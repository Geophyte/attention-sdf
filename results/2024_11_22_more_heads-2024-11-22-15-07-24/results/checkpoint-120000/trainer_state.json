{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 8.0,
  "eval_steps": 500,
  "global_step": 120000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0006666666666666666,
      "grad_norm": 2.687889814376831,
      "learning_rate": 4.999583333333333e-05,
      "loss": 0.0269,
      "step": 10
    },
    {
      "epoch": 0.0013333333333333333,
      "grad_norm": 2.5180647373199463,
      "learning_rate": 4.999166666666667e-05,
      "loss": 0.0115,
      "step": 20
    },
    {
      "epoch": 0.002,
      "grad_norm": 1.7175499200820923,
      "learning_rate": 4.99875e-05,
      "loss": 0.0083,
      "step": 30
    },
    {
      "epoch": 0.0026666666666666666,
      "grad_norm": 2.182140588760376,
      "learning_rate": 4.998333333333334e-05,
      "loss": 0.0069,
      "step": 40
    },
    {
      "epoch": 0.0033333333333333335,
      "grad_norm": 0.3744048774242401,
      "learning_rate": 4.997916666666667e-05,
      "loss": 0.0083,
      "step": 50
    },
    {
      "epoch": 0.004,
      "grad_norm": 1.4175785779953003,
      "learning_rate": 4.9975e-05,
      "loss": 0.0094,
      "step": 60
    },
    {
      "epoch": 0.004666666666666667,
      "grad_norm": 2.287304401397705,
      "learning_rate": 4.997083333333333e-05,
      "loss": 0.0083,
      "step": 70
    },
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 2.603260040283203,
      "learning_rate": 4.996666666666667e-05,
      "loss": 0.0104,
      "step": 80
    },
    {
      "epoch": 0.006,
      "grad_norm": 1.9657002687454224,
      "learning_rate": 4.99625e-05,
      "loss": 0.0089,
      "step": 90
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 1.6856985092163086,
      "learning_rate": 4.995833333333333e-05,
      "loss": 0.0086,
      "step": 100
    },
    {
      "epoch": 0.007333333333333333,
      "grad_norm": 2.7349603176116943,
      "learning_rate": 4.995416666666667e-05,
      "loss": 0.0076,
      "step": 110
    },
    {
      "epoch": 0.008,
      "grad_norm": 1.9388618469238281,
      "learning_rate": 4.995e-05,
      "loss": 0.0081,
      "step": 120
    },
    {
      "epoch": 0.008666666666666666,
      "grad_norm": 2.102595806121826,
      "learning_rate": 4.994583333333334e-05,
      "loss": 0.0076,
      "step": 130
    },
    {
      "epoch": 0.009333333333333334,
      "grad_norm": 2.4877140522003174,
      "learning_rate": 4.994166666666667e-05,
      "loss": 0.0074,
      "step": 140
    },
    {
      "epoch": 0.01,
      "grad_norm": 2.0649473667144775,
      "learning_rate": 4.99375e-05,
      "loss": 0.0067,
      "step": 150
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 2.642038345336914,
      "learning_rate": 4.993333333333334e-05,
      "loss": 0.0066,
      "step": 160
    },
    {
      "epoch": 0.011333333333333334,
      "grad_norm": 2.509730339050293,
      "learning_rate": 4.992916666666667e-05,
      "loss": 0.0069,
      "step": 170
    },
    {
      "epoch": 0.012,
      "grad_norm": 2.591891050338745,
      "learning_rate": 4.992500000000001e-05,
      "loss": 0.0082,
      "step": 180
    },
    {
      "epoch": 0.012666666666666666,
      "grad_norm": 0.7453887462615967,
      "learning_rate": 4.992083333333333e-05,
      "loss": 0.0074,
      "step": 190
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.7518396377563477,
      "learning_rate": 4.991666666666667e-05,
      "loss": 0.0087,
      "step": 200
    },
    {
      "epoch": 0.014,
      "grad_norm": 0.33387279510498047,
      "learning_rate": 4.99125e-05,
      "loss": 0.0076,
      "step": 210
    },
    {
      "epoch": 0.014666666666666666,
      "grad_norm": 0.5921304821968079,
      "learning_rate": 4.990833333333334e-05,
      "loss": 0.0085,
      "step": 220
    },
    {
      "epoch": 0.015333333333333332,
      "grad_norm": 2.8868088722229004,
      "learning_rate": 4.990416666666667e-05,
      "loss": 0.0076,
      "step": 230
    },
    {
      "epoch": 0.016,
      "grad_norm": 2.1087183952331543,
      "learning_rate": 4.99e-05,
      "loss": 0.0088,
      "step": 240
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 0.44767603278160095,
      "learning_rate": 4.989583333333334e-05,
      "loss": 0.0057,
      "step": 250
    },
    {
      "epoch": 0.017333333333333333,
      "grad_norm": 0.3023192286491394,
      "learning_rate": 4.989166666666667e-05,
      "loss": 0.0062,
      "step": 260
    },
    {
      "epoch": 0.018,
      "grad_norm": 1.714623212814331,
      "learning_rate": 4.9887500000000006e-05,
      "loss": 0.0069,
      "step": 270
    },
    {
      "epoch": 0.018666666666666668,
      "grad_norm": 0.7359893321990967,
      "learning_rate": 4.988333333333334e-05,
      "loss": 0.0077,
      "step": 280
    },
    {
      "epoch": 0.019333333333333334,
      "grad_norm": 2.080582618713379,
      "learning_rate": 4.987916666666667e-05,
      "loss": 0.007,
      "step": 290
    },
    {
      "epoch": 0.02,
      "grad_norm": 2.240441083908081,
      "learning_rate": 4.9875000000000006e-05,
      "loss": 0.0072,
      "step": 300
    },
    {
      "epoch": 0.020666666666666667,
      "grad_norm": 2.096586227416992,
      "learning_rate": 4.987083333333333e-05,
      "loss": 0.0069,
      "step": 310
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 1.0082346200942993,
      "learning_rate": 4.986666666666667e-05,
      "loss": 0.0079,
      "step": 320
    },
    {
      "epoch": 0.022,
      "grad_norm": 0.1421877145767212,
      "learning_rate": 4.98625e-05,
      "loss": 0.0097,
      "step": 330
    },
    {
      "epoch": 0.02266666666666667,
      "grad_norm": 2.083390712738037,
      "learning_rate": 4.985833333333334e-05,
      "loss": 0.0061,
      "step": 340
    },
    {
      "epoch": 0.023333333333333334,
      "grad_norm": 0.6039996147155762,
      "learning_rate": 4.985416666666667e-05,
      "loss": 0.0071,
      "step": 350
    },
    {
      "epoch": 0.024,
      "grad_norm": 2.350480318069458,
      "learning_rate": 4.9850000000000006e-05,
      "loss": 0.0054,
      "step": 360
    },
    {
      "epoch": 0.024666666666666667,
      "grad_norm": 0.8223791122436523,
      "learning_rate": 4.9845833333333336e-05,
      "loss": 0.0071,
      "step": 370
    },
    {
      "epoch": 0.025333333333333333,
      "grad_norm": 0.4627286195755005,
      "learning_rate": 4.984166666666667e-05,
      "loss": 0.0071,
      "step": 380
    },
    {
      "epoch": 0.026,
      "grad_norm": 0.4429260492324829,
      "learning_rate": 4.9837500000000005e-05,
      "loss": 0.0079,
      "step": 390
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 2.763302803039551,
      "learning_rate": 4.9833333333333336e-05,
      "loss": 0.0076,
      "step": 400
    },
    {
      "epoch": 0.027333333333333334,
      "grad_norm": 1.8006788492202759,
      "learning_rate": 4.9829166666666674e-05,
      "loss": 0.0078,
      "step": 410
    },
    {
      "epoch": 0.028,
      "grad_norm": 2.3051607608795166,
      "learning_rate": 4.9825000000000005e-05,
      "loss": 0.007,
      "step": 420
    },
    {
      "epoch": 0.028666666666666667,
      "grad_norm": 1.3215159177780151,
      "learning_rate": 4.9820833333333336e-05,
      "loss": 0.0087,
      "step": 430
    },
    {
      "epoch": 0.029333333333333333,
      "grad_norm": 1.5949712991714478,
      "learning_rate": 4.981666666666667e-05,
      "loss": 0.0066,
      "step": 440
    },
    {
      "epoch": 0.03,
      "grad_norm": 1.0610194206237793,
      "learning_rate": 4.98125e-05,
      "loss": 0.006,
      "step": 450
    },
    {
      "epoch": 0.030666666666666665,
      "grad_norm": 1.457953929901123,
      "learning_rate": 4.9808333333333336e-05,
      "loss": 0.0063,
      "step": 460
    },
    {
      "epoch": 0.03133333333333333,
      "grad_norm": 0.9462178349494934,
      "learning_rate": 4.9804166666666667e-05,
      "loss": 0.0071,
      "step": 470
    },
    {
      "epoch": 0.032,
      "grad_norm": 2.107923984527588,
      "learning_rate": 4.9800000000000004e-05,
      "loss": 0.0067,
      "step": 480
    },
    {
      "epoch": 0.03266666666666666,
      "grad_norm": 1.472015380859375,
      "learning_rate": 4.9795833333333335e-05,
      "loss": 0.006,
      "step": 490
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 2.6355910301208496,
      "learning_rate": 4.979166666666667e-05,
      "loss": 0.0068,
      "step": 500
    },
    {
      "epoch": 0.034,
      "grad_norm": 2.094083070755005,
      "learning_rate": 4.9787500000000004e-05,
      "loss": 0.0077,
      "step": 510
    },
    {
      "epoch": 0.034666666666666665,
      "grad_norm": 0.9447447061538696,
      "learning_rate": 4.9783333333333335e-05,
      "loss": 0.0083,
      "step": 520
    },
    {
      "epoch": 0.035333333333333335,
      "grad_norm": 0.3070347309112549,
      "learning_rate": 4.977916666666667e-05,
      "loss": 0.0075,
      "step": 530
    },
    {
      "epoch": 0.036,
      "grad_norm": 0.3250657916069031,
      "learning_rate": 4.9775000000000004e-05,
      "loss": 0.0058,
      "step": 540
    },
    {
      "epoch": 0.03666666666666667,
      "grad_norm": 0.23651407659053802,
      "learning_rate": 4.9770833333333335e-05,
      "loss": 0.0063,
      "step": 550
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 1.1844457387924194,
      "learning_rate": 4.9766666666666666e-05,
      "loss": 0.0082,
      "step": 560
    },
    {
      "epoch": 0.038,
      "grad_norm": 1.2069220542907715,
      "learning_rate": 4.9762500000000003e-05,
      "loss": 0.0059,
      "step": 570
    },
    {
      "epoch": 0.03866666666666667,
      "grad_norm": 1.602105975151062,
      "learning_rate": 4.9758333333333334e-05,
      "loss": 0.0072,
      "step": 580
    },
    {
      "epoch": 0.03933333333333333,
      "grad_norm": 1.9871618747711182,
      "learning_rate": 4.9754166666666665e-05,
      "loss": 0.0059,
      "step": 590
    },
    {
      "epoch": 0.04,
      "grad_norm": 1.594141960144043,
      "learning_rate": 4.975e-05,
      "loss": 0.0108,
      "step": 600
    },
    {
      "epoch": 0.04066666666666666,
      "grad_norm": 1.5921519994735718,
      "learning_rate": 4.9745833333333334e-05,
      "loss": 0.0068,
      "step": 610
    },
    {
      "epoch": 0.04133333333333333,
      "grad_norm": 1.9575122594833374,
      "learning_rate": 4.974166666666667e-05,
      "loss": 0.0049,
      "step": 620
    },
    {
      "epoch": 0.042,
      "grad_norm": 0.32186025381088257,
      "learning_rate": 4.97375e-05,
      "loss": 0.0075,
      "step": 630
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 0.9350440502166748,
      "learning_rate": 4.973333333333334e-05,
      "loss": 0.0059,
      "step": 640
    },
    {
      "epoch": 0.043333333333333335,
      "grad_norm": 0.5164177417755127,
      "learning_rate": 4.972916666666667e-05,
      "loss": 0.0054,
      "step": 650
    },
    {
      "epoch": 0.044,
      "grad_norm": 0.9221636652946472,
      "learning_rate": 4.9725e-05,
      "loss": 0.0054,
      "step": 660
    },
    {
      "epoch": 0.04466666666666667,
      "grad_norm": 0.2845630943775177,
      "learning_rate": 4.9720833333333333e-05,
      "loss": 0.0052,
      "step": 670
    },
    {
      "epoch": 0.04533333333333334,
      "grad_norm": 1.5565426349639893,
      "learning_rate": 4.9716666666666664e-05,
      "loss": 0.0051,
      "step": 680
    },
    {
      "epoch": 0.046,
      "grad_norm": 1.3275163173675537,
      "learning_rate": 4.97125e-05,
      "loss": 0.0048,
      "step": 690
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 1.3942781686782837,
      "learning_rate": 4.970833333333333e-05,
      "loss": 0.0054,
      "step": 700
    },
    {
      "epoch": 0.04733333333333333,
      "grad_norm": 0.49659135937690735,
      "learning_rate": 4.970416666666667e-05,
      "loss": 0.0054,
      "step": 710
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.9099952578544617,
      "learning_rate": 4.97e-05,
      "loss": 0.0036,
      "step": 720
    },
    {
      "epoch": 0.048666666666666664,
      "grad_norm": 0.3823022246360779,
      "learning_rate": 4.969583333333333e-05,
      "loss": 0.0064,
      "step": 730
    },
    {
      "epoch": 0.04933333333333333,
      "grad_norm": 2.051003932952881,
      "learning_rate": 4.969166666666667e-05,
      "loss": 0.0047,
      "step": 740
    },
    {
      "epoch": 0.05,
      "grad_norm": 1.9124919176101685,
      "learning_rate": 4.96875e-05,
      "loss": 0.0068,
      "step": 750
    },
    {
      "epoch": 0.050666666666666665,
      "grad_norm": 0.2817414104938507,
      "learning_rate": 4.968333333333334e-05,
      "loss": 0.0064,
      "step": 760
    },
    {
      "epoch": 0.051333333333333335,
      "grad_norm": 2.3136794567108154,
      "learning_rate": 4.967916666666667e-05,
      "loss": 0.0077,
      "step": 770
    },
    {
      "epoch": 0.052,
      "grad_norm": 1.3948156833648682,
      "learning_rate": 4.967500000000001e-05,
      "loss": 0.006,
      "step": 780
    },
    {
      "epoch": 0.05266666666666667,
      "grad_norm": 1.9274743795394897,
      "learning_rate": 4.967083333333333e-05,
      "loss": 0.0062,
      "step": 790
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.5747785568237305,
      "learning_rate": 4.966666666666667e-05,
      "loss": 0.0059,
      "step": 800
    },
    {
      "epoch": 0.054,
      "grad_norm": 2.271437883377075,
      "learning_rate": 4.96625e-05,
      "loss": 0.006,
      "step": 810
    },
    {
      "epoch": 0.05466666666666667,
      "grad_norm": 1.3000779151916504,
      "learning_rate": 4.965833333333333e-05,
      "loss": 0.0064,
      "step": 820
    },
    {
      "epoch": 0.05533333333333333,
      "grad_norm": 2.040259838104248,
      "learning_rate": 4.965416666666667e-05,
      "loss": 0.0056,
      "step": 830
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.9061023592948914,
      "learning_rate": 4.965e-05,
      "loss": 0.0061,
      "step": 840
    },
    {
      "epoch": 0.056666666666666664,
      "grad_norm": 0.32257193326950073,
      "learning_rate": 4.964583333333334e-05,
      "loss": 0.0078,
      "step": 850
    },
    {
      "epoch": 0.05733333333333333,
      "grad_norm": 1.1569247245788574,
      "learning_rate": 4.964166666666667e-05,
      "loss": 0.0064,
      "step": 860
    },
    {
      "epoch": 0.058,
      "grad_norm": 0.5167549848556519,
      "learning_rate": 4.96375e-05,
      "loss": 0.0042,
      "step": 870
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 2.3061137199401855,
      "learning_rate": 4.963333333333334e-05,
      "loss": 0.0048,
      "step": 880
    },
    {
      "epoch": 0.059333333333333335,
      "grad_norm": 0.7203113436698914,
      "learning_rate": 4.962916666666667e-05,
      "loss": 0.0066,
      "step": 890
    },
    {
      "epoch": 0.06,
      "grad_norm": 1.5470353364944458,
      "learning_rate": 4.962500000000001e-05,
      "loss": 0.0049,
      "step": 900
    },
    {
      "epoch": 0.06066666666666667,
      "grad_norm": 0.7597410082817078,
      "learning_rate": 4.962083333333333e-05,
      "loss": 0.0049,
      "step": 910
    },
    {
      "epoch": 0.06133333333333333,
      "grad_norm": 2.0909011363983154,
      "learning_rate": 4.961666666666667e-05,
      "loss": 0.0042,
      "step": 920
    },
    {
      "epoch": 0.062,
      "grad_norm": 1.3809174299240112,
      "learning_rate": 4.96125e-05,
      "loss": 0.0059,
      "step": 930
    },
    {
      "epoch": 0.06266666666666666,
      "grad_norm": 0.24983720481395721,
      "learning_rate": 4.960833333333334e-05,
      "loss": 0.0046,
      "step": 940
    },
    {
      "epoch": 0.06333333333333334,
      "grad_norm": 1.5308326482772827,
      "learning_rate": 4.960416666666667e-05,
      "loss": 0.0048,
      "step": 950
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.40418294072151184,
      "learning_rate": 4.96e-05,
      "loss": 0.0077,
      "step": 960
    },
    {
      "epoch": 0.06466666666666666,
      "grad_norm": 1.2458083629608154,
      "learning_rate": 4.959583333333334e-05,
      "loss": 0.0048,
      "step": 970
    },
    {
      "epoch": 0.06533333333333333,
      "grad_norm": 1.0168609619140625,
      "learning_rate": 4.959166666666667e-05,
      "loss": 0.0049,
      "step": 980
    },
    {
      "epoch": 0.066,
      "grad_norm": 1.6180636882781982,
      "learning_rate": 4.9587500000000006e-05,
      "loss": 0.0043,
      "step": 990
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.3621739447116852,
      "learning_rate": 4.958333333333334e-05,
      "loss": 0.0061,
      "step": 1000
    },
    {
      "epoch": 0.06733333333333333,
      "grad_norm": 0.2908506691455841,
      "learning_rate": 4.957916666666667e-05,
      "loss": 0.0042,
      "step": 1010
    },
    {
      "epoch": 0.068,
      "grad_norm": 1.2463761568069458,
      "learning_rate": 4.9575000000000006e-05,
      "loss": 0.0061,
      "step": 1020
    },
    {
      "epoch": 0.06866666666666667,
      "grad_norm": 1.9397252798080444,
      "learning_rate": 4.957083333333333e-05,
      "loss": 0.0046,
      "step": 1030
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 1.874546766281128,
      "learning_rate": 4.956666666666667e-05,
      "loss": 0.0051,
      "step": 1040
    },
    {
      "epoch": 0.07,
      "grad_norm": 1.6196098327636719,
      "learning_rate": 4.95625e-05,
      "loss": 0.0054,
      "step": 1050
    },
    {
      "epoch": 0.07066666666666667,
      "grad_norm": 2.2181780338287354,
      "learning_rate": 4.9558333333333336e-05,
      "loss": 0.0064,
      "step": 1060
    },
    {
      "epoch": 0.07133333333333333,
      "grad_norm": 2.0522148609161377,
      "learning_rate": 4.955416666666667e-05,
      "loss": 0.0055,
      "step": 1070
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.9989867806434631,
      "learning_rate": 4.9550000000000005e-05,
      "loss": 0.0053,
      "step": 1080
    },
    {
      "epoch": 0.07266666666666667,
      "grad_norm": 1.892066240310669,
      "learning_rate": 4.9545833333333336e-05,
      "loss": 0.0052,
      "step": 1090
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 1.84681236743927,
      "learning_rate": 4.954166666666667e-05,
      "loss": 0.0052,
      "step": 1100
    },
    {
      "epoch": 0.074,
      "grad_norm": 1.724343180656433,
      "learning_rate": 4.9537500000000005e-05,
      "loss": 0.0077,
      "step": 1110
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 2.450486660003662,
      "learning_rate": 4.9533333333333336e-05,
      "loss": 0.0065,
      "step": 1120
    },
    {
      "epoch": 0.07533333333333334,
      "grad_norm": 2.0932157039642334,
      "learning_rate": 4.9529166666666673e-05,
      "loss": 0.0045,
      "step": 1130
    },
    {
      "epoch": 0.076,
      "grad_norm": 1.9039332866668701,
      "learning_rate": 4.9525000000000004e-05,
      "loss": 0.006,
      "step": 1140
    },
    {
      "epoch": 0.07666666666666666,
      "grad_norm": 0.1422206163406372,
      "learning_rate": 4.9520833333333335e-05,
      "loss": 0.0054,
      "step": 1150
    },
    {
      "epoch": 0.07733333333333334,
      "grad_norm": 0.18728336691856384,
      "learning_rate": 4.9516666666666666e-05,
      "loss": 0.0042,
      "step": 1160
    },
    {
      "epoch": 0.078,
      "grad_norm": 0.7910377979278564,
      "learning_rate": 4.95125e-05,
      "loss": 0.0042,
      "step": 1170
    },
    {
      "epoch": 0.07866666666666666,
      "grad_norm": 1.9933587312698364,
      "learning_rate": 4.9508333333333335e-05,
      "loss": 0.0046,
      "step": 1180
    },
    {
      "epoch": 0.07933333333333334,
      "grad_norm": 1.1817282438278198,
      "learning_rate": 4.9504166666666666e-05,
      "loss": 0.0041,
      "step": 1190
    },
    {
      "epoch": 0.08,
      "grad_norm": 1.8434993028640747,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 0.004,
      "step": 1200
    },
    {
      "epoch": 0.08066666666666666,
      "grad_norm": 0.44779738783836365,
      "learning_rate": 4.9495833333333335e-05,
      "loss": 0.0057,
      "step": 1210
    },
    {
      "epoch": 0.08133333333333333,
      "grad_norm": 1.684939980506897,
      "learning_rate": 4.949166666666667e-05,
      "loss": 0.0047,
      "step": 1220
    },
    {
      "epoch": 0.082,
      "grad_norm": 0.3035814166069031,
      "learning_rate": 4.9487500000000003e-05,
      "loss": 0.0039,
      "step": 1230
    },
    {
      "epoch": 0.08266666666666667,
      "grad_norm": 0.09702815860509872,
      "learning_rate": 4.9483333333333334e-05,
      "loss": 0.0036,
      "step": 1240
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 0.3855328857898712,
      "learning_rate": 4.947916666666667e-05,
      "loss": 0.0057,
      "step": 1250
    },
    {
      "epoch": 0.084,
      "grad_norm": 0.26070961356163025,
      "learning_rate": 4.9475e-05,
      "loss": 0.0034,
      "step": 1260
    },
    {
      "epoch": 0.08466666666666667,
      "grad_norm": 1.0317233800888062,
      "learning_rate": 4.947083333333334e-05,
      "loss": 0.0051,
      "step": 1270
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 2.0853371620178223,
      "learning_rate": 4.9466666666666665e-05,
      "loss": 0.006,
      "step": 1280
    },
    {
      "epoch": 0.086,
      "grad_norm": 0.8072184920310974,
      "learning_rate": 4.94625e-05,
      "loss": 0.0068,
      "step": 1290
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.15999604761600494,
      "learning_rate": 4.9458333333333334e-05,
      "loss": 0.0045,
      "step": 1300
    },
    {
      "epoch": 0.08733333333333333,
      "grad_norm": 0.4228023588657379,
      "learning_rate": 4.9454166666666665e-05,
      "loss": 0.005,
      "step": 1310
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.19652524590492249,
      "learning_rate": 4.945e-05,
      "loss": 0.0067,
      "step": 1320
    },
    {
      "epoch": 0.08866666666666667,
      "grad_norm": 1.2225992679595947,
      "learning_rate": 4.9445833333333334e-05,
      "loss": 0.0044,
      "step": 1330
    },
    {
      "epoch": 0.08933333333333333,
      "grad_norm": 0.6290768980979919,
      "learning_rate": 4.944166666666667e-05,
      "loss": 0.0053,
      "step": 1340
    },
    {
      "epoch": 0.09,
      "grad_norm": 1.2130309343338013,
      "learning_rate": 4.94375e-05,
      "loss": 0.0055,
      "step": 1350
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 1.0222649574279785,
      "learning_rate": 4.943333333333334e-05,
      "loss": 0.005,
      "step": 1360
    },
    {
      "epoch": 0.09133333333333334,
      "grad_norm": 1.2218873500823975,
      "learning_rate": 4.942916666666667e-05,
      "loss": 0.0054,
      "step": 1370
    },
    {
      "epoch": 0.092,
      "grad_norm": 1.026261329650879,
      "learning_rate": 4.9425e-05,
      "loss": 0.0054,
      "step": 1380
    },
    {
      "epoch": 0.09266666666666666,
      "grad_norm": 1.0640934705734253,
      "learning_rate": 4.942083333333334e-05,
      "loss": 0.005,
      "step": 1390
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.9835082292556763,
      "learning_rate": 4.9416666666666664e-05,
      "loss": 0.005,
      "step": 1400
    },
    {
      "epoch": 0.094,
      "grad_norm": 1.4709945917129517,
      "learning_rate": 4.94125e-05,
      "loss": 0.0055,
      "step": 1410
    },
    {
      "epoch": 0.09466666666666666,
      "grad_norm": 1.802603840827942,
      "learning_rate": 4.940833333333333e-05,
      "loss": 0.0045,
      "step": 1420
    },
    {
      "epoch": 0.09533333333333334,
      "grad_norm": 2.090071201324463,
      "learning_rate": 4.940416666666667e-05,
      "loss": 0.0041,
      "step": 1430
    },
    {
      "epoch": 0.096,
      "grad_norm": 1.502015471458435,
      "learning_rate": 4.94e-05,
      "loss": 0.0044,
      "step": 1440
    },
    {
      "epoch": 0.09666666666666666,
      "grad_norm": 0.9825465083122253,
      "learning_rate": 4.939583333333333e-05,
      "loss": 0.0046,
      "step": 1450
    },
    {
      "epoch": 0.09733333333333333,
      "grad_norm": 2.1219735145568848,
      "learning_rate": 4.939166666666667e-05,
      "loss": 0.0051,
      "step": 1460
    },
    {
      "epoch": 0.098,
      "grad_norm": 1.6990768909454346,
      "learning_rate": 4.93875e-05,
      "loss": 0.0043,
      "step": 1470
    },
    {
      "epoch": 0.09866666666666667,
      "grad_norm": 1.8290860652923584,
      "learning_rate": 4.938333333333334e-05,
      "loss": 0.0049,
      "step": 1480
    },
    {
      "epoch": 0.09933333333333333,
      "grad_norm": 0.617807924747467,
      "learning_rate": 4.937916666666667e-05,
      "loss": 0.006,
      "step": 1490
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.17899876832962036,
      "learning_rate": 4.937500000000001e-05,
      "loss": 0.0054,
      "step": 1500
    },
    {
      "epoch": 0.10066666666666667,
      "grad_norm": 0.9627370238304138,
      "learning_rate": 4.937083333333334e-05,
      "loss": 0.0036,
      "step": 1510
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 1.5219734907150269,
      "learning_rate": 4.936666666666667e-05,
      "loss": 0.0046,
      "step": 1520
    },
    {
      "epoch": 0.102,
      "grad_norm": 1.9382555484771729,
      "learning_rate": 4.93625e-05,
      "loss": 0.0043,
      "step": 1530
    },
    {
      "epoch": 0.10266666666666667,
      "grad_norm": 1.3945759534835815,
      "learning_rate": 4.935833333333333e-05,
      "loss": 0.0043,
      "step": 1540
    },
    {
      "epoch": 0.10333333333333333,
      "grad_norm": 0.18389305472373962,
      "learning_rate": 4.935416666666667e-05,
      "loss": 0.0048,
      "step": 1550
    },
    {
      "epoch": 0.104,
      "grad_norm": 2.0050106048583984,
      "learning_rate": 4.935e-05,
      "loss": 0.0041,
      "step": 1560
    },
    {
      "epoch": 0.10466666666666667,
      "grad_norm": 0.14233778417110443,
      "learning_rate": 4.934583333333334e-05,
      "loss": 0.0048,
      "step": 1570
    },
    {
      "epoch": 0.10533333333333333,
      "grad_norm": 1.8984160423278809,
      "learning_rate": 4.934166666666667e-05,
      "loss": 0.0065,
      "step": 1580
    },
    {
      "epoch": 0.106,
      "grad_norm": 0.14440961182117462,
      "learning_rate": 4.93375e-05,
      "loss": 0.0056,
      "step": 1590
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 1.0003292560577393,
      "learning_rate": 4.933333333333334e-05,
      "loss": 0.005,
      "step": 1600
    },
    {
      "epoch": 0.10733333333333334,
      "grad_norm": 0.6129662990570068,
      "learning_rate": 4.932916666666667e-05,
      "loss": 0.0045,
      "step": 1610
    },
    {
      "epoch": 0.108,
      "grad_norm": 1.248452067375183,
      "learning_rate": 4.9325000000000006e-05,
      "loss": 0.0041,
      "step": 1620
    },
    {
      "epoch": 0.10866666666666666,
      "grad_norm": 1.1745842695236206,
      "learning_rate": 4.932083333333334e-05,
      "loss": 0.0036,
      "step": 1630
    },
    {
      "epoch": 0.10933333333333334,
      "grad_norm": 0.9977298974990845,
      "learning_rate": 4.931666666666667e-05,
      "loss": 0.006,
      "step": 1640
    },
    {
      "epoch": 0.11,
      "grad_norm": 1.354168176651001,
      "learning_rate": 4.93125e-05,
      "loss": 0.0051,
      "step": 1650
    },
    {
      "epoch": 0.11066666666666666,
      "grad_norm": 1.1450533866882324,
      "learning_rate": 4.930833333333334e-05,
      "loss": 0.0046,
      "step": 1660
    },
    {
      "epoch": 0.11133333333333334,
      "grad_norm": 0.27432289719581604,
      "learning_rate": 4.930416666666667e-05,
      "loss": 0.0034,
      "step": 1670
    },
    {
      "epoch": 0.112,
      "grad_norm": 1.2190369367599487,
      "learning_rate": 4.93e-05,
      "loss": 0.0048,
      "step": 1680
    },
    {
      "epoch": 0.11266666666666666,
      "grad_norm": 1.4096165895462036,
      "learning_rate": 4.929583333333334e-05,
      "loss": 0.0044,
      "step": 1690
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 0.6996694803237915,
      "learning_rate": 4.929166666666667e-05,
      "loss": 0.0053,
      "step": 1700
    },
    {
      "epoch": 0.114,
      "grad_norm": 0.7100768089294434,
      "learning_rate": 4.9287500000000005e-05,
      "loss": 0.0034,
      "step": 1710
    },
    {
      "epoch": 0.11466666666666667,
      "grad_norm": 1.0738505125045776,
      "learning_rate": 4.9283333333333336e-05,
      "loss": 0.0041,
      "step": 1720
    },
    {
      "epoch": 0.11533333333333333,
      "grad_norm": 0.9341318011283875,
      "learning_rate": 4.927916666666667e-05,
      "loss": 0.004,
      "step": 1730
    },
    {
      "epoch": 0.116,
      "grad_norm": 1.6426043510437012,
      "learning_rate": 4.9275000000000005e-05,
      "loss": 0.005,
      "step": 1740
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 2.0903573036193848,
      "learning_rate": 4.9270833333333336e-05,
      "loss": 0.0045,
      "step": 1750
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 1.5638717412948608,
      "learning_rate": 4.926666666666667e-05,
      "loss": 0.0044,
      "step": 1760
    },
    {
      "epoch": 0.118,
      "grad_norm": 0.8686596751213074,
      "learning_rate": 4.92625e-05,
      "loss": 0.0037,
      "step": 1770
    },
    {
      "epoch": 0.11866666666666667,
      "grad_norm": 0.9502230882644653,
      "learning_rate": 4.9258333333333336e-05,
      "loss": 0.0048,
      "step": 1780
    },
    {
      "epoch": 0.11933333333333333,
      "grad_norm": 1.8639684915542603,
      "learning_rate": 4.925416666666667e-05,
      "loss": 0.0052,
      "step": 1790
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.2408389002084732,
      "learning_rate": 4.9250000000000004e-05,
      "loss": 0.005,
      "step": 1800
    },
    {
      "epoch": 0.12066666666666667,
      "grad_norm": 1.2282332181930542,
      "learning_rate": 4.9245833333333335e-05,
      "loss": 0.0048,
      "step": 1810
    },
    {
      "epoch": 0.12133333333333333,
      "grad_norm": 0.5821278691291809,
      "learning_rate": 4.9241666666666666e-05,
      "loss": 0.0049,
      "step": 1820
    },
    {
      "epoch": 0.122,
      "grad_norm": 1.4686232805252075,
      "learning_rate": 4.9237500000000004e-05,
      "loss": 0.0045,
      "step": 1830
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 1.1544593572616577,
      "learning_rate": 4.9233333333333335e-05,
      "loss": 0.003,
      "step": 1840
    },
    {
      "epoch": 0.12333333333333334,
      "grad_norm": 0.30110153555870056,
      "learning_rate": 4.922916666666667e-05,
      "loss": 0.0053,
      "step": 1850
    },
    {
      "epoch": 0.124,
      "grad_norm": 0.7430373430252075,
      "learning_rate": 4.9225000000000004e-05,
      "loss": 0.0041,
      "step": 1860
    },
    {
      "epoch": 0.12466666666666666,
      "grad_norm": 0.9215880036354065,
      "learning_rate": 4.9220833333333335e-05,
      "loss": 0.0052,
      "step": 1870
    },
    {
      "epoch": 0.12533333333333332,
      "grad_norm": 0.3256371021270752,
      "learning_rate": 4.9216666666666666e-05,
      "loss": 0.0031,
      "step": 1880
    },
    {
      "epoch": 0.126,
      "grad_norm": 0.5018567442893982,
      "learning_rate": 4.9212500000000004e-05,
      "loss": 0.0052,
      "step": 1890
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.8100734353065491,
      "learning_rate": 4.9208333333333335e-05,
      "loss": 0.0033,
      "step": 1900
    },
    {
      "epoch": 0.12733333333333333,
      "grad_norm": 1.516318440437317,
      "learning_rate": 4.9204166666666666e-05,
      "loss": 0.0045,
      "step": 1910
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.31091219186782837,
      "learning_rate": 4.92e-05,
      "loss": 0.0039,
      "step": 1920
    },
    {
      "epoch": 0.12866666666666668,
      "grad_norm": 1.3161824941635132,
      "learning_rate": 4.9195833333333334e-05,
      "loss": 0.0051,
      "step": 1930
    },
    {
      "epoch": 0.12933333333333333,
      "grad_norm": 2.073185443878174,
      "learning_rate": 4.919166666666667e-05,
      "loss": 0.0047,
      "step": 1940
    },
    {
      "epoch": 0.13,
      "grad_norm": 1.5492433309555054,
      "learning_rate": 4.91875e-05,
      "loss": 0.004,
      "step": 1950
    },
    {
      "epoch": 0.13066666666666665,
      "grad_norm": 1.6267855167388916,
      "learning_rate": 4.9183333333333334e-05,
      "loss": 0.0051,
      "step": 1960
    },
    {
      "epoch": 0.13133333333333333,
      "grad_norm": 1.424028992652893,
      "learning_rate": 4.917916666666667e-05,
      "loss": 0.0057,
      "step": 1970
    },
    {
      "epoch": 0.132,
      "grad_norm": 1.9347048997879028,
      "learning_rate": 4.9175e-05,
      "loss": 0.0042,
      "step": 1980
    },
    {
      "epoch": 0.13266666666666665,
      "grad_norm": 0.701745331287384,
      "learning_rate": 4.917083333333334e-05,
      "loss": 0.0059,
      "step": 1990
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 1.6468569040298462,
      "learning_rate": 4.9166666666666665e-05,
      "loss": 0.0049,
      "step": 2000
    },
    {
      "epoch": 0.134,
      "grad_norm": 0.559154212474823,
      "learning_rate": 4.91625e-05,
      "loss": 0.0035,
      "step": 2010
    },
    {
      "epoch": 0.13466666666666666,
      "grad_norm": 1.3577990531921387,
      "learning_rate": 4.915833333333333e-05,
      "loss": 0.0043,
      "step": 2020
    },
    {
      "epoch": 0.13533333333333333,
      "grad_norm": 1.7815115451812744,
      "learning_rate": 4.915416666666667e-05,
      "loss": 0.0045,
      "step": 2030
    },
    {
      "epoch": 0.136,
      "grad_norm": 1.0324186086654663,
      "learning_rate": 4.915e-05,
      "loss": 0.0038,
      "step": 2040
    },
    {
      "epoch": 0.13666666666666666,
      "grad_norm": 1.5869017839431763,
      "learning_rate": 4.914583333333333e-05,
      "loss": 0.0047,
      "step": 2050
    },
    {
      "epoch": 0.13733333333333334,
      "grad_norm": 1.008241891860962,
      "learning_rate": 4.914166666666667e-05,
      "loss": 0.0055,
      "step": 2060
    },
    {
      "epoch": 0.138,
      "grad_norm": 1.1608525514602661,
      "learning_rate": 4.91375e-05,
      "loss": 0.0045,
      "step": 2070
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 0.5743528604507446,
      "learning_rate": 4.913333333333334e-05,
      "loss": 0.0046,
      "step": 2080
    },
    {
      "epoch": 0.13933333333333334,
      "grad_norm": 1.3593403100967407,
      "learning_rate": 4.912916666666667e-05,
      "loss": 0.0069,
      "step": 2090
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.772919237613678,
      "learning_rate": 4.9125e-05,
      "loss": 0.0058,
      "step": 2100
    },
    {
      "epoch": 0.14066666666666666,
      "grad_norm": 1.727744460105896,
      "learning_rate": 4.912083333333334e-05,
      "loss": 0.0053,
      "step": 2110
    },
    {
      "epoch": 0.14133333333333334,
      "grad_norm": 1.882303237915039,
      "learning_rate": 4.9116666666666663e-05,
      "loss": 0.0034,
      "step": 2120
    },
    {
      "epoch": 0.142,
      "grad_norm": 0.8944628834724426,
      "learning_rate": 4.91125e-05,
      "loss": 0.0038,
      "step": 2130
    },
    {
      "epoch": 0.14266666666666666,
      "grad_norm": 1.8632770776748657,
      "learning_rate": 4.910833333333333e-05,
      "loss": 0.0029,
      "step": 2140
    },
    {
      "epoch": 0.14333333333333334,
      "grad_norm": 1.4580906629562378,
      "learning_rate": 4.910416666666667e-05,
      "loss": 0.0036,
      "step": 2150
    },
    {
      "epoch": 0.144,
      "grad_norm": 2.1514947414398193,
      "learning_rate": 4.91e-05,
      "loss": 0.0047,
      "step": 2160
    },
    {
      "epoch": 0.14466666666666667,
      "grad_norm": 0.2943922281265259,
      "learning_rate": 4.909583333333334e-05,
      "loss": 0.005,
      "step": 2170
    },
    {
      "epoch": 0.14533333333333334,
      "grad_norm": 1.8729537725448608,
      "learning_rate": 4.909166666666667e-05,
      "loss": 0.0049,
      "step": 2180
    },
    {
      "epoch": 0.146,
      "grad_norm": 0.5553823709487915,
      "learning_rate": 4.90875e-05,
      "loss": 0.0052,
      "step": 2190
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 2.0334465503692627,
      "learning_rate": 4.908333333333334e-05,
      "loss": 0.0047,
      "step": 2200
    },
    {
      "epoch": 0.14733333333333334,
      "grad_norm": 0.9678438305854797,
      "learning_rate": 4.907916666666667e-05,
      "loss": 0.0055,
      "step": 2210
    },
    {
      "epoch": 0.148,
      "grad_norm": 0.4980819821357727,
      "learning_rate": 4.907500000000001e-05,
      "loss": 0.0045,
      "step": 2220
    },
    {
      "epoch": 0.14866666666666667,
      "grad_norm": 0.2367781549692154,
      "learning_rate": 4.907083333333334e-05,
      "loss": 0.0043,
      "step": 2230
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 1.8516407012939453,
      "learning_rate": 4.906666666666667e-05,
      "loss": 0.0043,
      "step": 2240
    },
    {
      "epoch": 0.15,
      "grad_norm": 1.1022050380706787,
      "learning_rate": 4.90625e-05,
      "loss": 0.003,
      "step": 2250
    },
    {
      "epoch": 0.15066666666666667,
      "grad_norm": 0.6544795632362366,
      "learning_rate": 4.905833333333333e-05,
      "loss": 0.003,
      "step": 2260
    },
    {
      "epoch": 0.15133333333333332,
      "grad_norm": 0.25831496715545654,
      "learning_rate": 4.905416666666667e-05,
      "loss": 0.0048,
      "step": 2270
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.8087701201438904,
      "learning_rate": 4.905e-05,
      "loss": 0.0034,
      "step": 2280
    },
    {
      "epoch": 0.15266666666666667,
      "grad_norm": 0.12911826372146606,
      "learning_rate": 4.904583333333334e-05,
      "loss": 0.0036,
      "step": 2290
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 0.8217871189117432,
      "learning_rate": 4.904166666666667e-05,
      "loss": 0.0034,
      "step": 2300
    },
    {
      "epoch": 0.154,
      "grad_norm": 0.5063501596450806,
      "learning_rate": 4.9037500000000006e-05,
      "loss": 0.0044,
      "step": 2310
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 0.2243434190750122,
      "learning_rate": 4.903333333333334e-05,
      "loss": 0.003,
      "step": 2320
    },
    {
      "epoch": 0.15533333333333332,
      "grad_norm": 1.4195349216461182,
      "learning_rate": 4.902916666666667e-05,
      "loss": 0.0033,
      "step": 2330
    },
    {
      "epoch": 0.156,
      "grad_norm": 1.5448921918869019,
      "learning_rate": 4.9025000000000006e-05,
      "loss": 0.0045,
      "step": 2340
    },
    {
      "epoch": 0.15666666666666668,
      "grad_norm": 1.4542838335037231,
      "learning_rate": 4.902083333333334e-05,
      "loss": 0.0038,
      "step": 2350
    },
    {
      "epoch": 0.15733333333333333,
      "grad_norm": 0.7689732909202576,
      "learning_rate": 4.901666666666667e-05,
      "loss": 0.0055,
      "step": 2360
    },
    {
      "epoch": 0.158,
      "grad_norm": 1.5535778999328613,
      "learning_rate": 4.90125e-05,
      "loss": 0.0052,
      "step": 2370
    },
    {
      "epoch": 0.15866666666666668,
      "grad_norm": 0.6276856660842896,
      "learning_rate": 4.9008333333333336e-05,
      "loss": 0.0036,
      "step": 2380
    },
    {
      "epoch": 0.15933333333333333,
      "grad_norm": 0.15002477169036865,
      "learning_rate": 4.900416666666667e-05,
      "loss": 0.004,
      "step": 2390
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.5298471450805664,
      "learning_rate": 4.9e-05,
      "loss": 0.0044,
      "step": 2400
    },
    {
      "epoch": 0.16066666666666668,
      "grad_norm": 0.2838834822177887,
      "learning_rate": 4.8995833333333336e-05,
      "loss": 0.0048,
      "step": 2410
    },
    {
      "epoch": 0.16133333333333333,
      "grad_norm": 1.3806260824203491,
      "learning_rate": 4.899166666666667e-05,
      "loss": 0.0027,
      "step": 2420
    },
    {
      "epoch": 0.162,
      "grad_norm": 1.1969081163406372,
      "learning_rate": 4.8987500000000005e-05,
      "loss": 0.0048,
      "step": 2430
    },
    {
      "epoch": 0.16266666666666665,
      "grad_norm": 0.7542228698730469,
      "learning_rate": 4.8983333333333336e-05,
      "loss": 0.0043,
      "step": 2440
    },
    {
      "epoch": 0.16333333333333333,
      "grad_norm": 0.5603057742118835,
      "learning_rate": 4.8979166666666674e-05,
      "loss": 0.005,
      "step": 2450
    },
    {
      "epoch": 0.164,
      "grad_norm": 0.9779190421104431,
      "learning_rate": 4.8975000000000005e-05,
      "loss": 0.0049,
      "step": 2460
    },
    {
      "epoch": 0.16466666666666666,
      "grad_norm": 0.7342962026596069,
      "learning_rate": 4.8970833333333336e-05,
      "loss": 0.0044,
      "step": 2470
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 0.3210488259792328,
      "learning_rate": 4.8966666666666667e-05,
      "loss": 0.004,
      "step": 2480
    },
    {
      "epoch": 0.166,
      "grad_norm": 1.4362150430679321,
      "learning_rate": 4.89625e-05,
      "loss": 0.0043,
      "step": 2490
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 1.2098182439804077,
      "learning_rate": 4.8958333333333335e-05,
      "loss": 0.0062,
      "step": 2500
    },
    {
      "epoch": 0.16733333333333333,
      "grad_norm": 0.6923810243606567,
      "learning_rate": 4.8954166666666666e-05,
      "loss": 0.0036,
      "step": 2510
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.4121471345424652,
      "learning_rate": 4.8950000000000004e-05,
      "loss": 0.0045,
      "step": 2520
    },
    {
      "epoch": 0.16866666666666666,
      "grad_norm": 1.2829712629318237,
      "learning_rate": 4.8945833333333335e-05,
      "loss": 0.0039,
      "step": 2530
    },
    {
      "epoch": 0.16933333333333334,
      "grad_norm": 1.7794783115386963,
      "learning_rate": 4.8941666666666666e-05,
      "loss": 0.0041,
      "step": 2540
    },
    {
      "epoch": 0.17,
      "grad_norm": 1.7277549505233765,
      "learning_rate": 4.8937500000000004e-05,
      "loss": 0.0037,
      "step": 2550
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 1.4464997053146362,
      "learning_rate": 4.8933333333333335e-05,
      "loss": 0.005,
      "step": 2560
    },
    {
      "epoch": 0.17133333333333334,
      "grad_norm": 1.2256356477737427,
      "learning_rate": 4.892916666666667e-05,
      "loss": 0.0041,
      "step": 2570
    },
    {
      "epoch": 0.172,
      "grad_norm": 0.6653989553451538,
      "learning_rate": 4.8925e-05,
      "loss": 0.0029,
      "step": 2580
    },
    {
      "epoch": 0.17266666666666666,
      "grad_norm": 0.17710629105567932,
      "learning_rate": 4.892083333333334e-05,
      "loss": 0.0034,
      "step": 2590
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.3589659035205841,
      "learning_rate": 4.891666666666667e-05,
      "loss": 0.0035,
      "step": 2600
    },
    {
      "epoch": 0.174,
      "grad_norm": 1.4798297882080078,
      "learning_rate": 4.89125e-05,
      "loss": 0.0048,
      "step": 2610
    },
    {
      "epoch": 0.17466666666666666,
      "grad_norm": 1.652923583984375,
      "learning_rate": 4.8908333333333334e-05,
      "loss": 0.0032,
      "step": 2620
    },
    {
      "epoch": 0.17533333333333334,
      "grad_norm": 0.35369905829429626,
      "learning_rate": 4.8904166666666665e-05,
      "loss": 0.0028,
      "step": 2630
    },
    {
      "epoch": 0.176,
      "grad_norm": 1.9048553705215454,
      "learning_rate": 4.89e-05,
      "loss": 0.003,
      "step": 2640
    },
    {
      "epoch": 0.17666666666666667,
      "grad_norm": 1.2963422536849976,
      "learning_rate": 4.8895833333333334e-05,
      "loss": 0.005,
      "step": 2650
    },
    {
      "epoch": 0.17733333333333334,
      "grad_norm": 0.318605899810791,
      "learning_rate": 4.889166666666667e-05,
      "loss": 0.0035,
      "step": 2660
    },
    {
      "epoch": 0.178,
      "grad_norm": 0.4306735396385193,
      "learning_rate": 4.88875e-05,
      "loss": 0.0035,
      "step": 2670
    },
    {
      "epoch": 0.17866666666666667,
      "grad_norm": 0.5057425498962402,
      "learning_rate": 4.8883333333333333e-05,
      "loss": 0.0037,
      "step": 2680
    },
    {
      "epoch": 0.17933333333333334,
      "grad_norm": 1.6777889728546143,
      "learning_rate": 4.887916666666667e-05,
      "loss": 0.0046,
      "step": 2690
    },
    {
      "epoch": 0.18,
      "grad_norm": 1.1787846088409424,
      "learning_rate": 4.8875e-05,
      "loss": 0.0033,
      "step": 2700
    },
    {
      "epoch": 0.18066666666666667,
      "grad_norm": 1.299659252166748,
      "learning_rate": 4.887083333333334e-05,
      "loss": 0.0038,
      "step": 2710
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 0.16670608520507812,
      "learning_rate": 4.886666666666667e-05,
      "loss": 0.0057,
      "step": 2720
    },
    {
      "epoch": 0.182,
      "grad_norm": 0.5205896496772766,
      "learning_rate": 4.88625e-05,
      "loss": 0.0042,
      "step": 2730
    },
    {
      "epoch": 0.18266666666666667,
      "grad_norm": 0.1926855444908142,
      "learning_rate": 4.885833333333333e-05,
      "loss": 0.0032,
      "step": 2740
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 1.2408947944641113,
      "learning_rate": 4.885416666666667e-05,
      "loss": 0.0044,
      "step": 2750
    },
    {
      "epoch": 0.184,
      "grad_norm": 1.5024627447128296,
      "learning_rate": 4.885e-05,
      "loss": 0.0036,
      "step": 2760
    },
    {
      "epoch": 0.18466666666666667,
      "grad_norm": 1.8886770009994507,
      "learning_rate": 4.884583333333333e-05,
      "loss": 0.0036,
      "step": 2770
    },
    {
      "epoch": 0.18533333333333332,
      "grad_norm": 0.9319820404052734,
      "learning_rate": 4.884166666666667e-05,
      "loss": 0.0041,
      "step": 2780
    },
    {
      "epoch": 0.186,
      "grad_norm": 1.5503147840499878,
      "learning_rate": 4.88375e-05,
      "loss": 0.0051,
      "step": 2790
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 1.3239165544509888,
      "learning_rate": 4.883333333333334e-05,
      "loss": 0.005,
      "step": 2800
    },
    {
      "epoch": 0.18733333333333332,
      "grad_norm": 2.0624029636383057,
      "learning_rate": 4.882916666666667e-05,
      "loss": 0.0047,
      "step": 2810
    },
    {
      "epoch": 0.188,
      "grad_norm": 0.5450664758682251,
      "learning_rate": 4.8825e-05,
      "loss": 0.0046,
      "step": 2820
    },
    {
      "epoch": 0.18866666666666668,
      "grad_norm": 0.5142188668251038,
      "learning_rate": 4.882083333333334e-05,
      "loss": 0.004,
      "step": 2830
    },
    {
      "epoch": 0.18933333333333333,
      "grad_norm": 0.22499696910381317,
      "learning_rate": 4.881666666666667e-05,
      "loss": 0.0035,
      "step": 2840
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.11159080266952515,
      "learning_rate": 4.88125e-05,
      "loss": 0.0051,
      "step": 2850
    },
    {
      "epoch": 0.19066666666666668,
      "grad_norm": 0.7362318634986877,
      "learning_rate": 4.880833333333333e-05,
      "loss": 0.0033,
      "step": 2860
    },
    {
      "epoch": 0.19133333333333333,
      "grad_norm": 0.5816321969032288,
      "learning_rate": 4.880416666666667e-05,
      "loss": 0.0036,
      "step": 2870
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.3838132619857788,
      "learning_rate": 4.88e-05,
      "loss": 0.0041,
      "step": 2880
    },
    {
      "epoch": 0.19266666666666668,
      "grad_norm": 0.736258327960968,
      "learning_rate": 4.879583333333334e-05,
      "loss": 0.0027,
      "step": 2890
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 1.8758949041366577,
      "learning_rate": 4.879166666666667e-05,
      "loss": 0.0049,
      "step": 2900
    },
    {
      "epoch": 0.194,
      "grad_norm": 1.1728335618972778,
      "learning_rate": 4.87875e-05,
      "loss": 0.0043,
      "step": 2910
    },
    {
      "epoch": 0.19466666666666665,
      "grad_norm": 0.1909654140472412,
      "learning_rate": 4.878333333333334e-05,
      "loss": 0.0052,
      "step": 2920
    },
    {
      "epoch": 0.19533333333333333,
      "grad_norm": 1.6326944828033447,
      "learning_rate": 4.877916666666667e-05,
      "loss": 0.0034,
      "step": 2930
    },
    {
      "epoch": 0.196,
      "grad_norm": 0.9271409511566162,
      "learning_rate": 4.8775000000000007e-05,
      "loss": 0.0038,
      "step": 2940
    },
    {
      "epoch": 0.19666666666666666,
      "grad_norm": 1.1889591217041016,
      "learning_rate": 4.877083333333334e-05,
      "loss": 0.0037,
      "step": 2950
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 0.5996056199073792,
      "learning_rate": 4.876666666666667e-05,
      "loss": 0.0039,
      "step": 2960
    },
    {
      "epoch": 0.198,
      "grad_norm": 0.8157148361206055,
      "learning_rate": 4.87625e-05,
      "loss": 0.0029,
      "step": 2970
    },
    {
      "epoch": 0.19866666666666666,
      "grad_norm": 0.7601659893989563,
      "learning_rate": 4.875833333333333e-05,
      "loss": 0.0052,
      "step": 2980
    },
    {
      "epoch": 0.19933333333333333,
      "grad_norm": 1.273533582687378,
      "learning_rate": 4.875416666666667e-05,
      "loss": 0.004,
      "step": 2990
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.0337233543395996,
      "learning_rate": 4.875e-05,
      "loss": 0.004,
      "step": 3000
    },
    {
      "epoch": 0.20066666666666666,
      "grad_norm": 0.5655863881111145,
      "learning_rate": 4.874583333333334e-05,
      "loss": 0.0033,
      "step": 3010
    },
    {
      "epoch": 0.20133333333333334,
      "grad_norm": 0.21488024294376373,
      "learning_rate": 4.874166666666667e-05,
      "loss": 0.003,
      "step": 3020
    },
    {
      "epoch": 0.202,
      "grad_norm": 1.3980647325515747,
      "learning_rate": 4.8737500000000006e-05,
      "loss": 0.005,
      "step": 3030
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 1.2247170209884644,
      "learning_rate": 4.8733333333333337e-05,
      "loss": 0.0033,
      "step": 3040
    },
    {
      "epoch": 0.20333333333333334,
      "grad_norm": 1.4913314580917358,
      "learning_rate": 4.872916666666667e-05,
      "loss": 0.004,
      "step": 3050
    },
    {
      "epoch": 0.204,
      "grad_norm": 0.8862841129302979,
      "learning_rate": 4.8725000000000005e-05,
      "loss": 0.0041,
      "step": 3060
    },
    {
      "epoch": 0.20466666666666666,
      "grad_norm": 0.8392050862312317,
      "learning_rate": 4.8720833333333336e-05,
      "loss": 0.0035,
      "step": 3070
    },
    {
      "epoch": 0.20533333333333334,
      "grad_norm": 1.1566914319992065,
      "learning_rate": 4.8716666666666674e-05,
      "loss": 0.0031,
      "step": 3080
    },
    {
      "epoch": 0.206,
      "grad_norm": 0.923065721988678,
      "learning_rate": 4.87125e-05,
      "loss": 0.0045,
      "step": 3090
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 0.6650733947753906,
      "learning_rate": 4.8708333333333336e-05,
      "loss": 0.0025,
      "step": 3100
    },
    {
      "epoch": 0.20733333333333334,
      "grad_norm": 0.2853628098964691,
      "learning_rate": 4.870416666666667e-05,
      "loss": 0.0036,
      "step": 3110
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.25514400005340576,
      "learning_rate": 4.87e-05,
      "loss": 0.0031,
      "step": 3120
    },
    {
      "epoch": 0.20866666666666667,
      "grad_norm": 0.7668795585632324,
      "learning_rate": 4.8695833333333336e-05,
      "loss": 0.0033,
      "step": 3130
    },
    {
      "epoch": 0.20933333333333334,
      "grad_norm": 0.9653162360191345,
      "learning_rate": 4.869166666666667e-05,
      "loss": 0.0035,
      "step": 3140
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.6910844445228577,
      "learning_rate": 4.8687500000000004e-05,
      "loss": 0.0034,
      "step": 3150
    },
    {
      "epoch": 0.21066666666666667,
      "grad_norm": 0.6205446124076843,
      "learning_rate": 4.8683333333333335e-05,
      "loss": 0.0041,
      "step": 3160
    },
    {
      "epoch": 0.21133333333333335,
      "grad_norm": 0.21098768711090088,
      "learning_rate": 4.867916666666667e-05,
      "loss": 0.0021,
      "step": 3170
    },
    {
      "epoch": 0.212,
      "grad_norm": 1.4098118543624878,
      "learning_rate": 4.8675000000000004e-05,
      "loss": 0.0037,
      "step": 3180
    },
    {
      "epoch": 0.21266666666666667,
      "grad_norm": 0.4289988577365875,
      "learning_rate": 4.8670833333333335e-05,
      "loss": 0.0035,
      "step": 3190
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 1.343937873840332,
      "learning_rate": 4.866666666666667e-05,
      "loss": 0.0042,
      "step": 3200
    },
    {
      "epoch": 0.214,
      "grad_norm": 1.6045048236846924,
      "learning_rate": 4.86625e-05,
      "loss": 0.0038,
      "step": 3210
    },
    {
      "epoch": 0.21466666666666667,
      "grad_norm": 1.2449259757995605,
      "learning_rate": 4.8658333333333335e-05,
      "loss": 0.0041,
      "step": 3220
    },
    {
      "epoch": 0.21533333333333332,
      "grad_norm": 0.29319921135902405,
      "learning_rate": 4.8654166666666666e-05,
      "loss": 0.0041,
      "step": 3230
    },
    {
      "epoch": 0.216,
      "grad_norm": 1.3857342004776,
      "learning_rate": 4.8650000000000003e-05,
      "loss": 0.0034,
      "step": 3240
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 0.812997579574585,
      "learning_rate": 4.8645833333333334e-05,
      "loss": 0.005,
      "step": 3250
    },
    {
      "epoch": 0.21733333333333332,
      "grad_norm": 0.22401073575019836,
      "learning_rate": 4.8641666666666665e-05,
      "loss": 0.0039,
      "step": 3260
    },
    {
      "epoch": 0.218,
      "grad_norm": 0.22259773313999176,
      "learning_rate": 4.86375e-05,
      "loss": 0.0036,
      "step": 3270
    },
    {
      "epoch": 0.21866666666666668,
      "grad_norm": 0.9581876993179321,
      "learning_rate": 4.8633333333333334e-05,
      "loss": 0.0044,
      "step": 3280
    },
    {
      "epoch": 0.21933333333333332,
      "grad_norm": 1.0750646591186523,
      "learning_rate": 4.862916666666667e-05,
      "loss": 0.0043,
      "step": 3290
    },
    {
      "epoch": 0.22,
      "grad_norm": 1.4765034914016724,
      "learning_rate": 4.8625e-05,
      "loss": 0.0047,
      "step": 3300
    },
    {
      "epoch": 0.22066666666666668,
      "grad_norm": 1.5766247510910034,
      "learning_rate": 4.862083333333334e-05,
      "loss": 0.0053,
      "step": 3310
    },
    {
      "epoch": 0.22133333333333333,
      "grad_norm": 1.2209542989730835,
      "learning_rate": 4.861666666666667e-05,
      "loss": 0.0027,
      "step": 3320
    },
    {
      "epoch": 0.222,
      "grad_norm": 0.4639345705509186,
      "learning_rate": 4.86125e-05,
      "loss": 0.0037,
      "step": 3330
    },
    {
      "epoch": 0.22266666666666668,
      "grad_norm": 0.551226019859314,
      "learning_rate": 4.8608333333333334e-05,
      "loss": 0.0045,
      "step": 3340
    },
    {
      "epoch": 0.22333333333333333,
      "grad_norm": 0.9307441115379333,
      "learning_rate": 4.8604166666666664e-05,
      "loss": 0.0044,
      "step": 3350
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.8940609097480774,
      "learning_rate": 4.86e-05,
      "loss": 0.0037,
      "step": 3360
    },
    {
      "epoch": 0.22466666666666665,
      "grad_norm": 0.3121626377105713,
      "learning_rate": 4.859583333333333e-05,
      "loss": 0.0037,
      "step": 3370
    },
    {
      "epoch": 0.22533333333333333,
      "grad_norm": 1.1699166297912598,
      "learning_rate": 4.859166666666667e-05,
      "loss": 0.0037,
      "step": 3380
    },
    {
      "epoch": 0.226,
      "grad_norm": 1.4883301258087158,
      "learning_rate": 4.85875e-05,
      "loss": 0.0049,
      "step": 3390
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.19341057538986206,
      "learning_rate": 4.858333333333333e-05,
      "loss": 0.0041,
      "step": 3400
    },
    {
      "epoch": 0.22733333333333333,
      "grad_norm": 0.6399067640304565,
      "learning_rate": 4.857916666666667e-05,
      "loss": 0.0039,
      "step": 3410
    },
    {
      "epoch": 0.228,
      "grad_norm": 1.4505231380462646,
      "learning_rate": 4.8575e-05,
      "loss": 0.0046,
      "step": 3420
    },
    {
      "epoch": 0.22866666666666666,
      "grad_norm": 1.8147964477539062,
      "learning_rate": 4.857083333333334e-05,
      "loss": 0.0033,
      "step": 3430
    },
    {
      "epoch": 0.22933333333333333,
      "grad_norm": 0.7774876356124878,
      "learning_rate": 4.856666666666667e-05,
      "loss": 0.0039,
      "step": 3440
    },
    {
      "epoch": 0.23,
      "grad_norm": 1.6367168426513672,
      "learning_rate": 4.85625e-05,
      "loss": 0.0042,
      "step": 3450
    },
    {
      "epoch": 0.23066666666666666,
      "grad_norm": 1.8197182416915894,
      "learning_rate": 4.855833333333333e-05,
      "loss": 0.0037,
      "step": 3460
    },
    {
      "epoch": 0.23133333333333334,
      "grad_norm": 1.5358400344848633,
      "learning_rate": 4.855416666666667e-05,
      "loss": 0.0033,
      "step": 3470
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.1465577930212021,
      "learning_rate": 4.855e-05,
      "loss": 0.0042,
      "step": 3480
    },
    {
      "epoch": 0.23266666666666666,
      "grad_norm": 0.19287191331386566,
      "learning_rate": 4.854583333333333e-05,
      "loss": 0.0053,
      "step": 3490
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.32873547077178955,
      "learning_rate": 4.854166666666667e-05,
      "loss": 0.0026,
      "step": 3500
    },
    {
      "epoch": 0.234,
      "grad_norm": 0.21482405066490173,
      "learning_rate": 4.85375e-05,
      "loss": 0.0038,
      "step": 3510
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 0.9758002758026123,
      "learning_rate": 4.853333333333334e-05,
      "loss": 0.0036,
      "step": 3520
    },
    {
      "epoch": 0.23533333333333334,
      "grad_norm": 1.3246201276779175,
      "learning_rate": 4.852916666666667e-05,
      "loss": 0.0046,
      "step": 3530
    },
    {
      "epoch": 0.236,
      "grad_norm": 0.46353378891944885,
      "learning_rate": 4.8525e-05,
      "loss": 0.0044,
      "step": 3540
    },
    {
      "epoch": 0.23666666666666666,
      "grad_norm": 0.9792349934577942,
      "learning_rate": 4.852083333333334e-05,
      "loss": 0.0033,
      "step": 3550
    },
    {
      "epoch": 0.23733333333333334,
      "grad_norm": 0.5032846331596375,
      "learning_rate": 4.851666666666667e-05,
      "loss": 0.0037,
      "step": 3560
    },
    {
      "epoch": 0.238,
      "grad_norm": 0.16497302055358887,
      "learning_rate": 4.85125e-05,
      "loss": 0.003,
      "step": 3570
    },
    {
      "epoch": 0.23866666666666667,
      "grad_norm": 1.0077036619186401,
      "learning_rate": 4.850833333333333e-05,
      "loss": 0.0057,
      "step": 3580
    },
    {
      "epoch": 0.23933333333333334,
      "grad_norm": 0.6009171009063721,
      "learning_rate": 4.850416666666667e-05,
      "loss": 0.0024,
      "step": 3590
    },
    {
      "epoch": 0.24,
      "grad_norm": 1.2811386585235596,
      "learning_rate": 4.85e-05,
      "loss": 0.0035,
      "step": 3600
    },
    {
      "epoch": 0.24066666666666667,
      "grad_norm": 0.45041462779045105,
      "learning_rate": 4.849583333333334e-05,
      "loss": 0.0046,
      "step": 3610
    },
    {
      "epoch": 0.24133333333333334,
      "grad_norm": 1.6127516031265259,
      "learning_rate": 4.849166666666667e-05,
      "loss": 0.0036,
      "step": 3620
    },
    {
      "epoch": 0.242,
      "grad_norm": 0.38222500681877136,
      "learning_rate": 4.84875e-05,
      "loss": 0.0031,
      "step": 3630
    },
    {
      "epoch": 0.24266666666666667,
      "grad_norm": 0.4823915958404541,
      "learning_rate": 4.848333333333334e-05,
      "loss": 0.0044,
      "step": 3640
    },
    {
      "epoch": 0.24333333333333335,
      "grad_norm": 0.6132540702819824,
      "learning_rate": 4.847916666666667e-05,
      "loss": 0.0026,
      "step": 3650
    },
    {
      "epoch": 0.244,
      "grad_norm": 0.6741745471954346,
      "learning_rate": 4.8475000000000006e-05,
      "loss": 0.0043,
      "step": 3660
    },
    {
      "epoch": 0.24466666666666667,
      "grad_norm": 1.9110480546951294,
      "learning_rate": 4.847083333333334e-05,
      "loss": 0.0027,
      "step": 3670
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 0.4399789869785309,
      "learning_rate": 4.8466666666666675e-05,
      "loss": 0.0051,
      "step": 3680
    },
    {
      "epoch": 0.246,
      "grad_norm": 0.2720473110675812,
      "learning_rate": 4.84625e-05,
      "loss": 0.0046,
      "step": 3690
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 1.3032920360565186,
      "learning_rate": 4.845833333333334e-05,
      "loss": 0.0045,
      "step": 3700
    },
    {
      "epoch": 0.24733333333333332,
      "grad_norm": 0.6443799138069153,
      "learning_rate": 4.845416666666667e-05,
      "loss": 0.0031,
      "step": 3710
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.5130902528762817,
      "learning_rate": 4.845e-05,
      "loss": 0.0031,
      "step": 3720
    },
    {
      "epoch": 0.24866666666666667,
      "grad_norm": 0.5912992358207703,
      "learning_rate": 4.8445833333333336e-05,
      "loss": 0.0041,
      "step": 3730
    },
    {
      "epoch": 0.24933333333333332,
      "grad_norm": 0.22220930457115173,
      "learning_rate": 4.844166666666667e-05,
      "loss": 0.0034,
      "step": 3740
    },
    {
      "epoch": 0.25,
      "grad_norm": 1.1421968936920166,
      "learning_rate": 4.8437500000000005e-05,
      "loss": 0.003,
      "step": 3750
    },
    {
      "epoch": 0.25066666666666665,
      "grad_norm": 1.1784101724624634,
      "learning_rate": 4.8433333333333336e-05,
      "loss": 0.006,
      "step": 3760
    },
    {
      "epoch": 0.25133333333333335,
      "grad_norm": 0.2224348783493042,
      "learning_rate": 4.842916666666667e-05,
      "loss": 0.0043,
      "step": 3770
    },
    {
      "epoch": 0.252,
      "grad_norm": 0.3709963858127594,
      "learning_rate": 4.8425000000000005e-05,
      "loss": 0.0056,
      "step": 3780
    },
    {
      "epoch": 0.25266666666666665,
      "grad_norm": 0.20942188799381256,
      "learning_rate": 4.8420833333333336e-05,
      "loss": 0.0038,
      "step": 3790
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.5331473350524902,
      "learning_rate": 4.8416666666666673e-05,
      "loss": 0.006,
      "step": 3800
    },
    {
      "epoch": 0.254,
      "grad_norm": 0.5783295631408691,
      "learning_rate": 4.8412500000000004e-05,
      "loss": 0.0049,
      "step": 3810
    },
    {
      "epoch": 0.25466666666666665,
      "grad_norm": 1.127259373664856,
      "learning_rate": 4.8408333333333335e-05,
      "loss": 0.003,
      "step": 3820
    },
    {
      "epoch": 0.25533333333333336,
      "grad_norm": 0.36087566614151,
      "learning_rate": 4.8404166666666666e-05,
      "loss": 0.0039,
      "step": 3830
    },
    {
      "epoch": 0.256,
      "grad_norm": 1.5388935804367065,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 0.0044,
      "step": 3840
    },
    {
      "epoch": 0.25666666666666665,
      "grad_norm": 1.0470846891403198,
      "learning_rate": 4.8395833333333335e-05,
      "loss": 0.0036,
      "step": 3850
    },
    {
      "epoch": 0.25733333333333336,
      "grad_norm": 0.14759401977062225,
      "learning_rate": 4.8391666666666666e-05,
      "loss": 0.0029,
      "step": 3860
    },
    {
      "epoch": 0.258,
      "grad_norm": 1.3647265434265137,
      "learning_rate": 4.8387500000000004e-05,
      "loss": 0.0036,
      "step": 3870
    },
    {
      "epoch": 0.25866666666666666,
      "grad_norm": 1.5020010471343994,
      "learning_rate": 4.8383333333333335e-05,
      "loss": 0.0055,
      "step": 3880
    },
    {
      "epoch": 0.25933333333333336,
      "grad_norm": 1.2420680522918701,
      "learning_rate": 4.837916666666667e-05,
      "loss": 0.005,
      "step": 3890
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.4880925714969635,
      "learning_rate": 4.8375000000000004e-05,
      "loss": 0.003,
      "step": 3900
    },
    {
      "epoch": 0.26066666666666666,
      "grad_norm": 1.5537973642349243,
      "learning_rate": 4.8370833333333335e-05,
      "loss": 0.0031,
      "step": 3910
    },
    {
      "epoch": 0.2613333333333333,
      "grad_norm": 0.9749120473861694,
      "learning_rate": 4.836666666666667e-05,
      "loss": 0.0038,
      "step": 3920
    },
    {
      "epoch": 0.262,
      "grad_norm": 0.1537284106016159,
      "learning_rate": 4.83625e-05,
      "loss": 0.0044,
      "step": 3930
    },
    {
      "epoch": 0.26266666666666666,
      "grad_norm": 0.8256956934928894,
      "learning_rate": 4.8358333333333334e-05,
      "loss": 0.003,
      "step": 3940
    },
    {
      "epoch": 0.2633333333333333,
      "grad_norm": 0.8492171168327332,
      "learning_rate": 4.8354166666666665e-05,
      "loss": 0.0048,
      "step": 3950
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.5048144459724426,
      "learning_rate": 4.835e-05,
      "loss": 0.0045,
      "step": 3960
    },
    {
      "epoch": 0.26466666666666666,
      "grad_norm": 1.6561084985733032,
      "learning_rate": 4.8345833333333334e-05,
      "loss": 0.005,
      "step": 3970
    },
    {
      "epoch": 0.2653333333333333,
      "grad_norm": 0.6071512699127197,
      "learning_rate": 4.834166666666667e-05,
      "loss": 0.0037,
      "step": 3980
    },
    {
      "epoch": 0.266,
      "grad_norm": 0.16577281057834625,
      "learning_rate": 4.83375e-05,
      "loss": 0.0037,
      "step": 3990
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.7631093263626099,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 0.0024,
      "step": 4000
    },
    {
      "epoch": 0.2673333333333333,
      "grad_norm": 0.3876414895057678,
      "learning_rate": 4.832916666666667e-05,
      "loss": 0.0028,
      "step": 4010
    },
    {
      "epoch": 0.268,
      "grad_norm": 0.12641192972660065,
      "learning_rate": 4.8325e-05,
      "loss": 0.0042,
      "step": 4020
    },
    {
      "epoch": 0.26866666666666666,
      "grad_norm": 1.5909169912338257,
      "learning_rate": 4.832083333333334e-05,
      "loss": 0.0037,
      "step": 4030
    },
    {
      "epoch": 0.2693333333333333,
      "grad_norm": 0.5312933325767517,
      "learning_rate": 4.831666666666667e-05,
      "loss": 0.0046,
      "step": 4040
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.17596659064292908,
      "learning_rate": 4.83125e-05,
      "loss": 0.0041,
      "step": 4050
    },
    {
      "epoch": 0.27066666666666667,
      "grad_norm": 1.6336170434951782,
      "learning_rate": 4.830833333333333e-05,
      "loss": 0.0028,
      "step": 4060
    },
    {
      "epoch": 0.2713333333333333,
      "grad_norm": 1.3257784843444824,
      "learning_rate": 4.8304166666666664e-05,
      "loss": 0.0034,
      "step": 4070
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.1878112256526947,
      "learning_rate": 4.83e-05,
      "loss": 0.0041,
      "step": 4080
    },
    {
      "epoch": 0.27266666666666667,
      "grad_norm": 1.5853601694107056,
      "learning_rate": 4.829583333333333e-05,
      "loss": 0.0043,
      "step": 4090
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 0.44803231954574585,
      "learning_rate": 4.829166666666667e-05,
      "loss": 0.0031,
      "step": 4100
    },
    {
      "epoch": 0.274,
      "grad_norm": 0.5079397559165955,
      "learning_rate": 4.82875e-05,
      "loss": 0.0037,
      "step": 4110
    },
    {
      "epoch": 0.27466666666666667,
      "grad_norm": 0.28729623556137085,
      "learning_rate": 4.828333333333334e-05,
      "loss": 0.0043,
      "step": 4120
    },
    {
      "epoch": 0.2753333333333333,
      "grad_norm": 0.6788010597229004,
      "learning_rate": 4.827916666666667e-05,
      "loss": 0.0033,
      "step": 4130
    },
    {
      "epoch": 0.276,
      "grad_norm": 0.767819344997406,
      "learning_rate": 4.8275e-05,
      "loss": 0.0034,
      "step": 4140
    },
    {
      "epoch": 0.27666666666666667,
      "grad_norm": 1.696294903755188,
      "learning_rate": 4.827083333333334e-05,
      "loss": 0.0033,
      "step": 4150
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 1.0523524284362793,
      "learning_rate": 4.826666666666667e-05,
      "loss": 0.0032,
      "step": 4160
    },
    {
      "epoch": 0.278,
      "grad_norm": 1.7324471473693848,
      "learning_rate": 4.826250000000001e-05,
      "loss": 0.0038,
      "step": 4170
    },
    {
      "epoch": 0.2786666666666667,
      "grad_norm": 0.949699878692627,
      "learning_rate": 4.825833333333333e-05,
      "loss": 0.003,
      "step": 4180
    },
    {
      "epoch": 0.2793333333333333,
      "grad_norm": 1.37506103515625,
      "learning_rate": 4.825416666666667e-05,
      "loss": 0.0037,
      "step": 4190
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.6282669901847839,
      "learning_rate": 4.825e-05,
      "loss": 0.0032,
      "step": 4200
    },
    {
      "epoch": 0.2806666666666667,
      "grad_norm": 1.235854983329773,
      "learning_rate": 4.824583333333333e-05,
      "loss": 0.0032,
      "step": 4210
    },
    {
      "epoch": 0.2813333333333333,
      "grad_norm": 0.15830881893634796,
      "learning_rate": 4.824166666666667e-05,
      "loss": 0.0032,
      "step": 4220
    },
    {
      "epoch": 0.282,
      "grad_norm": 0.7457111477851868,
      "learning_rate": 4.82375e-05,
      "loss": 0.0034,
      "step": 4230
    },
    {
      "epoch": 0.2826666666666667,
      "grad_norm": 0.12572747468948364,
      "learning_rate": 4.823333333333334e-05,
      "loss": 0.003,
      "step": 4240
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 0.5043851137161255,
      "learning_rate": 4.822916666666667e-05,
      "loss": 0.0035,
      "step": 4250
    },
    {
      "epoch": 0.284,
      "grad_norm": 1.567594289779663,
      "learning_rate": 4.822500000000001e-05,
      "loss": 0.003,
      "step": 4260
    },
    {
      "epoch": 0.2846666666666667,
      "grad_norm": 0.5433722138404846,
      "learning_rate": 4.822083333333334e-05,
      "loss": 0.0037,
      "step": 4270
    },
    {
      "epoch": 0.2853333333333333,
      "grad_norm": 0.6327200531959534,
      "learning_rate": 4.821666666666667e-05,
      "loss": 0.0041,
      "step": 4280
    },
    {
      "epoch": 0.286,
      "grad_norm": 0.5723459124565125,
      "learning_rate": 4.8212500000000006e-05,
      "loss": 0.0031,
      "step": 4290
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 1.4474555253982544,
      "learning_rate": 4.820833333333333e-05,
      "loss": 0.0045,
      "step": 4300
    },
    {
      "epoch": 0.28733333333333333,
      "grad_norm": 1.100568413734436,
      "learning_rate": 4.820416666666667e-05,
      "loss": 0.0034,
      "step": 4310
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.9009677767753601,
      "learning_rate": 4.82e-05,
      "loss": 0.0041,
      "step": 4320
    },
    {
      "epoch": 0.2886666666666667,
      "grad_norm": 0.8260223865509033,
      "learning_rate": 4.819583333333334e-05,
      "loss": 0.0026,
      "step": 4330
    },
    {
      "epoch": 0.28933333333333333,
      "grad_norm": 1.6583417654037476,
      "learning_rate": 4.819166666666667e-05,
      "loss": 0.0049,
      "step": 4340
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.9889898896217346,
      "learning_rate": 4.81875e-05,
      "loss": 0.0031,
      "step": 4350
    },
    {
      "epoch": 0.2906666666666667,
      "grad_norm": 0.7381437420845032,
      "learning_rate": 4.818333333333334e-05,
      "loss": 0.0033,
      "step": 4360
    },
    {
      "epoch": 0.29133333333333333,
      "grad_norm": 0.7285858988761902,
      "learning_rate": 4.817916666666667e-05,
      "loss": 0.0035,
      "step": 4370
    },
    {
      "epoch": 0.292,
      "grad_norm": 0.11148499697446823,
      "learning_rate": 4.8175000000000005e-05,
      "loss": 0.0036,
      "step": 4380
    },
    {
      "epoch": 0.2926666666666667,
      "grad_norm": 0.9979974031448364,
      "learning_rate": 4.8170833333333336e-05,
      "loss": 0.0044,
      "step": 4390
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 1.5291321277618408,
      "learning_rate": 4.8166666666666674e-05,
      "loss": 0.0031,
      "step": 4400
    },
    {
      "epoch": 0.294,
      "grad_norm": 0.6248039603233337,
      "learning_rate": 4.8162500000000005e-05,
      "loss": 0.0034,
      "step": 4410
    },
    {
      "epoch": 0.2946666666666667,
      "grad_norm": 0.8323062062263489,
      "learning_rate": 4.8158333333333336e-05,
      "loss": 0.0025,
      "step": 4420
    },
    {
      "epoch": 0.29533333333333334,
      "grad_norm": 1.3257341384887695,
      "learning_rate": 4.815416666666667e-05,
      "loss": 0.0035,
      "step": 4430
    },
    {
      "epoch": 0.296,
      "grad_norm": 1.1106271743774414,
      "learning_rate": 4.815e-05,
      "loss": 0.0053,
      "step": 4440
    },
    {
      "epoch": 0.2966666666666667,
      "grad_norm": 0.8304520845413208,
      "learning_rate": 4.8145833333333336e-05,
      "loss": 0.0033,
      "step": 4450
    },
    {
      "epoch": 0.29733333333333334,
      "grad_norm": 1.6073344945907593,
      "learning_rate": 4.814166666666667e-05,
      "loss": 0.0038,
      "step": 4460
    },
    {
      "epoch": 0.298,
      "grad_norm": 0.19167017936706543,
      "learning_rate": 4.8137500000000005e-05,
      "loss": 0.0028,
      "step": 4470
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 0.40391042828559875,
      "learning_rate": 4.8133333333333336e-05,
      "loss": 0.0031,
      "step": 4480
    },
    {
      "epoch": 0.29933333333333334,
      "grad_norm": 0.8214229345321655,
      "learning_rate": 4.8129166666666667e-05,
      "loss": 0.0036,
      "step": 4490
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.743028998374939,
      "learning_rate": 4.8125000000000004e-05,
      "loss": 0.0042,
      "step": 4500
    },
    {
      "epoch": 0.3006666666666667,
      "grad_norm": 0.2435729056596756,
      "learning_rate": 4.8120833333333335e-05,
      "loss": 0.0026,
      "step": 4510
    },
    {
      "epoch": 0.30133333333333334,
      "grad_norm": 0.7246306538581848,
      "learning_rate": 4.811666666666667e-05,
      "loss": 0.0028,
      "step": 4520
    },
    {
      "epoch": 0.302,
      "grad_norm": 0.6813671588897705,
      "learning_rate": 4.8112500000000004e-05,
      "loss": 0.0042,
      "step": 4530
    },
    {
      "epoch": 0.30266666666666664,
      "grad_norm": 0.7351318001747131,
      "learning_rate": 4.8108333333333335e-05,
      "loss": 0.0055,
      "step": 4540
    },
    {
      "epoch": 0.30333333333333334,
      "grad_norm": 1.2731684446334839,
      "learning_rate": 4.8104166666666666e-05,
      "loss": 0.0048,
      "step": 4550
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.9243801236152649,
      "learning_rate": 4.8100000000000004e-05,
      "loss": 0.0035,
      "step": 4560
    },
    {
      "epoch": 0.30466666666666664,
      "grad_norm": 1.2764942646026611,
      "learning_rate": 4.8095833333333335e-05,
      "loss": 0.0031,
      "step": 4570
    },
    {
      "epoch": 0.30533333333333335,
      "grad_norm": 0.2978487014770508,
      "learning_rate": 4.8091666666666666e-05,
      "loss": 0.0035,
      "step": 4580
    },
    {
      "epoch": 0.306,
      "grad_norm": 0.3037518858909607,
      "learning_rate": 4.80875e-05,
      "loss": 0.0034,
      "step": 4590
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.7138003706932068,
      "learning_rate": 4.8083333333333334e-05,
      "loss": 0.0035,
      "step": 4600
    },
    {
      "epoch": 0.30733333333333335,
      "grad_norm": 0.3994419276714325,
      "learning_rate": 4.807916666666667e-05,
      "loss": 0.0035,
      "step": 4610
    },
    {
      "epoch": 0.308,
      "grad_norm": 0.7798553109169006,
      "learning_rate": 4.8075e-05,
      "loss": 0.0035,
      "step": 4620
    },
    {
      "epoch": 0.30866666666666664,
      "grad_norm": 0.5827024579048157,
      "learning_rate": 4.8070833333333334e-05,
      "loss": 0.0023,
      "step": 4630
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 1.2398113012313843,
      "learning_rate": 4.806666666666667e-05,
      "loss": 0.0035,
      "step": 4640
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.4134829044342041,
      "learning_rate": 4.80625e-05,
      "loss": 0.0027,
      "step": 4650
    },
    {
      "epoch": 0.31066666666666665,
      "grad_norm": 1.4623171091079712,
      "learning_rate": 4.8058333333333334e-05,
      "loss": 0.004,
      "step": 4660
    },
    {
      "epoch": 0.31133333333333335,
      "grad_norm": 0.9309910535812378,
      "learning_rate": 4.8054166666666665e-05,
      "loss": 0.0056,
      "step": 4670
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.36495494842529297,
      "learning_rate": 4.805e-05,
      "loss": 0.0032,
      "step": 4680
    },
    {
      "epoch": 0.31266666666666665,
      "grad_norm": 1.5905787944793701,
      "learning_rate": 4.8045833333333333e-05,
      "loss": 0.0031,
      "step": 4690
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 0.5038665533065796,
      "learning_rate": 4.804166666666667e-05,
      "loss": 0.0047,
      "step": 4700
    },
    {
      "epoch": 0.314,
      "grad_norm": 0.8081688284873962,
      "learning_rate": 4.80375e-05,
      "loss": 0.0037,
      "step": 4710
    },
    {
      "epoch": 0.31466666666666665,
      "grad_norm": 0.17142543196678162,
      "learning_rate": 4.803333333333333e-05,
      "loss": 0.0027,
      "step": 4720
    },
    {
      "epoch": 0.31533333333333335,
      "grad_norm": 1.0508856773376465,
      "learning_rate": 4.802916666666667e-05,
      "loss": 0.0026,
      "step": 4730
    },
    {
      "epoch": 0.316,
      "grad_norm": 0.8721628785133362,
      "learning_rate": 4.8025e-05,
      "loss": 0.0031,
      "step": 4740
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 0.5854426026344299,
      "learning_rate": 4.802083333333334e-05,
      "loss": 0.003,
      "step": 4750
    },
    {
      "epoch": 0.31733333333333336,
      "grad_norm": 0.30236682295799255,
      "learning_rate": 4.801666666666667e-05,
      "loss": 0.0043,
      "step": 4760
    },
    {
      "epoch": 0.318,
      "grad_norm": 0.686276912689209,
      "learning_rate": 4.80125e-05,
      "loss": 0.0038,
      "step": 4770
    },
    {
      "epoch": 0.31866666666666665,
      "grad_norm": 0.4500109851360321,
      "learning_rate": 4.800833333333333e-05,
      "loss": 0.0044,
      "step": 4780
    },
    {
      "epoch": 0.31933333333333336,
      "grad_norm": 0.24446025490760803,
      "learning_rate": 4.8004166666666663e-05,
      "loss": 0.0028,
      "step": 4790
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.283463954925537,
      "learning_rate": 4.8e-05,
      "loss": 0.004,
      "step": 4800
    },
    {
      "epoch": 0.32066666666666666,
      "grad_norm": 0.3397277593612671,
      "learning_rate": 4.799583333333333e-05,
      "loss": 0.0042,
      "step": 4810
    },
    {
      "epoch": 0.32133333333333336,
      "grad_norm": 0.26729848980903625,
      "learning_rate": 4.799166666666667e-05,
      "loss": 0.0049,
      "step": 4820
    },
    {
      "epoch": 0.322,
      "grad_norm": 1.0646086931228638,
      "learning_rate": 4.79875e-05,
      "loss": 0.003,
      "step": 4830
    },
    {
      "epoch": 0.32266666666666666,
      "grad_norm": 1.0686935186386108,
      "learning_rate": 4.798333333333334e-05,
      "loss": 0.0039,
      "step": 4840
    },
    {
      "epoch": 0.3233333333333333,
      "grad_norm": 1.3095866441726685,
      "learning_rate": 4.797916666666667e-05,
      "loss": 0.006,
      "step": 4850
    },
    {
      "epoch": 0.324,
      "grad_norm": 0.7845761775970459,
      "learning_rate": 4.7975e-05,
      "loss": 0.0026,
      "step": 4860
    },
    {
      "epoch": 0.32466666666666666,
      "grad_norm": 0.8270766139030457,
      "learning_rate": 4.797083333333334e-05,
      "loss": 0.0054,
      "step": 4870
    },
    {
      "epoch": 0.3253333333333333,
      "grad_norm": 1.271499514579773,
      "learning_rate": 4.796666666666667e-05,
      "loss": 0.0036,
      "step": 4880
    },
    {
      "epoch": 0.326,
      "grad_norm": 0.3270185887813568,
      "learning_rate": 4.796250000000001e-05,
      "loss": 0.0032,
      "step": 4890
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 0.3722549378871918,
      "learning_rate": 4.795833333333333e-05,
      "loss": 0.0042,
      "step": 4900
    },
    {
      "epoch": 0.3273333333333333,
      "grad_norm": 1.003379225730896,
      "learning_rate": 4.795416666666667e-05,
      "loss": 0.0033,
      "step": 4910
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.34946534037590027,
      "learning_rate": 4.795e-05,
      "loss": 0.0035,
      "step": 4920
    },
    {
      "epoch": 0.32866666666666666,
      "grad_norm": 1.1024739742279053,
      "learning_rate": 4.794583333333333e-05,
      "loss": 0.004,
      "step": 4930
    },
    {
      "epoch": 0.3293333333333333,
      "grad_norm": 0.19007976353168488,
      "learning_rate": 4.794166666666667e-05,
      "loss": 0.003,
      "step": 4940
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.7492088079452515,
      "learning_rate": 4.79375e-05,
      "loss": 0.0024,
      "step": 4950
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 0.703568160533905,
      "learning_rate": 4.793333333333334e-05,
      "loss": 0.0037,
      "step": 4960
    },
    {
      "epoch": 0.3313333333333333,
      "grad_norm": 0.8097915649414062,
      "learning_rate": 4.792916666666667e-05,
      "loss": 0.004,
      "step": 4970
    },
    {
      "epoch": 0.332,
      "grad_norm": 1.24431312084198,
      "learning_rate": 4.7925000000000006e-05,
      "loss": 0.0037,
      "step": 4980
    },
    {
      "epoch": 0.33266666666666667,
      "grad_norm": 0.4113370478153229,
      "learning_rate": 4.792083333333334e-05,
      "loss": 0.0039,
      "step": 4990
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.29539990425109863,
      "learning_rate": 4.791666666666667e-05,
      "loss": 0.0036,
      "step": 5000
    },
    {
      "epoch": 0.334,
      "grad_norm": 0.49934840202331543,
      "learning_rate": 4.7912500000000006e-05,
      "loss": 0.0031,
      "step": 5010
    },
    {
      "epoch": 0.33466666666666667,
      "grad_norm": 0.9383407831192017,
      "learning_rate": 4.790833333333334e-05,
      "loss": 0.0032,
      "step": 5020
    },
    {
      "epoch": 0.3353333333333333,
      "grad_norm": 0.8043243885040283,
      "learning_rate": 4.790416666666667e-05,
      "loss": 0.0024,
      "step": 5030
    },
    {
      "epoch": 0.336,
      "grad_norm": 1.2579432725906372,
      "learning_rate": 4.79e-05,
      "loss": 0.0035,
      "step": 5040
    },
    {
      "epoch": 0.33666666666666667,
      "grad_norm": 1.2205970287322998,
      "learning_rate": 4.7895833333333337e-05,
      "loss": 0.0041,
      "step": 5050
    },
    {
      "epoch": 0.3373333333333333,
      "grad_norm": 0.5830954909324646,
      "learning_rate": 4.789166666666667e-05,
      "loss": 0.0038,
      "step": 5060
    },
    {
      "epoch": 0.338,
      "grad_norm": 0.743940532207489,
      "learning_rate": 4.78875e-05,
      "loss": 0.0037,
      "step": 5070
    },
    {
      "epoch": 0.33866666666666667,
      "grad_norm": 1.4050737619400024,
      "learning_rate": 4.7883333333333336e-05,
      "loss": 0.003,
      "step": 5080
    },
    {
      "epoch": 0.3393333333333333,
      "grad_norm": 0.723722517490387,
      "learning_rate": 4.787916666666667e-05,
      "loss": 0.0033,
      "step": 5090
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.2536763548851013,
      "learning_rate": 4.7875000000000005e-05,
      "loss": 0.0025,
      "step": 5100
    },
    {
      "epoch": 0.3406666666666667,
      "grad_norm": 0.1972314417362213,
      "learning_rate": 4.7870833333333336e-05,
      "loss": 0.0037,
      "step": 5110
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 0.2700187861919403,
      "learning_rate": 4.7866666666666674e-05,
      "loss": 0.0041,
      "step": 5120
    },
    {
      "epoch": 0.342,
      "grad_norm": 0.23115167021751404,
      "learning_rate": 4.7862500000000005e-05,
      "loss": 0.0049,
      "step": 5130
    },
    {
      "epoch": 0.3426666666666667,
      "grad_norm": 0.44922882318496704,
      "learning_rate": 4.7858333333333336e-05,
      "loss": 0.003,
      "step": 5140
    },
    {
      "epoch": 0.3433333333333333,
      "grad_norm": 1.315100073814392,
      "learning_rate": 4.7854166666666667e-05,
      "loss": 0.0049,
      "step": 5150
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.5988679528236389,
      "learning_rate": 4.785e-05,
      "loss": 0.0038,
      "step": 5160
    },
    {
      "epoch": 0.3446666666666667,
      "grad_norm": 0.4982264041900635,
      "learning_rate": 4.7845833333333335e-05,
      "loss": 0.0028,
      "step": 5170
    },
    {
      "epoch": 0.3453333333333333,
      "grad_norm": 0.13672465085983276,
      "learning_rate": 4.7841666666666666e-05,
      "loss": 0.0038,
      "step": 5180
    },
    {
      "epoch": 0.346,
      "grad_norm": 0.7067376375198364,
      "learning_rate": 4.7837500000000004e-05,
      "loss": 0.0042,
      "step": 5190
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.6612502932548523,
      "learning_rate": 4.7833333333333335e-05,
      "loss": 0.0025,
      "step": 5200
    },
    {
      "epoch": 0.3473333333333333,
      "grad_norm": 0.2434227168560028,
      "learning_rate": 4.7829166666666666e-05,
      "loss": 0.003,
      "step": 5210
    },
    {
      "epoch": 0.348,
      "grad_norm": 0.34814274311065674,
      "learning_rate": 4.7825000000000004e-05,
      "loss": 0.0034,
      "step": 5220
    },
    {
      "epoch": 0.3486666666666667,
      "grad_norm": 0.6760985851287842,
      "learning_rate": 4.7820833333333335e-05,
      "loss": 0.003,
      "step": 5230
    },
    {
      "epoch": 0.34933333333333333,
      "grad_norm": 1.5490659475326538,
      "learning_rate": 4.781666666666667e-05,
      "loss": 0.0029,
      "step": 5240
    },
    {
      "epoch": 0.35,
      "grad_norm": 1.2185773849487305,
      "learning_rate": 4.7812500000000003e-05,
      "loss": 0.0043,
      "step": 5250
    },
    {
      "epoch": 0.3506666666666667,
      "grad_norm": 0.12796491384506226,
      "learning_rate": 4.780833333333334e-05,
      "loss": 0.0029,
      "step": 5260
    },
    {
      "epoch": 0.35133333333333333,
      "grad_norm": 1.106927514076233,
      "learning_rate": 4.7804166666666665e-05,
      "loss": 0.0034,
      "step": 5270
    },
    {
      "epoch": 0.352,
      "grad_norm": 1.2633785009384155,
      "learning_rate": 4.78e-05,
      "loss": 0.0048,
      "step": 5280
    },
    {
      "epoch": 0.3526666666666667,
      "grad_norm": 0.30925342440605164,
      "learning_rate": 4.7795833333333334e-05,
      "loss": 0.0029,
      "step": 5290
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 0.2601206302642822,
      "learning_rate": 4.7791666666666665e-05,
      "loss": 0.0027,
      "step": 5300
    },
    {
      "epoch": 0.354,
      "grad_norm": 0.9646944403648376,
      "learning_rate": 4.77875e-05,
      "loss": 0.0025,
      "step": 5310
    },
    {
      "epoch": 0.3546666666666667,
      "grad_norm": 0.6143637895584106,
      "learning_rate": 4.7783333333333334e-05,
      "loss": 0.0033,
      "step": 5320
    },
    {
      "epoch": 0.35533333333333333,
      "grad_norm": 0.7691269516944885,
      "learning_rate": 4.777916666666667e-05,
      "loss": 0.0046,
      "step": 5330
    },
    {
      "epoch": 0.356,
      "grad_norm": 0.545270562171936,
      "learning_rate": 4.7775e-05,
      "loss": 0.0031,
      "step": 5340
    },
    {
      "epoch": 0.3566666666666667,
      "grad_norm": 0.9454483985900879,
      "learning_rate": 4.7770833333333333e-05,
      "loss": 0.0043,
      "step": 5350
    },
    {
      "epoch": 0.35733333333333334,
      "grad_norm": 0.4532085657119751,
      "learning_rate": 4.776666666666667e-05,
      "loss": 0.0035,
      "step": 5360
    },
    {
      "epoch": 0.358,
      "grad_norm": 1.1770453453063965,
      "learning_rate": 4.77625e-05,
      "loss": 0.0027,
      "step": 5370
    },
    {
      "epoch": 0.3586666666666667,
      "grad_norm": 0.6411908864974976,
      "learning_rate": 4.775833333333334e-05,
      "loss": 0.0048,
      "step": 5380
    },
    {
      "epoch": 0.35933333333333334,
      "grad_norm": 0.14000102877616882,
      "learning_rate": 4.7754166666666664e-05,
      "loss": 0.0042,
      "step": 5390
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.5090286135673523,
      "learning_rate": 4.775e-05,
      "loss": 0.0021,
      "step": 5400
    },
    {
      "epoch": 0.3606666666666667,
      "grad_norm": 0.9179437756538391,
      "learning_rate": 4.774583333333333e-05,
      "loss": 0.0036,
      "step": 5410
    },
    {
      "epoch": 0.36133333333333334,
      "grad_norm": 0.53196120262146,
      "learning_rate": 4.774166666666667e-05,
      "loss": 0.0038,
      "step": 5420
    },
    {
      "epoch": 0.362,
      "grad_norm": 1.0773279666900635,
      "learning_rate": 4.77375e-05,
      "loss": 0.0036,
      "step": 5430
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 1.316174030303955,
      "learning_rate": 4.773333333333333e-05,
      "loss": 0.0022,
      "step": 5440
    },
    {
      "epoch": 0.36333333333333334,
      "grad_norm": 0.179277241230011,
      "learning_rate": 4.772916666666667e-05,
      "loss": 0.0032,
      "step": 5450
    },
    {
      "epoch": 0.364,
      "grad_norm": 1.4768319129943848,
      "learning_rate": 4.7725e-05,
      "loss": 0.0034,
      "step": 5460
    },
    {
      "epoch": 0.36466666666666664,
      "grad_norm": 0.25801602005958557,
      "learning_rate": 4.772083333333334e-05,
      "loss": 0.0028,
      "step": 5470
    },
    {
      "epoch": 0.36533333333333334,
      "grad_norm": 0.5975989103317261,
      "learning_rate": 4.771666666666667e-05,
      "loss": 0.0039,
      "step": 5480
    },
    {
      "epoch": 0.366,
      "grad_norm": 1.4600045680999756,
      "learning_rate": 4.771250000000001e-05,
      "loss": 0.0036,
      "step": 5490
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.4384876787662506,
      "learning_rate": 4.770833333333334e-05,
      "loss": 0.0036,
      "step": 5500
    },
    {
      "epoch": 0.36733333333333335,
      "grad_norm": 1.266501545906067,
      "learning_rate": 4.770416666666667e-05,
      "loss": 0.0047,
      "step": 5510
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.785381019115448,
      "learning_rate": 4.77e-05,
      "loss": 0.0042,
      "step": 5520
    },
    {
      "epoch": 0.36866666666666664,
      "grad_norm": 1.309007167816162,
      "learning_rate": 4.769583333333333e-05,
      "loss": 0.0038,
      "step": 5530
    },
    {
      "epoch": 0.36933333333333335,
      "grad_norm": 1.0742741823196411,
      "learning_rate": 4.769166666666667e-05,
      "loss": 0.0044,
      "step": 5540
    },
    {
      "epoch": 0.37,
      "grad_norm": 1.2189104557037354,
      "learning_rate": 4.76875e-05,
      "loss": 0.0037,
      "step": 5550
    },
    {
      "epoch": 0.37066666666666664,
      "grad_norm": 1.112190842628479,
      "learning_rate": 4.768333333333334e-05,
      "loss": 0.0037,
      "step": 5560
    },
    {
      "epoch": 0.37133333333333335,
      "grad_norm": 0.6280006170272827,
      "learning_rate": 4.767916666666667e-05,
      "loss": 0.0045,
      "step": 5570
    },
    {
      "epoch": 0.372,
      "grad_norm": 0.16612908244132996,
      "learning_rate": 4.7675e-05,
      "loss": 0.0048,
      "step": 5580
    },
    {
      "epoch": 0.37266666666666665,
      "grad_norm": 1.0595130920410156,
      "learning_rate": 4.767083333333334e-05,
      "loss": 0.0052,
      "step": 5590
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 1.3914824724197388,
      "learning_rate": 4.766666666666667e-05,
      "loss": 0.0033,
      "step": 5600
    },
    {
      "epoch": 0.374,
      "grad_norm": 1.1376465559005737,
      "learning_rate": 4.7662500000000007e-05,
      "loss": 0.0044,
      "step": 5610
    },
    {
      "epoch": 0.37466666666666665,
      "grad_norm": 0.622086763381958,
      "learning_rate": 4.765833333333334e-05,
      "loss": 0.0026,
      "step": 5620
    },
    {
      "epoch": 0.37533333333333335,
      "grad_norm": 0.9849109649658203,
      "learning_rate": 4.765416666666667e-05,
      "loss": 0.0035,
      "step": 5630
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.35079964995384216,
      "learning_rate": 4.765e-05,
      "loss": 0.0045,
      "step": 5640
    },
    {
      "epoch": 0.37666666666666665,
      "grad_norm": 0.8485637903213501,
      "learning_rate": 4.764583333333334e-05,
      "loss": 0.0034,
      "step": 5650
    },
    {
      "epoch": 0.37733333333333335,
      "grad_norm": 0.4724384844303131,
      "learning_rate": 4.764166666666667e-05,
      "loss": 0.0031,
      "step": 5660
    },
    {
      "epoch": 0.378,
      "grad_norm": 0.145189106464386,
      "learning_rate": 4.76375e-05,
      "loss": 0.0044,
      "step": 5670
    },
    {
      "epoch": 0.37866666666666665,
      "grad_norm": 0.6295716166496277,
      "learning_rate": 4.763333333333334e-05,
      "loss": 0.0025,
      "step": 5680
    },
    {
      "epoch": 0.37933333333333336,
      "grad_norm": 0.6039990186691284,
      "learning_rate": 4.762916666666667e-05,
      "loss": 0.0038,
      "step": 5690
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.6908872723579407,
      "learning_rate": 4.7625000000000006e-05,
      "loss": 0.0023,
      "step": 5700
    },
    {
      "epoch": 0.38066666666666665,
      "grad_norm": 0.5229137539863586,
      "learning_rate": 4.762083333333334e-05,
      "loss": 0.0033,
      "step": 5710
    },
    {
      "epoch": 0.38133333333333336,
      "grad_norm": 1.1956480741500854,
      "learning_rate": 4.761666666666667e-05,
      "loss": 0.004,
      "step": 5720
    },
    {
      "epoch": 0.382,
      "grad_norm": 0.45639848709106445,
      "learning_rate": 4.7612500000000005e-05,
      "loss": 0.0045,
      "step": 5730
    },
    {
      "epoch": 0.38266666666666665,
      "grad_norm": 0.8845516443252563,
      "learning_rate": 4.7608333333333336e-05,
      "loss": 0.0027,
      "step": 5740
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 1.1998281478881836,
      "learning_rate": 4.760416666666667e-05,
      "loss": 0.0041,
      "step": 5750
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.9181052446365356,
      "learning_rate": 4.76e-05,
      "loss": 0.0039,
      "step": 5760
    },
    {
      "epoch": 0.38466666666666666,
      "grad_norm": 1.0669987201690674,
      "learning_rate": 4.7595833333333336e-05,
      "loss": 0.0042,
      "step": 5770
    },
    {
      "epoch": 0.38533333333333336,
      "grad_norm": 1.0665132999420166,
      "learning_rate": 4.759166666666667e-05,
      "loss": 0.0038,
      "step": 5780
    },
    {
      "epoch": 0.386,
      "grad_norm": 1.08854341506958,
      "learning_rate": 4.7587500000000005e-05,
      "loss": 0.0055,
      "step": 5790
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.6238972544670105,
      "learning_rate": 4.7583333333333336e-05,
      "loss": 0.0049,
      "step": 5800
    },
    {
      "epoch": 0.3873333333333333,
      "grad_norm": 0.3963947594165802,
      "learning_rate": 4.757916666666667e-05,
      "loss": 0.0037,
      "step": 5810
    },
    {
      "epoch": 0.388,
      "grad_norm": 0.4220937490463257,
      "learning_rate": 4.7575000000000004e-05,
      "loss": 0.004,
      "step": 5820
    },
    {
      "epoch": 0.38866666666666666,
      "grad_norm": 1.4122406244277954,
      "learning_rate": 4.7570833333333335e-05,
      "loss": 0.0033,
      "step": 5830
    },
    {
      "epoch": 0.3893333333333333,
      "grad_norm": 1.2112215757369995,
      "learning_rate": 4.756666666666667e-05,
      "loss": 0.0036,
      "step": 5840
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.8070688843727112,
      "learning_rate": 4.7562500000000004e-05,
      "loss": 0.0062,
      "step": 5850
    },
    {
      "epoch": 0.39066666666666666,
      "grad_norm": 1.035423994064331,
      "learning_rate": 4.7558333333333335e-05,
      "loss": 0.0032,
      "step": 5860
    },
    {
      "epoch": 0.3913333333333333,
      "grad_norm": 1.0879591703414917,
      "learning_rate": 4.7554166666666666e-05,
      "loss": 0.0042,
      "step": 5870
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.7021936774253845,
      "learning_rate": 4.755e-05,
      "loss": 0.0052,
      "step": 5880
    },
    {
      "epoch": 0.39266666666666666,
      "grad_norm": 1.2182891368865967,
      "learning_rate": 4.7545833333333335e-05,
      "loss": 0.003,
      "step": 5890
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 0.2532028555870056,
      "learning_rate": 4.7541666666666666e-05,
      "loss": 0.0045,
      "step": 5900
    },
    {
      "epoch": 0.394,
      "grad_norm": 1.216154932975769,
      "learning_rate": 4.7537500000000004e-05,
      "loss": 0.0032,
      "step": 5910
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 1.1722763776779175,
      "learning_rate": 4.7533333333333334e-05,
      "loss": 0.0046,
      "step": 5920
    },
    {
      "epoch": 0.3953333333333333,
      "grad_norm": 0.3179859220981598,
      "learning_rate": 4.752916666666667e-05,
      "loss": 0.0027,
      "step": 5930
    },
    {
      "epoch": 0.396,
      "grad_norm": 0.591046929359436,
      "learning_rate": 4.7525e-05,
      "loss": 0.0026,
      "step": 5940
    },
    {
      "epoch": 0.39666666666666667,
      "grad_norm": 0.3425639271736145,
      "learning_rate": 4.7520833333333334e-05,
      "loss": 0.0031,
      "step": 5950
    },
    {
      "epoch": 0.3973333333333333,
      "grad_norm": 0.8128775358200073,
      "learning_rate": 4.751666666666667e-05,
      "loss": 0.0027,
      "step": 5960
    },
    {
      "epoch": 0.398,
      "grad_norm": 1.3405418395996094,
      "learning_rate": 4.75125e-05,
      "loss": 0.0036,
      "step": 5970
    },
    {
      "epoch": 0.39866666666666667,
      "grad_norm": 0.5460469722747803,
      "learning_rate": 4.750833333333334e-05,
      "loss": 0.0036,
      "step": 5980
    },
    {
      "epoch": 0.3993333333333333,
      "grad_norm": 1.3988195657730103,
      "learning_rate": 4.7504166666666665e-05,
      "loss": 0.003,
      "step": 5990
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.5650808215141296,
      "learning_rate": 4.75e-05,
      "loss": 0.0029,
      "step": 6000
    },
    {
      "epoch": 0.40066666666666667,
      "grad_norm": 0.19818241894245148,
      "learning_rate": 4.7495833333333334e-05,
      "loss": 0.0024,
      "step": 6010
    },
    {
      "epoch": 0.4013333333333333,
      "grad_norm": 0.9094085097312927,
      "learning_rate": 4.7491666666666665e-05,
      "loss": 0.0026,
      "step": 6020
    },
    {
      "epoch": 0.402,
      "grad_norm": 0.7191340327262878,
      "learning_rate": 4.74875e-05,
      "loss": 0.0031,
      "step": 6030
    },
    {
      "epoch": 0.4026666666666667,
      "grad_norm": 0.08165238797664642,
      "learning_rate": 4.748333333333333e-05,
      "loss": 0.0027,
      "step": 6040
    },
    {
      "epoch": 0.4033333333333333,
      "grad_norm": 0.15347997844219208,
      "learning_rate": 4.747916666666667e-05,
      "loss": 0.003,
      "step": 6050
    },
    {
      "epoch": 0.404,
      "grad_norm": 0.3726889491081238,
      "learning_rate": 4.7475e-05,
      "loss": 0.0027,
      "step": 6060
    },
    {
      "epoch": 0.4046666666666667,
      "grad_norm": 0.3177434504032135,
      "learning_rate": 4.747083333333334e-05,
      "loss": 0.0026,
      "step": 6070
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 0.5192985534667969,
      "learning_rate": 4.746666666666667e-05,
      "loss": 0.0044,
      "step": 6080
    },
    {
      "epoch": 0.406,
      "grad_norm": 0.3464012145996094,
      "learning_rate": 4.74625e-05,
      "loss": 0.0037,
      "step": 6090
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 0.28068476915359497,
      "learning_rate": 4.745833333333334e-05,
      "loss": 0.0042,
      "step": 6100
    },
    {
      "epoch": 0.4073333333333333,
      "grad_norm": 0.717705488204956,
      "learning_rate": 4.7454166666666664e-05,
      "loss": 0.0023,
      "step": 6110
    },
    {
      "epoch": 0.408,
      "grad_norm": 1.3457310199737549,
      "learning_rate": 4.745e-05,
      "loss": 0.0037,
      "step": 6120
    },
    {
      "epoch": 0.4086666666666667,
      "grad_norm": 0.5778967142105103,
      "learning_rate": 4.744583333333333e-05,
      "loss": 0.0029,
      "step": 6130
    },
    {
      "epoch": 0.4093333333333333,
      "grad_norm": 0.21783347427845,
      "learning_rate": 4.744166666666667e-05,
      "loss": 0.0033,
      "step": 6140
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.3436151444911957,
      "learning_rate": 4.74375e-05,
      "loss": 0.0049,
      "step": 6150
    },
    {
      "epoch": 0.4106666666666667,
      "grad_norm": 1.3634257316589355,
      "learning_rate": 4.743333333333333e-05,
      "loss": 0.0047,
      "step": 6160
    },
    {
      "epoch": 0.41133333333333333,
      "grad_norm": 1.1544675827026367,
      "learning_rate": 4.742916666666667e-05,
      "loss": 0.003,
      "step": 6170
    },
    {
      "epoch": 0.412,
      "grad_norm": 1.3471033573150635,
      "learning_rate": 4.7425e-05,
      "loss": 0.003,
      "step": 6180
    },
    {
      "epoch": 0.4126666666666667,
      "grad_norm": 1.6496061086654663,
      "learning_rate": 4.742083333333334e-05,
      "loss": 0.0042,
      "step": 6190
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.4875029921531677,
      "learning_rate": 4.741666666666667e-05,
      "loss": 0.0029,
      "step": 6200
    },
    {
      "epoch": 0.414,
      "grad_norm": 0.24130858480930328,
      "learning_rate": 4.741250000000001e-05,
      "loss": 0.0034,
      "step": 6210
    },
    {
      "epoch": 0.4146666666666667,
      "grad_norm": 0.250049889087677,
      "learning_rate": 4.740833333333334e-05,
      "loss": 0.0049,
      "step": 6220
    },
    {
      "epoch": 0.41533333333333333,
      "grad_norm": 0.49794477224349976,
      "learning_rate": 4.740416666666667e-05,
      "loss": 0.0043,
      "step": 6230
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.6020777821540833,
      "learning_rate": 4.74e-05,
      "loss": 0.0032,
      "step": 6240
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 0.20689159631729126,
      "learning_rate": 4.739583333333333e-05,
      "loss": 0.0038,
      "step": 6250
    },
    {
      "epoch": 0.41733333333333333,
      "grad_norm": 1.3652961254119873,
      "learning_rate": 4.739166666666667e-05,
      "loss": 0.0036,
      "step": 6260
    },
    {
      "epoch": 0.418,
      "grad_norm": 0.8831093311309814,
      "learning_rate": 4.73875e-05,
      "loss": 0.0037,
      "step": 6270
    },
    {
      "epoch": 0.4186666666666667,
      "grad_norm": 0.23814231157302856,
      "learning_rate": 4.738333333333334e-05,
      "loss": 0.0022,
      "step": 6280
    },
    {
      "epoch": 0.41933333333333334,
      "grad_norm": 0.3732949197292328,
      "learning_rate": 4.737916666666667e-05,
      "loss": 0.0043,
      "step": 6290
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.6194766163825989,
      "learning_rate": 4.7375e-05,
      "loss": 0.0031,
      "step": 6300
    },
    {
      "epoch": 0.4206666666666667,
      "grad_norm": 1.0941001176834106,
      "learning_rate": 4.737083333333334e-05,
      "loss": 0.0032,
      "step": 6310
    },
    {
      "epoch": 0.42133333333333334,
      "grad_norm": 0.1539364457130432,
      "learning_rate": 4.736666666666667e-05,
      "loss": 0.0044,
      "step": 6320
    },
    {
      "epoch": 0.422,
      "grad_norm": 0.5705458521842957,
      "learning_rate": 4.7362500000000006e-05,
      "loss": 0.0041,
      "step": 6330
    },
    {
      "epoch": 0.4226666666666667,
      "grad_norm": 1.105122447013855,
      "learning_rate": 4.735833333333334e-05,
      "loss": 0.0031,
      "step": 6340
    },
    {
      "epoch": 0.42333333333333334,
      "grad_norm": 0.13483896851539612,
      "learning_rate": 4.7354166666666675e-05,
      "loss": 0.004,
      "step": 6350
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.4946076273918152,
      "learning_rate": 4.735e-05,
      "loss": 0.0032,
      "step": 6360
    },
    {
      "epoch": 0.4246666666666667,
      "grad_norm": 1.5495728254318237,
      "learning_rate": 4.734583333333334e-05,
      "loss": 0.004,
      "step": 6370
    },
    {
      "epoch": 0.42533333333333334,
      "grad_norm": 0.9181692600250244,
      "learning_rate": 4.734166666666667e-05,
      "loss": 0.0033,
      "step": 6380
    },
    {
      "epoch": 0.426,
      "grad_norm": 1.1203128099441528,
      "learning_rate": 4.73375e-05,
      "loss": 0.0048,
      "step": 6390
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.9522242546081543,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 0.0035,
      "step": 6400
    },
    {
      "epoch": 0.42733333333333334,
      "grad_norm": 0.39684727787971497,
      "learning_rate": 4.732916666666667e-05,
      "loss": 0.0032,
      "step": 6410
    },
    {
      "epoch": 0.428,
      "grad_norm": 0.9013205170631409,
      "learning_rate": 4.7325000000000005e-05,
      "loss": 0.0023,
      "step": 6420
    },
    {
      "epoch": 0.42866666666666664,
      "grad_norm": 1.140989899635315,
      "learning_rate": 4.7320833333333336e-05,
      "loss": 0.0055,
      "step": 6430
    },
    {
      "epoch": 0.42933333333333334,
      "grad_norm": 0.39161232113838196,
      "learning_rate": 4.731666666666667e-05,
      "loss": 0.0037,
      "step": 6440
    },
    {
      "epoch": 0.43,
      "grad_norm": 1.001819133758545,
      "learning_rate": 4.7312500000000005e-05,
      "loss": 0.0034,
      "step": 6450
    },
    {
      "epoch": 0.43066666666666664,
      "grad_norm": 0.7495710253715515,
      "learning_rate": 4.7308333333333336e-05,
      "loss": 0.0028,
      "step": 6460
    },
    {
      "epoch": 0.43133333333333335,
      "grad_norm": 1.0681759119033813,
      "learning_rate": 4.7304166666666674e-05,
      "loss": 0.0044,
      "step": 6470
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.1543983668088913,
      "learning_rate": 4.73e-05,
      "loss": 0.0034,
      "step": 6480
    },
    {
      "epoch": 0.43266666666666664,
      "grad_norm": 1.0543341636657715,
      "learning_rate": 4.7295833333333336e-05,
      "loss": 0.0037,
      "step": 6490
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 1.0398129224777222,
      "learning_rate": 4.7291666666666666e-05,
      "loss": 0.0033,
      "step": 6500
    },
    {
      "epoch": 0.434,
      "grad_norm": 0.43851062655448914,
      "learning_rate": 4.7287500000000004e-05,
      "loss": 0.0029,
      "step": 6510
    },
    {
      "epoch": 0.43466666666666665,
      "grad_norm": 1.2313169240951538,
      "learning_rate": 4.7283333333333335e-05,
      "loss": 0.0024,
      "step": 6520
    },
    {
      "epoch": 0.43533333333333335,
      "grad_norm": 0.9640794992446899,
      "learning_rate": 4.7279166666666666e-05,
      "loss": 0.0024,
      "step": 6530
    },
    {
      "epoch": 0.436,
      "grad_norm": 1.5300507545471191,
      "learning_rate": 4.7275000000000004e-05,
      "loss": 0.0037,
      "step": 6540
    },
    {
      "epoch": 0.43666666666666665,
      "grad_norm": 0.6262165904045105,
      "learning_rate": 4.7270833333333335e-05,
      "loss": 0.0039,
      "step": 6550
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 0.3128182590007782,
      "learning_rate": 4.726666666666667e-05,
      "loss": 0.0025,
      "step": 6560
    },
    {
      "epoch": 0.438,
      "grad_norm": 0.3754892349243164,
      "learning_rate": 4.7262500000000004e-05,
      "loss": 0.0032,
      "step": 6570
    },
    {
      "epoch": 0.43866666666666665,
      "grad_norm": 1.3885008096694946,
      "learning_rate": 4.7258333333333335e-05,
      "loss": 0.0031,
      "step": 6580
    },
    {
      "epoch": 0.43933333333333335,
      "grad_norm": 0.3914533853530884,
      "learning_rate": 4.725416666666667e-05,
      "loss": 0.0038,
      "step": 6590
    },
    {
      "epoch": 0.44,
      "grad_norm": 1.1891227960586548,
      "learning_rate": 4.7249999999999997e-05,
      "loss": 0.0037,
      "step": 6600
    },
    {
      "epoch": 0.44066666666666665,
      "grad_norm": 0.237665593624115,
      "learning_rate": 4.7245833333333334e-05,
      "loss": 0.0031,
      "step": 6610
    },
    {
      "epoch": 0.44133333333333336,
      "grad_norm": 1.2387306690216064,
      "learning_rate": 4.7241666666666665e-05,
      "loss": 0.0036,
      "step": 6620
    },
    {
      "epoch": 0.442,
      "grad_norm": 1.3421975374221802,
      "learning_rate": 4.72375e-05,
      "loss": 0.0023,
      "step": 6630
    },
    {
      "epoch": 0.44266666666666665,
      "grad_norm": 1.4200483560562134,
      "learning_rate": 4.7233333333333334e-05,
      "loss": 0.0031,
      "step": 6640
    },
    {
      "epoch": 0.44333333333333336,
      "grad_norm": 0.3169054687023163,
      "learning_rate": 4.722916666666667e-05,
      "loss": 0.0048,
      "step": 6650
    },
    {
      "epoch": 0.444,
      "grad_norm": 0.5668976902961731,
      "learning_rate": 4.7225e-05,
      "loss": 0.0032,
      "step": 6660
    },
    {
      "epoch": 0.44466666666666665,
      "grad_norm": 0.6643959879875183,
      "learning_rate": 4.7220833333333334e-05,
      "loss": 0.0022,
      "step": 6670
    },
    {
      "epoch": 0.44533333333333336,
      "grad_norm": 0.3943842649459839,
      "learning_rate": 4.721666666666667e-05,
      "loss": 0.0041,
      "step": 6680
    },
    {
      "epoch": 0.446,
      "grad_norm": 0.7634662389755249,
      "learning_rate": 4.72125e-05,
      "loss": 0.0037,
      "step": 6690
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 0.4377771317958832,
      "learning_rate": 4.720833333333334e-05,
      "loss": 0.0038,
      "step": 6700
    },
    {
      "epoch": 0.44733333333333336,
      "grad_norm": 0.73165363073349,
      "learning_rate": 4.720416666666667e-05,
      "loss": 0.0039,
      "step": 6710
    },
    {
      "epoch": 0.448,
      "grad_norm": 1.6871544122695923,
      "learning_rate": 4.72e-05,
      "loss": 0.0034,
      "step": 6720
    },
    {
      "epoch": 0.44866666666666666,
      "grad_norm": 0.3579109311103821,
      "learning_rate": 4.719583333333333e-05,
      "loss": 0.0024,
      "step": 6730
    },
    {
      "epoch": 0.4493333333333333,
      "grad_norm": 0.37441763281822205,
      "learning_rate": 4.7191666666666664e-05,
      "loss": 0.0027,
      "step": 6740
    },
    {
      "epoch": 0.45,
      "grad_norm": 1.9039689302444458,
      "learning_rate": 4.71875e-05,
      "loss": 0.0035,
      "step": 6750
    },
    {
      "epoch": 0.45066666666666666,
      "grad_norm": 0.9259925484657288,
      "learning_rate": 4.718333333333333e-05,
      "loss": 0.003,
      "step": 6760
    },
    {
      "epoch": 0.4513333333333333,
      "grad_norm": 0.1290828287601471,
      "learning_rate": 4.717916666666667e-05,
      "loss": 0.0025,
      "step": 6770
    },
    {
      "epoch": 0.452,
      "grad_norm": 0.3274081349372864,
      "learning_rate": 4.7175e-05,
      "loss": 0.0047,
      "step": 6780
    },
    {
      "epoch": 0.45266666666666666,
      "grad_norm": 1.002066969871521,
      "learning_rate": 4.717083333333334e-05,
      "loss": 0.004,
      "step": 6790
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.8654166460037231,
      "learning_rate": 4.716666666666667e-05,
      "loss": 0.0036,
      "step": 6800
    },
    {
      "epoch": 0.454,
      "grad_norm": 0.624860942363739,
      "learning_rate": 4.71625e-05,
      "loss": 0.0026,
      "step": 6810
    },
    {
      "epoch": 0.45466666666666666,
      "grad_norm": 0.8111307621002197,
      "learning_rate": 4.715833333333334e-05,
      "loss": 0.0037,
      "step": 6820
    },
    {
      "epoch": 0.4553333333333333,
      "grad_norm": 0.997177243232727,
      "learning_rate": 4.715416666666667e-05,
      "loss": 0.0038,
      "step": 6830
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.8582230806350708,
      "learning_rate": 4.715e-05,
      "loss": 0.004,
      "step": 6840
    },
    {
      "epoch": 0.45666666666666667,
      "grad_norm": 0.7811107039451599,
      "learning_rate": 4.714583333333333e-05,
      "loss": 0.0048,
      "step": 6850
    },
    {
      "epoch": 0.4573333333333333,
      "grad_norm": 1.1598961353302002,
      "learning_rate": 4.714166666666667e-05,
      "loss": 0.0035,
      "step": 6860
    },
    {
      "epoch": 0.458,
      "grad_norm": 0.19769680500030518,
      "learning_rate": 4.71375e-05,
      "loss": 0.0036,
      "step": 6870
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 1.3699878454208374,
      "learning_rate": 4.713333333333333e-05,
      "loss": 0.0031,
      "step": 6880
    },
    {
      "epoch": 0.4593333333333333,
      "grad_norm": 1.2844600677490234,
      "learning_rate": 4.712916666666667e-05,
      "loss": 0.0034,
      "step": 6890
    },
    {
      "epoch": 0.46,
      "grad_norm": 1.274626612663269,
      "learning_rate": 4.7125e-05,
      "loss": 0.0028,
      "step": 6900
    },
    {
      "epoch": 0.46066666666666667,
      "grad_norm": 0.24075983464717865,
      "learning_rate": 4.712083333333334e-05,
      "loss": 0.0037,
      "step": 6910
    },
    {
      "epoch": 0.4613333333333333,
      "grad_norm": 0.5111427307128906,
      "learning_rate": 4.711666666666667e-05,
      "loss": 0.0028,
      "step": 6920
    },
    {
      "epoch": 0.462,
      "grad_norm": 0.19077765941619873,
      "learning_rate": 4.711250000000001e-05,
      "loss": 0.003,
      "step": 6930
    },
    {
      "epoch": 0.46266666666666667,
      "grad_norm": 0.7718935012817383,
      "learning_rate": 4.710833333333334e-05,
      "loss": 0.0037,
      "step": 6940
    },
    {
      "epoch": 0.4633333333333333,
      "grad_norm": 0.3568619191646576,
      "learning_rate": 4.710416666666667e-05,
      "loss": 0.0032,
      "step": 6950
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.846760630607605,
      "learning_rate": 4.71e-05,
      "loss": 0.0027,
      "step": 6960
    },
    {
      "epoch": 0.4646666666666667,
      "grad_norm": 0.11220426857471466,
      "learning_rate": 4.709583333333333e-05,
      "loss": 0.0048,
      "step": 6970
    },
    {
      "epoch": 0.4653333333333333,
      "grad_norm": 0.2050345540046692,
      "learning_rate": 4.709166666666667e-05,
      "loss": 0.0036,
      "step": 6980
    },
    {
      "epoch": 0.466,
      "grad_norm": 0.05757554620504379,
      "learning_rate": 4.70875e-05,
      "loss": 0.0028,
      "step": 6990
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.6158124804496765,
      "learning_rate": 4.708333333333334e-05,
      "loss": 0.005,
      "step": 7000
    },
    {
      "epoch": 0.4673333333333333,
      "grad_norm": 0.47863760590553284,
      "learning_rate": 4.707916666666667e-05,
      "loss": 0.0031,
      "step": 7010
    },
    {
      "epoch": 0.468,
      "grad_norm": 0.9858037829399109,
      "learning_rate": 4.7075e-05,
      "loss": 0.0049,
      "step": 7020
    },
    {
      "epoch": 0.4686666666666667,
      "grad_norm": 0.5417238473892212,
      "learning_rate": 4.707083333333334e-05,
      "loss": 0.0038,
      "step": 7030
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 1.0493773221969604,
      "learning_rate": 4.706666666666667e-05,
      "loss": 0.0033,
      "step": 7040
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.2001587301492691,
      "learning_rate": 4.7062500000000006e-05,
      "loss": 0.0031,
      "step": 7050
    },
    {
      "epoch": 0.4706666666666667,
      "grad_norm": 0.6835939288139343,
      "learning_rate": 4.7058333333333337e-05,
      "loss": 0.004,
      "step": 7060
    },
    {
      "epoch": 0.4713333333333333,
      "grad_norm": 0.967454731464386,
      "learning_rate": 4.7054166666666674e-05,
      "loss": 0.0034,
      "step": 7070
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.4322623610496521,
      "learning_rate": 4.705e-05,
      "loss": 0.0029,
      "step": 7080
    },
    {
      "epoch": 0.4726666666666667,
      "grad_norm": 0.8441066741943359,
      "learning_rate": 4.7045833333333336e-05,
      "loss": 0.0037,
      "step": 7090
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 0.5720183253288269,
      "learning_rate": 4.704166666666667e-05,
      "loss": 0.0031,
      "step": 7100
    },
    {
      "epoch": 0.474,
      "grad_norm": 0.5745135545730591,
      "learning_rate": 4.70375e-05,
      "loss": 0.003,
      "step": 7110
    },
    {
      "epoch": 0.4746666666666667,
      "grad_norm": 1.0389615297317505,
      "learning_rate": 4.7033333333333336e-05,
      "loss": 0.004,
      "step": 7120
    },
    {
      "epoch": 0.47533333333333333,
      "grad_norm": 0.17166617512702942,
      "learning_rate": 4.702916666666667e-05,
      "loss": 0.002,
      "step": 7130
    },
    {
      "epoch": 0.476,
      "grad_norm": 0.19200383126735687,
      "learning_rate": 4.7025000000000005e-05,
      "loss": 0.0024,
      "step": 7140
    },
    {
      "epoch": 0.4766666666666667,
      "grad_norm": 0.055007338523864746,
      "learning_rate": 4.7020833333333336e-05,
      "loss": 0.0025,
      "step": 7150
    },
    {
      "epoch": 0.47733333333333333,
      "grad_norm": 0.8698751926422119,
      "learning_rate": 4.701666666666667e-05,
      "loss": 0.003,
      "step": 7160
    },
    {
      "epoch": 0.478,
      "grad_norm": 1.0754244327545166,
      "learning_rate": 4.7012500000000004e-05,
      "loss": 0.003,
      "step": 7170
    },
    {
      "epoch": 0.4786666666666667,
      "grad_norm": 1.2130942344665527,
      "learning_rate": 4.7008333333333335e-05,
      "loss": 0.0041,
      "step": 7180
    },
    {
      "epoch": 0.47933333333333333,
      "grad_norm": 1.1191133260726929,
      "learning_rate": 4.700416666666667e-05,
      "loss": 0.0037,
      "step": 7190
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.4775145947933197,
      "learning_rate": 4.7e-05,
      "loss": 0.0033,
      "step": 7200
    },
    {
      "epoch": 0.4806666666666667,
      "grad_norm": 0.616603672504425,
      "learning_rate": 4.6995833333333335e-05,
      "loss": 0.005,
      "step": 7210
    },
    {
      "epoch": 0.48133333333333334,
      "grad_norm": 0.9081879258155823,
      "learning_rate": 4.6991666666666666e-05,
      "loss": 0.0041,
      "step": 7220
    },
    {
      "epoch": 0.482,
      "grad_norm": 0.3827323317527771,
      "learning_rate": 4.6987500000000004e-05,
      "loss": 0.0039,
      "step": 7230
    },
    {
      "epoch": 0.4826666666666667,
      "grad_norm": 0.8824182152748108,
      "learning_rate": 4.6983333333333335e-05,
      "loss": 0.0033,
      "step": 7240
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 0.6164167523384094,
      "learning_rate": 4.6979166666666666e-05,
      "loss": 0.0038,
      "step": 7250
    },
    {
      "epoch": 0.484,
      "grad_norm": 0.386981725692749,
      "learning_rate": 4.6975000000000003e-05,
      "loss": 0.0019,
      "step": 7260
    },
    {
      "epoch": 0.4846666666666667,
      "grad_norm": 0.9425315260887146,
      "learning_rate": 4.6970833333333334e-05,
      "loss": 0.0028,
      "step": 7270
    },
    {
      "epoch": 0.48533333333333334,
      "grad_norm": 0.8045969009399414,
      "learning_rate": 4.696666666666667e-05,
      "loss": 0.0029,
      "step": 7280
    },
    {
      "epoch": 0.486,
      "grad_norm": 0.5894646644592285,
      "learning_rate": 4.69625e-05,
      "loss": 0.0041,
      "step": 7290
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 0.5126438736915588,
      "learning_rate": 4.695833333333334e-05,
      "loss": 0.0023,
      "step": 7300
    },
    {
      "epoch": 0.48733333333333334,
      "grad_norm": 0.46210429072380066,
      "learning_rate": 4.695416666666667e-05,
      "loss": 0.0037,
      "step": 7310
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.8825369477272034,
      "learning_rate": 4.695e-05,
      "loss": 0.0027,
      "step": 7320
    },
    {
      "epoch": 0.4886666666666667,
      "grad_norm": 1.1082416772842407,
      "learning_rate": 4.6945833333333334e-05,
      "loss": 0.0028,
      "step": 7330
    },
    {
      "epoch": 0.48933333333333334,
      "grad_norm": 0.8209580779075623,
      "learning_rate": 4.6941666666666665e-05,
      "loss": 0.0025,
      "step": 7340
    },
    {
      "epoch": 0.49,
      "grad_norm": 1.102725863456726,
      "learning_rate": 4.69375e-05,
      "loss": 0.003,
      "step": 7350
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 0.34013018012046814,
      "learning_rate": 4.6933333333333333e-05,
      "loss": 0.0023,
      "step": 7360
    },
    {
      "epoch": 0.49133333333333334,
      "grad_norm": 0.7171105146408081,
      "learning_rate": 4.692916666666667e-05,
      "loss": 0.003,
      "step": 7370
    },
    {
      "epoch": 0.492,
      "grad_norm": 0.4831714928150177,
      "learning_rate": 4.6925e-05,
      "loss": 0.003,
      "step": 7380
    },
    {
      "epoch": 0.49266666666666664,
      "grad_norm": 0.8023403286933899,
      "learning_rate": 4.692083333333333e-05,
      "loss": 0.0026,
      "step": 7390
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.9952295422554016,
      "learning_rate": 4.691666666666667e-05,
      "loss": 0.0045,
      "step": 7400
    },
    {
      "epoch": 0.494,
      "grad_norm": 0.21376368403434753,
      "learning_rate": 4.69125e-05,
      "loss": 0.003,
      "step": 7410
    },
    {
      "epoch": 0.49466666666666664,
      "grad_norm": 0.7930742502212524,
      "learning_rate": 4.690833333333334e-05,
      "loss": 0.0021,
      "step": 7420
    },
    {
      "epoch": 0.49533333333333335,
      "grad_norm": 1.1648060083389282,
      "learning_rate": 4.690416666666667e-05,
      "loss": 0.0024,
      "step": 7430
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.22721542418003082,
      "learning_rate": 4.69e-05,
      "loss": 0.003,
      "step": 7440
    },
    {
      "epoch": 0.49666666666666665,
      "grad_norm": 0.13500039279460907,
      "learning_rate": 4.689583333333333e-05,
      "loss": 0.0038,
      "step": 7450
    },
    {
      "epoch": 0.49733333333333335,
      "grad_norm": 0.47111940383911133,
      "learning_rate": 4.689166666666667e-05,
      "loss": 0.0038,
      "step": 7460
    },
    {
      "epoch": 0.498,
      "grad_norm": 1.3000483512878418,
      "learning_rate": 4.68875e-05,
      "loss": 0.0033,
      "step": 7470
    },
    {
      "epoch": 0.49866666666666665,
      "grad_norm": 1.4326071739196777,
      "learning_rate": 4.688333333333333e-05,
      "loss": 0.0031,
      "step": 7480
    },
    {
      "epoch": 0.49933333333333335,
      "grad_norm": 0.5591257214546204,
      "learning_rate": 4.687916666666667e-05,
      "loss": 0.0037,
      "step": 7490
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.3368886709213257,
      "learning_rate": 4.6875e-05,
      "loss": 0.0037,
      "step": 7500
    },
    {
      "epoch": 0.5006666666666667,
      "grad_norm": 0.9935293197631836,
      "learning_rate": 4.687083333333334e-05,
      "loss": 0.0029,
      "step": 7510
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 0.08880732953548431,
      "learning_rate": 4.686666666666667e-05,
      "loss": 0.0028,
      "step": 7520
    },
    {
      "epoch": 0.502,
      "grad_norm": 1.0724366903305054,
      "learning_rate": 4.68625e-05,
      "loss": 0.0041,
      "step": 7530
    },
    {
      "epoch": 0.5026666666666667,
      "grad_norm": 0.9901062250137329,
      "learning_rate": 4.685833333333334e-05,
      "loss": 0.003,
      "step": 7540
    },
    {
      "epoch": 0.5033333333333333,
      "grad_norm": 1.056032419204712,
      "learning_rate": 4.685416666666667e-05,
      "loss": 0.0023,
      "step": 7550
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.12905927002429962,
      "learning_rate": 4.685000000000001e-05,
      "loss": 0.0039,
      "step": 7560
    },
    {
      "epoch": 0.5046666666666667,
      "grad_norm": 0.8935486078262329,
      "learning_rate": 4.684583333333333e-05,
      "loss": 0.0029,
      "step": 7570
    },
    {
      "epoch": 0.5053333333333333,
      "grad_norm": 0.09911147505044937,
      "learning_rate": 4.684166666666667e-05,
      "loss": 0.0029,
      "step": 7580
    },
    {
      "epoch": 0.506,
      "grad_norm": 0.8634986877441406,
      "learning_rate": 4.68375e-05,
      "loss": 0.0024,
      "step": 7590
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.9787014722824097,
      "learning_rate": 4.683333333333334e-05,
      "loss": 0.0024,
      "step": 7600
    },
    {
      "epoch": 0.5073333333333333,
      "grad_norm": 0.7814839482307434,
      "learning_rate": 4.682916666666667e-05,
      "loss": 0.0037,
      "step": 7610
    },
    {
      "epoch": 0.508,
      "grad_norm": 0.19144971668720245,
      "learning_rate": 4.6825e-05,
      "loss": 0.003,
      "step": 7620
    },
    {
      "epoch": 0.5086666666666667,
      "grad_norm": 0.31765690445899963,
      "learning_rate": 4.682083333333334e-05,
      "loss": 0.003,
      "step": 7630
    },
    {
      "epoch": 0.5093333333333333,
      "grad_norm": 0.34773796796798706,
      "learning_rate": 4.681666666666667e-05,
      "loss": 0.0027,
      "step": 7640
    },
    {
      "epoch": 0.51,
      "grad_norm": 1.0382593870162964,
      "learning_rate": 4.6812500000000006e-05,
      "loss": 0.0035,
      "step": 7650
    },
    {
      "epoch": 0.5106666666666667,
      "grad_norm": 1.03468918800354,
      "learning_rate": 4.680833333333334e-05,
      "loss": 0.0034,
      "step": 7660
    },
    {
      "epoch": 0.5113333333333333,
      "grad_norm": 0.4875366687774658,
      "learning_rate": 4.680416666666667e-05,
      "loss": 0.0029,
      "step": 7670
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.7914777994155884,
      "learning_rate": 4.6800000000000006e-05,
      "loss": 0.0032,
      "step": 7680
    },
    {
      "epoch": 0.5126666666666667,
      "grad_norm": 0.6481848359107971,
      "learning_rate": 4.679583333333333e-05,
      "loss": 0.0044,
      "step": 7690
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 0.4713221788406372,
      "learning_rate": 4.679166666666667e-05,
      "loss": 0.0029,
      "step": 7700
    },
    {
      "epoch": 0.514,
      "grad_norm": 0.6199143528938293,
      "learning_rate": 4.67875e-05,
      "loss": 0.0029,
      "step": 7710
    },
    {
      "epoch": 0.5146666666666667,
      "grad_norm": 0.3679966926574707,
      "learning_rate": 4.6783333333333337e-05,
      "loss": 0.0033,
      "step": 7720
    },
    {
      "epoch": 0.5153333333333333,
      "grad_norm": 0.20226413011550903,
      "learning_rate": 4.677916666666667e-05,
      "loss": 0.0037,
      "step": 7730
    },
    {
      "epoch": 0.516,
      "grad_norm": 0.16939008235931396,
      "learning_rate": 4.6775000000000005e-05,
      "loss": 0.0043,
      "step": 7740
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 1.162490725517273,
      "learning_rate": 4.6770833333333336e-05,
      "loss": 0.0026,
      "step": 7750
    },
    {
      "epoch": 0.5173333333333333,
      "grad_norm": 1.079908013343811,
      "learning_rate": 4.676666666666667e-05,
      "loss": 0.0031,
      "step": 7760
    },
    {
      "epoch": 0.518,
      "grad_norm": 0.2593943178653717,
      "learning_rate": 4.6762500000000005e-05,
      "loss": 0.0025,
      "step": 7770
    },
    {
      "epoch": 0.5186666666666667,
      "grad_norm": 0.24365046620368958,
      "learning_rate": 4.6758333333333336e-05,
      "loss": 0.003,
      "step": 7780
    },
    {
      "epoch": 0.5193333333333333,
      "grad_norm": 0.23522493243217468,
      "learning_rate": 4.6754166666666674e-05,
      "loss": 0.0031,
      "step": 7790
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.291524052619934,
      "learning_rate": 4.6750000000000005e-05,
      "loss": 0.0044,
      "step": 7800
    },
    {
      "epoch": 0.5206666666666667,
      "grad_norm": 0.3900526762008667,
      "learning_rate": 4.6745833333333336e-05,
      "loss": 0.0028,
      "step": 7810
    },
    {
      "epoch": 0.5213333333333333,
      "grad_norm": 1.093462347984314,
      "learning_rate": 4.674166666666667e-05,
      "loss": 0.003,
      "step": 7820
    },
    {
      "epoch": 0.522,
      "grad_norm": 0.18082574009895325,
      "learning_rate": 4.67375e-05,
      "loss": 0.0028,
      "step": 7830
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 0.8786438703536987,
      "learning_rate": 4.6733333333333335e-05,
      "loss": 0.0022,
      "step": 7840
    },
    {
      "epoch": 0.5233333333333333,
      "grad_norm": 0.5500067472457886,
      "learning_rate": 4.6729166666666666e-05,
      "loss": 0.004,
      "step": 7850
    },
    {
      "epoch": 0.524,
      "grad_norm": 0.29829180240631104,
      "learning_rate": 4.6725000000000004e-05,
      "loss": 0.0044,
      "step": 7860
    },
    {
      "epoch": 0.5246666666666666,
      "grad_norm": 0.6851005554199219,
      "learning_rate": 4.6720833333333335e-05,
      "loss": 0.0035,
      "step": 7870
    },
    {
      "epoch": 0.5253333333333333,
      "grad_norm": 1.4130644798278809,
      "learning_rate": 4.671666666666667e-05,
      "loss": 0.0033,
      "step": 7880
    },
    {
      "epoch": 0.526,
      "grad_norm": 1.145278811454773,
      "learning_rate": 4.6712500000000004e-05,
      "loss": 0.0028,
      "step": 7890
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 1.0258182287216187,
      "learning_rate": 4.6708333333333335e-05,
      "loss": 0.0039,
      "step": 7900
    },
    {
      "epoch": 0.5273333333333333,
      "grad_norm": 0.4897812306880951,
      "learning_rate": 4.670416666666667e-05,
      "loss": 0.0035,
      "step": 7910
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.19958694279193878,
      "learning_rate": 4.6700000000000003e-05,
      "loss": 0.0031,
      "step": 7920
    },
    {
      "epoch": 0.5286666666666666,
      "grad_norm": 0.6877722144126892,
      "learning_rate": 4.6695833333333334e-05,
      "loss": 0.0023,
      "step": 7930
    },
    {
      "epoch": 0.5293333333333333,
      "grad_norm": 1.1642333269119263,
      "learning_rate": 4.6691666666666665e-05,
      "loss": 0.0037,
      "step": 7940
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.6175572276115417,
      "learning_rate": 4.66875e-05,
      "loss": 0.0033,
      "step": 7950
    },
    {
      "epoch": 0.5306666666666666,
      "grad_norm": 1.0581073760986328,
      "learning_rate": 4.6683333333333334e-05,
      "loss": 0.003,
      "step": 7960
    },
    {
      "epoch": 0.5313333333333333,
      "grad_norm": 0.5496383905410767,
      "learning_rate": 4.6679166666666665e-05,
      "loss": 0.0033,
      "step": 7970
    },
    {
      "epoch": 0.532,
      "grad_norm": 0.8766186833381653,
      "learning_rate": 4.6675e-05,
      "loss": 0.0028,
      "step": 7980
    },
    {
      "epoch": 0.5326666666666666,
      "grad_norm": 0.5359751582145691,
      "learning_rate": 4.6670833333333334e-05,
      "loss": 0.0035,
      "step": 7990
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 1.319885492324829,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.0031,
      "step": 8000
    },
    {
      "epoch": 0.534,
      "grad_norm": 1.347113013267517,
      "learning_rate": 4.66625e-05,
      "loss": 0.0031,
      "step": 8010
    },
    {
      "epoch": 0.5346666666666666,
      "grad_norm": 0.9883711338043213,
      "learning_rate": 4.665833333333334e-05,
      "loss": 0.0035,
      "step": 8020
    },
    {
      "epoch": 0.5353333333333333,
      "grad_norm": 0.31607934832572937,
      "learning_rate": 4.665416666666667e-05,
      "loss": 0.0027,
      "step": 8030
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.90781569480896,
      "learning_rate": 4.665e-05,
      "loss": 0.0037,
      "step": 8040
    },
    {
      "epoch": 0.5366666666666666,
      "grad_norm": 0.26487666368484497,
      "learning_rate": 4.664583333333333e-05,
      "loss": 0.003,
      "step": 8050
    },
    {
      "epoch": 0.5373333333333333,
      "grad_norm": 0.6396721005439758,
      "learning_rate": 4.6641666666666664e-05,
      "loss": 0.0038,
      "step": 8060
    },
    {
      "epoch": 0.538,
      "grad_norm": 0.9844906330108643,
      "learning_rate": 4.66375e-05,
      "loss": 0.0039,
      "step": 8070
    },
    {
      "epoch": 0.5386666666666666,
      "grad_norm": 1.1933115720748901,
      "learning_rate": 4.663333333333333e-05,
      "loss": 0.0032,
      "step": 8080
    },
    {
      "epoch": 0.5393333333333333,
      "grad_norm": 0.994701087474823,
      "learning_rate": 4.662916666666667e-05,
      "loss": 0.0033,
      "step": 8090
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.7458093166351318,
      "learning_rate": 4.6625e-05,
      "loss": 0.0025,
      "step": 8100
    },
    {
      "epoch": 0.5406666666666666,
      "grad_norm": 1.14090895652771,
      "learning_rate": 4.662083333333333e-05,
      "loss": 0.0031,
      "step": 8110
    },
    {
      "epoch": 0.5413333333333333,
      "grad_norm": 1.2085529565811157,
      "learning_rate": 4.661666666666667e-05,
      "loss": 0.0025,
      "step": 8120
    },
    {
      "epoch": 0.542,
      "grad_norm": 0.923292338848114,
      "learning_rate": 4.66125e-05,
      "loss": 0.0026,
      "step": 8130
    },
    {
      "epoch": 0.5426666666666666,
      "grad_norm": 0.30414631962776184,
      "learning_rate": 4.660833333333334e-05,
      "loss": 0.0035,
      "step": 8140
    },
    {
      "epoch": 0.5433333333333333,
      "grad_norm": 1.060979962348938,
      "learning_rate": 4.660416666666667e-05,
      "loss": 0.0033,
      "step": 8150
    },
    {
      "epoch": 0.544,
      "grad_norm": 1.3235619068145752,
      "learning_rate": 4.660000000000001e-05,
      "loss": 0.0024,
      "step": 8160
    },
    {
      "epoch": 0.5446666666666666,
      "grad_norm": 0.7848600745201111,
      "learning_rate": 4.659583333333333e-05,
      "loss": 0.0039,
      "step": 8170
    },
    {
      "epoch": 0.5453333333333333,
      "grad_norm": 1.2432903051376343,
      "learning_rate": 4.659166666666667e-05,
      "loss": 0.0036,
      "step": 8180
    },
    {
      "epoch": 0.546,
      "grad_norm": 0.4028705060482025,
      "learning_rate": 4.65875e-05,
      "loss": 0.0031,
      "step": 8190
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.5863414406776428,
      "learning_rate": 4.658333333333333e-05,
      "loss": 0.0029,
      "step": 8200
    },
    {
      "epoch": 0.5473333333333333,
      "grad_norm": 0.7820116281509399,
      "learning_rate": 4.657916666666667e-05,
      "loss": 0.0028,
      "step": 8210
    },
    {
      "epoch": 0.548,
      "grad_norm": 0.3230266273021698,
      "learning_rate": 4.6575e-05,
      "loss": 0.0034,
      "step": 8220
    },
    {
      "epoch": 0.5486666666666666,
      "grad_norm": 0.2446272224187851,
      "learning_rate": 4.657083333333334e-05,
      "loss": 0.0037,
      "step": 8230
    },
    {
      "epoch": 0.5493333333333333,
      "grad_norm": 0.599113941192627,
      "learning_rate": 4.656666666666667e-05,
      "loss": 0.0026,
      "step": 8240
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.6008501052856445,
      "learning_rate": 4.65625e-05,
      "loss": 0.0026,
      "step": 8250
    },
    {
      "epoch": 0.5506666666666666,
      "grad_norm": 0.23771975934505463,
      "learning_rate": 4.655833333333334e-05,
      "loss": 0.0037,
      "step": 8260
    },
    {
      "epoch": 0.5513333333333333,
      "grad_norm": 0.4095011353492737,
      "learning_rate": 4.655416666666667e-05,
      "loss": 0.0027,
      "step": 8270
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.3448532521724701,
      "learning_rate": 4.655000000000001e-05,
      "loss": 0.0027,
      "step": 8280
    },
    {
      "epoch": 0.5526666666666666,
      "grad_norm": 0.47537171840667725,
      "learning_rate": 4.654583333333333e-05,
      "loss": 0.0023,
      "step": 8290
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 0.16391398012638092,
      "learning_rate": 4.654166666666667e-05,
      "loss": 0.0026,
      "step": 8300
    },
    {
      "epoch": 0.554,
      "grad_norm": 0.10481083393096924,
      "learning_rate": 4.65375e-05,
      "loss": 0.0029,
      "step": 8310
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 1.1720741987228394,
      "learning_rate": 4.653333333333334e-05,
      "loss": 0.0031,
      "step": 8320
    },
    {
      "epoch": 0.5553333333333333,
      "grad_norm": 1.0424450635910034,
      "learning_rate": 4.652916666666667e-05,
      "loss": 0.0034,
      "step": 8330
    },
    {
      "epoch": 0.556,
      "grad_norm": 0.6740330457687378,
      "learning_rate": 4.6525e-05,
      "loss": 0.0027,
      "step": 8340
    },
    {
      "epoch": 0.5566666666666666,
      "grad_norm": 0.8315352201461792,
      "learning_rate": 4.652083333333334e-05,
      "loss": 0.0035,
      "step": 8350
    },
    {
      "epoch": 0.5573333333333333,
      "grad_norm": 0.10667600482702255,
      "learning_rate": 4.651666666666667e-05,
      "loss": 0.0032,
      "step": 8360
    },
    {
      "epoch": 0.558,
      "grad_norm": 0.19686678051948547,
      "learning_rate": 4.6512500000000006e-05,
      "loss": 0.0031,
      "step": 8370
    },
    {
      "epoch": 0.5586666666666666,
      "grad_norm": 0.6587392687797546,
      "learning_rate": 4.650833333333334e-05,
      "loss": 0.0025,
      "step": 8380
    },
    {
      "epoch": 0.5593333333333333,
      "grad_norm": 0.7324837446212769,
      "learning_rate": 4.650416666666667e-05,
      "loss": 0.0035,
      "step": 8390
    },
    {
      "epoch": 0.56,
      "grad_norm": 1.6470643281936646,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 0.0044,
      "step": 8400
    },
    {
      "epoch": 0.5606666666666666,
      "grad_norm": 0.4107409715652466,
      "learning_rate": 4.649583333333333e-05,
      "loss": 0.0036,
      "step": 8410
    },
    {
      "epoch": 0.5613333333333334,
      "grad_norm": 0.46201029419898987,
      "learning_rate": 4.649166666666667e-05,
      "loss": 0.0035,
      "step": 8420
    },
    {
      "epoch": 0.562,
      "grad_norm": 0.3923068642616272,
      "learning_rate": 4.64875e-05,
      "loss": 0.0024,
      "step": 8430
    },
    {
      "epoch": 0.5626666666666666,
      "grad_norm": 1.0055702924728394,
      "learning_rate": 4.6483333333333336e-05,
      "loss": 0.0048,
      "step": 8440
    },
    {
      "epoch": 0.5633333333333334,
      "grad_norm": 0.6667426824569702,
      "learning_rate": 4.647916666666667e-05,
      "loss": 0.0044,
      "step": 8450
    },
    {
      "epoch": 0.564,
      "grad_norm": 1.16754150390625,
      "learning_rate": 4.6475000000000005e-05,
      "loss": 0.0031,
      "step": 8460
    },
    {
      "epoch": 0.5646666666666667,
      "grad_norm": 0.5485078692436218,
      "learning_rate": 4.6470833333333336e-05,
      "loss": 0.0039,
      "step": 8470
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 0.8982669115066528,
      "learning_rate": 4.646666666666667e-05,
      "loss": 0.0025,
      "step": 8480
    },
    {
      "epoch": 0.566,
      "grad_norm": 0.21844996511936188,
      "learning_rate": 4.6462500000000005e-05,
      "loss": 0.0029,
      "step": 8490
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.6834937334060669,
      "learning_rate": 4.6458333333333335e-05,
      "loss": 0.0039,
      "step": 8500
    },
    {
      "epoch": 0.5673333333333334,
      "grad_norm": 0.3036920428276062,
      "learning_rate": 4.645416666666667e-05,
      "loss": 0.003,
      "step": 8510
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.0811871662735939,
      "learning_rate": 4.6450000000000004e-05,
      "loss": 0.0018,
      "step": 8520
    },
    {
      "epoch": 0.5686666666666667,
      "grad_norm": 0.35242709517478943,
      "learning_rate": 4.6445833333333335e-05,
      "loss": 0.0033,
      "step": 8530
    },
    {
      "epoch": 0.5693333333333334,
      "grad_norm": 1.061696171760559,
      "learning_rate": 4.6441666666666666e-05,
      "loss": 0.0016,
      "step": 8540
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.10383457690477371,
      "learning_rate": 4.64375e-05,
      "loss": 0.0049,
      "step": 8550
    },
    {
      "epoch": 0.5706666666666667,
      "grad_norm": 0.14021512866020203,
      "learning_rate": 4.6433333333333335e-05,
      "loss": 0.0025,
      "step": 8560
    },
    {
      "epoch": 0.5713333333333334,
      "grad_norm": 0.9904906153678894,
      "learning_rate": 4.6429166666666666e-05,
      "loss": 0.0034,
      "step": 8570
    },
    {
      "epoch": 0.572,
      "grad_norm": 1.158008098602295,
      "learning_rate": 4.6425000000000004e-05,
      "loss": 0.0028,
      "step": 8580
    },
    {
      "epoch": 0.5726666666666667,
      "grad_norm": 0.5885018706321716,
      "learning_rate": 4.6420833333333335e-05,
      "loss": 0.0039,
      "step": 8590
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.3903132677078247,
      "learning_rate": 4.641666666666667e-05,
      "loss": 0.0031,
      "step": 8600
    },
    {
      "epoch": 0.574,
      "grad_norm": 0.6248076558113098,
      "learning_rate": 4.64125e-05,
      "loss": 0.0034,
      "step": 8610
    },
    {
      "epoch": 0.5746666666666667,
      "grad_norm": 1.4984347820281982,
      "learning_rate": 4.6408333333333334e-05,
      "loss": 0.0034,
      "step": 8620
    },
    {
      "epoch": 0.5753333333333334,
      "grad_norm": 0.8769806027412415,
      "learning_rate": 4.640416666666667e-05,
      "loss": 0.0029,
      "step": 8630
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.2491668164730072,
      "learning_rate": 4.64e-05,
      "loss": 0.0033,
      "step": 8640
    },
    {
      "epoch": 0.5766666666666667,
      "grad_norm": 0.5386205315589905,
      "learning_rate": 4.6395833333333334e-05,
      "loss": 0.0031,
      "step": 8650
    },
    {
      "epoch": 0.5773333333333334,
      "grad_norm": 0.44899293780326843,
      "learning_rate": 4.6391666666666665e-05,
      "loss": 0.0042,
      "step": 8660
    },
    {
      "epoch": 0.578,
      "grad_norm": 0.44233396649360657,
      "learning_rate": 4.63875e-05,
      "loss": 0.0025,
      "step": 8670
    },
    {
      "epoch": 0.5786666666666667,
      "grad_norm": 1.0911688804626465,
      "learning_rate": 4.6383333333333334e-05,
      "loss": 0.0031,
      "step": 8680
    },
    {
      "epoch": 0.5793333333333334,
      "grad_norm": 0.24998144805431366,
      "learning_rate": 4.6379166666666665e-05,
      "loss": 0.0024,
      "step": 8690
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.2050565481185913,
      "learning_rate": 4.6375e-05,
      "loss": 0.0036,
      "step": 8700
    },
    {
      "epoch": 0.5806666666666667,
      "grad_norm": 0.8872929811477661,
      "learning_rate": 4.637083333333333e-05,
      "loss": 0.0029,
      "step": 8710
    },
    {
      "epoch": 0.5813333333333334,
      "grad_norm": 0.7157166600227356,
      "learning_rate": 4.636666666666667e-05,
      "loss": 0.0042,
      "step": 8720
    },
    {
      "epoch": 0.582,
      "grad_norm": 0.3088557720184326,
      "learning_rate": 4.63625e-05,
      "loss": 0.0015,
      "step": 8730
    },
    {
      "epoch": 0.5826666666666667,
      "grad_norm": 0.6299300789833069,
      "learning_rate": 4.635833333333334e-05,
      "loss": 0.0022,
      "step": 8740
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 0.20415258407592773,
      "learning_rate": 4.635416666666667e-05,
      "loss": 0.0033,
      "step": 8750
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.6919320225715637,
      "learning_rate": 4.635e-05,
      "loss": 0.0039,
      "step": 8760
    },
    {
      "epoch": 0.5846666666666667,
      "grad_norm": 0.46862685680389404,
      "learning_rate": 4.634583333333334e-05,
      "loss": 0.0032,
      "step": 8770
    },
    {
      "epoch": 0.5853333333333334,
      "grad_norm": 0.18583400547504425,
      "learning_rate": 4.6341666666666664e-05,
      "loss": 0.0025,
      "step": 8780
    },
    {
      "epoch": 0.586,
      "grad_norm": 0.7964715361595154,
      "learning_rate": 4.63375e-05,
      "loss": 0.003,
      "step": 8790
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.09524043649435043,
      "learning_rate": 4.633333333333333e-05,
      "loss": 0.0026,
      "step": 8800
    },
    {
      "epoch": 0.5873333333333334,
      "grad_norm": 1.1233859062194824,
      "learning_rate": 4.632916666666667e-05,
      "loss": 0.0026,
      "step": 8810
    },
    {
      "epoch": 0.588,
      "grad_norm": 0.6356949806213379,
      "learning_rate": 4.6325e-05,
      "loss": 0.0047,
      "step": 8820
    },
    {
      "epoch": 0.5886666666666667,
      "grad_norm": 0.44174692034721375,
      "learning_rate": 4.632083333333334e-05,
      "loss": 0.004,
      "step": 8830
    },
    {
      "epoch": 0.5893333333333334,
      "grad_norm": 0.6953902244567871,
      "learning_rate": 4.631666666666667e-05,
      "loss": 0.0019,
      "step": 8840
    },
    {
      "epoch": 0.59,
      "grad_norm": 1.313276767730713,
      "learning_rate": 4.63125e-05,
      "loss": 0.0027,
      "step": 8850
    },
    {
      "epoch": 0.5906666666666667,
      "grad_norm": 1.6151323318481445,
      "learning_rate": 4.630833333333334e-05,
      "loss": 0.0039,
      "step": 8860
    },
    {
      "epoch": 0.5913333333333334,
      "grad_norm": 1.1157060861587524,
      "learning_rate": 4.630416666666667e-05,
      "loss": 0.003,
      "step": 8870
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.9295045137405396,
      "learning_rate": 4.630000000000001e-05,
      "loss": 0.0036,
      "step": 8880
    },
    {
      "epoch": 0.5926666666666667,
      "grad_norm": 0.9371067881584167,
      "learning_rate": 4.629583333333334e-05,
      "loss": 0.0031,
      "step": 8890
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 0.4506186544895172,
      "learning_rate": 4.629166666666667e-05,
      "loss": 0.003,
      "step": 8900
    },
    {
      "epoch": 0.594,
      "grad_norm": 0.746969997882843,
      "learning_rate": 4.62875e-05,
      "loss": 0.0025,
      "step": 8910
    },
    {
      "epoch": 0.5946666666666667,
      "grad_norm": 0.24340027570724487,
      "learning_rate": 4.628333333333333e-05,
      "loss": 0.0023,
      "step": 8920
    },
    {
      "epoch": 0.5953333333333334,
      "grad_norm": 0.4648166000843048,
      "learning_rate": 4.627916666666667e-05,
      "loss": 0.0043,
      "step": 8930
    },
    {
      "epoch": 0.596,
      "grad_norm": 1.2858775854110718,
      "learning_rate": 4.6275e-05,
      "loss": 0.0034,
      "step": 8940
    },
    {
      "epoch": 0.5966666666666667,
      "grad_norm": 0.45109128952026367,
      "learning_rate": 4.627083333333334e-05,
      "loss": 0.0036,
      "step": 8950
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.48080238699913025,
      "learning_rate": 4.626666666666667e-05,
      "loss": 0.0032,
      "step": 8960
    },
    {
      "epoch": 0.598,
      "grad_norm": 0.06861840188503265,
      "learning_rate": 4.6262500000000006e-05,
      "loss": 0.0021,
      "step": 8970
    },
    {
      "epoch": 0.5986666666666667,
      "grad_norm": 0.4707047641277313,
      "learning_rate": 4.625833333333334e-05,
      "loss": 0.0043,
      "step": 8980
    },
    {
      "epoch": 0.5993333333333334,
      "grad_norm": 0.3583952486515045,
      "learning_rate": 4.625416666666667e-05,
      "loss": 0.0029,
      "step": 8990
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.9828819632530212,
      "learning_rate": 4.6250000000000006e-05,
      "loss": 0.0037,
      "step": 9000
    },
    {
      "epoch": 0.6006666666666667,
      "grad_norm": 0.7385402321815491,
      "learning_rate": 4.624583333333334e-05,
      "loss": 0.0036,
      "step": 9010
    },
    {
      "epoch": 0.6013333333333334,
      "grad_norm": 0.8390965461730957,
      "learning_rate": 4.624166666666667e-05,
      "loss": 0.0023,
      "step": 9020
    },
    {
      "epoch": 0.602,
      "grad_norm": 0.88664710521698,
      "learning_rate": 4.62375e-05,
      "loss": 0.0025,
      "step": 9030
    },
    {
      "epoch": 0.6026666666666667,
      "grad_norm": 1.0098686218261719,
      "learning_rate": 4.623333333333334e-05,
      "loss": 0.0029,
      "step": 9040
    },
    {
      "epoch": 0.6033333333333334,
      "grad_norm": 0.5803182125091553,
      "learning_rate": 4.622916666666667e-05,
      "loss": 0.0026,
      "step": 9050
    },
    {
      "epoch": 0.604,
      "grad_norm": 0.3732638955116272,
      "learning_rate": 4.6225e-05,
      "loss": 0.0035,
      "step": 9060
    },
    {
      "epoch": 0.6046666666666667,
      "grad_norm": 0.18793118000030518,
      "learning_rate": 4.6220833333333336e-05,
      "loss": 0.003,
      "step": 9070
    },
    {
      "epoch": 0.6053333333333333,
      "grad_norm": 1.2215304374694824,
      "learning_rate": 4.621666666666667e-05,
      "loss": 0.0025,
      "step": 9080
    },
    {
      "epoch": 0.606,
      "grad_norm": 0.1286872923374176,
      "learning_rate": 4.6212500000000005e-05,
      "loss": 0.004,
      "step": 9090
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 0.6634997129440308,
      "learning_rate": 4.6208333333333336e-05,
      "loss": 0.0028,
      "step": 9100
    },
    {
      "epoch": 0.6073333333333333,
      "grad_norm": 0.7316662669181824,
      "learning_rate": 4.6204166666666674e-05,
      "loss": 0.0033,
      "step": 9110
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.615901529788971,
      "learning_rate": 4.6200000000000005e-05,
      "loss": 0.0035,
      "step": 9120
    },
    {
      "epoch": 0.6086666666666667,
      "grad_norm": 0.09509195387363434,
      "learning_rate": 4.6195833333333336e-05,
      "loss": 0.0024,
      "step": 9130
    },
    {
      "epoch": 0.6093333333333333,
      "grad_norm": 0.8571414947509766,
      "learning_rate": 4.619166666666667e-05,
      "loss": 0.0037,
      "step": 9140
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.2320180982351303,
      "learning_rate": 4.61875e-05,
      "loss": 0.0022,
      "step": 9150
    },
    {
      "epoch": 0.6106666666666667,
      "grad_norm": 0.3978809714317322,
      "learning_rate": 4.6183333333333336e-05,
      "loss": 0.0017,
      "step": 9160
    },
    {
      "epoch": 0.6113333333333333,
      "grad_norm": 1.2998017072677612,
      "learning_rate": 4.6179166666666667e-05,
      "loss": 0.0033,
      "step": 9170
    },
    {
      "epoch": 0.612,
      "grad_norm": 1.0570355653762817,
      "learning_rate": 4.6175000000000004e-05,
      "loss": 0.0039,
      "step": 9180
    },
    {
      "epoch": 0.6126666666666667,
      "grad_norm": 0.8396913409233093,
      "learning_rate": 4.6170833333333335e-05,
      "loss": 0.0023,
      "step": 9190
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.5052846670150757,
      "learning_rate": 4.6166666666666666e-05,
      "loss": 0.0035,
      "step": 9200
    },
    {
      "epoch": 0.614,
      "grad_norm": 0.39822566509246826,
      "learning_rate": 4.6162500000000004e-05,
      "loss": 0.0025,
      "step": 9210
    },
    {
      "epoch": 0.6146666666666667,
      "grad_norm": 0.1951274424791336,
      "learning_rate": 4.6158333333333335e-05,
      "loss": 0.0038,
      "step": 9220
    },
    {
      "epoch": 0.6153333333333333,
      "grad_norm": 0.7806199193000793,
      "learning_rate": 4.615416666666667e-05,
      "loss": 0.0021,
      "step": 9230
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.4183606207370758,
      "learning_rate": 4.6150000000000004e-05,
      "loss": 0.0027,
      "step": 9240
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 0.3736613392829895,
      "learning_rate": 4.614583333333334e-05,
      "loss": 0.0036,
      "step": 9250
    },
    {
      "epoch": 0.6173333333333333,
      "grad_norm": 0.09533596783876419,
      "learning_rate": 4.6141666666666666e-05,
      "loss": 0.0035,
      "step": 9260
    },
    {
      "epoch": 0.618,
      "grad_norm": 1.0984983444213867,
      "learning_rate": 4.61375e-05,
      "loss": 0.002,
      "step": 9270
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 0.96000075340271,
      "learning_rate": 4.6133333333333334e-05,
      "loss": 0.0022,
      "step": 9280
    },
    {
      "epoch": 0.6193333333333333,
      "grad_norm": 1.0264577865600586,
      "learning_rate": 4.6129166666666665e-05,
      "loss": 0.0026,
      "step": 9290
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.9212391376495361,
      "learning_rate": 4.6125e-05,
      "loss": 0.0039,
      "step": 9300
    },
    {
      "epoch": 0.6206666666666667,
      "grad_norm": 0.6148560047149658,
      "learning_rate": 4.6120833333333334e-05,
      "loss": 0.0035,
      "step": 9310
    },
    {
      "epoch": 0.6213333333333333,
      "grad_norm": 0.7822883129119873,
      "learning_rate": 4.611666666666667e-05,
      "loss": 0.004,
      "step": 9320
    },
    {
      "epoch": 0.622,
      "grad_norm": 0.8912437558174133,
      "learning_rate": 4.61125e-05,
      "loss": 0.0037,
      "step": 9330
    },
    {
      "epoch": 0.6226666666666667,
      "grad_norm": 0.08324223756790161,
      "learning_rate": 4.6108333333333334e-05,
      "loss": 0.0048,
      "step": 9340
    },
    {
      "epoch": 0.6233333333333333,
      "grad_norm": 0.272810161113739,
      "learning_rate": 4.610416666666667e-05,
      "loss": 0.0026,
      "step": 9350
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.3325026333332062,
      "learning_rate": 4.61e-05,
      "loss": 0.003,
      "step": 9360
    },
    {
      "epoch": 0.6246666666666667,
      "grad_norm": 0.11440366506576538,
      "learning_rate": 4.609583333333334e-05,
      "loss": 0.0039,
      "step": 9370
    },
    {
      "epoch": 0.6253333333333333,
      "grad_norm": 0.2581756114959717,
      "learning_rate": 4.6091666666666664e-05,
      "loss": 0.0025,
      "step": 9380
    },
    {
      "epoch": 0.626,
      "grad_norm": 0.23460496962070465,
      "learning_rate": 4.60875e-05,
      "loss": 0.0029,
      "step": 9390
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.6241137981414795,
      "learning_rate": 4.608333333333333e-05,
      "loss": 0.0026,
      "step": 9400
    },
    {
      "epoch": 0.6273333333333333,
      "grad_norm": 0.4919661581516266,
      "learning_rate": 4.607916666666667e-05,
      "loss": 0.0023,
      "step": 9410
    },
    {
      "epoch": 0.628,
      "grad_norm": 1.056361198425293,
      "learning_rate": 4.6075e-05,
      "loss": 0.0041,
      "step": 9420
    },
    {
      "epoch": 0.6286666666666667,
      "grad_norm": 0.23763540387153625,
      "learning_rate": 4.607083333333333e-05,
      "loss": 0.0036,
      "step": 9430
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 1.148130178451538,
      "learning_rate": 4.606666666666667e-05,
      "loss": 0.0052,
      "step": 9440
    },
    {
      "epoch": 0.63,
      "grad_norm": 1.3376634120941162,
      "learning_rate": 4.60625e-05,
      "loss": 0.003,
      "step": 9450
    },
    {
      "epoch": 0.6306666666666667,
      "grad_norm": 1.051477313041687,
      "learning_rate": 4.605833333333334e-05,
      "loss": 0.0036,
      "step": 9460
    },
    {
      "epoch": 0.6313333333333333,
      "grad_norm": 0.8741955757141113,
      "learning_rate": 4.605416666666667e-05,
      "loss": 0.0046,
      "step": 9470
    },
    {
      "epoch": 0.632,
      "grad_norm": 1.1838881969451904,
      "learning_rate": 4.605e-05,
      "loss": 0.0033,
      "step": 9480
    },
    {
      "epoch": 0.6326666666666667,
      "grad_norm": 0.9652798175811768,
      "learning_rate": 4.604583333333334e-05,
      "loss": 0.004,
      "step": 9490
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.40856504440307617,
      "learning_rate": 4.604166666666666e-05,
      "loss": 0.0034,
      "step": 9500
    },
    {
      "epoch": 0.634,
      "grad_norm": 0.4731910228729248,
      "learning_rate": 4.60375e-05,
      "loss": 0.0034,
      "step": 9510
    },
    {
      "epoch": 0.6346666666666667,
      "grad_norm": 0.5590527057647705,
      "learning_rate": 4.603333333333333e-05,
      "loss": 0.0042,
      "step": 9520
    },
    {
      "epoch": 0.6353333333333333,
      "grad_norm": 0.7392047047615051,
      "learning_rate": 4.602916666666667e-05,
      "loss": 0.0036,
      "step": 9530
    },
    {
      "epoch": 0.636,
      "grad_norm": 0.06287161260843277,
      "learning_rate": 4.6025e-05,
      "loss": 0.0028,
      "step": 9540
    },
    {
      "epoch": 0.6366666666666667,
      "grad_norm": 0.313904732465744,
      "learning_rate": 4.602083333333334e-05,
      "loss": 0.0031,
      "step": 9550
    },
    {
      "epoch": 0.6373333333333333,
      "grad_norm": 0.23957572877407074,
      "learning_rate": 4.601666666666667e-05,
      "loss": 0.0027,
      "step": 9560
    },
    {
      "epoch": 0.638,
      "grad_norm": 0.23911409080028534,
      "learning_rate": 4.60125e-05,
      "loss": 0.0022,
      "step": 9570
    },
    {
      "epoch": 0.6386666666666667,
      "grad_norm": 1.07850980758667,
      "learning_rate": 4.600833333333334e-05,
      "loss": 0.0045,
      "step": 9580
    },
    {
      "epoch": 0.6393333333333333,
      "grad_norm": 0.702487587928772,
      "learning_rate": 4.600416666666667e-05,
      "loss": 0.0037,
      "step": 9590
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.6862812638282776,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.0042,
      "step": 9600
    },
    {
      "epoch": 0.6406666666666667,
      "grad_norm": 0.08973562717437744,
      "learning_rate": 4.599583333333334e-05,
      "loss": 0.0046,
      "step": 9610
    },
    {
      "epoch": 0.6413333333333333,
      "grad_norm": 0.7605029344558716,
      "learning_rate": 4.599166666666667e-05,
      "loss": 0.003,
      "step": 9620
    },
    {
      "epoch": 0.642,
      "grad_norm": 0.7993279099464417,
      "learning_rate": 4.59875e-05,
      "loss": 0.0027,
      "step": 9630
    },
    {
      "epoch": 0.6426666666666667,
      "grad_norm": 0.19174088537693024,
      "learning_rate": 4.598333333333333e-05,
      "loss": 0.003,
      "step": 9640
    },
    {
      "epoch": 0.6433333333333333,
      "grad_norm": 1.0544168949127197,
      "learning_rate": 4.597916666666667e-05,
      "loss": 0.0032,
      "step": 9650
    },
    {
      "epoch": 0.644,
      "grad_norm": 0.21286815404891968,
      "learning_rate": 4.5975e-05,
      "loss": 0.0039,
      "step": 9660
    },
    {
      "epoch": 0.6446666666666667,
      "grad_norm": 1.17289137840271,
      "learning_rate": 4.597083333333334e-05,
      "loss": 0.0027,
      "step": 9670
    },
    {
      "epoch": 0.6453333333333333,
      "grad_norm": 1.4085222482681274,
      "learning_rate": 4.596666666666667e-05,
      "loss": 0.0039,
      "step": 9680
    },
    {
      "epoch": 0.646,
      "grad_norm": 0.5941648483276367,
      "learning_rate": 4.5962500000000006e-05,
      "loss": 0.0029,
      "step": 9690
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 0.1120481789112091,
      "learning_rate": 4.595833333333334e-05,
      "loss": 0.0043,
      "step": 9700
    },
    {
      "epoch": 0.6473333333333333,
      "grad_norm": 0.7098726630210876,
      "learning_rate": 4.595416666666667e-05,
      "loss": 0.002,
      "step": 9710
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.7927956581115723,
      "learning_rate": 4.5950000000000006e-05,
      "loss": 0.0029,
      "step": 9720
    },
    {
      "epoch": 0.6486666666666666,
      "grad_norm": 0.7039307355880737,
      "learning_rate": 4.5945833333333337e-05,
      "loss": 0.005,
      "step": 9730
    },
    {
      "epoch": 0.6493333333333333,
      "grad_norm": 0.8112756013870239,
      "learning_rate": 4.594166666666667e-05,
      "loss": 0.0042,
      "step": 9740
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.5554519891738892,
      "learning_rate": 4.59375e-05,
      "loss": 0.0032,
      "step": 9750
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 0.31018930673599243,
      "learning_rate": 4.5933333333333336e-05,
      "loss": 0.0036,
      "step": 9760
    },
    {
      "epoch": 0.6513333333333333,
      "grad_norm": 0.44359633326530457,
      "learning_rate": 4.592916666666667e-05,
      "loss": 0.0031,
      "step": 9770
    },
    {
      "epoch": 0.652,
      "grad_norm": 0.783682107925415,
      "learning_rate": 4.5925e-05,
      "loss": 0.0038,
      "step": 9780
    },
    {
      "epoch": 0.6526666666666666,
      "grad_norm": 0.11703767627477646,
      "learning_rate": 4.5920833333333336e-05,
      "loss": 0.0023,
      "step": 9790
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.3127841055393219,
      "learning_rate": 4.591666666666667e-05,
      "loss": 0.0032,
      "step": 9800
    },
    {
      "epoch": 0.654,
      "grad_norm": 1.3408257961273193,
      "learning_rate": 4.5912500000000005e-05,
      "loss": 0.0038,
      "step": 9810
    },
    {
      "epoch": 0.6546666666666666,
      "grad_norm": 1.3224749565124512,
      "learning_rate": 4.5908333333333336e-05,
      "loss": 0.0056,
      "step": 9820
    },
    {
      "epoch": 0.6553333333333333,
      "grad_norm": 1.1501249074935913,
      "learning_rate": 4.5904166666666673e-05,
      "loss": 0.0029,
      "step": 9830
    },
    {
      "epoch": 0.656,
      "grad_norm": 1.0437207221984863,
      "learning_rate": 4.5900000000000004e-05,
      "loss": 0.0028,
      "step": 9840
    },
    {
      "epoch": 0.6566666666666666,
      "grad_norm": 0.3603673279285431,
      "learning_rate": 4.5895833333333335e-05,
      "loss": 0.0034,
      "step": 9850
    },
    {
      "epoch": 0.6573333333333333,
      "grad_norm": 0.8513641953468323,
      "learning_rate": 4.5891666666666666e-05,
      "loss": 0.0039,
      "step": 9860
    },
    {
      "epoch": 0.658,
      "grad_norm": 0.580266535282135,
      "learning_rate": 4.58875e-05,
      "loss": 0.0027,
      "step": 9870
    },
    {
      "epoch": 0.6586666666666666,
      "grad_norm": 0.2292431890964508,
      "learning_rate": 4.5883333333333335e-05,
      "loss": 0.0031,
      "step": 9880
    },
    {
      "epoch": 0.6593333333333333,
      "grad_norm": 0.9454001188278198,
      "learning_rate": 4.5879166666666666e-05,
      "loss": 0.0029,
      "step": 9890
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.3743993043899536,
      "learning_rate": 4.5875000000000004e-05,
      "loss": 0.0049,
      "step": 9900
    },
    {
      "epoch": 0.6606666666666666,
      "grad_norm": 0.2012978494167328,
      "learning_rate": 4.5870833333333335e-05,
      "loss": 0.0033,
      "step": 9910
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 1.266609787940979,
      "learning_rate": 4.5866666666666666e-05,
      "loss": 0.0028,
      "step": 9920
    },
    {
      "epoch": 0.662,
      "grad_norm": 1.2941646575927734,
      "learning_rate": 4.5862500000000003e-05,
      "loss": 0.0023,
      "step": 9930
    },
    {
      "epoch": 0.6626666666666666,
      "grad_norm": 0.722312867641449,
      "learning_rate": 4.5858333333333334e-05,
      "loss": 0.0036,
      "step": 9940
    },
    {
      "epoch": 0.6633333333333333,
      "grad_norm": 1.0222426652908325,
      "learning_rate": 4.585416666666667e-05,
      "loss": 0.0034,
      "step": 9950
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.493381142616272,
      "learning_rate": 4.585e-05,
      "loss": 0.0032,
      "step": 9960
    },
    {
      "epoch": 0.6646666666666666,
      "grad_norm": 0.673309862613678,
      "learning_rate": 4.584583333333334e-05,
      "loss": 0.0035,
      "step": 9970
    },
    {
      "epoch": 0.6653333333333333,
      "grad_norm": 0.4746003746986389,
      "learning_rate": 4.5841666666666665e-05,
      "loss": 0.0033,
      "step": 9980
    },
    {
      "epoch": 0.666,
      "grad_norm": 0.8307003974914551,
      "learning_rate": 4.58375e-05,
      "loss": 0.0051,
      "step": 9990
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.7897984981536865,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 0.0032,
      "step": 10000
    },
    {
      "epoch": 0.6673333333333333,
      "grad_norm": 1.160436987876892,
      "learning_rate": 4.5829166666666665e-05,
      "loss": 0.0047,
      "step": 10010
    },
    {
      "epoch": 0.668,
      "grad_norm": 0.1552751660346985,
      "learning_rate": 4.5825e-05,
      "loss": 0.0032,
      "step": 10020
    },
    {
      "epoch": 0.6686666666666666,
      "grad_norm": 0.1279294341802597,
      "learning_rate": 4.5820833333333334e-05,
      "loss": 0.0021,
      "step": 10030
    },
    {
      "epoch": 0.6693333333333333,
      "grad_norm": 0.4001120626926422,
      "learning_rate": 4.581666666666667e-05,
      "loss": 0.0038,
      "step": 10040
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.686513364315033,
      "learning_rate": 4.58125e-05,
      "loss": 0.0027,
      "step": 10050
    },
    {
      "epoch": 0.6706666666666666,
      "grad_norm": 0.7942013144493103,
      "learning_rate": 4.580833333333333e-05,
      "loss": 0.0029,
      "step": 10060
    },
    {
      "epoch": 0.6713333333333333,
      "grad_norm": 0.4370158612728119,
      "learning_rate": 4.580416666666667e-05,
      "loss": 0.0041,
      "step": 10070
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.781352162361145,
      "learning_rate": 4.58e-05,
      "loss": 0.0022,
      "step": 10080
    },
    {
      "epoch": 0.6726666666666666,
      "grad_norm": 0.586739718914032,
      "learning_rate": 4.579583333333334e-05,
      "loss": 0.0048,
      "step": 10090
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 0.11766049265861511,
      "learning_rate": 4.579166666666667e-05,
      "loss": 0.0024,
      "step": 10100
    },
    {
      "epoch": 0.674,
      "grad_norm": 0.8278858661651611,
      "learning_rate": 4.57875e-05,
      "loss": 0.0033,
      "step": 10110
    },
    {
      "epoch": 0.6746666666666666,
      "grad_norm": 0.48799630999565125,
      "learning_rate": 4.578333333333333e-05,
      "loss": 0.0024,
      "step": 10120
    },
    {
      "epoch": 0.6753333333333333,
      "grad_norm": 0.45968759059906006,
      "learning_rate": 4.577916666666667e-05,
      "loss": 0.0023,
      "step": 10130
    },
    {
      "epoch": 0.676,
      "grad_norm": 1.0115867853164673,
      "learning_rate": 4.5775e-05,
      "loss": 0.0031,
      "step": 10140
    },
    {
      "epoch": 0.6766666666666666,
      "grad_norm": 0.9286671280860901,
      "learning_rate": 4.577083333333333e-05,
      "loss": 0.0038,
      "step": 10150
    },
    {
      "epoch": 0.6773333333333333,
      "grad_norm": 1.1518845558166504,
      "learning_rate": 4.576666666666667e-05,
      "loss": 0.0031,
      "step": 10160
    },
    {
      "epoch": 0.678,
      "grad_norm": 0.3028412163257599,
      "learning_rate": 4.57625e-05,
      "loss": 0.0041,
      "step": 10170
    },
    {
      "epoch": 0.6786666666666666,
      "grad_norm": 0.1783774048089981,
      "learning_rate": 4.575833333333334e-05,
      "loss": 0.0032,
      "step": 10180
    },
    {
      "epoch": 0.6793333333333333,
      "grad_norm": 0.2257266342639923,
      "learning_rate": 4.575416666666667e-05,
      "loss": 0.004,
      "step": 10190
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.2934758961200714,
      "learning_rate": 4.575e-05,
      "loss": 0.0027,
      "step": 10200
    },
    {
      "epoch": 0.6806666666666666,
      "grad_norm": 0.9798299074172974,
      "learning_rate": 4.574583333333334e-05,
      "loss": 0.0022,
      "step": 10210
    },
    {
      "epoch": 0.6813333333333333,
      "grad_norm": 1.1665856838226318,
      "learning_rate": 4.574166666666667e-05,
      "loss": 0.0026,
      "step": 10220
    },
    {
      "epoch": 0.682,
      "grad_norm": 0.25879472494125366,
      "learning_rate": 4.57375e-05,
      "loss": 0.0032,
      "step": 10230
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 0.3933226466178894,
      "learning_rate": 4.573333333333333e-05,
      "loss": 0.0037,
      "step": 10240
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 0.6352521181106567,
      "learning_rate": 4.572916666666667e-05,
      "loss": 0.0027,
      "step": 10250
    },
    {
      "epoch": 0.684,
      "grad_norm": 0.5791943669319153,
      "learning_rate": 4.5725e-05,
      "loss": 0.0024,
      "step": 10260
    },
    {
      "epoch": 0.6846666666666666,
      "grad_norm": 1.0956140756607056,
      "learning_rate": 4.572083333333334e-05,
      "loss": 0.0036,
      "step": 10270
    },
    {
      "epoch": 0.6853333333333333,
      "grad_norm": 0.7145956158638,
      "learning_rate": 4.571666666666667e-05,
      "loss": 0.0032,
      "step": 10280
    },
    {
      "epoch": 0.686,
      "grad_norm": 0.300973117351532,
      "learning_rate": 4.57125e-05,
      "loss": 0.003,
      "step": 10290
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 0.6156970262527466,
      "learning_rate": 4.570833333333334e-05,
      "loss": 0.0022,
      "step": 10300
    },
    {
      "epoch": 0.6873333333333334,
      "grad_norm": 1.2845503091812134,
      "learning_rate": 4.570416666666667e-05,
      "loss": 0.0034,
      "step": 10310
    },
    {
      "epoch": 0.688,
      "grad_norm": 1.1500122547149658,
      "learning_rate": 4.5700000000000006e-05,
      "loss": 0.0033,
      "step": 10320
    },
    {
      "epoch": 0.6886666666666666,
      "grad_norm": 0.4211852252483368,
      "learning_rate": 4.569583333333334e-05,
      "loss": 0.0037,
      "step": 10330
    },
    {
      "epoch": 0.6893333333333334,
      "grad_norm": 0.18112067878246307,
      "learning_rate": 4.569166666666667e-05,
      "loss": 0.0032,
      "step": 10340
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.26825863122940063,
      "learning_rate": 4.56875e-05,
      "loss": 0.0025,
      "step": 10350
    },
    {
      "epoch": 0.6906666666666667,
      "grad_norm": 0.8390005230903625,
      "learning_rate": 4.568333333333333e-05,
      "loss": 0.0037,
      "step": 10360
    },
    {
      "epoch": 0.6913333333333334,
      "grad_norm": 1.1059726476669312,
      "learning_rate": 4.567916666666667e-05,
      "loss": 0.0032,
      "step": 10370
    },
    {
      "epoch": 0.692,
      "grad_norm": 0.1606425791978836,
      "learning_rate": 4.5675e-05,
      "loss": 0.003,
      "step": 10380
    },
    {
      "epoch": 0.6926666666666667,
      "grad_norm": 0.9596524238586426,
      "learning_rate": 4.567083333333334e-05,
      "loss": 0.003,
      "step": 10390
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.8924880623817444,
      "learning_rate": 4.566666666666667e-05,
      "loss": 0.0039,
      "step": 10400
    },
    {
      "epoch": 0.694,
      "grad_norm": 0.3337685763835907,
      "learning_rate": 4.5662500000000005e-05,
      "loss": 0.0022,
      "step": 10410
    },
    {
      "epoch": 0.6946666666666667,
      "grad_norm": 0.18485257029533386,
      "learning_rate": 4.5658333333333336e-05,
      "loss": 0.0019,
      "step": 10420
    },
    {
      "epoch": 0.6953333333333334,
      "grad_norm": 0.10284704715013504,
      "learning_rate": 4.565416666666667e-05,
      "loss": 0.0031,
      "step": 10430
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.7194899916648865,
      "learning_rate": 4.5650000000000005e-05,
      "loss": 0.0025,
      "step": 10440
    },
    {
      "epoch": 0.6966666666666667,
      "grad_norm": 0.12158221751451492,
      "learning_rate": 4.5645833333333336e-05,
      "loss": 0.0037,
      "step": 10450
    },
    {
      "epoch": 0.6973333333333334,
      "grad_norm": 0.581950843334198,
      "learning_rate": 4.5641666666666674e-05,
      "loss": 0.0023,
      "step": 10460
    },
    {
      "epoch": 0.698,
      "grad_norm": 0.8674358129501343,
      "learning_rate": 4.56375e-05,
      "loss": 0.0037,
      "step": 10470
    },
    {
      "epoch": 0.6986666666666667,
      "grad_norm": 0.5727643370628357,
      "learning_rate": 4.5633333333333336e-05,
      "loss": 0.0027,
      "step": 10480
    },
    {
      "epoch": 0.6993333333333334,
      "grad_norm": 0.17506816983222961,
      "learning_rate": 4.562916666666667e-05,
      "loss": 0.0042,
      "step": 10490
    },
    {
      "epoch": 0.7,
      "grad_norm": 1.0412509441375732,
      "learning_rate": 4.5625e-05,
      "loss": 0.0032,
      "step": 10500
    },
    {
      "epoch": 0.7006666666666667,
      "grad_norm": 0.7141419053077698,
      "learning_rate": 4.5620833333333335e-05,
      "loss": 0.0043,
      "step": 10510
    },
    {
      "epoch": 0.7013333333333334,
      "grad_norm": 0.20489001274108887,
      "learning_rate": 4.5616666666666666e-05,
      "loss": 0.0018,
      "step": 10520
    },
    {
      "epoch": 0.702,
      "grad_norm": 0.8247308731079102,
      "learning_rate": 4.5612500000000004e-05,
      "loss": 0.0036,
      "step": 10530
    },
    {
      "epoch": 0.7026666666666667,
      "grad_norm": 0.24989645183086395,
      "learning_rate": 4.5608333333333335e-05,
      "loss": 0.0033,
      "step": 10540
    },
    {
      "epoch": 0.7033333333333334,
      "grad_norm": 1.0198004245758057,
      "learning_rate": 4.560416666666667e-05,
      "loss": 0.0033,
      "step": 10550
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.707115650177002,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 0.0028,
      "step": 10560
    },
    {
      "epoch": 0.7046666666666667,
      "grad_norm": 0.4247843027114868,
      "learning_rate": 4.5595833333333335e-05,
      "loss": 0.004,
      "step": 10570
    },
    {
      "epoch": 0.7053333333333334,
      "grad_norm": 0.14071838557720184,
      "learning_rate": 4.559166666666667e-05,
      "loss": 0.003,
      "step": 10580
    },
    {
      "epoch": 0.706,
      "grad_norm": 0.16296133399009705,
      "learning_rate": 4.55875e-05,
      "loss": 0.0028,
      "step": 10590
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.44576790928840637,
      "learning_rate": 4.5583333333333335e-05,
      "loss": 0.0022,
      "step": 10600
    },
    {
      "epoch": 0.7073333333333334,
      "grad_norm": 0.2131728082895279,
      "learning_rate": 4.5579166666666666e-05,
      "loss": 0.0019,
      "step": 10610
    },
    {
      "epoch": 0.708,
      "grad_norm": 0.3286387622356415,
      "learning_rate": 4.5575e-05,
      "loss": 0.004,
      "step": 10620
    },
    {
      "epoch": 0.7086666666666667,
      "grad_norm": 0.47729650139808655,
      "learning_rate": 4.5570833333333334e-05,
      "loss": 0.0032,
      "step": 10630
    },
    {
      "epoch": 0.7093333333333334,
      "grad_norm": 1.0246660709381104,
      "learning_rate": 4.556666666666667e-05,
      "loss": 0.0033,
      "step": 10640
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.8980647921562195,
      "learning_rate": 4.55625e-05,
      "loss": 0.0031,
      "step": 10650
    },
    {
      "epoch": 0.7106666666666667,
      "grad_norm": 0.9457372426986694,
      "learning_rate": 4.5558333333333334e-05,
      "loss": 0.0033,
      "step": 10660
    },
    {
      "epoch": 0.7113333333333334,
      "grad_norm": 0.10882475972175598,
      "learning_rate": 4.555416666666667e-05,
      "loss": 0.0042,
      "step": 10670
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.088493213057518,
      "learning_rate": 4.555e-05,
      "loss": 0.0036,
      "step": 10680
    },
    {
      "epoch": 0.7126666666666667,
      "grad_norm": 1.0745741128921509,
      "learning_rate": 4.554583333333334e-05,
      "loss": 0.0032,
      "step": 10690
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 0.14251326024532318,
      "learning_rate": 4.554166666666667e-05,
      "loss": 0.0022,
      "step": 10700
    },
    {
      "epoch": 0.714,
      "grad_norm": 0.25451189279556274,
      "learning_rate": 4.55375e-05,
      "loss": 0.002,
      "step": 10710
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 0.7797801494598389,
      "learning_rate": 4.553333333333333e-05,
      "loss": 0.0022,
      "step": 10720
    },
    {
      "epoch": 0.7153333333333334,
      "grad_norm": 1.1726237535476685,
      "learning_rate": 4.5529166666666664e-05,
      "loss": 0.0028,
      "step": 10730
    },
    {
      "epoch": 0.716,
      "grad_norm": 1.0407788753509521,
      "learning_rate": 4.5525e-05,
      "loss": 0.0046,
      "step": 10740
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 1.6648318767547607,
      "learning_rate": 4.552083333333333e-05,
      "loss": 0.0029,
      "step": 10750
    },
    {
      "epoch": 0.7173333333333334,
      "grad_norm": 1.2012168169021606,
      "learning_rate": 4.551666666666667e-05,
      "loss": 0.0033,
      "step": 10760
    },
    {
      "epoch": 0.718,
      "grad_norm": 0.6372371912002563,
      "learning_rate": 4.55125e-05,
      "loss": 0.0029,
      "step": 10770
    },
    {
      "epoch": 0.7186666666666667,
      "grad_norm": 0.22726909816265106,
      "learning_rate": 4.550833333333334e-05,
      "loss": 0.0029,
      "step": 10780
    },
    {
      "epoch": 0.7193333333333334,
      "grad_norm": 0.5419561266899109,
      "learning_rate": 4.550416666666667e-05,
      "loss": 0.003,
      "step": 10790
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.1884751319885254,
      "learning_rate": 4.55e-05,
      "loss": 0.0025,
      "step": 10800
    },
    {
      "epoch": 0.7206666666666667,
      "grad_norm": 0.8440871238708496,
      "learning_rate": 4.549583333333334e-05,
      "loss": 0.0025,
      "step": 10810
    },
    {
      "epoch": 0.7213333333333334,
      "grad_norm": 0.4872582256793976,
      "learning_rate": 4.549166666666667e-05,
      "loss": 0.0038,
      "step": 10820
    },
    {
      "epoch": 0.722,
      "grad_norm": 0.7797776460647583,
      "learning_rate": 4.54875e-05,
      "loss": 0.0047,
      "step": 10830
    },
    {
      "epoch": 0.7226666666666667,
      "grad_norm": 0.3894754946231842,
      "learning_rate": 4.548333333333333e-05,
      "loss": 0.0036,
      "step": 10840
    },
    {
      "epoch": 0.7233333333333334,
      "grad_norm": 0.8452012538909912,
      "learning_rate": 4.547916666666667e-05,
      "loss": 0.0038,
      "step": 10850
    },
    {
      "epoch": 0.724,
      "grad_norm": 0.08613352477550507,
      "learning_rate": 4.5475e-05,
      "loss": 0.0046,
      "step": 10860
    },
    {
      "epoch": 0.7246666666666667,
      "grad_norm": 0.11296644061803818,
      "learning_rate": 4.547083333333333e-05,
      "loss": 0.0038,
      "step": 10870
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 0.11576303094625473,
      "learning_rate": 4.546666666666667e-05,
      "loss": 0.0028,
      "step": 10880
    },
    {
      "epoch": 0.726,
      "grad_norm": 0.21670395135879517,
      "learning_rate": 4.54625e-05,
      "loss": 0.003,
      "step": 10890
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 1.2697731256484985,
      "learning_rate": 4.545833333333334e-05,
      "loss": 0.0035,
      "step": 10900
    },
    {
      "epoch": 0.7273333333333334,
      "grad_norm": 0.8110384941101074,
      "learning_rate": 4.545416666666667e-05,
      "loss": 0.0039,
      "step": 10910
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.39505818486213684,
      "learning_rate": 4.545000000000001e-05,
      "loss": 0.0043,
      "step": 10920
    },
    {
      "epoch": 0.7286666666666667,
      "grad_norm": 0.18462619185447693,
      "learning_rate": 4.544583333333334e-05,
      "loss": 0.0026,
      "step": 10930
    },
    {
      "epoch": 0.7293333333333333,
      "grad_norm": 0.9429273009300232,
      "learning_rate": 4.544166666666667e-05,
      "loss": 0.0031,
      "step": 10940
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.5989305973052979,
      "learning_rate": 4.54375e-05,
      "loss": 0.0034,
      "step": 10950
    },
    {
      "epoch": 0.7306666666666667,
      "grad_norm": 0.6454309225082397,
      "learning_rate": 4.543333333333333e-05,
      "loss": 0.0021,
      "step": 10960
    },
    {
      "epoch": 0.7313333333333333,
      "grad_norm": 0.2091936469078064,
      "learning_rate": 4.542916666666667e-05,
      "loss": 0.0033,
      "step": 10970
    },
    {
      "epoch": 0.732,
      "grad_norm": 0.9190095663070679,
      "learning_rate": 4.5425e-05,
      "loss": 0.0042,
      "step": 10980
    },
    {
      "epoch": 0.7326666666666667,
      "grad_norm": 1.123993158340454,
      "learning_rate": 4.542083333333334e-05,
      "loss": 0.0038,
      "step": 10990
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.41843947768211365,
      "learning_rate": 4.541666666666667e-05,
      "loss": 0.0033,
      "step": 11000
    },
    {
      "epoch": 0.734,
      "grad_norm": 0.5048002600669861,
      "learning_rate": 4.54125e-05,
      "loss": 0.0036,
      "step": 11010
    },
    {
      "epoch": 0.7346666666666667,
      "grad_norm": 0.4917817711830139,
      "learning_rate": 4.540833333333334e-05,
      "loss": 0.0033,
      "step": 11020
    },
    {
      "epoch": 0.7353333333333333,
      "grad_norm": 0.8219631910324097,
      "learning_rate": 4.540416666666667e-05,
      "loss": 0.0029,
      "step": 11030
    },
    {
      "epoch": 0.736,
      "grad_norm": 1.1549521684646606,
      "learning_rate": 4.5400000000000006e-05,
      "loss": 0.0029,
      "step": 11040
    },
    {
      "epoch": 0.7366666666666667,
      "grad_norm": 0.1284063160419464,
      "learning_rate": 4.539583333333334e-05,
      "loss": 0.0027,
      "step": 11050
    },
    {
      "epoch": 0.7373333333333333,
      "grad_norm": 0.21520088613033295,
      "learning_rate": 4.5391666666666675e-05,
      "loss": 0.0023,
      "step": 11060
    },
    {
      "epoch": 0.738,
      "grad_norm": 0.23022113740444183,
      "learning_rate": 4.53875e-05,
      "loss": 0.0039,
      "step": 11070
    },
    {
      "epoch": 0.7386666666666667,
      "grad_norm": 0.4606292247772217,
      "learning_rate": 4.5383333333333336e-05,
      "loss": 0.0033,
      "step": 11080
    },
    {
      "epoch": 0.7393333333333333,
      "grad_norm": 0.36557736992836,
      "learning_rate": 4.537916666666667e-05,
      "loss": 0.0016,
      "step": 11090
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.5872055292129517,
      "learning_rate": 4.5375e-05,
      "loss": 0.0034,
      "step": 11100
    },
    {
      "epoch": 0.7406666666666667,
      "grad_norm": 0.5598261952400208,
      "learning_rate": 4.5370833333333336e-05,
      "loss": 0.0037,
      "step": 11110
    },
    {
      "epoch": 0.7413333333333333,
      "grad_norm": 0.31718870997428894,
      "learning_rate": 4.536666666666667e-05,
      "loss": 0.0028,
      "step": 11120
    },
    {
      "epoch": 0.742,
      "grad_norm": 0.6006004810333252,
      "learning_rate": 4.5362500000000005e-05,
      "loss": 0.0036,
      "step": 11130
    },
    {
      "epoch": 0.7426666666666667,
      "grad_norm": 0.8583232760429382,
      "learning_rate": 4.5358333333333336e-05,
      "loss": 0.0023,
      "step": 11140
    },
    {
      "epoch": 0.7433333333333333,
      "grad_norm": 0.4827457070350647,
      "learning_rate": 4.535416666666667e-05,
      "loss": 0.004,
      "step": 11150
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.9677068591117859,
      "learning_rate": 4.5350000000000005e-05,
      "loss": 0.003,
      "step": 11160
    },
    {
      "epoch": 0.7446666666666667,
      "grad_norm": 0.1857290267944336,
      "learning_rate": 4.5345833333333336e-05,
      "loss": 0.0019,
      "step": 11170
    },
    {
      "epoch": 0.7453333333333333,
      "grad_norm": 0.6744040846824646,
      "learning_rate": 4.534166666666667e-05,
      "loss": 0.0032,
      "step": 11180
    },
    {
      "epoch": 0.746,
      "grad_norm": 0.6050589680671692,
      "learning_rate": 4.53375e-05,
      "loss": 0.0032,
      "step": 11190
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.3010072112083435,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 0.0047,
      "step": 11200
    },
    {
      "epoch": 0.7473333333333333,
      "grad_norm": 0.8466507792472839,
      "learning_rate": 4.5329166666666666e-05,
      "loss": 0.0032,
      "step": 11210
    },
    {
      "epoch": 0.748,
      "grad_norm": 0.8195092678070068,
      "learning_rate": 4.5325000000000004e-05,
      "loss": 0.0027,
      "step": 11220
    },
    {
      "epoch": 0.7486666666666667,
      "grad_norm": 1.4706485271453857,
      "learning_rate": 4.5320833333333335e-05,
      "loss": 0.0029,
      "step": 11230
    },
    {
      "epoch": 0.7493333333333333,
      "grad_norm": 0.6633300185203552,
      "learning_rate": 4.5316666666666666e-05,
      "loss": 0.0055,
      "step": 11240
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.31567221879959106,
      "learning_rate": 4.5312500000000004e-05,
      "loss": 0.0033,
      "step": 11250
    },
    {
      "epoch": 0.7506666666666667,
      "grad_norm": 0.4959214925765991,
      "learning_rate": 4.5308333333333335e-05,
      "loss": 0.0045,
      "step": 11260
    },
    {
      "epoch": 0.7513333333333333,
      "grad_norm": 0.21275098621845245,
      "learning_rate": 4.530416666666667e-05,
      "loss": 0.0041,
      "step": 11270
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.6484524607658386,
      "learning_rate": 4.53e-05,
      "loss": 0.0031,
      "step": 11280
    },
    {
      "epoch": 0.7526666666666667,
      "grad_norm": 0.41409942507743835,
      "learning_rate": 4.5295833333333334e-05,
      "loss": 0.0035,
      "step": 11290
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 0.4681242108345032,
      "learning_rate": 4.529166666666667e-05,
      "loss": 0.0041,
      "step": 11300
    },
    {
      "epoch": 0.754,
      "grad_norm": 0.12490593641996384,
      "learning_rate": 4.52875e-05,
      "loss": 0.0033,
      "step": 11310
    },
    {
      "epoch": 0.7546666666666667,
      "grad_norm": 0.5997568368911743,
      "learning_rate": 4.5283333333333334e-05,
      "loss": 0.0032,
      "step": 11320
    },
    {
      "epoch": 0.7553333333333333,
      "grad_norm": 1.1679630279541016,
      "learning_rate": 4.5279166666666665e-05,
      "loss": 0.0034,
      "step": 11330
    },
    {
      "epoch": 0.756,
      "grad_norm": 0.49292999505996704,
      "learning_rate": 4.5275e-05,
      "loss": 0.0052,
      "step": 11340
    },
    {
      "epoch": 0.7566666666666667,
      "grad_norm": 0.6767358779907227,
      "learning_rate": 4.5270833333333334e-05,
      "loss": 0.0029,
      "step": 11350
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 0.14737917482852936,
      "learning_rate": 4.526666666666667e-05,
      "loss": 0.0022,
      "step": 11360
    },
    {
      "epoch": 0.758,
      "grad_norm": 0.26692327857017517,
      "learning_rate": 4.52625e-05,
      "loss": 0.0023,
      "step": 11370
    },
    {
      "epoch": 0.7586666666666667,
      "grad_norm": 0.5545374751091003,
      "learning_rate": 4.5258333333333333e-05,
      "loss": 0.0024,
      "step": 11380
    },
    {
      "epoch": 0.7593333333333333,
      "grad_norm": 0.14175352454185486,
      "learning_rate": 4.525416666666667e-05,
      "loss": 0.0018,
      "step": 11390
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.9274383187294006,
      "learning_rate": 4.525e-05,
      "loss": 0.0027,
      "step": 11400
    },
    {
      "epoch": 0.7606666666666667,
      "grad_norm": 0.09730137884616852,
      "learning_rate": 4.524583333333334e-05,
      "loss": 0.0031,
      "step": 11410
    },
    {
      "epoch": 0.7613333333333333,
      "grad_norm": 0.4398835599422455,
      "learning_rate": 4.524166666666667e-05,
      "loss": 0.0028,
      "step": 11420
    },
    {
      "epoch": 0.762,
      "grad_norm": 0.3220072090625763,
      "learning_rate": 4.52375e-05,
      "loss": 0.0034,
      "step": 11430
    },
    {
      "epoch": 0.7626666666666667,
      "grad_norm": 0.29002657532691956,
      "learning_rate": 4.523333333333333e-05,
      "loss": 0.0033,
      "step": 11440
    },
    {
      "epoch": 0.7633333333333333,
      "grad_norm": 0.6950721144676208,
      "learning_rate": 4.5229166666666664e-05,
      "loss": 0.0032,
      "step": 11450
    },
    {
      "epoch": 0.764,
      "grad_norm": 0.5193151831626892,
      "learning_rate": 4.5225e-05,
      "loss": 0.0023,
      "step": 11460
    },
    {
      "epoch": 0.7646666666666667,
      "grad_norm": 0.3372756242752075,
      "learning_rate": 4.522083333333333e-05,
      "loss": 0.0042,
      "step": 11470
    },
    {
      "epoch": 0.7653333333333333,
      "grad_norm": 0.7728881239891052,
      "learning_rate": 4.521666666666667e-05,
      "loss": 0.0032,
      "step": 11480
    },
    {
      "epoch": 0.766,
      "grad_norm": 0.709328830242157,
      "learning_rate": 4.52125e-05,
      "loss": 0.0029,
      "step": 11490
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.4800125062465668,
      "learning_rate": 4.520833333333334e-05,
      "loss": 0.0032,
      "step": 11500
    },
    {
      "epoch": 0.7673333333333333,
      "grad_norm": 0.42122623324394226,
      "learning_rate": 4.520416666666667e-05,
      "loss": 0.0028,
      "step": 11510
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.17112590372562408,
      "learning_rate": 4.52e-05,
      "loss": 0.003,
      "step": 11520
    },
    {
      "epoch": 0.7686666666666667,
      "grad_norm": 0.28188735246658325,
      "learning_rate": 4.519583333333334e-05,
      "loss": 0.0023,
      "step": 11530
    },
    {
      "epoch": 0.7693333333333333,
      "grad_norm": 0.09826356172561646,
      "learning_rate": 4.519166666666667e-05,
      "loss": 0.0033,
      "step": 11540
    },
    {
      "epoch": 0.77,
      "grad_norm": 1.0396307706832886,
      "learning_rate": 4.518750000000001e-05,
      "loss": 0.0043,
      "step": 11550
    },
    {
      "epoch": 0.7706666666666667,
      "grad_norm": 0.14232678711414337,
      "learning_rate": 4.518333333333333e-05,
      "loss": 0.0034,
      "step": 11560
    },
    {
      "epoch": 0.7713333333333333,
      "grad_norm": 0.061556581407785416,
      "learning_rate": 4.517916666666667e-05,
      "loss": 0.0031,
      "step": 11570
    },
    {
      "epoch": 0.772,
      "grad_norm": 0.25229889154434204,
      "learning_rate": 4.5175e-05,
      "loss": 0.0028,
      "step": 11580
    },
    {
      "epoch": 0.7726666666666666,
      "grad_norm": 1.0059847831726074,
      "learning_rate": 4.517083333333333e-05,
      "loss": 0.0046,
      "step": 11590
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.3334507942199707,
      "learning_rate": 4.516666666666667e-05,
      "loss": 0.003,
      "step": 11600
    },
    {
      "epoch": 0.774,
      "grad_norm": 0.7942342758178711,
      "learning_rate": 4.51625e-05,
      "loss": 0.0029,
      "step": 11610
    },
    {
      "epoch": 0.7746666666666666,
      "grad_norm": 0.33865657448768616,
      "learning_rate": 4.515833333333334e-05,
      "loss": 0.0019,
      "step": 11620
    },
    {
      "epoch": 0.7753333333333333,
      "grad_norm": 0.32865265011787415,
      "learning_rate": 4.515416666666667e-05,
      "loss": 0.0023,
      "step": 11630
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.3665650486946106,
      "learning_rate": 4.5150000000000006e-05,
      "loss": 0.0039,
      "step": 11640
    },
    {
      "epoch": 0.7766666666666666,
      "grad_norm": 0.6353673934936523,
      "learning_rate": 4.514583333333334e-05,
      "loss": 0.0033,
      "step": 11650
    },
    {
      "epoch": 0.7773333333333333,
      "grad_norm": 0.30479830503463745,
      "learning_rate": 4.514166666666667e-05,
      "loss": 0.0025,
      "step": 11660
    },
    {
      "epoch": 0.778,
      "grad_norm": 0.3909425735473633,
      "learning_rate": 4.5137500000000006e-05,
      "loss": 0.0028,
      "step": 11670
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 1.1012159585952759,
      "learning_rate": 4.513333333333333e-05,
      "loss": 0.0024,
      "step": 11680
    },
    {
      "epoch": 0.7793333333333333,
      "grad_norm": 0.14579246938228607,
      "learning_rate": 4.512916666666667e-05,
      "loss": 0.0037,
      "step": 11690
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.6720571517944336,
      "learning_rate": 4.5125e-05,
      "loss": 0.0047,
      "step": 11700
    },
    {
      "epoch": 0.7806666666666666,
      "grad_norm": 0.8117564916610718,
      "learning_rate": 4.512083333333334e-05,
      "loss": 0.0031,
      "step": 11710
    },
    {
      "epoch": 0.7813333333333333,
      "grad_norm": 0.2771739363670349,
      "learning_rate": 4.511666666666667e-05,
      "loss": 0.0026,
      "step": 11720
    },
    {
      "epoch": 0.782,
      "grad_norm": 0.2761460542678833,
      "learning_rate": 4.51125e-05,
      "loss": 0.0029,
      "step": 11730
    },
    {
      "epoch": 0.7826666666666666,
      "grad_norm": 0.0898132175207138,
      "learning_rate": 4.5108333333333337e-05,
      "loss": 0.0024,
      "step": 11740
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 0.030185798183083534,
      "learning_rate": 4.510416666666667e-05,
      "loss": 0.0032,
      "step": 11750
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.3878960907459259,
      "learning_rate": 4.5100000000000005e-05,
      "loss": 0.0021,
      "step": 11760
    },
    {
      "epoch": 0.7846666666666666,
      "grad_norm": 0.424974650144577,
      "learning_rate": 4.5095833333333336e-05,
      "loss": 0.0033,
      "step": 11770
    },
    {
      "epoch": 0.7853333333333333,
      "grad_norm": 0.29139378666877747,
      "learning_rate": 4.5091666666666674e-05,
      "loss": 0.003,
      "step": 11780
    },
    {
      "epoch": 0.786,
      "grad_norm": 0.3313540518283844,
      "learning_rate": 4.5087500000000005e-05,
      "loss": 0.002,
      "step": 11790
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.6885078549385071,
      "learning_rate": 4.5083333333333336e-05,
      "loss": 0.0033,
      "step": 11800
    },
    {
      "epoch": 0.7873333333333333,
      "grad_norm": 0.6741114258766174,
      "learning_rate": 4.507916666666667e-05,
      "loss": 0.0034,
      "step": 11810
    },
    {
      "epoch": 0.788,
      "grad_norm": 0.8380358815193176,
      "learning_rate": 4.5075e-05,
      "loss": 0.0028,
      "step": 11820
    },
    {
      "epoch": 0.7886666666666666,
      "grad_norm": 0.5647667050361633,
      "learning_rate": 4.5070833333333336e-05,
      "loss": 0.003,
      "step": 11830
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 0.05261912941932678,
      "learning_rate": 4.5066666666666667e-05,
      "loss": 0.0023,
      "step": 11840
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.7480394840240479,
      "learning_rate": 4.5062500000000004e-05,
      "loss": 0.0019,
      "step": 11850
    },
    {
      "epoch": 0.7906666666666666,
      "grad_norm": 1.0476523637771606,
      "learning_rate": 4.5058333333333335e-05,
      "loss": 0.0026,
      "step": 11860
    },
    {
      "epoch": 0.7913333333333333,
      "grad_norm": 0.8394904732704163,
      "learning_rate": 4.5054166666666666e-05,
      "loss": 0.0027,
      "step": 11870
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.24753746390342712,
      "learning_rate": 4.5050000000000004e-05,
      "loss": 0.003,
      "step": 11880
    },
    {
      "epoch": 0.7926666666666666,
      "grad_norm": 0.1321079581975937,
      "learning_rate": 4.5045833333333335e-05,
      "loss": 0.0028,
      "step": 11890
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 0.5135877132415771,
      "learning_rate": 4.504166666666667e-05,
      "loss": 0.0023,
      "step": 11900
    },
    {
      "epoch": 0.794,
      "grad_norm": 0.21700866520404816,
      "learning_rate": 4.5037500000000004e-05,
      "loss": 0.0025,
      "step": 11910
    },
    {
      "epoch": 0.7946666666666666,
      "grad_norm": 0.9723278284072876,
      "learning_rate": 4.5033333333333335e-05,
      "loss": 0.0034,
      "step": 11920
    },
    {
      "epoch": 0.7953333333333333,
      "grad_norm": 1.0074517726898193,
      "learning_rate": 4.5029166666666666e-05,
      "loss": 0.0036,
      "step": 11930
    },
    {
      "epoch": 0.796,
      "grad_norm": 0.42347756028175354,
      "learning_rate": 4.5025000000000003e-05,
      "loss": 0.0036,
      "step": 11940
    },
    {
      "epoch": 0.7966666666666666,
      "grad_norm": 1.1943180561065674,
      "learning_rate": 4.5020833333333334e-05,
      "loss": 0.002,
      "step": 11950
    },
    {
      "epoch": 0.7973333333333333,
      "grad_norm": 0.1064043790102005,
      "learning_rate": 4.5016666666666665e-05,
      "loss": 0.0028,
      "step": 11960
    },
    {
      "epoch": 0.798,
      "grad_norm": 0.3705398738384247,
      "learning_rate": 4.50125e-05,
      "loss": 0.0026,
      "step": 11970
    },
    {
      "epoch": 0.7986666666666666,
      "grad_norm": 0.2817496359348297,
      "learning_rate": 4.5008333333333334e-05,
      "loss": 0.003,
      "step": 11980
    },
    {
      "epoch": 0.7993333333333333,
      "grad_norm": 0.10197600722312927,
      "learning_rate": 4.500416666666667e-05,
      "loss": 0.0033,
      "step": 11990
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.15601368248462677,
      "learning_rate": 4.5e-05,
      "loss": 0.0035,
      "step": 12000
    },
    {
      "epoch": 0.8006666666666666,
      "grad_norm": 1.1378324031829834,
      "learning_rate": 4.4995833333333334e-05,
      "loss": 0.0027,
      "step": 12010
    },
    {
      "epoch": 0.8013333333333333,
      "grad_norm": 0.7395380139350891,
      "learning_rate": 4.499166666666667e-05,
      "loss": 0.0046,
      "step": 12020
    },
    {
      "epoch": 0.802,
      "grad_norm": 1.1542295217514038,
      "learning_rate": 4.49875e-05,
      "loss": 0.0032,
      "step": 12030
    },
    {
      "epoch": 0.8026666666666666,
      "grad_norm": 0.6135126352310181,
      "learning_rate": 4.4983333333333334e-05,
      "loss": 0.003,
      "step": 12040
    },
    {
      "epoch": 0.8033333333333333,
      "grad_norm": 1.1830508708953857,
      "learning_rate": 4.4979166666666664e-05,
      "loss": 0.0034,
      "step": 12050
    },
    {
      "epoch": 0.804,
      "grad_norm": 0.8529956340789795,
      "learning_rate": 4.4975e-05,
      "loss": 0.0042,
      "step": 12060
    },
    {
      "epoch": 0.8046666666666666,
      "grad_norm": 0.2978699207305908,
      "learning_rate": 4.497083333333333e-05,
      "loss": 0.0031,
      "step": 12070
    },
    {
      "epoch": 0.8053333333333333,
      "grad_norm": 0.15780775249004364,
      "learning_rate": 4.496666666666667e-05,
      "loss": 0.0029,
      "step": 12080
    },
    {
      "epoch": 0.806,
      "grad_norm": 0.3685638904571533,
      "learning_rate": 4.49625e-05,
      "loss": 0.0035,
      "step": 12090
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 0.11933346837759018,
      "learning_rate": 4.495833333333333e-05,
      "loss": 0.0039,
      "step": 12100
    },
    {
      "epoch": 0.8073333333333333,
      "grad_norm": 0.5013530850410461,
      "learning_rate": 4.495416666666667e-05,
      "loss": 0.004,
      "step": 12110
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.425663560628891,
      "learning_rate": 4.495e-05,
      "loss": 0.0032,
      "step": 12120
    },
    {
      "epoch": 0.8086666666666666,
      "grad_norm": 1.416636347770691,
      "learning_rate": 4.494583333333334e-05,
      "loss": 0.0034,
      "step": 12130
    },
    {
      "epoch": 0.8093333333333333,
      "grad_norm": 1.2612769603729248,
      "learning_rate": 4.494166666666667e-05,
      "loss": 0.0037,
      "step": 12140
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.20066505670547485,
      "learning_rate": 4.49375e-05,
      "loss": 0.0049,
      "step": 12150
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 0.8245548009872437,
      "learning_rate": 4.493333333333333e-05,
      "loss": 0.0025,
      "step": 12160
    },
    {
      "epoch": 0.8113333333333334,
      "grad_norm": 0.043942373245954514,
      "learning_rate": 4.492916666666666e-05,
      "loss": 0.0038,
      "step": 12170
    },
    {
      "epoch": 0.812,
      "grad_norm": 0.5324233770370483,
      "learning_rate": 4.4925e-05,
      "loss": 0.0028,
      "step": 12180
    },
    {
      "epoch": 0.8126666666666666,
      "grad_norm": 0.1914980113506317,
      "learning_rate": 4.492083333333333e-05,
      "loss": 0.002,
      "step": 12190
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.6252433061599731,
      "learning_rate": 4.491666666666667e-05,
      "loss": 0.0026,
      "step": 12200
    },
    {
      "epoch": 0.814,
      "grad_norm": 0.8017792105674744,
      "learning_rate": 4.49125e-05,
      "loss": 0.0023,
      "step": 12210
    },
    {
      "epoch": 0.8146666666666667,
      "grad_norm": 0.9907324314117432,
      "learning_rate": 4.490833333333334e-05,
      "loss": 0.0021,
      "step": 12220
    },
    {
      "epoch": 0.8153333333333334,
      "grad_norm": 0.3095623850822449,
      "learning_rate": 4.490416666666667e-05,
      "loss": 0.0043,
      "step": 12230
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.271290123462677,
      "learning_rate": 4.49e-05,
      "loss": 0.0037,
      "step": 12240
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 0.42180001735687256,
      "learning_rate": 4.489583333333334e-05,
      "loss": 0.0028,
      "step": 12250
    },
    {
      "epoch": 0.8173333333333334,
      "grad_norm": 1.0318479537963867,
      "learning_rate": 4.489166666666667e-05,
      "loss": 0.0041,
      "step": 12260
    },
    {
      "epoch": 0.818,
      "grad_norm": 0.21538928151130676,
      "learning_rate": 4.488750000000001e-05,
      "loss": 0.0028,
      "step": 12270
    },
    {
      "epoch": 0.8186666666666667,
      "grad_norm": 0.33314990997314453,
      "learning_rate": 4.488333333333333e-05,
      "loss": 0.0028,
      "step": 12280
    },
    {
      "epoch": 0.8193333333333334,
      "grad_norm": 0.6759951710700989,
      "learning_rate": 4.487916666666667e-05,
      "loss": 0.0032,
      "step": 12290
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.6357985138893127,
      "learning_rate": 4.4875e-05,
      "loss": 0.0023,
      "step": 12300
    },
    {
      "epoch": 0.8206666666666667,
      "grad_norm": 0.1100924089550972,
      "learning_rate": 4.487083333333334e-05,
      "loss": 0.0028,
      "step": 12310
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 0.5348169207572937,
      "learning_rate": 4.486666666666667e-05,
      "loss": 0.0025,
      "step": 12320
    },
    {
      "epoch": 0.822,
      "grad_norm": 0.9176608324050903,
      "learning_rate": 4.48625e-05,
      "loss": 0.0025,
      "step": 12330
    },
    {
      "epoch": 0.8226666666666667,
      "grad_norm": 0.1803511679172516,
      "learning_rate": 4.485833333333334e-05,
      "loss": 0.0033,
      "step": 12340
    },
    {
      "epoch": 0.8233333333333334,
      "grad_norm": 0.3778398334980011,
      "learning_rate": 4.485416666666667e-05,
      "loss": 0.004,
      "step": 12350
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.44850024580955505,
      "learning_rate": 4.4850000000000006e-05,
      "loss": 0.0038,
      "step": 12360
    },
    {
      "epoch": 0.8246666666666667,
      "grad_norm": 0.633565366268158,
      "learning_rate": 4.484583333333334e-05,
      "loss": 0.0027,
      "step": 12370
    },
    {
      "epoch": 0.8253333333333334,
      "grad_norm": 0.40872350335121155,
      "learning_rate": 4.484166666666667e-05,
      "loss": 0.004,
      "step": 12380
    },
    {
      "epoch": 0.826,
      "grad_norm": 0.41781893372535706,
      "learning_rate": 4.4837500000000006e-05,
      "loss": 0.0038,
      "step": 12390
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.7713727355003357,
      "learning_rate": 4.483333333333333e-05,
      "loss": 0.0039,
      "step": 12400
    },
    {
      "epoch": 0.8273333333333334,
      "grad_norm": 0.2339436262845993,
      "learning_rate": 4.482916666666667e-05,
      "loss": 0.0036,
      "step": 12410
    },
    {
      "epoch": 0.828,
      "grad_norm": 0.6110034584999084,
      "learning_rate": 4.4825e-05,
      "loss": 0.0031,
      "step": 12420
    },
    {
      "epoch": 0.8286666666666667,
      "grad_norm": 0.3677428662776947,
      "learning_rate": 4.4820833333333336e-05,
      "loss": 0.0043,
      "step": 12430
    },
    {
      "epoch": 0.8293333333333334,
      "grad_norm": 0.5017076730728149,
      "learning_rate": 4.481666666666667e-05,
      "loss": 0.0037,
      "step": 12440
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.3876483738422394,
      "learning_rate": 4.4812500000000005e-05,
      "loss": 0.0043,
      "step": 12450
    },
    {
      "epoch": 0.8306666666666667,
      "grad_norm": 0.33270466327667236,
      "learning_rate": 4.4808333333333336e-05,
      "loss": 0.0031,
      "step": 12460
    },
    {
      "epoch": 0.8313333333333334,
      "grad_norm": 0.41740596294403076,
      "learning_rate": 4.480416666666667e-05,
      "loss": 0.0037,
      "step": 12470
    },
    {
      "epoch": 0.832,
      "grad_norm": 1.1583224534988403,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 0.0024,
      "step": 12480
    },
    {
      "epoch": 0.8326666666666667,
      "grad_norm": 0.49345487356185913,
      "learning_rate": 4.4795833333333336e-05,
      "loss": 0.003,
      "step": 12490
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.3070317804813385,
      "learning_rate": 4.4791666666666673e-05,
      "loss": 0.0019,
      "step": 12500
    },
    {
      "epoch": 0.834,
      "grad_norm": 0.2033541053533554,
      "learning_rate": 4.4787500000000004e-05,
      "loss": 0.003,
      "step": 12510
    },
    {
      "epoch": 0.8346666666666667,
      "grad_norm": 0.547938883304596,
      "learning_rate": 4.4783333333333335e-05,
      "loss": 0.0029,
      "step": 12520
    },
    {
      "epoch": 0.8353333333333334,
      "grad_norm": 0.6176834106445312,
      "learning_rate": 4.4779166666666666e-05,
      "loss": 0.0033,
      "step": 12530
    },
    {
      "epoch": 0.836,
      "grad_norm": 0.6150005459785461,
      "learning_rate": 4.4775e-05,
      "loss": 0.003,
      "step": 12540
    },
    {
      "epoch": 0.8366666666666667,
      "grad_norm": 0.597814679145813,
      "learning_rate": 4.4770833333333335e-05,
      "loss": 0.0024,
      "step": 12550
    },
    {
      "epoch": 0.8373333333333334,
      "grad_norm": 0.16864538192749023,
      "learning_rate": 4.4766666666666666e-05,
      "loss": 0.0028,
      "step": 12560
    },
    {
      "epoch": 0.838,
      "grad_norm": 0.7119025588035583,
      "learning_rate": 4.4762500000000004e-05,
      "loss": 0.0027,
      "step": 12570
    },
    {
      "epoch": 0.8386666666666667,
      "grad_norm": 0.6000737547874451,
      "learning_rate": 4.4758333333333335e-05,
      "loss": 0.0032,
      "step": 12580
    },
    {
      "epoch": 0.8393333333333334,
      "grad_norm": 0.15685871243476868,
      "learning_rate": 4.475416666666667e-05,
      "loss": 0.0042,
      "step": 12590
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.14687541127204895,
      "learning_rate": 4.4750000000000004e-05,
      "loss": 0.0023,
      "step": 12600
    },
    {
      "epoch": 0.8406666666666667,
      "grad_norm": 0.6199765801429749,
      "learning_rate": 4.4745833333333335e-05,
      "loss": 0.0024,
      "step": 12610
    },
    {
      "epoch": 0.8413333333333334,
      "grad_norm": 0.28593626618385315,
      "learning_rate": 4.474166666666667e-05,
      "loss": 0.0045,
      "step": 12620
    },
    {
      "epoch": 0.842,
      "grad_norm": 0.25212129950523376,
      "learning_rate": 4.47375e-05,
      "loss": 0.0035,
      "step": 12630
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 0.569578230381012,
      "learning_rate": 4.473333333333334e-05,
      "loss": 0.0028,
      "step": 12640
    },
    {
      "epoch": 0.8433333333333334,
      "grad_norm": 0.4211925268173218,
      "learning_rate": 4.4729166666666665e-05,
      "loss": 0.0031,
      "step": 12650
    },
    {
      "epoch": 0.844,
      "grad_norm": 0.4197618067264557,
      "learning_rate": 4.4725e-05,
      "loss": 0.0029,
      "step": 12660
    },
    {
      "epoch": 0.8446666666666667,
      "grad_norm": 0.5447720885276794,
      "learning_rate": 4.4720833333333334e-05,
      "loss": 0.0027,
      "step": 12670
    },
    {
      "epoch": 0.8453333333333334,
      "grad_norm": 0.1839693784713745,
      "learning_rate": 4.4716666666666665e-05,
      "loss": 0.0027,
      "step": 12680
    },
    {
      "epoch": 0.846,
      "grad_norm": 0.5458518862724304,
      "learning_rate": 4.47125e-05,
      "loss": 0.0025,
      "step": 12690
    },
    {
      "epoch": 0.8466666666666667,
      "grad_norm": 0.32736819982528687,
      "learning_rate": 4.4708333333333334e-05,
      "loss": 0.0024,
      "step": 12700
    },
    {
      "epoch": 0.8473333333333334,
      "grad_norm": 0.1939363032579422,
      "learning_rate": 4.470416666666667e-05,
      "loss": 0.0031,
      "step": 12710
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.5534693598747253,
      "learning_rate": 4.47e-05,
      "loss": 0.0023,
      "step": 12720
    },
    {
      "epoch": 0.8486666666666667,
      "grad_norm": 0.5717672109603882,
      "learning_rate": 4.469583333333334e-05,
      "loss": 0.0038,
      "step": 12730
    },
    {
      "epoch": 0.8493333333333334,
      "grad_norm": 0.13535718619823456,
      "learning_rate": 4.469166666666667e-05,
      "loss": 0.0044,
      "step": 12740
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.4287067651748657,
      "learning_rate": 4.46875e-05,
      "loss": 0.0037,
      "step": 12750
    },
    {
      "epoch": 0.8506666666666667,
      "grad_norm": 1.0321316719055176,
      "learning_rate": 4.468333333333334e-05,
      "loss": 0.0027,
      "step": 12760
    },
    {
      "epoch": 0.8513333333333334,
      "grad_norm": 0.21332238614559174,
      "learning_rate": 4.4679166666666664e-05,
      "loss": 0.004,
      "step": 12770
    },
    {
      "epoch": 0.852,
      "grad_norm": 0.40421488881111145,
      "learning_rate": 4.4675e-05,
      "loss": 0.0032,
      "step": 12780
    },
    {
      "epoch": 0.8526666666666667,
      "grad_norm": 0.2068783938884735,
      "learning_rate": 4.467083333333333e-05,
      "loss": 0.0029,
      "step": 12790
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.8558272123336792,
      "learning_rate": 4.466666666666667e-05,
      "loss": 0.0034,
      "step": 12800
    },
    {
      "epoch": 0.854,
      "grad_norm": 0.8127270936965942,
      "learning_rate": 4.46625e-05,
      "loss": 0.0031,
      "step": 12810
    },
    {
      "epoch": 0.8546666666666667,
      "grad_norm": 0.6071747541427612,
      "learning_rate": 4.465833333333333e-05,
      "loss": 0.0031,
      "step": 12820
    },
    {
      "epoch": 0.8553333333333333,
      "grad_norm": 0.8912677764892578,
      "learning_rate": 4.465416666666667e-05,
      "loss": 0.0027,
      "step": 12830
    },
    {
      "epoch": 0.856,
      "grad_norm": 1.4151532649993896,
      "learning_rate": 4.465e-05,
      "loss": 0.0035,
      "step": 12840
    },
    {
      "epoch": 0.8566666666666667,
      "grad_norm": 1.222810983657837,
      "learning_rate": 4.464583333333334e-05,
      "loss": 0.0032,
      "step": 12850
    },
    {
      "epoch": 0.8573333333333333,
      "grad_norm": 1.206697702407837,
      "learning_rate": 4.464166666666667e-05,
      "loss": 0.0026,
      "step": 12860
    },
    {
      "epoch": 0.858,
      "grad_norm": 0.5348423719406128,
      "learning_rate": 4.463750000000001e-05,
      "loss": 0.0038,
      "step": 12870
    },
    {
      "epoch": 0.8586666666666667,
      "grad_norm": 0.6115939617156982,
      "learning_rate": 4.463333333333334e-05,
      "loss": 0.0035,
      "step": 12880
    },
    {
      "epoch": 0.8593333333333333,
      "grad_norm": 0.8924676775932312,
      "learning_rate": 4.462916666666667e-05,
      "loss": 0.0037,
      "step": 12890
    },
    {
      "epoch": 0.86,
      "grad_norm": 1.2757421731948853,
      "learning_rate": 4.4625e-05,
      "loss": 0.0019,
      "step": 12900
    },
    {
      "epoch": 0.8606666666666667,
      "grad_norm": 0.29000771045684814,
      "learning_rate": 4.462083333333333e-05,
      "loss": 0.0023,
      "step": 12910
    },
    {
      "epoch": 0.8613333333333333,
      "grad_norm": 0.5045168995857239,
      "learning_rate": 4.461666666666667e-05,
      "loss": 0.0048,
      "step": 12920
    },
    {
      "epoch": 0.862,
      "grad_norm": 0.9427114725112915,
      "learning_rate": 4.46125e-05,
      "loss": 0.0029,
      "step": 12930
    },
    {
      "epoch": 0.8626666666666667,
      "grad_norm": 0.500393271446228,
      "learning_rate": 4.460833333333334e-05,
      "loss": 0.0026,
      "step": 12940
    },
    {
      "epoch": 0.8633333333333333,
      "grad_norm": 0.510538637638092,
      "learning_rate": 4.460416666666667e-05,
      "loss": 0.0059,
      "step": 12950
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.6222529411315918,
      "learning_rate": 4.46e-05,
      "loss": 0.0031,
      "step": 12960
    },
    {
      "epoch": 0.8646666666666667,
      "grad_norm": 0.7429229021072388,
      "learning_rate": 4.459583333333334e-05,
      "loss": 0.0034,
      "step": 12970
    },
    {
      "epoch": 0.8653333333333333,
      "grad_norm": 1.121616244316101,
      "learning_rate": 4.459166666666667e-05,
      "loss": 0.0032,
      "step": 12980
    },
    {
      "epoch": 0.866,
      "grad_norm": 0.46082621812820435,
      "learning_rate": 4.4587500000000006e-05,
      "loss": 0.0025,
      "step": 12990
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.8850328326225281,
      "learning_rate": 4.458333333333334e-05,
      "loss": 0.004,
      "step": 13000
    },
    {
      "epoch": 0.8673333333333333,
      "grad_norm": 0.8958327770233154,
      "learning_rate": 4.457916666666667e-05,
      "loss": 0.0025,
      "step": 13010
    },
    {
      "epoch": 0.868,
      "grad_norm": 0.44689804315567017,
      "learning_rate": 4.4575e-05,
      "loss": 0.0021,
      "step": 13020
    },
    {
      "epoch": 0.8686666666666667,
      "grad_norm": 0.5381309986114502,
      "learning_rate": 4.457083333333334e-05,
      "loss": 0.0039,
      "step": 13030
    },
    {
      "epoch": 0.8693333333333333,
      "grad_norm": 0.30697008967399597,
      "learning_rate": 4.456666666666667e-05,
      "loss": 0.0031,
      "step": 13040
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.16462615132331848,
      "learning_rate": 4.45625e-05,
      "loss": 0.0037,
      "step": 13050
    },
    {
      "epoch": 0.8706666666666667,
      "grad_norm": 0.6886553168296814,
      "learning_rate": 4.455833333333334e-05,
      "loss": 0.0037,
      "step": 13060
    },
    {
      "epoch": 0.8713333333333333,
      "grad_norm": 0.7300302982330322,
      "learning_rate": 4.455416666666667e-05,
      "loss": 0.0032,
      "step": 13070
    },
    {
      "epoch": 0.872,
      "grad_norm": 1.1294234991073608,
      "learning_rate": 4.4550000000000005e-05,
      "loss": 0.0039,
      "step": 13080
    },
    {
      "epoch": 0.8726666666666667,
      "grad_norm": 1.097439169883728,
      "learning_rate": 4.4545833333333336e-05,
      "loss": 0.0041,
      "step": 13090
    },
    {
      "epoch": 0.8733333333333333,
      "grad_norm": 1.0874309539794922,
      "learning_rate": 4.454166666666667e-05,
      "loss": 0.0039,
      "step": 13100
    },
    {
      "epoch": 0.874,
      "grad_norm": 0.7977234125137329,
      "learning_rate": 4.4537500000000005e-05,
      "loss": 0.0044,
      "step": 13110
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 0.8143104910850525,
      "learning_rate": 4.4533333333333336e-05,
      "loss": 0.0033,
      "step": 13120
    },
    {
      "epoch": 0.8753333333333333,
      "grad_norm": 0.3197081983089447,
      "learning_rate": 4.452916666666667e-05,
      "loss": 0.0044,
      "step": 13130
    },
    {
      "epoch": 0.876,
      "grad_norm": 0.09217169135808945,
      "learning_rate": 4.4525e-05,
      "loss": 0.0024,
      "step": 13140
    },
    {
      "epoch": 0.8766666666666667,
      "grad_norm": 0.6577485203742981,
      "learning_rate": 4.4520833333333336e-05,
      "loss": 0.0022,
      "step": 13150
    },
    {
      "epoch": 0.8773333333333333,
      "grad_norm": 0.392840713262558,
      "learning_rate": 4.451666666666667e-05,
      "loss": 0.0053,
      "step": 13160
    },
    {
      "epoch": 0.878,
      "grad_norm": 0.7582976222038269,
      "learning_rate": 4.4512500000000005e-05,
      "loss": 0.0037,
      "step": 13170
    },
    {
      "epoch": 0.8786666666666667,
      "grad_norm": 1.052558422088623,
      "learning_rate": 4.4508333333333336e-05,
      "loss": 0.0028,
      "step": 13180
    },
    {
      "epoch": 0.8793333333333333,
      "grad_norm": 1.3642386198043823,
      "learning_rate": 4.4504166666666666e-05,
      "loss": 0.0027,
      "step": 13190
    },
    {
      "epoch": 0.88,
      "grad_norm": 1.0663237571716309,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 0.003,
      "step": 13200
    },
    {
      "epoch": 0.8806666666666667,
      "grad_norm": 0.4441225230693817,
      "learning_rate": 4.4495833333333335e-05,
      "loss": 0.0036,
      "step": 13210
    },
    {
      "epoch": 0.8813333333333333,
      "grad_norm": 0.18965338170528412,
      "learning_rate": 4.449166666666667e-05,
      "loss": 0.0027,
      "step": 13220
    },
    {
      "epoch": 0.882,
      "grad_norm": 0.35034143924713135,
      "learning_rate": 4.4487500000000004e-05,
      "loss": 0.0034,
      "step": 13230
    },
    {
      "epoch": 0.8826666666666667,
      "grad_norm": 0.73508620262146,
      "learning_rate": 4.4483333333333335e-05,
      "loss": 0.0026,
      "step": 13240
    },
    {
      "epoch": 0.8833333333333333,
      "grad_norm": 0.5407818555831909,
      "learning_rate": 4.4479166666666666e-05,
      "loss": 0.0026,
      "step": 13250
    },
    {
      "epoch": 0.884,
      "grad_norm": 0.5074117183685303,
      "learning_rate": 4.4475e-05,
      "loss": 0.0023,
      "step": 13260
    },
    {
      "epoch": 0.8846666666666667,
      "grad_norm": 0.2861482799053192,
      "learning_rate": 4.4470833333333335e-05,
      "loss": 0.0023,
      "step": 13270
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 0.37365397810935974,
      "learning_rate": 4.4466666666666666e-05,
      "loss": 0.0031,
      "step": 13280
    },
    {
      "epoch": 0.886,
      "grad_norm": 0.1768886297941208,
      "learning_rate": 4.44625e-05,
      "loss": 0.0044,
      "step": 13290
    },
    {
      "epoch": 0.8866666666666667,
      "grad_norm": 0.4531075358390808,
      "learning_rate": 4.4458333333333334e-05,
      "loss": 0.0015,
      "step": 13300
    },
    {
      "epoch": 0.8873333333333333,
      "grad_norm": 0.9478448033332825,
      "learning_rate": 4.445416666666667e-05,
      "loss": 0.0037,
      "step": 13310
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.4938031733036041,
      "learning_rate": 4.445e-05,
      "loss": 0.0024,
      "step": 13320
    },
    {
      "epoch": 0.8886666666666667,
      "grad_norm": 0.5494163036346436,
      "learning_rate": 4.4445833333333334e-05,
      "loss": 0.0029,
      "step": 13330
    },
    {
      "epoch": 0.8893333333333333,
      "grad_norm": 0.885095477104187,
      "learning_rate": 4.444166666666667e-05,
      "loss": 0.0025,
      "step": 13340
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.8526294827461243,
      "learning_rate": 4.44375e-05,
      "loss": 0.003,
      "step": 13350
    },
    {
      "epoch": 0.8906666666666667,
      "grad_norm": 0.3224727213382721,
      "learning_rate": 4.443333333333334e-05,
      "loss": 0.0024,
      "step": 13360
    },
    {
      "epoch": 0.8913333333333333,
      "grad_norm": 0.38193610310554504,
      "learning_rate": 4.4429166666666665e-05,
      "loss": 0.0022,
      "step": 13370
    },
    {
      "epoch": 0.892,
      "grad_norm": 0.5347357392311096,
      "learning_rate": 4.4425e-05,
      "loss": 0.0032,
      "step": 13380
    },
    {
      "epoch": 0.8926666666666667,
      "grad_norm": 0.2947915494441986,
      "learning_rate": 4.442083333333333e-05,
      "loss": 0.0032,
      "step": 13390
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.7208769917488098,
      "learning_rate": 4.4416666666666664e-05,
      "loss": 0.002,
      "step": 13400
    },
    {
      "epoch": 0.894,
      "grad_norm": 0.22857627272605896,
      "learning_rate": 4.44125e-05,
      "loss": 0.0018,
      "step": 13410
    },
    {
      "epoch": 0.8946666666666667,
      "grad_norm": 1.064958095550537,
      "learning_rate": 4.440833333333333e-05,
      "loss": 0.0027,
      "step": 13420
    },
    {
      "epoch": 0.8953333333333333,
      "grad_norm": 0.14004550874233246,
      "learning_rate": 4.440416666666667e-05,
      "loss": 0.0032,
      "step": 13430
    },
    {
      "epoch": 0.896,
      "grad_norm": 1.0592644214630127,
      "learning_rate": 4.44e-05,
      "loss": 0.0027,
      "step": 13440
    },
    {
      "epoch": 0.8966666666666666,
      "grad_norm": 0.5641069412231445,
      "learning_rate": 4.439583333333334e-05,
      "loss": 0.0028,
      "step": 13450
    },
    {
      "epoch": 0.8973333333333333,
      "grad_norm": 0.3862629234790802,
      "learning_rate": 4.439166666666667e-05,
      "loss": 0.0026,
      "step": 13460
    },
    {
      "epoch": 0.898,
      "grad_norm": 0.7759215235710144,
      "learning_rate": 4.43875e-05,
      "loss": 0.003,
      "step": 13470
    },
    {
      "epoch": 0.8986666666666666,
      "grad_norm": 0.5228913426399231,
      "learning_rate": 4.438333333333334e-05,
      "loss": 0.0041,
      "step": 13480
    },
    {
      "epoch": 0.8993333333333333,
      "grad_norm": 0.18067482113838196,
      "learning_rate": 4.4379166666666663e-05,
      "loss": 0.003,
      "step": 13490
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.18700090050697327,
      "learning_rate": 4.4375e-05,
      "loss": 0.0024,
      "step": 13500
    },
    {
      "epoch": 0.9006666666666666,
      "grad_norm": 0.6871339678764343,
      "learning_rate": 4.437083333333333e-05,
      "loss": 0.0041,
      "step": 13510
    },
    {
      "epoch": 0.9013333333333333,
      "grad_norm": 0.17113208770751953,
      "learning_rate": 4.436666666666667e-05,
      "loss": 0.0028,
      "step": 13520
    },
    {
      "epoch": 0.902,
      "grad_norm": 0.6654245853424072,
      "learning_rate": 4.43625e-05,
      "loss": 0.0041,
      "step": 13530
    },
    {
      "epoch": 0.9026666666666666,
      "grad_norm": 0.8511804342269897,
      "learning_rate": 4.435833333333333e-05,
      "loss": 0.0035,
      "step": 13540
    },
    {
      "epoch": 0.9033333333333333,
      "grad_norm": 0.3048671782016754,
      "learning_rate": 4.435416666666667e-05,
      "loss": 0.0021,
      "step": 13550
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.6493951082229614,
      "learning_rate": 4.435e-05,
      "loss": 0.0028,
      "step": 13560
    },
    {
      "epoch": 0.9046666666666666,
      "grad_norm": 1.057681918144226,
      "learning_rate": 4.434583333333334e-05,
      "loss": 0.0033,
      "step": 13570
    },
    {
      "epoch": 0.9053333333333333,
      "grad_norm": 0.37901821732521057,
      "learning_rate": 4.434166666666667e-05,
      "loss": 0.0026,
      "step": 13580
    },
    {
      "epoch": 0.906,
      "grad_norm": 0.25044116377830505,
      "learning_rate": 4.433750000000001e-05,
      "loss": 0.005,
      "step": 13590
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.5347663164138794,
      "learning_rate": 4.433333333333334e-05,
      "loss": 0.0028,
      "step": 13600
    },
    {
      "epoch": 0.9073333333333333,
      "grad_norm": 0.788580596446991,
      "learning_rate": 4.432916666666667e-05,
      "loss": 0.0052,
      "step": 13610
    },
    {
      "epoch": 0.908,
      "grad_norm": 0.9449475407600403,
      "learning_rate": 4.4325e-05,
      "loss": 0.0033,
      "step": 13620
    },
    {
      "epoch": 0.9086666666666666,
      "grad_norm": 0.10577093809843063,
      "learning_rate": 4.432083333333333e-05,
      "loss": 0.0037,
      "step": 13630
    },
    {
      "epoch": 0.9093333333333333,
      "grad_norm": 0.6174083352088928,
      "learning_rate": 4.431666666666667e-05,
      "loss": 0.0036,
      "step": 13640
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.3103034794330597,
      "learning_rate": 4.43125e-05,
      "loss": 0.0033,
      "step": 13650
    },
    {
      "epoch": 0.9106666666666666,
      "grad_norm": 0.16507774591445923,
      "learning_rate": 4.430833333333334e-05,
      "loss": 0.002,
      "step": 13660
    },
    {
      "epoch": 0.9113333333333333,
      "grad_norm": 0.6442999243736267,
      "learning_rate": 4.430416666666667e-05,
      "loss": 0.0029,
      "step": 13670
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.8017317652702332,
      "learning_rate": 4.43e-05,
      "loss": 0.0031,
      "step": 13680
    },
    {
      "epoch": 0.9126666666666666,
      "grad_norm": 0.577670156955719,
      "learning_rate": 4.429583333333334e-05,
      "loss": 0.0019,
      "step": 13690
    },
    {
      "epoch": 0.9133333333333333,
      "grad_norm": 0.3461441397666931,
      "learning_rate": 4.429166666666667e-05,
      "loss": 0.0027,
      "step": 13700
    },
    {
      "epoch": 0.914,
      "grad_norm": 0.9867244362831116,
      "learning_rate": 4.4287500000000006e-05,
      "loss": 0.0045,
      "step": 13710
    },
    {
      "epoch": 0.9146666666666666,
      "grad_norm": 0.20225241780281067,
      "learning_rate": 4.428333333333334e-05,
      "loss": 0.0017,
      "step": 13720
    },
    {
      "epoch": 0.9153333333333333,
      "grad_norm": 0.8887898921966553,
      "learning_rate": 4.427916666666667e-05,
      "loss": 0.0028,
      "step": 13730
    },
    {
      "epoch": 0.916,
      "grad_norm": 0.6184044480323792,
      "learning_rate": 4.4275e-05,
      "loss": 0.0025,
      "step": 13740
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 0.6196648478507996,
      "learning_rate": 4.4270833333333337e-05,
      "loss": 0.0028,
      "step": 13750
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 0.943156898021698,
      "learning_rate": 4.426666666666667e-05,
      "loss": 0.0034,
      "step": 13760
    },
    {
      "epoch": 0.918,
      "grad_norm": 0.5863364338874817,
      "learning_rate": 4.42625e-05,
      "loss": 0.0032,
      "step": 13770
    },
    {
      "epoch": 0.9186666666666666,
      "grad_norm": 0.19547727704048157,
      "learning_rate": 4.4258333333333336e-05,
      "loss": 0.0036,
      "step": 13780
    },
    {
      "epoch": 0.9193333333333333,
      "grad_norm": 0.3386250138282776,
      "learning_rate": 4.425416666666667e-05,
      "loss": 0.0023,
      "step": 13790
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.6164978742599487,
      "learning_rate": 4.4250000000000005e-05,
      "loss": 0.0024,
      "step": 13800
    },
    {
      "epoch": 0.9206666666666666,
      "grad_norm": 0.25068503618240356,
      "learning_rate": 4.4245833333333336e-05,
      "loss": 0.0031,
      "step": 13810
    },
    {
      "epoch": 0.9213333333333333,
      "grad_norm": 0.5296918749809265,
      "learning_rate": 4.424166666666667e-05,
      "loss": 0.0038,
      "step": 13820
    },
    {
      "epoch": 0.922,
      "grad_norm": 1.426409363746643,
      "learning_rate": 4.4237500000000005e-05,
      "loss": 0.0038,
      "step": 13830
    },
    {
      "epoch": 0.9226666666666666,
      "grad_norm": 0.3649054169654846,
      "learning_rate": 4.4233333333333336e-05,
      "loss": 0.0026,
      "step": 13840
    },
    {
      "epoch": 0.9233333333333333,
      "grad_norm": 1.186626672744751,
      "learning_rate": 4.422916666666667e-05,
      "loss": 0.0029,
      "step": 13850
    },
    {
      "epoch": 0.924,
      "grad_norm": 1.1508736610412598,
      "learning_rate": 4.4225e-05,
      "loss": 0.003,
      "step": 13860
    },
    {
      "epoch": 0.9246666666666666,
      "grad_norm": 0.11094779521226883,
      "learning_rate": 4.4220833333333335e-05,
      "loss": 0.0028,
      "step": 13870
    },
    {
      "epoch": 0.9253333333333333,
      "grad_norm": 0.6185861229896545,
      "learning_rate": 4.4216666666666666e-05,
      "loss": 0.0028,
      "step": 13880
    },
    {
      "epoch": 0.926,
      "grad_norm": 0.16489268839359283,
      "learning_rate": 4.4212500000000004e-05,
      "loss": 0.0023,
      "step": 13890
    },
    {
      "epoch": 0.9266666666666666,
      "grad_norm": 0.5694699883460999,
      "learning_rate": 4.4208333333333335e-05,
      "loss": 0.0025,
      "step": 13900
    },
    {
      "epoch": 0.9273333333333333,
      "grad_norm": 0.18217246234416962,
      "learning_rate": 4.4204166666666666e-05,
      "loss": 0.0031,
      "step": 13910
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.2126873880624771,
      "learning_rate": 4.4200000000000004e-05,
      "loss": 0.0039,
      "step": 13920
    },
    {
      "epoch": 0.9286666666666666,
      "grad_norm": 0.5108299851417542,
      "learning_rate": 4.4195833333333335e-05,
      "loss": 0.0034,
      "step": 13930
    },
    {
      "epoch": 0.9293333333333333,
      "grad_norm": 0.12363399565219879,
      "learning_rate": 4.419166666666667e-05,
      "loss": 0.0028,
      "step": 13940
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.3614823818206787,
      "learning_rate": 4.4187500000000003e-05,
      "loss": 0.0032,
      "step": 13950
    },
    {
      "epoch": 0.9306666666666666,
      "grad_norm": 0.23815493285655975,
      "learning_rate": 4.4183333333333334e-05,
      "loss": 0.0026,
      "step": 13960
    },
    {
      "epoch": 0.9313333333333333,
      "grad_norm": 0.21635980904102325,
      "learning_rate": 4.417916666666667e-05,
      "loss": 0.0042,
      "step": 13970
    },
    {
      "epoch": 0.932,
      "grad_norm": 0.7780157923698425,
      "learning_rate": 4.4174999999999996e-05,
      "loss": 0.003,
      "step": 13980
    },
    {
      "epoch": 0.9326666666666666,
      "grad_norm": 0.694342315196991,
      "learning_rate": 4.4170833333333334e-05,
      "loss": 0.0035,
      "step": 13990
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.7133123278617859,
      "learning_rate": 4.4166666666666665e-05,
      "loss": 0.0048,
      "step": 14000
    },
    {
      "epoch": 0.934,
      "grad_norm": 0.18327116966247559,
      "learning_rate": 4.41625e-05,
      "loss": 0.0027,
      "step": 14010
    },
    {
      "epoch": 0.9346666666666666,
      "grad_norm": 1.090716004371643,
      "learning_rate": 4.4158333333333334e-05,
      "loss": 0.0019,
      "step": 14020
    },
    {
      "epoch": 0.9353333333333333,
      "grad_norm": 0.5816233158111572,
      "learning_rate": 4.415416666666667e-05,
      "loss": 0.0023,
      "step": 14030
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.5924586653709412,
      "learning_rate": 4.415e-05,
      "loss": 0.0029,
      "step": 14040
    },
    {
      "epoch": 0.9366666666666666,
      "grad_norm": 0.6636008024215698,
      "learning_rate": 4.4145833333333333e-05,
      "loss": 0.0039,
      "step": 14050
    },
    {
      "epoch": 0.9373333333333334,
      "grad_norm": 0.7355638742446899,
      "learning_rate": 4.414166666666667e-05,
      "loss": 0.0024,
      "step": 14060
    },
    {
      "epoch": 0.938,
      "grad_norm": 0.3868750333786011,
      "learning_rate": 4.41375e-05,
      "loss": 0.0034,
      "step": 14070
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 1.2196568250656128,
      "learning_rate": 4.413333333333334e-05,
      "loss": 0.0034,
      "step": 14080
    },
    {
      "epoch": 0.9393333333333334,
      "grad_norm": 0.3742434084415436,
      "learning_rate": 4.412916666666667e-05,
      "loss": 0.002,
      "step": 14090
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.6017754673957825,
      "learning_rate": 4.4125e-05,
      "loss": 0.0036,
      "step": 14100
    },
    {
      "epoch": 0.9406666666666667,
      "grad_norm": 1.0215835571289062,
      "learning_rate": 4.412083333333333e-05,
      "loss": 0.0029,
      "step": 14110
    },
    {
      "epoch": 0.9413333333333334,
      "grad_norm": 0.8706844449043274,
      "learning_rate": 4.411666666666667e-05,
      "loss": 0.0042,
      "step": 14120
    },
    {
      "epoch": 0.942,
      "grad_norm": 0.29984599351882935,
      "learning_rate": 4.41125e-05,
      "loss": 0.0037,
      "step": 14130
    },
    {
      "epoch": 0.9426666666666667,
      "grad_norm": 0.9059067964553833,
      "learning_rate": 4.410833333333333e-05,
      "loss": 0.0037,
      "step": 14140
    },
    {
      "epoch": 0.9433333333333334,
      "grad_norm": 0.6153349280357361,
      "learning_rate": 4.410416666666667e-05,
      "loss": 0.0021,
      "step": 14150
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.5736953020095825,
      "learning_rate": 4.41e-05,
      "loss": 0.0033,
      "step": 14160
    },
    {
      "epoch": 0.9446666666666667,
      "grad_norm": 1.3336840867996216,
      "learning_rate": 4.409583333333334e-05,
      "loss": 0.0039,
      "step": 14170
    },
    {
      "epoch": 0.9453333333333334,
      "grad_norm": 0.20187035202980042,
      "learning_rate": 4.409166666666667e-05,
      "loss": 0.0038,
      "step": 14180
    },
    {
      "epoch": 0.946,
      "grad_norm": 0.2625346779823303,
      "learning_rate": 4.40875e-05,
      "loss": 0.0027,
      "step": 14190
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.6711329817771912,
      "learning_rate": 4.408333333333334e-05,
      "loss": 0.0024,
      "step": 14200
    },
    {
      "epoch": 0.9473333333333334,
      "grad_norm": 0.6580400466918945,
      "learning_rate": 4.407916666666667e-05,
      "loss": 0.0038,
      "step": 14210
    },
    {
      "epoch": 0.948,
      "grad_norm": 0.33964136242866516,
      "learning_rate": 4.4075e-05,
      "loss": 0.003,
      "step": 14220
    },
    {
      "epoch": 0.9486666666666667,
      "grad_norm": 0.4823460876941681,
      "learning_rate": 4.407083333333333e-05,
      "loss": 0.0033,
      "step": 14230
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 0.3969941735267639,
      "learning_rate": 4.406666666666667e-05,
      "loss": 0.0021,
      "step": 14240
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.7342988848686218,
      "learning_rate": 4.40625e-05,
      "loss": 0.0022,
      "step": 14250
    },
    {
      "epoch": 0.9506666666666667,
      "grad_norm": 0.6180643439292908,
      "learning_rate": 4.405833333333334e-05,
      "loss": 0.0016,
      "step": 14260
    },
    {
      "epoch": 0.9513333333333334,
      "grad_norm": 0.09944672137498856,
      "learning_rate": 4.405416666666667e-05,
      "loss": 0.0031,
      "step": 14270
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.8674693703651428,
      "learning_rate": 4.405e-05,
      "loss": 0.0027,
      "step": 14280
    },
    {
      "epoch": 0.9526666666666667,
      "grad_norm": 0.8658291101455688,
      "learning_rate": 4.404583333333334e-05,
      "loss": 0.0027,
      "step": 14290
    },
    {
      "epoch": 0.9533333333333334,
      "grad_norm": 1.0384314060211182,
      "learning_rate": 4.404166666666667e-05,
      "loss": 0.0029,
      "step": 14300
    },
    {
      "epoch": 0.954,
      "grad_norm": 0.8977053165435791,
      "learning_rate": 4.4037500000000007e-05,
      "loss": 0.0039,
      "step": 14310
    },
    {
      "epoch": 0.9546666666666667,
      "grad_norm": 0.9640757441520691,
      "learning_rate": 4.403333333333334e-05,
      "loss": 0.003,
      "step": 14320
    },
    {
      "epoch": 0.9553333333333334,
      "grad_norm": 0.9236149191856384,
      "learning_rate": 4.402916666666667e-05,
      "loss": 0.0034,
      "step": 14330
    },
    {
      "epoch": 0.956,
      "grad_norm": 0.9699156880378723,
      "learning_rate": 4.4025e-05,
      "loss": 0.0039,
      "step": 14340
    },
    {
      "epoch": 0.9566666666666667,
      "grad_norm": 0.5525704622268677,
      "learning_rate": 4.402083333333333e-05,
      "loss": 0.0037,
      "step": 14350
    },
    {
      "epoch": 0.9573333333333334,
      "grad_norm": 0.6476650238037109,
      "learning_rate": 4.401666666666667e-05,
      "loss": 0.0037,
      "step": 14360
    },
    {
      "epoch": 0.958,
      "grad_norm": 0.19083403050899506,
      "learning_rate": 4.40125e-05,
      "loss": 0.0023,
      "step": 14370
    },
    {
      "epoch": 0.9586666666666667,
      "grad_norm": 0.41724082827568054,
      "learning_rate": 4.400833333333334e-05,
      "loss": 0.0024,
      "step": 14380
    },
    {
      "epoch": 0.9593333333333334,
      "grad_norm": 0.41164249181747437,
      "learning_rate": 4.400416666666667e-05,
      "loss": 0.0025,
      "step": 14390
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.7541658878326416,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.0028,
      "step": 14400
    },
    {
      "epoch": 0.9606666666666667,
      "grad_norm": 0.39931797981262207,
      "learning_rate": 4.3995833333333337e-05,
      "loss": 0.0041,
      "step": 14410
    },
    {
      "epoch": 0.9613333333333334,
      "grad_norm": 0.24391460418701172,
      "learning_rate": 4.399166666666667e-05,
      "loss": 0.0026,
      "step": 14420
    },
    {
      "epoch": 0.962,
      "grad_norm": 1.3802276849746704,
      "learning_rate": 4.3987500000000005e-05,
      "loss": 0.0038,
      "step": 14430
    },
    {
      "epoch": 0.9626666666666667,
      "grad_norm": 1.2811144590377808,
      "learning_rate": 4.3983333333333336e-05,
      "loss": 0.0046,
      "step": 14440
    },
    {
      "epoch": 0.9633333333333334,
      "grad_norm": 1.2488057613372803,
      "learning_rate": 4.3979166666666674e-05,
      "loss": 0.0036,
      "step": 14450
    },
    {
      "epoch": 0.964,
      "grad_norm": 1.3897088766098022,
      "learning_rate": 4.3975e-05,
      "loss": 0.0039,
      "step": 14460
    },
    {
      "epoch": 0.9646666666666667,
      "grad_norm": 0.19494271278381348,
      "learning_rate": 4.3970833333333336e-05,
      "loss": 0.0045,
      "step": 14470
    },
    {
      "epoch": 0.9653333333333334,
      "grad_norm": 0.43184635043144226,
      "learning_rate": 4.396666666666667e-05,
      "loss": 0.0035,
      "step": 14480
    },
    {
      "epoch": 0.966,
      "grad_norm": 1.101858377456665,
      "learning_rate": 4.39625e-05,
      "loss": 0.0039,
      "step": 14490
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 0.3560122847557068,
      "learning_rate": 4.3958333333333336e-05,
      "loss": 0.0043,
      "step": 14500
    },
    {
      "epoch": 0.9673333333333334,
      "grad_norm": 0.7279670238494873,
      "learning_rate": 4.395416666666667e-05,
      "loss": 0.0022,
      "step": 14510
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.1627611219882965,
      "learning_rate": 4.3950000000000004e-05,
      "loss": 0.0029,
      "step": 14520
    },
    {
      "epoch": 0.9686666666666667,
      "grad_norm": 0.33345726132392883,
      "learning_rate": 4.3945833333333335e-05,
      "loss": 0.0038,
      "step": 14530
    },
    {
      "epoch": 0.9693333333333334,
      "grad_norm": 0.8973556160926819,
      "learning_rate": 4.394166666666667e-05,
      "loss": 0.0032,
      "step": 14540
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.6952127814292908,
      "learning_rate": 4.3937500000000004e-05,
      "loss": 0.003,
      "step": 14550
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 0.07205013185739517,
      "learning_rate": 4.3933333333333335e-05,
      "loss": 0.0031,
      "step": 14560
    },
    {
      "epoch": 0.9713333333333334,
      "grad_norm": 0.29193004965782166,
      "learning_rate": 4.392916666666667e-05,
      "loss": 0.0042,
      "step": 14570
    },
    {
      "epoch": 0.972,
      "grad_norm": 0.44450414180755615,
      "learning_rate": 4.3925e-05,
      "loss": 0.0041,
      "step": 14580
    },
    {
      "epoch": 0.9726666666666667,
      "grad_norm": 0.4924672842025757,
      "learning_rate": 4.3920833333333335e-05,
      "loss": 0.0024,
      "step": 14590
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.5448008179664612,
      "learning_rate": 4.3916666666666666e-05,
      "loss": 0.0027,
      "step": 14600
    },
    {
      "epoch": 0.974,
      "grad_norm": 0.6112403869628906,
      "learning_rate": 4.3912500000000004e-05,
      "loss": 0.0042,
      "step": 14610
    },
    {
      "epoch": 0.9746666666666667,
      "grad_norm": 0.6666039228439331,
      "learning_rate": 4.3908333333333334e-05,
      "loss": 0.0028,
      "step": 14620
    },
    {
      "epoch": 0.9753333333333334,
      "grad_norm": 0.785449743270874,
      "learning_rate": 4.3904166666666665e-05,
      "loss": 0.0039,
      "step": 14630
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.7823684215545654,
      "learning_rate": 4.39e-05,
      "loss": 0.0044,
      "step": 14640
    },
    {
      "epoch": 0.9766666666666667,
      "grad_norm": 0.19646432995796204,
      "learning_rate": 4.3895833333333334e-05,
      "loss": 0.0021,
      "step": 14650
    },
    {
      "epoch": 0.9773333333333334,
      "grad_norm": 0.3543362021446228,
      "learning_rate": 4.389166666666667e-05,
      "loss": 0.0028,
      "step": 14660
    },
    {
      "epoch": 0.978,
      "grad_norm": 0.5841280817985535,
      "learning_rate": 4.38875e-05,
      "loss": 0.0027,
      "step": 14670
    },
    {
      "epoch": 0.9786666666666667,
      "grad_norm": 0.09792373329401016,
      "learning_rate": 4.388333333333334e-05,
      "loss": 0.0034,
      "step": 14680
    },
    {
      "epoch": 0.9793333333333333,
      "grad_norm": 0.8436099886894226,
      "learning_rate": 4.387916666666667e-05,
      "loss": 0.0031,
      "step": 14690
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.3551631569862366,
      "learning_rate": 4.3875e-05,
      "loss": 0.0039,
      "step": 14700
    },
    {
      "epoch": 0.9806666666666667,
      "grad_norm": 0.2181927114725113,
      "learning_rate": 4.3870833333333334e-05,
      "loss": 0.0024,
      "step": 14710
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 0.21677838265895844,
      "learning_rate": 4.3866666666666665e-05,
      "loss": 0.0033,
      "step": 14720
    },
    {
      "epoch": 0.982,
      "grad_norm": 0.6093011498451233,
      "learning_rate": 4.38625e-05,
      "loss": 0.0043,
      "step": 14730
    },
    {
      "epoch": 0.9826666666666667,
      "grad_norm": 0.1683003008365631,
      "learning_rate": 4.385833333333333e-05,
      "loss": 0.0022,
      "step": 14740
    },
    {
      "epoch": 0.9833333333333333,
      "grad_norm": 1.2578442096710205,
      "learning_rate": 4.385416666666667e-05,
      "loss": 0.0036,
      "step": 14750
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.27903932332992554,
      "learning_rate": 4.385e-05,
      "loss": 0.003,
      "step": 14760
    },
    {
      "epoch": 0.9846666666666667,
      "grad_norm": 0.8943186402320862,
      "learning_rate": 4.384583333333333e-05,
      "loss": 0.003,
      "step": 14770
    },
    {
      "epoch": 0.9853333333333333,
      "grad_norm": 0.3123840391635895,
      "learning_rate": 4.384166666666667e-05,
      "loss": 0.003,
      "step": 14780
    },
    {
      "epoch": 0.986,
      "grad_norm": 0.42779967188835144,
      "learning_rate": 4.38375e-05,
      "loss": 0.0026,
      "step": 14790
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.6735125184059143,
      "learning_rate": 4.383333333333334e-05,
      "loss": 0.003,
      "step": 14800
    },
    {
      "epoch": 0.9873333333333333,
      "grad_norm": 0.49851059913635254,
      "learning_rate": 4.382916666666667e-05,
      "loss": 0.0038,
      "step": 14810
    },
    {
      "epoch": 0.988,
      "grad_norm": 0.19038806855678558,
      "learning_rate": 4.3825e-05,
      "loss": 0.0015,
      "step": 14820
    },
    {
      "epoch": 0.9886666666666667,
      "grad_norm": 1.140405297279358,
      "learning_rate": 4.382083333333333e-05,
      "loss": 0.0023,
      "step": 14830
    },
    {
      "epoch": 0.9893333333333333,
      "grad_norm": 1.1212797164916992,
      "learning_rate": 4.381666666666667e-05,
      "loss": 0.0033,
      "step": 14840
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.6010251045227051,
      "learning_rate": 4.38125e-05,
      "loss": 0.0023,
      "step": 14850
    },
    {
      "epoch": 0.9906666666666667,
      "grad_norm": 0.3865770101547241,
      "learning_rate": 4.380833333333333e-05,
      "loss": 0.0027,
      "step": 14860
    },
    {
      "epoch": 0.9913333333333333,
      "grad_norm": 1.1019055843353271,
      "learning_rate": 4.380416666666667e-05,
      "loss": 0.0026,
      "step": 14870
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.5443168878555298,
      "learning_rate": 4.38e-05,
      "loss": 0.0036,
      "step": 14880
    },
    {
      "epoch": 0.9926666666666667,
      "grad_norm": 0.18858429789543152,
      "learning_rate": 4.379583333333334e-05,
      "loss": 0.0042,
      "step": 14890
    },
    {
      "epoch": 0.9933333333333333,
      "grad_norm": 0.9782700538635254,
      "learning_rate": 4.379166666666667e-05,
      "loss": 0.0036,
      "step": 14900
    },
    {
      "epoch": 0.994,
      "grad_norm": 0.2344132512807846,
      "learning_rate": 4.37875e-05,
      "loss": 0.0033,
      "step": 14910
    },
    {
      "epoch": 0.9946666666666667,
      "grad_norm": 1.2408190965652466,
      "learning_rate": 4.378333333333334e-05,
      "loss": 0.0041,
      "step": 14920
    },
    {
      "epoch": 0.9953333333333333,
      "grad_norm": 0.7862167954444885,
      "learning_rate": 4.377916666666667e-05,
      "loss": 0.0038,
      "step": 14930
    },
    {
      "epoch": 0.996,
      "grad_norm": 0.27766838669776917,
      "learning_rate": 4.3775e-05,
      "loss": 0.0035,
      "step": 14940
    },
    {
      "epoch": 0.9966666666666667,
      "grad_norm": 0.7775807976722717,
      "learning_rate": 4.377083333333333e-05,
      "loss": 0.0019,
      "step": 14950
    },
    {
      "epoch": 0.9973333333333333,
      "grad_norm": 0.8621087670326233,
      "learning_rate": 4.376666666666667e-05,
      "loss": 0.0034,
      "step": 14960
    },
    {
      "epoch": 0.998,
      "grad_norm": 0.6853444576263428,
      "learning_rate": 4.37625e-05,
      "loss": 0.0025,
      "step": 14970
    },
    {
      "epoch": 0.9986666666666667,
      "grad_norm": 0.28422173857688904,
      "learning_rate": 4.375833333333334e-05,
      "loss": 0.0026,
      "step": 14980
    },
    {
      "epoch": 0.9993333333333333,
      "grad_norm": 0.1070985198020935,
      "learning_rate": 4.375416666666667e-05,
      "loss": 0.0029,
      "step": 14990
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.13487491011619568,
      "learning_rate": 4.375e-05,
      "loss": 0.0024,
      "step": 15000
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.003109253942966461,
      "eval_runtime": 153.4018,
      "eval_samples_per_second": 1303.765,
      "eval_steps_per_second": 32.594,
      "step": 15000
    },
    {
      "epoch": 1.0006666666666666,
      "grad_norm": 0.697627604007721,
      "learning_rate": 4.374583333333334e-05,
      "loss": 0.0035,
      "step": 15010
    },
    {
      "epoch": 1.0013333333333334,
      "grad_norm": 0.6428651213645935,
      "learning_rate": 4.374166666666667e-05,
      "loss": 0.0045,
      "step": 15020
    },
    {
      "epoch": 1.002,
      "grad_norm": 0.6877717971801758,
      "learning_rate": 4.3737500000000006e-05,
      "loss": 0.0036,
      "step": 15030
    },
    {
      "epoch": 1.0026666666666666,
      "grad_norm": 0.8344091176986694,
      "learning_rate": 4.373333333333334e-05,
      "loss": 0.0033,
      "step": 15040
    },
    {
      "epoch": 1.0033333333333334,
      "grad_norm": 0.17249363660812378,
      "learning_rate": 4.372916666666667e-05,
      "loss": 0.0026,
      "step": 15050
    },
    {
      "epoch": 1.004,
      "grad_norm": 0.19551633298397064,
      "learning_rate": 4.3725000000000006e-05,
      "loss": 0.0031,
      "step": 15060
    },
    {
      "epoch": 1.0046666666666666,
      "grad_norm": 0.7089709043502808,
      "learning_rate": 4.372083333333333e-05,
      "loss": 0.004,
      "step": 15070
    },
    {
      "epoch": 1.0053333333333334,
      "grad_norm": 0.6577184796333313,
      "learning_rate": 4.371666666666667e-05,
      "loss": 0.0027,
      "step": 15080
    },
    {
      "epoch": 1.006,
      "grad_norm": 0.20560282468795776,
      "learning_rate": 4.37125e-05,
      "loss": 0.0036,
      "step": 15090
    },
    {
      "epoch": 1.0066666666666666,
      "grad_norm": 0.14089438319206238,
      "learning_rate": 4.3708333333333336e-05,
      "loss": 0.0022,
      "step": 15100
    },
    {
      "epoch": 1.0073333333333334,
      "grad_norm": 0.48730167746543884,
      "learning_rate": 4.370416666666667e-05,
      "loss": 0.0028,
      "step": 15110
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.9510084390640259,
      "learning_rate": 4.3700000000000005e-05,
      "loss": 0.0025,
      "step": 15120
    },
    {
      "epoch": 1.0086666666666666,
      "grad_norm": 0.28255516290664673,
      "learning_rate": 4.3695833333333336e-05,
      "loss": 0.0037,
      "step": 15130
    },
    {
      "epoch": 1.0093333333333334,
      "grad_norm": 0.46518293023109436,
      "learning_rate": 4.369166666666667e-05,
      "loss": 0.0035,
      "step": 15140
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.9164302945137024,
      "learning_rate": 4.3687500000000005e-05,
      "loss": 0.0024,
      "step": 15150
    },
    {
      "epoch": 1.0106666666666666,
      "grad_norm": 0.9832173585891724,
      "learning_rate": 4.3683333333333336e-05,
      "loss": 0.0046,
      "step": 15160
    },
    {
      "epoch": 1.0113333333333334,
      "grad_norm": 1.0902379751205444,
      "learning_rate": 4.3679166666666674e-05,
      "loss": 0.0033,
      "step": 15170
    },
    {
      "epoch": 1.012,
      "grad_norm": 1.0054339170455933,
      "learning_rate": 4.3675000000000005e-05,
      "loss": 0.0032,
      "step": 15180
    },
    {
      "epoch": 1.0126666666666666,
      "grad_norm": 0.5107312202453613,
      "learning_rate": 4.3670833333333335e-05,
      "loss": 0.0035,
      "step": 15190
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.14889051020145416,
      "learning_rate": 4.3666666666666666e-05,
      "loss": 0.0031,
      "step": 15200
    },
    {
      "epoch": 1.014,
      "grad_norm": 0.08718938380479813,
      "learning_rate": 4.36625e-05,
      "loss": 0.002,
      "step": 15210
    },
    {
      "epoch": 1.0146666666666666,
      "grad_norm": 0.5207959413528442,
      "learning_rate": 4.3658333333333335e-05,
      "loss": 0.0043,
      "step": 15220
    },
    {
      "epoch": 1.0153333333333334,
      "grad_norm": 0.4079402685165405,
      "learning_rate": 4.3654166666666666e-05,
      "loss": 0.0021,
      "step": 15230
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.6568962335586548,
      "learning_rate": 4.3650000000000004e-05,
      "loss": 0.0029,
      "step": 15240
    },
    {
      "epoch": 1.0166666666666666,
      "grad_norm": 1.0530493259429932,
      "learning_rate": 4.3645833333333335e-05,
      "loss": 0.002,
      "step": 15250
    },
    {
      "epoch": 1.0173333333333334,
      "grad_norm": 0.474057137966156,
      "learning_rate": 4.364166666666667e-05,
      "loss": 0.0036,
      "step": 15260
    },
    {
      "epoch": 1.018,
      "grad_norm": 0.20332913100719452,
      "learning_rate": 4.3637500000000004e-05,
      "loss": 0.0027,
      "step": 15270
    },
    {
      "epoch": 1.0186666666666666,
      "grad_norm": 0.3409653902053833,
      "learning_rate": 4.3633333333333335e-05,
      "loss": 0.0025,
      "step": 15280
    },
    {
      "epoch": 1.0193333333333334,
      "grad_norm": 0.2946847975254059,
      "learning_rate": 4.362916666666667e-05,
      "loss": 0.0022,
      "step": 15290
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.23887082934379578,
      "learning_rate": 4.3625e-05,
      "loss": 0.0032,
      "step": 15300
    },
    {
      "epoch": 1.0206666666666666,
      "grad_norm": 0.23025403916835785,
      "learning_rate": 4.3620833333333334e-05,
      "loss": 0.0023,
      "step": 15310
    },
    {
      "epoch": 1.0213333333333334,
      "grad_norm": 0.9429076910018921,
      "learning_rate": 4.3616666666666665e-05,
      "loss": 0.0032,
      "step": 15320
    },
    {
      "epoch": 1.022,
      "grad_norm": 0.2566322386264801,
      "learning_rate": 4.36125e-05,
      "loss": 0.0039,
      "step": 15330
    },
    {
      "epoch": 1.0226666666666666,
      "grad_norm": 1.0613672733306885,
      "learning_rate": 4.3608333333333334e-05,
      "loss": 0.0032,
      "step": 15340
    },
    {
      "epoch": 1.0233333333333334,
      "grad_norm": 0.8095762729644775,
      "learning_rate": 4.3604166666666665e-05,
      "loss": 0.0034,
      "step": 15350
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.9758205413818359,
      "learning_rate": 4.36e-05,
      "loss": 0.0015,
      "step": 15360
    },
    {
      "epoch": 1.0246666666666666,
      "grad_norm": 0.2695779502391815,
      "learning_rate": 4.3595833333333334e-05,
      "loss": 0.003,
      "step": 15370
    },
    {
      "epoch": 1.0253333333333334,
      "grad_norm": 0.36153504252433777,
      "learning_rate": 4.359166666666667e-05,
      "loss": 0.0024,
      "step": 15380
    },
    {
      "epoch": 1.026,
      "grad_norm": 0.48960039019584656,
      "learning_rate": 4.35875e-05,
      "loss": 0.0032,
      "step": 15390
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.8749327063560486,
      "learning_rate": 4.358333333333334e-05,
      "loss": 0.0026,
      "step": 15400
    },
    {
      "epoch": 1.0273333333333334,
      "grad_norm": 1.1001514196395874,
      "learning_rate": 4.357916666666667e-05,
      "loss": 0.0032,
      "step": 15410
    },
    {
      "epoch": 1.028,
      "grad_norm": 0.9369826316833496,
      "learning_rate": 4.3575e-05,
      "loss": 0.0036,
      "step": 15420
    },
    {
      "epoch": 1.0286666666666666,
      "grad_norm": 0.7574819922447205,
      "learning_rate": 4.357083333333333e-05,
      "loss": 0.0033,
      "step": 15430
    },
    {
      "epoch": 1.0293333333333334,
      "grad_norm": 0.8951776623725891,
      "learning_rate": 4.3566666666666664e-05,
      "loss": 0.0028,
      "step": 15440
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.5906083583831787,
      "learning_rate": 4.35625e-05,
      "loss": 0.0025,
      "step": 15450
    },
    {
      "epoch": 1.0306666666666666,
      "grad_norm": 1.0272799730300903,
      "learning_rate": 4.355833333333333e-05,
      "loss": 0.0021,
      "step": 15460
    },
    {
      "epoch": 1.0313333333333334,
      "grad_norm": 0.4238758683204651,
      "learning_rate": 4.355416666666667e-05,
      "loss": 0.0029,
      "step": 15470
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.10638025403022766,
      "learning_rate": 4.355e-05,
      "loss": 0.003,
      "step": 15480
    },
    {
      "epoch": 1.0326666666666666,
      "grad_norm": 0.06461833417415619,
      "learning_rate": 4.354583333333333e-05,
      "loss": 0.0031,
      "step": 15490
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 0.43881526589393616,
      "learning_rate": 4.354166666666667e-05,
      "loss": 0.0028,
      "step": 15500
    },
    {
      "epoch": 1.034,
      "grad_norm": 0.3499971032142639,
      "learning_rate": 4.35375e-05,
      "loss": 0.0039,
      "step": 15510
    },
    {
      "epoch": 1.0346666666666666,
      "grad_norm": 0.7522558569908142,
      "learning_rate": 4.353333333333334e-05,
      "loss": 0.0034,
      "step": 15520
    },
    {
      "epoch": 1.0353333333333334,
      "grad_norm": 0.4647603929042816,
      "learning_rate": 4.352916666666667e-05,
      "loss": 0.0029,
      "step": 15530
    },
    {
      "epoch": 1.036,
      "grad_norm": 0.21475759148597717,
      "learning_rate": 4.352500000000001e-05,
      "loss": 0.0031,
      "step": 15540
    },
    {
      "epoch": 1.0366666666666666,
      "grad_norm": 0.34484344720840454,
      "learning_rate": 4.352083333333333e-05,
      "loss": 0.002,
      "step": 15550
    },
    {
      "epoch": 1.0373333333333334,
      "grad_norm": 0.3809765875339508,
      "learning_rate": 4.351666666666667e-05,
      "loss": 0.0035,
      "step": 15560
    },
    {
      "epoch": 1.038,
      "grad_norm": 1.1011743545532227,
      "learning_rate": 4.35125e-05,
      "loss": 0.0035,
      "step": 15570
    },
    {
      "epoch": 1.0386666666666666,
      "grad_norm": 0.25778859853744507,
      "learning_rate": 4.350833333333333e-05,
      "loss": 0.0024,
      "step": 15580
    },
    {
      "epoch": 1.0393333333333334,
      "grad_norm": 0.9360677003860474,
      "learning_rate": 4.350416666666667e-05,
      "loss": 0.0027,
      "step": 15590
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.8566505908966064,
      "learning_rate": 4.35e-05,
      "loss": 0.0032,
      "step": 15600
    },
    {
      "epoch": 1.0406666666666666,
      "grad_norm": 1.0503677129745483,
      "learning_rate": 4.349583333333334e-05,
      "loss": 0.0027,
      "step": 15610
    },
    {
      "epoch": 1.0413333333333332,
      "grad_norm": 0.5194066762924194,
      "learning_rate": 4.349166666666667e-05,
      "loss": 0.0038,
      "step": 15620
    },
    {
      "epoch": 1.042,
      "grad_norm": 0.3681616485118866,
      "learning_rate": 4.34875e-05,
      "loss": 0.0035,
      "step": 15630
    },
    {
      "epoch": 1.0426666666666666,
      "grad_norm": 0.10814276337623596,
      "learning_rate": 4.348333333333334e-05,
      "loss": 0.0036,
      "step": 15640
    },
    {
      "epoch": 1.0433333333333334,
      "grad_norm": 0.33485788106918335,
      "learning_rate": 4.347916666666667e-05,
      "loss": 0.0044,
      "step": 15650
    },
    {
      "epoch": 1.044,
      "grad_norm": 0.5043490529060364,
      "learning_rate": 4.3475000000000006e-05,
      "loss": 0.0035,
      "step": 15660
    },
    {
      "epoch": 1.0446666666666666,
      "grad_norm": 0.5308102965354919,
      "learning_rate": 4.347083333333333e-05,
      "loss": 0.0042,
      "step": 15670
    },
    {
      "epoch": 1.0453333333333332,
      "grad_norm": 0.36357542872428894,
      "learning_rate": 4.346666666666667e-05,
      "loss": 0.0037,
      "step": 15680
    },
    {
      "epoch": 1.046,
      "grad_norm": 0.1753464788198471,
      "learning_rate": 4.34625e-05,
      "loss": 0.0032,
      "step": 15690
    },
    {
      "epoch": 1.0466666666666666,
      "grad_norm": 1.1184136867523193,
      "learning_rate": 4.345833333333334e-05,
      "loss": 0.0026,
      "step": 15700
    },
    {
      "epoch": 1.0473333333333332,
      "grad_norm": 0.07074309885501862,
      "learning_rate": 4.345416666666667e-05,
      "loss": 0.0038,
      "step": 15710
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.3884972035884857,
      "learning_rate": 4.345e-05,
      "loss": 0.0039,
      "step": 15720
    },
    {
      "epoch": 1.0486666666666666,
      "grad_norm": 0.22054849565029144,
      "learning_rate": 4.344583333333334e-05,
      "loss": 0.0028,
      "step": 15730
    },
    {
      "epoch": 1.0493333333333332,
      "grad_norm": 0.38592803478240967,
      "learning_rate": 4.344166666666667e-05,
      "loss": 0.0034,
      "step": 15740
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.46819937229156494,
      "learning_rate": 4.3437500000000006e-05,
      "loss": 0.0035,
      "step": 15750
    },
    {
      "epoch": 1.0506666666666666,
      "grad_norm": 0.31922873854637146,
      "learning_rate": 4.3433333333333336e-05,
      "loss": 0.0024,
      "step": 15760
    },
    {
      "epoch": 1.0513333333333332,
      "grad_norm": 0.09930934756994247,
      "learning_rate": 4.342916666666667e-05,
      "loss": 0.0057,
      "step": 15770
    },
    {
      "epoch": 1.052,
      "grad_norm": 0.7708150744438171,
      "learning_rate": 4.3425000000000005e-05,
      "loss": 0.0023,
      "step": 15780
    },
    {
      "epoch": 1.0526666666666666,
      "grad_norm": 0.7911482453346252,
      "learning_rate": 4.3420833333333336e-05,
      "loss": 0.0025,
      "step": 15790
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.43167024850845337,
      "learning_rate": 4.341666666666667e-05,
      "loss": 0.0027,
      "step": 15800
    },
    {
      "epoch": 1.054,
      "grad_norm": 0.35747113823890686,
      "learning_rate": 4.34125e-05,
      "loss": 0.0025,
      "step": 15810
    },
    {
      "epoch": 1.0546666666666666,
      "grad_norm": 0.29674333333969116,
      "learning_rate": 4.3408333333333336e-05,
      "loss": 0.0037,
      "step": 15820
    },
    {
      "epoch": 1.0553333333333332,
      "grad_norm": 0.5224533677101135,
      "learning_rate": 4.340416666666667e-05,
      "loss": 0.0029,
      "step": 15830
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.6980332136154175,
      "learning_rate": 4.3400000000000005e-05,
      "loss": 0.0034,
      "step": 15840
    },
    {
      "epoch": 1.0566666666666666,
      "grad_norm": 0.6574023365974426,
      "learning_rate": 4.3395833333333336e-05,
      "loss": 0.0027,
      "step": 15850
    },
    {
      "epoch": 1.0573333333333332,
      "grad_norm": 0.9544275403022766,
      "learning_rate": 4.3391666666666667e-05,
      "loss": 0.0027,
      "step": 15860
    },
    {
      "epoch": 1.058,
      "grad_norm": 0.08466529101133347,
      "learning_rate": 4.3387500000000004e-05,
      "loss": 0.0026,
      "step": 15870
    },
    {
      "epoch": 1.0586666666666666,
      "grad_norm": 0.1637524664402008,
      "learning_rate": 4.3383333333333335e-05,
      "loss": 0.003,
      "step": 15880
    },
    {
      "epoch": 1.0593333333333332,
      "grad_norm": 0.446405827999115,
      "learning_rate": 4.337916666666667e-05,
      "loss": 0.0021,
      "step": 15890
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.7880664467811584,
      "learning_rate": 4.3375000000000004e-05,
      "loss": 0.0028,
      "step": 15900
    },
    {
      "epoch": 1.0606666666666666,
      "grad_norm": 0.6868352890014648,
      "learning_rate": 4.3370833333333335e-05,
      "loss": 0.0032,
      "step": 15910
    },
    {
      "epoch": 1.0613333333333332,
      "grad_norm": 1.020516276359558,
      "learning_rate": 4.3366666666666666e-05,
      "loss": 0.004,
      "step": 15920
    },
    {
      "epoch": 1.062,
      "grad_norm": 0.4230600893497467,
      "learning_rate": 4.3362500000000004e-05,
      "loss": 0.003,
      "step": 15930
    },
    {
      "epoch": 1.0626666666666666,
      "grad_norm": 0.513536274433136,
      "learning_rate": 4.3358333333333335e-05,
      "loss": 0.0032,
      "step": 15940
    },
    {
      "epoch": 1.0633333333333332,
      "grad_norm": 0.25662484765052795,
      "learning_rate": 4.3354166666666666e-05,
      "loss": 0.0037,
      "step": 15950
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.27701425552368164,
      "learning_rate": 4.335e-05,
      "loss": 0.0042,
      "step": 15960
    },
    {
      "epoch": 1.0646666666666667,
      "grad_norm": 0.10156527906656265,
      "learning_rate": 4.3345833333333334e-05,
      "loss": 0.0022,
      "step": 15970
    },
    {
      "epoch": 1.0653333333333332,
      "grad_norm": 0.13112163543701172,
      "learning_rate": 4.334166666666667e-05,
      "loss": 0.0018,
      "step": 15980
    },
    {
      "epoch": 1.066,
      "grad_norm": 0.14877696335315704,
      "learning_rate": 4.33375e-05,
      "loss": 0.0022,
      "step": 15990
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.4347112774848938,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.0026,
      "step": 16000
    },
    {
      "epoch": 1.0673333333333332,
      "grad_norm": 0.21113678812980652,
      "learning_rate": 4.332916666666667e-05,
      "loss": 0.0041,
      "step": 16010
    },
    {
      "epoch": 1.068,
      "grad_norm": 0.46659913659095764,
      "learning_rate": 4.3325e-05,
      "loss": 0.0042,
      "step": 16020
    },
    {
      "epoch": 1.0686666666666667,
      "grad_norm": 0.49539613723754883,
      "learning_rate": 4.3320833333333334e-05,
      "loss": 0.004,
      "step": 16030
    },
    {
      "epoch": 1.0693333333333332,
      "grad_norm": 0.3978278934955597,
      "learning_rate": 4.3316666666666665e-05,
      "loss": 0.0039,
      "step": 16040
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.9888454079627991,
      "learning_rate": 4.33125e-05,
      "loss": 0.0039,
      "step": 16050
    },
    {
      "epoch": 1.0706666666666667,
      "grad_norm": 0.9482277631759644,
      "learning_rate": 4.3308333333333333e-05,
      "loss": 0.0043,
      "step": 16060
    },
    {
      "epoch": 1.0713333333333332,
      "grad_norm": 0.2931158244609833,
      "learning_rate": 4.330416666666667e-05,
      "loss": 0.0023,
      "step": 16070
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.32174256443977356,
      "learning_rate": 4.33e-05,
      "loss": 0.0029,
      "step": 16080
    },
    {
      "epoch": 1.0726666666666667,
      "grad_norm": 0.16014313697814941,
      "learning_rate": 4.329583333333333e-05,
      "loss": 0.0044,
      "step": 16090
    },
    {
      "epoch": 1.0733333333333333,
      "grad_norm": 0.4107897877693176,
      "learning_rate": 4.329166666666667e-05,
      "loss": 0.0026,
      "step": 16100
    },
    {
      "epoch": 1.074,
      "grad_norm": 0.22558750212192535,
      "learning_rate": 4.32875e-05,
      "loss": 0.0037,
      "step": 16110
    },
    {
      "epoch": 1.0746666666666667,
      "grad_norm": 0.14480052888393402,
      "learning_rate": 4.328333333333334e-05,
      "loss": 0.0027,
      "step": 16120
    },
    {
      "epoch": 1.0753333333333333,
      "grad_norm": 1.036080002784729,
      "learning_rate": 4.327916666666667e-05,
      "loss": 0.0031,
      "step": 16130
    },
    {
      "epoch": 1.076,
      "grad_norm": 0.24995380640029907,
      "learning_rate": 4.3275e-05,
      "loss": 0.0037,
      "step": 16140
    },
    {
      "epoch": 1.0766666666666667,
      "grad_norm": 0.8419128656387329,
      "learning_rate": 4.327083333333333e-05,
      "loss": 0.0026,
      "step": 16150
    },
    {
      "epoch": 1.0773333333333333,
      "grad_norm": 0.6271123886108398,
      "learning_rate": 4.3266666666666664e-05,
      "loss": 0.0017,
      "step": 16160
    },
    {
      "epoch": 1.078,
      "grad_norm": 0.6899564266204834,
      "learning_rate": 4.32625e-05,
      "loss": 0.004,
      "step": 16170
    },
    {
      "epoch": 1.0786666666666667,
      "grad_norm": 0.19814404845237732,
      "learning_rate": 4.325833333333333e-05,
      "loss": 0.0027,
      "step": 16180
    },
    {
      "epoch": 1.0793333333333333,
      "grad_norm": 0.29801034927368164,
      "learning_rate": 4.325416666666667e-05,
      "loss": 0.0026,
      "step": 16190
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.09663139283657074,
      "learning_rate": 4.325e-05,
      "loss": 0.0045,
      "step": 16200
    },
    {
      "epoch": 1.0806666666666667,
      "grad_norm": 0.2708817422389984,
      "learning_rate": 4.324583333333334e-05,
      "loss": 0.0037,
      "step": 16210
    },
    {
      "epoch": 1.0813333333333333,
      "grad_norm": 0.33642905950546265,
      "learning_rate": 4.324166666666667e-05,
      "loss": 0.0031,
      "step": 16220
    },
    {
      "epoch": 1.082,
      "grad_norm": 0.31380945444107056,
      "learning_rate": 4.32375e-05,
      "loss": 0.002,
      "step": 16230
    },
    {
      "epoch": 1.0826666666666667,
      "grad_norm": 0.7965397834777832,
      "learning_rate": 4.323333333333334e-05,
      "loss": 0.0024,
      "step": 16240
    },
    {
      "epoch": 1.0833333333333333,
      "grad_norm": 0.617834210395813,
      "learning_rate": 4.322916666666667e-05,
      "loss": 0.0044,
      "step": 16250
    },
    {
      "epoch": 1.084,
      "grad_norm": 0.17274780571460724,
      "learning_rate": 4.322500000000001e-05,
      "loss": 0.0027,
      "step": 16260
    },
    {
      "epoch": 1.0846666666666667,
      "grad_norm": 0.13909225165843964,
      "learning_rate": 4.322083333333334e-05,
      "loss": 0.0041,
      "step": 16270
    },
    {
      "epoch": 1.0853333333333333,
      "grad_norm": 0.24710772931575775,
      "learning_rate": 4.321666666666667e-05,
      "loss": 0.0035,
      "step": 16280
    },
    {
      "epoch": 1.086,
      "grad_norm": 0.42235267162323,
      "learning_rate": 4.32125e-05,
      "loss": 0.003,
      "step": 16290
    },
    {
      "epoch": 1.0866666666666667,
      "grad_norm": 0.7826117873191833,
      "learning_rate": 4.320833333333333e-05,
      "loss": 0.0016,
      "step": 16300
    },
    {
      "epoch": 1.0873333333333333,
      "grad_norm": 0.9345592260360718,
      "learning_rate": 4.320416666666667e-05,
      "loss": 0.0046,
      "step": 16310
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.5380436182022095,
      "learning_rate": 4.32e-05,
      "loss": 0.0022,
      "step": 16320
    },
    {
      "epoch": 1.0886666666666667,
      "grad_norm": 1.1079750061035156,
      "learning_rate": 4.319583333333334e-05,
      "loss": 0.0029,
      "step": 16330
    },
    {
      "epoch": 1.0893333333333333,
      "grad_norm": 0.5352602601051331,
      "learning_rate": 4.319166666666667e-05,
      "loss": 0.0031,
      "step": 16340
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.08669618517160416,
      "learning_rate": 4.3187500000000006e-05,
      "loss": 0.0023,
      "step": 16350
    },
    {
      "epoch": 1.0906666666666667,
      "grad_norm": 0.8090957999229431,
      "learning_rate": 4.318333333333334e-05,
      "loss": 0.0026,
      "step": 16360
    },
    {
      "epoch": 1.0913333333333333,
      "grad_norm": 1.106215476989746,
      "learning_rate": 4.317916666666667e-05,
      "loss": 0.0025,
      "step": 16370
    },
    {
      "epoch": 1.092,
      "grad_norm": 0.7559458613395691,
      "learning_rate": 4.3175000000000006e-05,
      "loss": 0.0023,
      "step": 16380
    },
    {
      "epoch": 1.0926666666666667,
      "grad_norm": 0.20867516100406647,
      "learning_rate": 4.317083333333334e-05,
      "loss": 0.0017,
      "step": 16390
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.12823599576950073,
      "learning_rate": 4.316666666666667e-05,
      "loss": 0.0024,
      "step": 16400
    },
    {
      "epoch": 1.094,
      "grad_norm": 0.2544817328453064,
      "learning_rate": 4.31625e-05,
      "loss": 0.0029,
      "step": 16410
    },
    {
      "epoch": 1.0946666666666667,
      "grad_norm": 0.47724342346191406,
      "learning_rate": 4.3158333333333337e-05,
      "loss": 0.0033,
      "step": 16420
    },
    {
      "epoch": 1.0953333333333333,
      "grad_norm": 0.7558513283729553,
      "learning_rate": 4.315416666666667e-05,
      "loss": 0.0033,
      "step": 16430
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.9375197887420654,
      "learning_rate": 4.315e-05,
      "loss": 0.0029,
      "step": 16440
    },
    {
      "epoch": 1.0966666666666667,
      "grad_norm": 0.5770492553710938,
      "learning_rate": 4.3145833333333336e-05,
      "loss": 0.0023,
      "step": 16450
    },
    {
      "epoch": 1.0973333333333333,
      "grad_norm": 0.5200848579406738,
      "learning_rate": 4.314166666666667e-05,
      "loss": 0.0036,
      "step": 16460
    },
    {
      "epoch": 1.098,
      "grad_norm": 0.25181278586387634,
      "learning_rate": 4.3137500000000005e-05,
      "loss": 0.0032,
      "step": 16470
    },
    {
      "epoch": 1.0986666666666667,
      "grad_norm": 0.5652074813842773,
      "learning_rate": 4.3133333333333336e-05,
      "loss": 0.0029,
      "step": 16480
    },
    {
      "epoch": 1.0993333333333333,
      "grad_norm": 0.6519011855125427,
      "learning_rate": 4.3129166666666674e-05,
      "loss": 0.0024,
      "step": 16490
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.22085130214691162,
      "learning_rate": 4.3125000000000005e-05,
      "loss": 0.0025,
      "step": 16500
    },
    {
      "epoch": 1.1006666666666667,
      "grad_norm": 0.45431095361709595,
      "learning_rate": 4.3120833333333336e-05,
      "loss": 0.0041,
      "step": 16510
    },
    {
      "epoch": 1.1013333333333333,
      "grad_norm": 0.6444717645645142,
      "learning_rate": 4.311666666666667e-05,
      "loss": 0.003,
      "step": 16520
    },
    {
      "epoch": 1.102,
      "grad_norm": 0.2778013050556183,
      "learning_rate": 4.31125e-05,
      "loss": 0.0034,
      "step": 16530
    },
    {
      "epoch": 1.1026666666666667,
      "grad_norm": 0.644057035446167,
      "learning_rate": 4.3108333333333335e-05,
      "loss": 0.0026,
      "step": 16540
    },
    {
      "epoch": 1.1033333333333333,
      "grad_norm": 0.41554272174835205,
      "learning_rate": 4.3104166666666666e-05,
      "loss": 0.0027,
      "step": 16550
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.5737496018409729,
      "learning_rate": 4.3100000000000004e-05,
      "loss": 0.0024,
      "step": 16560
    },
    {
      "epoch": 1.1046666666666667,
      "grad_norm": 0.4676462411880493,
      "learning_rate": 4.3095833333333335e-05,
      "loss": 0.0024,
      "step": 16570
    },
    {
      "epoch": 1.1053333333333333,
      "grad_norm": 0.30989375710487366,
      "learning_rate": 4.3091666666666666e-05,
      "loss": 0.005,
      "step": 16580
    },
    {
      "epoch": 1.106,
      "grad_norm": 0.13311907649040222,
      "learning_rate": 4.3087500000000004e-05,
      "loss": 0.0038,
      "step": 16590
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.08954590559005737,
      "learning_rate": 4.3083333333333335e-05,
      "loss": 0.0026,
      "step": 16600
    },
    {
      "epoch": 1.1073333333333333,
      "grad_norm": 0.07789053022861481,
      "learning_rate": 4.307916666666667e-05,
      "loss": 0.0048,
      "step": 16610
    },
    {
      "epoch": 1.108,
      "grad_norm": 0.5416013598442078,
      "learning_rate": 4.3075000000000003e-05,
      "loss": 0.0039,
      "step": 16620
    },
    {
      "epoch": 1.1086666666666667,
      "grad_norm": 0.7347038984298706,
      "learning_rate": 4.307083333333334e-05,
      "loss": 0.0034,
      "step": 16630
    },
    {
      "epoch": 1.1093333333333333,
      "grad_norm": 0.8880168199539185,
      "learning_rate": 4.3066666666666665e-05,
      "loss": 0.0012,
      "step": 16640
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.3742395043373108,
      "learning_rate": 4.30625e-05,
      "loss": 0.0031,
      "step": 16650
    },
    {
      "epoch": 1.1106666666666667,
      "grad_norm": 0.10410688072443008,
      "learning_rate": 4.3058333333333334e-05,
      "loss": 0.0027,
      "step": 16660
    },
    {
      "epoch": 1.1113333333333333,
      "grad_norm": 0.1599966436624527,
      "learning_rate": 4.3054166666666665e-05,
      "loss": 0.0037,
      "step": 16670
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.5260887742042542,
      "learning_rate": 4.305e-05,
      "loss": 0.0038,
      "step": 16680
    },
    {
      "epoch": 1.1126666666666667,
      "grad_norm": 0.7099977731704712,
      "learning_rate": 4.3045833333333334e-05,
      "loss": 0.0017,
      "step": 16690
    },
    {
      "epoch": 1.1133333333333333,
      "grad_norm": 0.3247945308685303,
      "learning_rate": 4.304166666666667e-05,
      "loss": 0.0017,
      "step": 16700
    },
    {
      "epoch": 1.114,
      "grad_norm": 0.13702988624572754,
      "learning_rate": 4.30375e-05,
      "loss": 0.0027,
      "step": 16710
    },
    {
      "epoch": 1.1146666666666667,
      "grad_norm": 0.6352514624595642,
      "learning_rate": 4.3033333333333334e-05,
      "loss": 0.0025,
      "step": 16720
    },
    {
      "epoch": 1.1153333333333333,
      "grad_norm": 0.6044010519981384,
      "learning_rate": 4.302916666666667e-05,
      "loss": 0.0024,
      "step": 16730
    },
    {
      "epoch": 1.116,
      "grad_norm": 0.09082909673452377,
      "learning_rate": 4.3025e-05,
      "loss": 0.0024,
      "step": 16740
    },
    {
      "epoch": 1.1166666666666667,
      "grad_norm": 0.8244155049324036,
      "learning_rate": 4.302083333333334e-05,
      "loss": 0.0023,
      "step": 16750
    },
    {
      "epoch": 1.1173333333333333,
      "grad_norm": 1.0858659744262695,
      "learning_rate": 4.3016666666666664e-05,
      "loss": 0.0021,
      "step": 16760
    },
    {
      "epoch": 1.1179999999999999,
      "grad_norm": 0.8656182289123535,
      "learning_rate": 4.30125e-05,
      "loss": 0.0028,
      "step": 16770
    },
    {
      "epoch": 1.1186666666666667,
      "grad_norm": 0.07764218747615814,
      "learning_rate": 4.300833333333333e-05,
      "loss": 0.0042,
      "step": 16780
    },
    {
      "epoch": 1.1193333333333333,
      "grad_norm": 0.3003460466861725,
      "learning_rate": 4.300416666666667e-05,
      "loss": 0.0045,
      "step": 16790
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.7759639024734497,
      "learning_rate": 4.3e-05,
      "loss": 0.0028,
      "step": 16800
    },
    {
      "epoch": 1.1206666666666667,
      "grad_norm": 0.6509681344032288,
      "learning_rate": 4.299583333333333e-05,
      "loss": 0.0029,
      "step": 16810
    },
    {
      "epoch": 1.1213333333333333,
      "grad_norm": 0.9978811740875244,
      "learning_rate": 4.299166666666667e-05,
      "loss": 0.0021,
      "step": 16820
    },
    {
      "epoch": 1.1219999999999999,
      "grad_norm": 0.9608122110366821,
      "learning_rate": 4.29875e-05,
      "loss": 0.0031,
      "step": 16830
    },
    {
      "epoch": 1.1226666666666667,
      "grad_norm": 0.17764310538768768,
      "learning_rate": 4.298333333333334e-05,
      "loss": 0.0022,
      "step": 16840
    },
    {
      "epoch": 1.1233333333333333,
      "grad_norm": 0.057392947375774384,
      "learning_rate": 4.297916666666667e-05,
      "loss": 0.0025,
      "step": 16850
    },
    {
      "epoch": 1.124,
      "grad_norm": 0.8552237153053284,
      "learning_rate": 4.2975e-05,
      "loss": 0.0029,
      "step": 16860
    },
    {
      "epoch": 1.1246666666666667,
      "grad_norm": 0.2558671236038208,
      "learning_rate": 4.297083333333334e-05,
      "loss": 0.0028,
      "step": 16870
    },
    {
      "epoch": 1.1253333333333333,
      "grad_norm": 0.12221630662679672,
      "learning_rate": 4.296666666666666e-05,
      "loss": 0.0022,
      "step": 16880
    },
    {
      "epoch": 1.126,
      "grad_norm": 0.10363336652517319,
      "learning_rate": 4.29625e-05,
      "loss": 0.0022,
      "step": 16890
    },
    {
      "epoch": 1.1266666666666667,
      "grad_norm": 0.5888798236846924,
      "learning_rate": 4.295833333333333e-05,
      "loss": 0.0022,
      "step": 16900
    },
    {
      "epoch": 1.1273333333333333,
      "grad_norm": 0.7109224200248718,
      "learning_rate": 4.295416666666667e-05,
      "loss": 0.004,
      "step": 16910
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.28798240423202515,
      "learning_rate": 4.295e-05,
      "loss": 0.0031,
      "step": 16920
    },
    {
      "epoch": 1.1286666666666667,
      "grad_norm": 0.1342717856168747,
      "learning_rate": 4.294583333333334e-05,
      "loss": 0.0018,
      "step": 16930
    },
    {
      "epoch": 1.1293333333333333,
      "grad_norm": 0.21974624693393707,
      "learning_rate": 4.294166666666667e-05,
      "loss": 0.0024,
      "step": 16940
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.7514297366142273,
      "learning_rate": 4.29375e-05,
      "loss": 0.0025,
      "step": 16950
    },
    {
      "epoch": 1.1306666666666667,
      "grad_norm": 0.6845746040344238,
      "learning_rate": 4.293333333333334e-05,
      "loss": 0.0035,
      "step": 16960
    },
    {
      "epoch": 1.1313333333333333,
      "grad_norm": 0.29044172167778015,
      "learning_rate": 4.292916666666667e-05,
      "loss": 0.003,
      "step": 16970
    },
    {
      "epoch": 1.1320000000000001,
      "grad_norm": 0.344938188791275,
      "learning_rate": 4.2925000000000007e-05,
      "loss": 0.0035,
      "step": 16980
    },
    {
      "epoch": 1.1326666666666667,
      "grad_norm": 0.18416862189769745,
      "learning_rate": 4.292083333333334e-05,
      "loss": 0.0026,
      "step": 16990
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.7153632044792175,
      "learning_rate": 4.291666666666667e-05,
      "loss": 0.0033,
      "step": 17000
    },
    {
      "epoch": 1.134,
      "grad_norm": 0.2999369204044342,
      "learning_rate": 4.29125e-05,
      "loss": 0.002,
      "step": 17010
    },
    {
      "epoch": 1.1346666666666667,
      "grad_norm": 0.28848543763160706,
      "learning_rate": 4.290833333333333e-05,
      "loss": 0.0025,
      "step": 17020
    },
    {
      "epoch": 1.1353333333333333,
      "grad_norm": 0.13188759982585907,
      "learning_rate": 4.290416666666667e-05,
      "loss": 0.0028,
      "step": 17030
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.5274955630302429,
      "learning_rate": 4.29e-05,
      "loss": 0.0021,
      "step": 17040
    },
    {
      "epoch": 1.1366666666666667,
      "grad_norm": 0.6028823256492615,
      "learning_rate": 4.289583333333334e-05,
      "loss": 0.0019,
      "step": 17050
    },
    {
      "epoch": 1.1373333333333333,
      "grad_norm": 0.25549790263175964,
      "learning_rate": 4.289166666666667e-05,
      "loss": 0.0043,
      "step": 17060
    },
    {
      "epoch": 1.138,
      "grad_norm": 0.09710012376308441,
      "learning_rate": 4.2887500000000006e-05,
      "loss": 0.0028,
      "step": 17070
    },
    {
      "epoch": 1.1386666666666667,
      "grad_norm": 0.3010818064212799,
      "learning_rate": 4.288333333333334e-05,
      "loss": 0.0025,
      "step": 17080
    },
    {
      "epoch": 1.1393333333333333,
      "grad_norm": 0.20068107545375824,
      "learning_rate": 4.287916666666667e-05,
      "loss": 0.0025,
      "step": 17090
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 0.5692033171653748,
      "learning_rate": 4.2875000000000005e-05,
      "loss": 0.0019,
      "step": 17100
    },
    {
      "epoch": 1.1406666666666667,
      "grad_norm": 0.2033115178346634,
      "learning_rate": 4.2870833333333336e-05,
      "loss": 0.0027,
      "step": 17110
    },
    {
      "epoch": 1.1413333333333333,
      "grad_norm": 0.17420850694179535,
      "learning_rate": 4.286666666666667e-05,
      "loss": 0.0033,
      "step": 17120
    },
    {
      "epoch": 1.142,
      "grad_norm": 0.4106374680995941,
      "learning_rate": 4.28625e-05,
      "loss": 0.0044,
      "step": 17130
    },
    {
      "epoch": 1.1426666666666667,
      "grad_norm": 0.7944263219833374,
      "learning_rate": 4.2858333333333336e-05,
      "loss": 0.0032,
      "step": 17140
    },
    {
      "epoch": 1.1433333333333333,
      "grad_norm": 1.057433843612671,
      "learning_rate": 4.285416666666667e-05,
      "loss": 0.0033,
      "step": 17150
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.3568693995475769,
      "learning_rate": 4.285e-05,
      "loss": 0.0037,
      "step": 17160
    },
    {
      "epoch": 1.1446666666666667,
      "grad_norm": 0.763723611831665,
      "learning_rate": 4.2845833333333336e-05,
      "loss": 0.0025,
      "step": 17170
    },
    {
      "epoch": 1.1453333333333333,
      "grad_norm": 0.5579987168312073,
      "learning_rate": 4.284166666666667e-05,
      "loss": 0.0027,
      "step": 17180
    },
    {
      "epoch": 1.146,
      "grad_norm": 0.45498910546302795,
      "learning_rate": 4.2837500000000004e-05,
      "loss": 0.0033,
      "step": 17190
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.3145712614059448,
      "learning_rate": 4.2833333333333335e-05,
      "loss": 0.0033,
      "step": 17200
    },
    {
      "epoch": 1.1473333333333333,
      "grad_norm": 0.9923133850097656,
      "learning_rate": 4.282916666666667e-05,
      "loss": 0.0022,
      "step": 17210
    },
    {
      "epoch": 1.148,
      "grad_norm": 0.19554542005062103,
      "learning_rate": 4.2825000000000004e-05,
      "loss": 0.0037,
      "step": 17220
    },
    {
      "epoch": 1.1486666666666667,
      "grad_norm": 0.43883493542671204,
      "learning_rate": 4.2820833333333335e-05,
      "loss": 0.003,
      "step": 17230
    },
    {
      "epoch": 1.1493333333333333,
      "grad_norm": 1.3777443170547485,
      "learning_rate": 4.2816666666666666e-05,
      "loss": 0.0019,
      "step": 17240
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.4421817362308502,
      "learning_rate": 4.28125e-05,
      "loss": 0.003,
      "step": 17250
    },
    {
      "epoch": 1.1506666666666667,
      "grad_norm": 0.06329435110092163,
      "learning_rate": 4.2808333333333335e-05,
      "loss": 0.0039,
      "step": 17260
    },
    {
      "epoch": 1.1513333333333333,
      "grad_norm": 1.109626054763794,
      "learning_rate": 4.2804166666666666e-05,
      "loss": 0.0038,
      "step": 17270
    },
    {
      "epoch": 1.152,
      "grad_norm": 1.0805284976959229,
      "learning_rate": 4.2800000000000004e-05,
      "loss": 0.0031,
      "step": 17280
    },
    {
      "epoch": 1.1526666666666667,
      "grad_norm": 1.002534031867981,
      "learning_rate": 4.2795833333333335e-05,
      "loss": 0.003,
      "step": 17290
    },
    {
      "epoch": 1.1533333333333333,
      "grad_norm": 0.814581573009491,
      "learning_rate": 4.2791666666666666e-05,
      "loss": 0.0031,
      "step": 17300
    },
    {
      "epoch": 1.154,
      "grad_norm": 1.3833633661270142,
      "learning_rate": 4.27875e-05,
      "loss": 0.0033,
      "step": 17310
    },
    {
      "epoch": 1.1546666666666667,
      "grad_norm": 0.4092886447906494,
      "learning_rate": 4.2783333333333334e-05,
      "loss": 0.003,
      "step": 17320
    },
    {
      "epoch": 1.1553333333333333,
      "grad_norm": 1.0128064155578613,
      "learning_rate": 4.277916666666667e-05,
      "loss": 0.003,
      "step": 17330
    },
    {
      "epoch": 1.156,
      "grad_norm": 0.32037708163261414,
      "learning_rate": 4.2775e-05,
      "loss": 0.0024,
      "step": 17340
    },
    {
      "epoch": 1.1566666666666667,
      "grad_norm": 0.24581781029701233,
      "learning_rate": 4.277083333333334e-05,
      "loss": 0.0031,
      "step": 17350
    },
    {
      "epoch": 1.1573333333333333,
      "grad_norm": 0.9626024961471558,
      "learning_rate": 4.2766666666666665e-05,
      "loss": 0.0024,
      "step": 17360
    },
    {
      "epoch": 1.158,
      "grad_norm": 0.7122421264648438,
      "learning_rate": 4.27625e-05,
      "loss": 0.003,
      "step": 17370
    },
    {
      "epoch": 1.1586666666666667,
      "grad_norm": 0.8567328453063965,
      "learning_rate": 4.2758333333333334e-05,
      "loss": 0.0024,
      "step": 17380
    },
    {
      "epoch": 1.1593333333333333,
      "grad_norm": 1.104278564453125,
      "learning_rate": 4.2754166666666665e-05,
      "loss": 0.002,
      "step": 17390
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.15066936612129211,
      "learning_rate": 4.275e-05,
      "loss": 0.0022,
      "step": 17400
    },
    {
      "epoch": 1.1606666666666667,
      "grad_norm": 0.5407692790031433,
      "learning_rate": 4.274583333333333e-05,
      "loss": 0.0031,
      "step": 17410
    },
    {
      "epoch": 1.1613333333333333,
      "grad_norm": 0.7186828255653381,
      "learning_rate": 4.274166666666667e-05,
      "loss": 0.0024,
      "step": 17420
    },
    {
      "epoch": 1.162,
      "grad_norm": 0.4077160656452179,
      "learning_rate": 4.27375e-05,
      "loss": 0.0028,
      "step": 17430
    },
    {
      "epoch": 1.1626666666666667,
      "grad_norm": 1.041038155555725,
      "learning_rate": 4.273333333333333e-05,
      "loss": 0.0022,
      "step": 17440
    },
    {
      "epoch": 1.1633333333333333,
      "grad_norm": 0.42838895320892334,
      "learning_rate": 4.272916666666667e-05,
      "loss": 0.0036,
      "step": 17450
    },
    {
      "epoch": 1.164,
      "grad_norm": 0.43651995062828064,
      "learning_rate": 4.2725e-05,
      "loss": 0.0027,
      "step": 17460
    },
    {
      "epoch": 1.1646666666666667,
      "grad_norm": 0.5598724484443665,
      "learning_rate": 4.272083333333334e-05,
      "loss": 0.0033,
      "step": 17470
    },
    {
      "epoch": 1.1653333333333333,
      "grad_norm": 0.6014231443405151,
      "learning_rate": 4.2716666666666664e-05,
      "loss": 0.0035,
      "step": 17480
    },
    {
      "epoch": 1.166,
      "grad_norm": 0.4877697229385376,
      "learning_rate": 4.27125e-05,
      "loss": 0.0039,
      "step": 17490
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.7529546022415161,
      "learning_rate": 4.270833333333333e-05,
      "loss": 0.0043,
      "step": 17500
    },
    {
      "epoch": 1.1673333333333333,
      "grad_norm": 0.11976964771747589,
      "learning_rate": 4.270416666666667e-05,
      "loss": 0.0032,
      "step": 17510
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.4085616171360016,
      "learning_rate": 4.27e-05,
      "loss": 0.0022,
      "step": 17520
    },
    {
      "epoch": 1.1686666666666667,
      "grad_norm": 0.14800798892974854,
      "learning_rate": 4.269583333333333e-05,
      "loss": 0.0034,
      "step": 17530
    },
    {
      "epoch": 1.1693333333333333,
      "grad_norm": 0.27743953466415405,
      "learning_rate": 4.269166666666667e-05,
      "loss": 0.0036,
      "step": 17540
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.5862003564834595,
      "learning_rate": 4.26875e-05,
      "loss": 0.003,
      "step": 17550
    },
    {
      "epoch": 1.1706666666666667,
      "grad_norm": 0.4588654935359955,
      "learning_rate": 4.268333333333334e-05,
      "loss": 0.0023,
      "step": 17560
    },
    {
      "epoch": 1.1713333333333333,
      "grad_norm": 0.5397027134895325,
      "learning_rate": 4.267916666666667e-05,
      "loss": 0.0033,
      "step": 17570
    },
    {
      "epoch": 1.172,
      "grad_norm": 0.26749664545059204,
      "learning_rate": 4.2675e-05,
      "loss": 0.0025,
      "step": 17580
    },
    {
      "epoch": 1.1726666666666667,
      "grad_norm": 0.1339610517024994,
      "learning_rate": 4.267083333333334e-05,
      "loss": 0.0021,
      "step": 17590
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.5885279178619385,
      "learning_rate": 4.266666666666667e-05,
      "loss": 0.0038,
      "step": 17600
    },
    {
      "epoch": 1.174,
      "grad_norm": 0.25147706270217896,
      "learning_rate": 4.26625e-05,
      "loss": 0.002,
      "step": 17610
    },
    {
      "epoch": 1.1746666666666667,
      "grad_norm": 0.13697664439678192,
      "learning_rate": 4.265833333333333e-05,
      "loss": 0.0024,
      "step": 17620
    },
    {
      "epoch": 1.1753333333333333,
      "grad_norm": 0.3974483907222748,
      "learning_rate": 4.265416666666667e-05,
      "loss": 0.0038,
      "step": 17630
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.11278647929430008,
      "learning_rate": 4.265e-05,
      "loss": 0.002,
      "step": 17640
    },
    {
      "epoch": 1.1766666666666667,
      "grad_norm": 0.6423064470291138,
      "learning_rate": 4.264583333333334e-05,
      "loss": 0.0052,
      "step": 17650
    },
    {
      "epoch": 1.1773333333333333,
      "grad_norm": 0.5540502667427063,
      "learning_rate": 4.264166666666667e-05,
      "loss": 0.0031,
      "step": 17660
    },
    {
      "epoch": 1.178,
      "grad_norm": 0.1334056556224823,
      "learning_rate": 4.26375e-05,
      "loss": 0.0028,
      "step": 17670
    },
    {
      "epoch": 1.1786666666666668,
      "grad_norm": 0.20777322351932526,
      "learning_rate": 4.263333333333334e-05,
      "loss": 0.0018,
      "step": 17680
    },
    {
      "epoch": 1.1793333333333333,
      "grad_norm": 0.9543066620826721,
      "learning_rate": 4.262916666666667e-05,
      "loss": 0.0027,
      "step": 17690
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.8437192440032959,
      "learning_rate": 4.2625000000000006e-05,
      "loss": 0.0022,
      "step": 17700
    },
    {
      "epoch": 1.1806666666666668,
      "grad_norm": 0.9841067790985107,
      "learning_rate": 4.262083333333334e-05,
      "loss": 0.0029,
      "step": 17710
    },
    {
      "epoch": 1.1813333333333333,
      "grad_norm": 0.24532201886177063,
      "learning_rate": 4.261666666666667e-05,
      "loss": 0.0027,
      "step": 17720
    },
    {
      "epoch": 1.182,
      "grad_norm": 0.16024866700172424,
      "learning_rate": 4.26125e-05,
      "loss": 0.0031,
      "step": 17730
    },
    {
      "epoch": 1.1826666666666668,
      "grad_norm": 0.09877262264490128,
      "learning_rate": 4.260833333333334e-05,
      "loss": 0.0033,
      "step": 17740
    },
    {
      "epoch": 1.1833333333333333,
      "grad_norm": 0.1639205515384674,
      "learning_rate": 4.260416666666667e-05,
      "loss": 0.0019,
      "step": 17750
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.13721603155136108,
      "learning_rate": 4.26e-05,
      "loss": 0.0022,
      "step": 17760
    },
    {
      "epoch": 1.1846666666666668,
      "grad_norm": 0.05594771355390549,
      "learning_rate": 4.2595833333333336e-05,
      "loss": 0.0026,
      "step": 17770
    },
    {
      "epoch": 1.1853333333333333,
      "grad_norm": 0.7231850624084473,
      "learning_rate": 4.259166666666667e-05,
      "loss": 0.0022,
      "step": 17780
    },
    {
      "epoch": 1.186,
      "grad_norm": 0.34145352244377136,
      "learning_rate": 4.2587500000000005e-05,
      "loss": 0.0027,
      "step": 17790
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.43824535608291626,
      "learning_rate": 4.2583333333333336e-05,
      "loss": 0.0017,
      "step": 17800
    },
    {
      "epoch": 1.1873333333333334,
      "grad_norm": 0.5232293605804443,
      "learning_rate": 4.257916666666667e-05,
      "loss": 0.0024,
      "step": 17810
    },
    {
      "epoch": 1.188,
      "grad_norm": 0.27255362272262573,
      "learning_rate": 4.2575000000000005e-05,
      "loss": 0.0034,
      "step": 17820
    },
    {
      "epoch": 1.1886666666666668,
      "grad_norm": 0.33556684851646423,
      "learning_rate": 4.2570833333333336e-05,
      "loss": 0.0033,
      "step": 17830
    },
    {
      "epoch": 1.1893333333333334,
      "grad_norm": 0.2936986982822418,
      "learning_rate": 4.2566666666666674e-05,
      "loss": 0.0024,
      "step": 17840
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.21527788043022156,
      "learning_rate": 4.25625e-05,
      "loss": 0.0032,
      "step": 17850
    },
    {
      "epoch": 1.1906666666666668,
      "grad_norm": 0.4787556529045105,
      "learning_rate": 4.2558333333333336e-05,
      "loss": 0.0035,
      "step": 17860
    },
    {
      "epoch": 1.1913333333333334,
      "grad_norm": 0.09777940064668655,
      "learning_rate": 4.2554166666666667e-05,
      "loss": 0.0038,
      "step": 17870
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.3093728721141815,
      "learning_rate": 4.2550000000000004e-05,
      "loss": 0.0031,
      "step": 17880
    },
    {
      "epoch": 1.1926666666666668,
      "grad_norm": 0.5925867557525635,
      "learning_rate": 4.2545833333333335e-05,
      "loss": 0.003,
      "step": 17890
    },
    {
      "epoch": 1.1933333333333334,
      "grad_norm": 0.11591342091560364,
      "learning_rate": 4.2541666666666666e-05,
      "loss": 0.0033,
      "step": 17900
    },
    {
      "epoch": 1.194,
      "grad_norm": 0.2502188980579376,
      "learning_rate": 4.2537500000000004e-05,
      "loss": 0.0017,
      "step": 17910
    },
    {
      "epoch": 1.1946666666666665,
      "grad_norm": 0.40733981132507324,
      "learning_rate": 4.2533333333333335e-05,
      "loss": 0.0029,
      "step": 17920
    },
    {
      "epoch": 1.1953333333333334,
      "grad_norm": 0.8201043605804443,
      "learning_rate": 4.252916666666667e-05,
      "loss": 0.0025,
      "step": 17930
    },
    {
      "epoch": 1.196,
      "grad_norm": 1.0556780099868774,
      "learning_rate": 4.2525000000000004e-05,
      "loss": 0.0025,
      "step": 17940
    },
    {
      "epoch": 1.1966666666666668,
      "grad_norm": 0.3760412037372589,
      "learning_rate": 4.2520833333333335e-05,
      "loss": 0.0033,
      "step": 17950
    },
    {
      "epoch": 1.1973333333333334,
      "grad_norm": 0.08521221578121185,
      "learning_rate": 4.251666666666667e-05,
      "loss": 0.002,
      "step": 17960
    },
    {
      "epoch": 1.198,
      "grad_norm": 0.581464946269989,
      "learning_rate": 4.2512499999999997e-05,
      "loss": 0.0025,
      "step": 17970
    },
    {
      "epoch": 1.1986666666666665,
      "grad_norm": 0.3946532905101776,
      "learning_rate": 4.2508333333333334e-05,
      "loss": 0.0024,
      "step": 17980
    },
    {
      "epoch": 1.1993333333333334,
      "grad_norm": 0.9881945848464966,
      "learning_rate": 4.2504166666666665e-05,
      "loss": 0.0033,
      "step": 17990
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.0601205825805664,
      "learning_rate": 4.25e-05,
      "loss": 0.0026,
      "step": 18000
    },
    {
      "epoch": 1.2006666666666668,
      "grad_norm": 0.6968905925750732,
      "learning_rate": 4.2495833333333334e-05,
      "loss": 0.0029,
      "step": 18010
    },
    {
      "epoch": 1.2013333333333334,
      "grad_norm": 0.3233848214149475,
      "learning_rate": 4.249166666666667e-05,
      "loss": 0.0026,
      "step": 18020
    },
    {
      "epoch": 1.202,
      "grad_norm": 0.6506301164627075,
      "learning_rate": 4.24875e-05,
      "loss": 0.0031,
      "step": 18030
    },
    {
      "epoch": 1.2026666666666666,
      "grad_norm": 0.1070367768406868,
      "learning_rate": 4.2483333333333334e-05,
      "loss": 0.0022,
      "step": 18040
    },
    {
      "epoch": 1.2033333333333334,
      "grad_norm": 0.03685839846730232,
      "learning_rate": 4.247916666666667e-05,
      "loss": 0.002,
      "step": 18050
    },
    {
      "epoch": 1.204,
      "grad_norm": 0.19430501759052277,
      "learning_rate": 4.2475e-05,
      "loss": 0.0018,
      "step": 18060
    },
    {
      "epoch": 1.2046666666666668,
      "grad_norm": 0.23306898772716522,
      "learning_rate": 4.247083333333334e-05,
      "loss": 0.0029,
      "step": 18070
    },
    {
      "epoch": 1.2053333333333334,
      "grad_norm": 0.0956631526350975,
      "learning_rate": 4.246666666666667e-05,
      "loss": 0.0034,
      "step": 18080
    },
    {
      "epoch": 1.206,
      "grad_norm": 0.3964204490184784,
      "learning_rate": 4.24625e-05,
      "loss": 0.0025,
      "step": 18090
    },
    {
      "epoch": 1.2066666666666666,
      "grad_norm": 0.3951898515224457,
      "learning_rate": 4.245833333333333e-05,
      "loss": 0.0025,
      "step": 18100
    },
    {
      "epoch": 1.2073333333333334,
      "grad_norm": 0.14279857277870178,
      "learning_rate": 4.2454166666666664e-05,
      "loss": 0.0036,
      "step": 18110
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.9548529982566833,
      "learning_rate": 4.245e-05,
      "loss": 0.0041,
      "step": 18120
    },
    {
      "epoch": 1.2086666666666668,
      "grad_norm": 0.19394126534461975,
      "learning_rate": 4.244583333333333e-05,
      "loss": 0.0021,
      "step": 18130
    },
    {
      "epoch": 1.2093333333333334,
      "grad_norm": 0.1009499579668045,
      "learning_rate": 4.244166666666667e-05,
      "loss": 0.0034,
      "step": 18140
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.9041916131973267,
      "learning_rate": 4.24375e-05,
      "loss": 0.0036,
      "step": 18150
    },
    {
      "epoch": 1.2106666666666666,
      "grad_norm": 0.401198148727417,
      "learning_rate": 4.243333333333334e-05,
      "loss": 0.003,
      "step": 18160
    },
    {
      "epoch": 1.2113333333333334,
      "grad_norm": 0.5310690999031067,
      "learning_rate": 4.242916666666667e-05,
      "loss": 0.0046,
      "step": 18170
    },
    {
      "epoch": 1.212,
      "grad_norm": 0.44379130005836487,
      "learning_rate": 4.2425e-05,
      "loss": 0.0027,
      "step": 18180
    },
    {
      "epoch": 1.2126666666666668,
      "grad_norm": 0.8390856981277466,
      "learning_rate": 4.242083333333334e-05,
      "loss": 0.0017,
      "step": 18190
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.20873652398586273,
      "learning_rate": 4.241666666666667e-05,
      "loss": 0.0033,
      "step": 18200
    },
    {
      "epoch": 1.214,
      "grad_norm": 0.31028056144714355,
      "learning_rate": 4.24125e-05,
      "loss": 0.0029,
      "step": 18210
    },
    {
      "epoch": 1.2146666666666666,
      "grad_norm": 0.2617231607437134,
      "learning_rate": 4.240833333333333e-05,
      "loss": 0.0046,
      "step": 18220
    },
    {
      "epoch": 1.2153333333333334,
      "grad_norm": 0.12382151931524277,
      "learning_rate": 4.240416666666667e-05,
      "loss": 0.0033,
      "step": 18230
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.4362099766731262,
      "learning_rate": 4.24e-05,
      "loss": 0.0024,
      "step": 18240
    },
    {
      "epoch": 1.2166666666666668,
      "grad_norm": 0.9938681125640869,
      "learning_rate": 4.239583333333333e-05,
      "loss": 0.0035,
      "step": 18250
    },
    {
      "epoch": 1.2173333333333334,
      "grad_norm": 0.6748813390731812,
      "learning_rate": 4.239166666666667e-05,
      "loss": 0.0019,
      "step": 18260
    },
    {
      "epoch": 1.218,
      "grad_norm": 0.40477049350738525,
      "learning_rate": 4.23875e-05,
      "loss": 0.0026,
      "step": 18270
    },
    {
      "epoch": 1.2186666666666666,
      "grad_norm": 0.16860659420490265,
      "learning_rate": 4.238333333333334e-05,
      "loss": 0.003,
      "step": 18280
    },
    {
      "epoch": 1.2193333333333334,
      "grad_norm": 0.08150430023670197,
      "learning_rate": 4.237916666666667e-05,
      "loss": 0.0034,
      "step": 18290
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.5733270049095154,
      "learning_rate": 4.237500000000001e-05,
      "loss": 0.0021,
      "step": 18300
    },
    {
      "epoch": 1.2206666666666668,
      "grad_norm": 0.1177632138133049,
      "learning_rate": 4.237083333333334e-05,
      "loss": 0.0049,
      "step": 18310
    },
    {
      "epoch": 1.2213333333333334,
      "grad_norm": 0.304506778717041,
      "learning_rate": 4.236666666666667e-05,
      "loss": 0.0025,
      "step": 18320
    },
    {
      "epoch": 1.222,
      "grad_norm": 0.5707967877388,
      "learning_rate": 4.23625e-05,
      "loss": 0.0023,
      "step": 18330
    },
    {
      "epoch": 1.2226666666666666,
      "grad_norm": 0.38678526878356934,
      "learning_rate": 4.235833333333333e-05,
      "loss": 0.0016,
      "step": 18340
    },
    {
      "epoch": 1.2233333333333334,
      "grad_norm": 0.9732305407524109,
      "learning_rate": 4.235416666666667e-05,
      "loss": 0.0032,
      "step": 18350
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.45473048090934753,
      "learning_rate": 4.235e-05,
      "loss": 0.0025,
      "step": 18360
    },
    {
      "epoch": 1.2246666666666666,
      "grad_norm": 0.08808263391256332,
      "learning_rate": 4.234583333333334e-05,
      "loss": 0.0023,
      "step": 18370
    },
    {
      "epoch": 1.2253333333333334,
      "grad_norm": 0.2583725154399872,
      "learning_rate": 4.234166666666667e-05,
      "loss": 0.0028,
      "step": 18380
    },
    {
      "epoch": 1.226,
      "grad_norm": 0.22821007668972015,
      "learning_rate": 4.23375e-05,
      "loss": 0.0029,
      "step": 18390
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.16919313371181488,
      "learning_rate": 4.233333333333334e-05,
      "loss": 0.0017,
      "step": 18400
    },
    {
      "epoch": 1.2273333333333334,
      "grad_norm": 0.1491602212190628,
      "learning_rate": 4.232916666666667e-05,
      "loss": 0.0029,
      "step": 18410
    },
    {
      "epoch": 1.228,
      "grad_norm": 0.6657520532608032,
      "learning_rate": 4.2325000000000006e-05,
      "loss": 0.002,
      "step": 18420
    },
    {
      "epoch": 1.2286666666666666,
      "grad_norm": 0.09776560962200165,
      "learning_rate": 4.2320833333333337e-05,
      "loss": 0.0018,
      "step": 18430
    },
    {
      "epoch": 1.2293333333333334,
      "grad_norm": 0.4258517324924469,
      "learning_rate": 4.2316666666666674e-05,
      "loss": 0.0025,
      "step": 18440
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.32818835973739624,
      "learning_rate": 4.23125e-05,
      "loss": 0.0043,
      "step": 18450
    },
    {
      "epoch": 1.2306666666666666,
      "grad_norm": 0.2602196931838989,
      "learning_rate": 4.2308333333333336e-05,
      "loss": 0.0019,
      "step": 18460
    },
    {
      "epoch": 1.2313333333333334,
      "grad_norm": 0.38768553733825684,
      "learning_rate": 4.230416666666667e-05,
      "loss": 0.0022,
      "step": 18470
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.1214609146118164,
      "learning_rate": 4.23e-05,
      "loss": 0.0039,
      "step": 18480
    },
    {
      "epoch": 1.2326666666666666,
      "grad_norm": 0.9214126467704773,
      "learning_rate": 4.2295833333333336e-05,
      "loss": 0.0025,
      "step": 18490
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 0.8150554895401001,
      "learning_rate": 4.229166666666667e-05,
      "loss": 0.0036,
      "step": 18500
    },
    {
      "epoch": 1.234,
      "grad_norm": 0.7425876259803772,
      "learning_rate": 4.2287500000000005e-05,
      "loss": 0.0038,
      "step": 18510
    },
    {
      "epoch": 1.2346666666666666,
      "grad_norm": 0.7534584999084473,
      "learning_rate": 4.2283333333333336e-05,
      "loss": 0.0047,
      "step": 18520
    },
    {
      "epoch": 1.2353333333333334,
      "grad_norm": 0.7900316119194031,
      "learning_rate": 4.2279166666666667e-05,
      "loss": 0.0028,
      "step": 18530
    },
    {
      "epoch": 1.236,
      "grad_norm": 0.6538499593734741,
      "learning_rate": 4.2275000000000004e-05,
      "loss": 0.0017,
      "step": 18540
    },
    {
      "epoch": 1.2366666666666666,
      "grad_norm": 1.172054409980774,
      "learning_rate": 4.2270833333333335e-05,
      "loss": 0.0026,
      "step": 18550
    },
    {
      "epoch": 1.2373333333333334,
      "grad_norm": 0.499604731798172,
      "learning_rate": 4.226666666666667e-05,
      "loss": 0.0027,
      "step": 18560
    },
    {
      "epoch": 1.238,
      "grad_norm": 0.6313722729682922,
      "learning_rate": 4.22625e-05,
      "loss": 0.0028,
      "step": 18570
    },
    {
      "epoch": 1.2386666666666666,
      "grad_norm": 0.8537113666534424,
      "learning_rate": 4.2258333333333335e-05,
      "loss": 0.0025,
      "step": 18580
    },
    {
      "epoch": 1.2393333333333334,
      "grad_norm": 0.3374284505844116,
      "learning_rate": 4.2254166666666666e-05,
      "loss": 0.0028,
      "step": 18590
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.5187137126922607,
      "learning_rate": 4.2250000000000004e-05,
      "loss": 0.0026,
      "step": 18600
    },
    {
      "epoch": 1.2406666666666666,
      "grad_norm": 0.3450215458869934,
      "learning_rate": 4.2245833333333335e-05,
      "loss": 0.0029,
      "step": 18610
    },
    {
      "epoch": 1.2413333333333334,
      "grad_norm": 0.6030356287956238,
      "learning_rate": 4.2241666666666666e-05,
      "loss": 0.003,
      "step": 18620
    },
    {
      "epoch": 1.242,
      "grad_norm": 0.2664659023284912,
      "learning_rate": 4.2237500000000003e-05,
      "loss": 0.0033,
      "step": 18630
    },
    {
      "epoch": 1.2426666666666666,
      "grad_norm": 0.6945118308067322,
      "learning_rate": 4.2233333333333334e-05,
      "loss": 0.0033,
      "step": 18640
    },
    {
      "epoch": 1.2433333333333334,
      "grad_norm": 0.553815484046936,
      "learning_rate": 4.222916666666667e-05,
      "loss": 0.0023,
      "step": 18650
    },
    {
      "epoch": 1.244,
      "grad_norm": 0.5293287634849548,
      "learning_rate": 4.2225e-05,
      "loss": 0.0029,
      "step": 18660
    },
    {
      "epoch": 1.2446666666666666,
      "grad_norm": 0.09952086955308914,
      "learning_rate": 4.2220833333333334e-05,
      "loss": 0.0047,
      "step": 18670
    },
    {
      "epoch": 1.2453333333333334,
      "grad_norm": 0.7210724353790283,
      "learning_rate": 4.221666666666667e-05,
      "loss": 0.0022,
      "step": 18680
    },
    {
      "epoch": 1.246,
      "grad_norm": 0.2973707616329193,
      "learning_rate": 4.2212499999999996e-05,
      "loss": 0.0022,
      "step": 18690
    },
    {
      "epoch": 1.2466666666666666,
      "grad_norm": 0.44024717807769775,
      "learning_rate": 4.2208333333333334e-05,
      "loss": 0.0037,
      "step": 18700
    },
    {
      "epoch": 1.2473333333333334,
      "grad_norm": 0.26593017578125,
      "learning_rate": 4.2204166666666665e-05,
      "loss": 0.0025,
      "step": 18710
    },
    {
      "epoch": 1.248,
      "grad_norm": 1.0272551774978638,
      "learning_rate": 4.22e-05,
      "loss": 0.0025,
      "step": 18720
    },
    {
      "epoch": 1.2486666666666666,
      "grad_norm": 1.0671108961105347,
      "learning_rate": 4.2195833333333334e-05,
      "loss": 0.0024,
      "step": 18730
    },
    {
      "epoch": 1.2493333333333334,
      "grad_norm": 0.3093143105506897,
      "learning_rate": 4.219166666666667e-05,
      "loss": 0.0021,
      "step": 18740
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.461513489484787,
      "learning_rate": 4.21875e-05,
      "loss": 0.0022,
      "step": 18750
    },
    {
      "epoch": 1.2506666666666666,
      "grad_norm": 0.5991500020027161,
      "learning_rate": 4.218333333333333e-05,
      "loss": 0.004,
      "step": 18760
    },
    {
      "epoch": 1.2513333333333334,
      "grad_norm": 0.7178164720535278,
      "learning_rate": 4.217916666666667e-05,
      "loss": 0.0018,
      "step": 18770
    },
    {
      "epoch": 1.252,
      "grad_norm": 0.9762794375419617,
      "learning_rate": 4.2175e-05,
      "loss": 0.0027,
      "step": 18780
    },
    {
      "epoch": 1.2526666666666666,
      "grad_norm": 0.05332712456583977,
      "learning_rate": 4.217083333333334e-05,
      "loss": 0.002,
      "step": 18790
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.9341694116592407,
      "learning_rate": 4.216666666666667e-05,
      "loss": 0.0044,
      "step": 18800
    },
    {
      "epoch": 1.254,
      "grad_norm": 0.24740491807460785,
      "learning_rate": 4.21625e-05,
      "loss": 0.0036,
      "step": 18810
    },
    {
      "epoch": 1.2546666666666666,
      "grad_norm": 0.6034910082817078,
      "learning_rate": 4.215833333333333e-05,
      "loss": 0.0029,
      "step": 18820
    },
    {
      "epoch": 1.2553333333333334,
      "grad_norm": 0.9574969410896301,
      "learning_rate": 4.2154166666666664e-05,
      "loss": 0.0031,
      "step": 18830
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.05933641642332077,
      "learning_rate": 4.215e-05,
      "loss": 0.0029,
      "step": 18840
    },
    {
      "epoch": 1.2566666666666666,
      "grad_norm": 0.4619636535644531,
      "learning_rate": 4.214583333333333e-05,
      "loss": 0.003,
      "step": 18850
    },
    {
      "epoch": 1.2573333333333334,
      "grad_norm": 0.4707726538181305,
      "learning_rate": 4.214166666666667e-05,
      "loss": 0.003,
      "step": 18860
    },
    {
      "epoch": 1.258,
      "grad_norm": 0.7014325857162476,
      "learning_rate": 4.21375e-05,
      "loss": 0.0022,
      "step": 18870
    },
    {
      "epoch": 1.2586666666666666,
      "grad_norm": 0.38178274035453796,
      "learning_rate": 4.213333333333334e-05,
      "loss": 0.0043,
      "step": 18880
    },
    {
      "epoch": 1.2593333333333334,
      "grad_norm": 1.364989161491394,
      "learning_rate": 4.212916666666667e-05,
      "loss": 0.002,
      "step": 18890
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.31286564469337463,
      "learning_rate": 4.2125e-05,
      "loss": 0.0022,
      "step": 18900
    },
    {
      "epoch": 1.2606666666666666,
      "grad_norm": 0.3908011317253113,
      "learning_rate": 4.212083333333334e-05,
      "loss": 0.0027,
      "step": 18910
    },
    {
      "epoch": 1.2613333333333334,
      "grad_norm": 0.8569197654724121,
      "learning_rate": 4.211666666666667e-05,
      "loss": 0.0035,
      "step": 18920
    },
    {
      "epoch": 1.262,
      "grad_norm": 0.74076247215271,
      "learning_rate": 4.211250000000001e-05,
      "loss": 0.003,
      "step": 18930
    },
    {
      "epoch": 1.2626666666666666,
      "grad_norm": 0.737048327922821,
      "learning_rate": 4.210833333333333e-05,
      "loss": 0.0022,
      "step": 18940
    },
    {
      "epoch": 1.2633333333333332,
      "grad_norm": 0.5027147531509399,
      "learning_rate": 4.210416666666667e-05,
      "loss": 0.0022,
      "step": 18950
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.6663207411766052,
      "learning_rate": 4.21e-05,
      "loss": 0.0022,
      "step": 18960
    },
    {
      "epoch": 1.2646666666666666,
      "grad_norm": 0.1398841291666031,
      "learning_rate": 4.209583333333333e-05,
      "loss": 0.0029,
      "step": 18970
    },
    {
      "epoch": 1.2653333333333334,
      "grad_norm": 0.5088528394699097,
      "learning_rate": 4.209166666666667e-05,
      "loss": 0.004,
      "step": 18980
    },
    {
      "epoch": 1.266,
      "grad_norm": 0.17235732078552246,
      "learning_rate": 4.20875e-05,
      "loss": 0.0025,
      "step": 18990
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.748904824256897,
      "learning_rate": 4.208333333333334e-05,
      "loss": 0.0032,
      "step": 19000
    },
    {
      "epoch": 1.2673333333333332,
      "grad_norm": 0.3365386724472046,
      "learning_rate": 4.207916666666667e-05,
      "loss": 0.0039,
      "step": 19010
    },
    {
      "epoch": 1.268,
      "grad_norm": 0.6297146677970886,
      "learning_rate": 4.2075000000000006e-05,
      "loss": 0.0028,
      "step": 19020
    },
    {
      "epoch": 1.2686666666666666,
      "grad_norm": 0.38243404030799866,
      "learning_rate": 4.207083333333334e-05,
      "loss": 0.0034,
      "step": 19030
    },
    {
      "epoch": 1.2693333333333334,
      "grad_norm": 0.8482142686843872,
      "learning_rate": 4.206666666666667e-05,
      "loss": 0.0029,
      "step": 19040
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.09194524586200714,
      "learning_rate": 4.2062500000000006e-05,
      "loss": 0.0019,
      "step": 19050
    },
    {
      "epoch": 1.2706666666666666,
      "grad_norm": 0.2627158463001251,
      "learning_rate": 4.205833333333333e-05,
      "loss": 0.0013,
      "step": 19060
    },
    {
      "epoch": 1.2713333333333332,
      "grad_norm": 0.5707187652587891,
      "learning_rate": 4.205416666666667e-05,
      "loss": 0.003,
      "step": 19070
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.7687414288520813,
      "learning_rate": 4.205e-05,
      "loss": 0.0021,
      "step": 19080
    },
    {
      "epoch": 1.2726666666666666,
      "grad_norm": 0.32029250264167786,
      "learning_rate": 4.204583333333334e-05,
      "loss": 0.0019,
      "step": 19090
    },
    {
      "epoch": 1.2733333333333334,
      "grad_norm": 0.5460361838340759,
      "learning_rate": 4.204166666666667e-05,
      "loss": 0.0025,
      "step": 19100
    },
    {
      "epoch": 1.274,
      "grad_norm": 1.1947776079177856,
      "learning_rate": 4.20375e-05,
      "loss": 0.0034,
      "step": 19110
    },
    {
      "epoch": 1.2746666666666666,
      "grad_norm": 0.8044322729110718,
      "learning_rate": 4.2033333333333336e-05,
      "loss": 0.0016,
      "step": 19120
    },
    {
      "epoch": 1.2753333333333332,
      "grad_norm": 0.1344808042049408,
      "learning_rate": 4.202916666666667e-05,
      "loss": 0.0031,
      "step": 19130
    },
    {
      "epoch": 1.276,
      "grad_norm": 0.8732065558433533,
      "learning_rate": 4.2025000000000005e-05,
      "loss": 0.0027,
      "step": 19140
    },
    {
      "epoch": 1.2766666666666666,
      "grad_norm": 0.44703781604766846,
      "learning_rate": 4.2020833333333336e-05,
      "loss": 0.0029,
      "step": 19150
    },
    {
      "epoch": 1.2773333333333334,
      "grad_norm": 0.7383649945259094,
      "learning_rate": 4.2016666666666674e-05,
      "loss": 0.0016,
      "step": 19160
    },
    {
      "epoch": 1.278,
      "grad_norm": 0.48508694767951965,
      "learning_rate": 4.2012500000000005e-05,
      "loss": 0.0034,
      "step": 19170
    },
    {
      "epoch": 1.2786666666666666,
      "grad_norm": 0.3536411225795746,
      "learning_rate": 4.2008333333333336e-05,
      "loss": 0.0019,
      "step": 19180
    },
    {
      "epoch": 1.2793333333333332,
      "grad_norm": 0.551531970500946,
      "learning_rate": 4.200416666666667e-05,
      "loss": 0.0025,
      "step": 19190
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.5569199323654175,
      "learning_rate": 4.2e-05,
      "loss": 0.0028,
      "step": 19200
    },
    {
      "epoch": 1.2806666666666666,
      "grad_norm": 0.21971477568149567,
      "learning_rate": 4.1995833333333335e-05,
      "loss": 0.0031,
      "step": 19210
    },
    {
      "epoch": 1.2813333333333334,
      "grad_norm": 0.2575913965702057,
      "learning_rate": 4.1991666666666666e-05,
      "loss": 0.0037,
      "step": 19220
    },
    {
      "epoch": 1.282,
      "grad_norm": 1.0125553607940674,
      "learning_rate": 4.1987500000000004e-05,
      "loss": 0.0022,
      "step": 19230
    },
    {
      "epoch": 1.2826666666666666,
      "grad_norm": 0.8481112122535706,
      "learning_rate": 4.1983333333333335e-05,
      "loss": 0.0043,
      "step": 19240
    },
    {
      "epoch": 1.2833333333333332,
      "grad_norm": 0.3364207148551941,
      "learning_rate": 4.1979166666666666e-05,
      "loss": 0.0031,
      "step": 19250
    },
    {
      "epoch": 1.284,
      "grad_norm": 0.792439341545105,
      "learning_rate": 4.1975000000000004e-05,
      "loss": 0.0032,
      "step": 19260
    },
    {
      "epoch": 1.2846666666666666,
      "grad_norm": 1.0344587564468384,
      "learning_rate": 4.1970833333333335e-05,
      "loss": 0.0033,
      "step": 19270
    },
    {
      "epoch": 1.2853333333333334,
      "grad_norm": 0.48783886432647705,
      "learning_rate": 4.196666666666667e-05,
      "loss": 0.0018,
      "step": 19280
    },
    {
      "epoch": 1.286,
      "grad_norm": 0.040258847177028656,
      "learning_rate": 4.1962500000000004e-05,
      "loss": 0.0034,
      "step": 19290
    },
    {
      "epoch": 1.2866666666666666,
      "grad_norm": 0.6158556938171387,
      "learning_rate": 4.1958333333333335e-05,
      "loss": 0.0039,
      "step": 19300
    },
    {
      "epoch": 1.2873333333333332,
      "grad_norm": 1.4035725593566895,
      "learning_rate": 4.1954166666666665e-05,
      "loss": 0.002,
      "step": 19310
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.31263482570648193,
      "learning_rate": 4.195e-05,
      "loss": 0.0028,
      "step": 19320
    },
    {
      "epoch": 1.2886666666666666,
      "grad_norm": 0.38205409049987793,
      "learning_rate": 4.1945833333333334e-05,
      "loss": 0.0027,
      "step": 19330
    },
    {
      "epoch": 1.2893333333333334,
      "grad_norm": 0.45188790559768677,
      "learning_rate": 4.1941666666666665e-05,
      "loss": 0.0033,
      "step": 19340
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.7657871246337891,
      "learning_rate": 4.19375e-05,
      "loss": 0.0029,
      "step": 19350
    },
    {
      "epoch": 1.2906666666666666,
      "grad_norm": 0.8966028690338135,
      "learning_rate": 4.1933333333333334e-05,
      "loss": 0.003,
      "step": 19360
    },
    {
      "epoch": 1.2913333333333332,
      "grad_norm": 0.41492947936058044,
      "learning_rate": 4.192916666666667e-05,
      "loss": 0.0037,
      "step": 19370
    },
    {
      "epoch": 1.292,
      "grad_norm": 0.09066565334796906,
      "learning_rate": 4.1925e-05,
      "loss": 0.0031,
      "step": 19380
    },
    {
      "epoch": 1.2926666666666666,
      "grad_norm": 0.5882505178451538,
      "learning_rate": 4.1920833333333334e-05,
      "loss": 0.0022,
      "step": 19390
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.6901441812515259,
      "learning_rate": 4.191666666666667e-05,
      "loss": 0.0024,
      "step": 19400
    },
    {
      "epoch": 1.294,
      "grad_norm": 0.9565029144287109,
      "learning_rate": 4.19125e-05,
      "loss": 0.0024,
      "step": 19410
    },
    {
      "epoch": 1.2946666666666666,
      "grad_norm": 0.6011980175971985,
      "learning_rate": 4.190833333333333e-05,
      "loss": 0.0029,
      "step": 19420
    },
    {
      "epoch": 1.2953333333333332,
      "grad_norm": 0.3261764347553253,
      "learning_rate": 4.1904166666666664e-05,
      "loss": 0.0032,
      "step": 19430
    },
    {
      "epoch": 1.296,
      "grad_norm": 1.0026732683181763,
      "learning_rate": 4.19e-05,
      "loss": 0.0025,
      "step": 19440
    },
    {
      "epoch": 1.2966666666666666,
      "grad_norm": 0.7979898452758789,
      "learning_rate": 4.189583333333333e-05,
      "loss": 0.0022,
      "step": 19450
    },
    {
      "epoch": 1.2973333333333334,
      "grad_norm": 0.638335108757019,
      "learning_rate": 4.189166666666667e-05,
      "loss": 0.0033,
      "step": 19460
    },
    {
      "epoch": 1.298,
      "grad_norm": 0.11867257207632065,
      "learning_rate": 4.18875e-05,
      "loss": 0.0019,
      "step": 19470
    },
    {
      "epoch": 1.2986666666666666,
      "grad_norm": 0.7213040590286255,
      "learning_rate": 4.188333333333333e-05,
      "loss": 0.0038,
      "step": 19480
    },
    {
      "epoch": 1.2993333333333332,
      "grad_norm": 0.4275548756122589,
      "learning_rate": 4.187916666666667e-05,
      "loss": 0.003,
      "step": 19490
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.15788599848747253,
      "learning_rate": 4.1875e-05,
      "loss": 0.0027,
      "step": 19500
    },
    {
      "epoch": 1.3006666666666666,
      "grad_norm": 0.26776668429374695,
      "learning_rate": 4.187083333333334e-05,
      "loss": 0.0022,
      "step": 19510
    },
    {
      "epoch": 1.3013333333333335,
      "grad_norm": 0.32298871874809265,
      "learning_rate": 4.186666666666667e-05,
      "loss": 0.0022,
      "step": 19520
    },
    {
      "epoch": 1.302,
      "grad_norm": 0.6545456051826477,
      "learning_rate": 4.18625e-05,
      "loss": 0.0034,
      "step": 19530
    },
    {
      "epoch": 1.3026666666666666,
      "grad_norm": 0.6162941455841064,
      "learning_rate": 4.185833333333333e-05,
      "loss": 0.0029,
      "step": 19540
    },
    {
      "epoch": 1.3033333333333332,
      "grad_norm": 0.14126557111740112,
      "learning_rate": 4.185416666666667e-05,
      "loss": 0.0035,
      "step": 19550
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.396397203207016,
      "learning_rate": 4.185e-05,
      "loss": 0.0025,
      "step": 19560
    },
    {
      "epoch": 1.3046666666666666,
      "grad_norm": 0.0470837764441967,
      "learning_rate": 4.184583333333333e-05,
      "loss": 0.0034,
      "step": 19570
    },
    {
      "epoch": 1.3053333333333335,
      "grad_norm": 0.32542890310287476,
      "learning_rate": 4.184166666666667e-05,
      "loss": 0.0031,
      "step": 19580
    },
    {
      "epoch": 1.306,
      "grad_norm": 0.17222490906715393,
      "learning_rate": 4.18375e-05,
      "loss": 0.0027,
      "step": 19590
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.12076021730899811,
      "learning_rate": 4.183333333333334e-05,
      "loss": 0.0033,
      "step": 19600
    },
    {
      "epoch": 1.3073333333333332,
      "grad_norm": 0.2957140803337097,
      "learning_rate": 4.182916666666667e-05,
      "loss": 0.0027,
      "step": 19610
    },
    {
      "epoch": 1.308,
      "grad_norm": 0.4189658463001251,
      "learning_rate": 4.1825e-05,
      "loss": 0.0033,
      "step": 19620
    },
    {
      "epoch": 1.3086666666666666,
      "grad_norm": 0.8066274523735046,
      "learning_rate": 4.182083333333334e-05,
      "loss": 0.0036,
      "step": 19630
    },
    {
      "epoch": 1.3093333333333335,
      "grad_norm": 0.24932484328746796,
      "learning_rate": 4.181666666666667e-05,
      "loss": 0.0024,
      "step": 19640
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.2584092319011688,
      "learning_rate": 4.181250000000001e-05,
      "loss": 0.0023,
      "step": 19650
    },
    {
      "epoch": 1.3106666666666666,
      "grad_norm": 0.3366481363773346,
      "learning_rate": 4.180833333333333e-05,
      "loss": 0.0028,
      "step": 19660
    },
    {
      "epoch": 1.3113333333333332,
      "grad_norm": 1.3007460832595825,
      "learning_rate": 4.180416666666667e-05,
      "loss": 0.0022,
      "step": 19670
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.5409169793128967,
      "learning_rate": 4.18e-05,
      "loss": 0.0024,
      "step": 19680
    },
    {
      "epoch": 1.3126666666666666,
      "grad_norm": 0.40138837695121765,
      "learning_rate": 4.179583333333334e-05,
      "loss": 0.0025,
      "step": 19690
    },
    {
      "epoch": 1.3133333333333335,
      "grad_norm": 0.08688221871852875,
      "learning_rate": 4.179166666666667e-05,
      "loss": 0.0025,
      "step": 19700
    },
    {
      "epoch": 1.314,
      "grad_norm": 0.07758647203445435,
      "learning_rate": 4.17875e-05,
      "loss": 0.0036,
      "step": 19710
    },
    {
      "epoch": 1.3146666666666667,
      "grad_norm": 0.49551472067832947,
      "learning_rate": 4.178333333333334e-05,
      "loss": 0.003,
      "step": 19720
    },
    {
      "epoch": 1.3153333333333332,
      "grad_norm": 0.3216613829135895,
      "learning_rate": 4.177916666666667e-05,
      "loss": 0.0027,
      "step": 19730
    },
    {
      "epoch": 1.316,
      "grad_norm": 0.2361927181482315,
      "learning_rate": 4.1775000000000006e-05,
      "loss": 0.0028,
      "step": 19740
    },
    {
      "epoch": 1.3166666666666667,
      "grad_norm": 0.6229206919670105,
      "learning_rate": 4.177083333333334e-05,
      "loss": 0.0041,
      "step": 19750
    },
    {
      "epoch": 1.3173333333333335,
      "grad_norm": 1.4048595428466797,
      "learning_rate": 4.176666666666667e-05,
      "loss": 0.0044,
      "step": 19760
    },
    {
      "epoch": 1.318,
      "grad_norm": 0.3951840102672577,
      "learning_rate": 4.1762500000000005e-05,
      "loss": 0.0022,
      "step": 19770
    },
    {
      "epoch": 1.3186666666666667,
      "grad_norm": 0.6201620697975159,
      "learning_rate": 4.175833333333333e-05,
      "loss": 0.004,
      "step": 19780
    },
    {
      "epoch": 1.3193333333333332,
      "grad_norm": 0.6682941317558289,
      "learning_rate": 4.175416666666667e-05,
      "loss": 0.002,
      "step": 19790
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.697148859500885,
      "learning_rate": 4.175e-05,
      "loss": 0.004,
      "step": 19800
    },
    {
      "epoch": 1.3206666666666667,
      "grad_norm": 0.5596291422843933,
      "learning_rate": 4.1745833333333336e-05,
      "loss": 0.0031,
      "step": 19810
    },
    {
      "epoch": 1.3213333333333335,
      "grad_norm": 0.914688766002655,
      "learning_rate": 4.174166666666667e-05,
      "loss": 0.003,
      "step": 19820
    },
    {
      "epoch": 1.322,
      "grad_norm": 0.6342896819114685,
      "learning_rate": 4.1737500000000005e-05,
      "loss": 0.0032,
      "step": 19830
    },
    {
      "epoch": 1.3226666666666667,
      "grad_norm": 0.796278178691864,
      "learning_rate": 4.1733333333333336e-05,
      "loss": 0.0047,
      "step": 19840
    },
    {
      "epoch": 1.3233333333333333,
      "grad_norm": 0.4198700487613678,
      "learning_rate": 4.172916666666667e-05,
      "loss": 0.0033,
      "step": 19850
    },
    {
      "epoch": 1.324,
      "grad_norm": 0.24230194091796875,
      "learning_rate": 4.1725000000000005e-05,
      "loss": 0.0043,
      "step": 19860
    },
    {
      "epoch": 1.3246666666666667,
      "grad_norm": 0.05296439677476883,
      "learning_rate": 4.1720833333333336e-05,
      "loss": 0.0037,
      "step": 19870
    },
    {
      "epoch": 1.3253333333333333,
      "grad_norm": 0.19899214804172516,
      "learning_rate": 4.171666666666667e-05,
      "loss": 0.0025,
      "step": 19880
    },
    {
      "epoch": 1.326,
      "grad_norm": 0.36516639590263367,
      "learning_rate": 4.1712500000000004e-05,
      "loss": 0.0022,
      "step": 19890
    },
    {
      "epoch": 1.3266666666666667,
      "grad_norm": 0.351129412651062,
      "learning_rate": 4.1708333333333335e-05,
      "loss": 0.0029,
      "step": 19900
    },
    {
      "epoch": 1.3273333333333333,
      "grad_norm": 0.5416398644447327,
      "learning_rate": 4.1704166666666666e-05,
      "loss": 0.0027,
      "step": 19910
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.5336840748786926,
      "learning_rate": 4.17e-05,
      "loss": 0.0026,
      "step": 19920
    },
    {
      "epoch": 1.3286666666666667,
      "grad_norm": 0.2943773865699768,
      "learning_rate": 4.1695833333333335e-05,
      "loss": 0.0033,
      "step": 19930
    },
    {
      "epoch": 1.3293333333333333,
      "grad_norm": 0.7706975340843201,
      "learning_rate": 4.1691666666666666e-05,
      "loss": 0.0021,
      "step": 19940
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.6636399030685425,
      "learning_rate": 4.1687500000000004e-05,
      "loss": 0.0027,
      "step": 19950
    },
    {
      "epoch": 1.3306666666666667,
      "grad_norm": 0.0675722137093544,
      "learning_rate": 4.1683333333333335e-05,
      "loss": 0.0031,
      "step": 19960
    },
    {
      "epoch": 1.3313333333333333,
      "grad_norm": 0.8456348180770874,
      "learning_rate": 4.167916666666667e-05,
      "loss": 0.0021,
      "step": 19970
    },
    {
      "epoch": 1.332,
      "grad_norm": 0.10473170876502991,
      "learning_rate": 4.1675e-05,
      "loss": 0.0026,
      "step": 19980
    },
    {
      "epoch": 1.3326666666666667,
      "grad_norm": 1.1279903650283813,
      "learning_rate": 4.1670833333333334e-05,
      "loss": 0.0032,
      "step": 19990
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.6063686013221741,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.0028,
      "step": 20000
    },
    {
      "epoch": 1.334,
      "grad_norm": 0.3554234206676483,
      "learning_rate": 4.16625e-05,
      "loss": 0.0037,
      "step": 20010
    },
    {
      "epoch": 1.3346666666666667,
      "grad_norm": 0.6481096744537354,
      "learning_rate": 4.165833333333334e-05,
      "loss": 0.0034,
      "step": 20020
    },
    {
      "epoch": 1.3353333333333333,
      "grad_norm": 0.6420700550079346,
      "learning_rate": 4.1654166666666665e-05,
      "loss": 0.0026,
      "step": 20030
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.6664952635765076,
      "learning_rate": 4.165e-05,
      "loss": 0.0019,
      "step": 20040
    },
    {
      "epoch": 1.3366666666666667,
      "grad_norm": 0.31693190336227417,
      "learning_rate": 4.1645833333333334e-05,
      "loss": 0.0037,
      "step": 20050
    },
    {
      "epoch": 1.3373333333333333,
      "grad_norm": 0.5434345006942749,
      "learning_rate": 4.1641666666666665e-05,
      "loss": 0.0027,
      "step": 20060
    },
    {
      "epoch": 1.338,
      "grad_norm": 0.9111323952674866,
      "learning_rate": 4.16375e-05,
      "loss": 0.0026,
      "step": 20070
    },
    {
      "epoch": 1.3386666666666667,
      "grad_norm": 0.16293969750404358,
      "learning_rate": 4.1633333333333333e-05,
      "loss": 0.0028,
      "step": 20080
    },
    {
      "epoch": 1.3393333333333333,
      "grad_norm": 0.2405148148536682,
      "learning_rate": 4.162916666666667e-05,
      "loss": 0.0023,
      "step": 20090
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.44678643345832825,
      "learning_rate": 4.1625e-05,
      "loss": 0.0031,
      "step": 20100
    },
    {
      "epoch": 1.3406666666666667,
      "grad_norm": 0.17048369348049164,
      "learning_rate": 4.162083333333334e-05,
      "loss": 0.0029,
      "step": 20110
    },
    {
      "epoch": 1.3413333333333333,
      "grad_norm": 0.3727347254753113,
      "learning_rate": 4.161666666666667e-05,
      "loss": 0.0031,
      "step": 20120
    },
    {
      "epoch": 1.342,
      "grad_norm": 0.2231757491827011,
      "learning_rate": 4.16125e-05,
      "loss": 0.0031,
      "step": 20130
    },
    {
      "epoch": 1.3426666666666667,
      "grad_norm": 0.21877744793891907,
      "learning_rate": 4.160833333333334e-05,
      "loss": 0.0031,
      "step": 20140
    },
    {
      "epoch": 1.3433333333333333,
      "grad_norm": 0.8092394471168518,
      "learning_rate": 4.1604166666666664e-05,
      "loss": 0.0031,
      "step": 20150
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.2865182161331177,
      "learning_rate": 4.16e-05,
      "loss": 0.0022,
      "step": 20160
    },
    {
      "epoch": 1.3446666666666667,
      "grad_norm": 0.2205280214548111,
      "learning_rate": 4.159583333333333e-05,
      "loss": 0.0029,
      "step": 20170
    },
    {
      "epoch": 1.3453333333333333,
      "grad_norm": 0.6746542453765869,
      "learning_rate": 4.159166666666667e-05,
      "loss": 0.0028,
      "step": 20180
    },
    {
      "epoch": 1.346,
      "grad_norm": 0.38312551379203796,
      "learning_rate": 4.15875e-05,
      "loss": 0.0041,
      "step": 20190
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.4455987513065338,
      "learning_rate": 4.158333333333333e-05,
      "loss": 0.0024,
      "step": 20200
    },
    {
      "epoch": 1.3473333333333333,
      "grad_norm": 0.25966182351112366,
      "learning_rate": 4.157916666666667e-05,
      "loss": 0.0026,
      "step": 20210
    },
    {
      "epoch": 1.3479999999999999,
      "grad_norm": 0.5249268412590027,
      "learning_rate": 4.1575e-05,
      "loss": 0.0029,
      "step": 20220
    },
    {
      "epoch": 1.3486666666666667,
      "grad_norm": 0.27353930473327637,
      "learning_rate": 4.157083333333334e-05,
      "loss": 0.0025,
      "step": 20230
    },
    {
      "epoch": 1.3493333333333333,
      "grad_norm": 0.45138251781463623,
      "learning_rate": 4.156666666666667e-05,
      "loss": 0.0024,
      "step": 20240
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.4091966152191162,
      "learning_rate": 4.156250000000001e-05,
      "loss": 0.0023,
      "step": 20250
    },
    {
      "epoch": 1.3506666666666667,
      "grad_norm": 0.6980469822883606,
      "learning_rate": 4.155833333333334e-05,
      "loss": 0.0023,
      "step": 20260
    },
    {
      "epoch": 1.3513333333333333,
      "grad_norm": 0.7084101438522339,
      "learning_rate": 4.155416666666667e-05,
      "loss": 0.0028,
      "step": 20270
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.7132492065429688,
      "learning_rate": 4.155e-05,
      "loss": 0.0021,
      "step": 20280
    },
    {
      "epoch": 1.3526666666666667,
      "grad_norm": 0.45146802067756653,
      "learning_rate": 4.154583333333333e-05,
      "loss": 0.0034,
      "step": 20290
    },
    {
      "epoch": 1.3533333333333333,
      "grad_norm": 0.44565314054489136,
      "learning_rate": 4.154166666666667e-05,
      "loss": 0.0019,
      "step": 20300
    },
    {
      "epoch": 1.354,
      "grad_norm": 0.5071864724159241,
      "learning_rate": 4.15375e-05,
      "loss": 0.0028,
      "step": 20310
    },
    {
      "epoch": 1.3546666666666667,
      "grad_norm": 0.16450658440589905,
      "learning_rate": 4.153333333333334e-05,
      "loss": 0.0028,
      "step": 20320
    },
    {
      "epoch": 1.3553333333333333,
      "grad_norm": 0.9508104920387268,
      "learning_rate": 4.152916666666667e-05,
      "loss": 0.0022,
      "step": 20330
    },
    {
      "epoch": 1.3559999999999999,
      "grad_norm": 0.501818835735321,
      "learning_rate": 4.1525e-05,
      "loss": 0.0023,
      "step": 20340
    },
    {
      "epoch": 1.3566666666666667,
      "grad_norm": 1.111602544784546,
      "learning_rate": 4.152083333333334e-05,
      "loss": 0.0017,
      "step": 20350
    },
    {
      "epoch": 1.3573333333333333,
      "grad_norm": 0.4265870749950409,
      "learning_rate": 4.151666666666667e-05,
      "loss": 0.0028,
      "step": 20360
    },
    {
      "epoch": 1.358,
      "grad_norm": 0.5697423815727234,
      "learning_rate": 4.1512500000000006e-05,
      "loss": 0.0034,
      "step": 20370
    },
    {
      "epoch": 1.3586666666666667,
      "grad_norm": 0.7881168127059937,
      "learning_rate": 4.150833333333334e-05,
      "loss": 0.0033,
      "step": 20380
    },
    {
      "epoch": 1.3593333333333333,
      "grad_norm": 0.32718202471733093,
      "learning_rate": 4.150416666666667e-05,
      "loss": 0.0028,
      "step": 20390
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.42172306776046753,
      "learning_rate": 4.15e-05,
      "loss": 0.0027,
      "step": 20400
    },
    {
      "epoch": 1.3606666666666667,
      "grad_norm": 1.047472596168518,
      "learning_rate": 4.149583333333334e-05,
      "loss": 0.0025,
      "step": 20410
    },
    {
      "epoch": 1.3613333333333333,
      "grad_norm": 0.6239863038063049,
      "learning_rate": 4.149166666666667e-05,
      "loss": 0.0039,
      "step": 20420
    },
    {
      "epoch": 1.362,
      "grad_norm": 0.30672651529312134,
      "learning_rate": 4.14875e-05,
      "loss": 0.0036,
      "step": 20430
    },
    {
      "epoch": 1.3626666666666667,
      "grad_norm": 0.5292906761169434,
      "learning_rate": 4.1483333333333337e-05,
      "loss": 0.0035,
      "step": 20440
    },
    {
      "epoch": 1.3633333333333333,
      "grad_norm": 0.4186714291572571,
      "learning_rate": 4.147916666666667e-05,
      "loss": 0.0019,
      "step": 20450
    },
    {
      "epoch": 1.3639999999999999,
      "grad_norm": 0.1343940645456314,
      "learning_rate": 4.1475000000000005e-05,
      "loss": 0.003,
      "step": 20460
    },
    {
      "epoch": 1.3646666666666667,
      "grad_norm": 0.4810989499092102,
      "learning_rate": 4.1470833333333336e-05,
      "loss": 0.0033,
      "step": 20470
    },
    {
      "epoch": 1.3653333333333333,
      "grad_norm": 1.0149660110473633,
      "learning_rate": 4.146666666666667e-05,
      "loss": 0.0026,
      "step": 20480
    },
    {
      "epoch": 1.366,
      "grad_norm": 0.6332932710647583,
      "learning_rate": 4.1462500000000005e-05,
      "loss": 0.0021,
      "step": 20490
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 0.14431317150592804,
      "learning_rate": 4.1458333333333336e-05,
      "loss": 0.0021,
      "step": 20500
    },
    {
      "epoch": 1.3673333333333333,
      "grad_norm": 0.6930856108665466,
      "learning_rate": 4.145416666666667e-05,
      "loss": 0.0038,
      "step": 20510
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.16928571462631226,
      "learning_rate": 4.145e-05,
      "loss": 0.0023,
      "step": 20520
    },
    {
      "epoch": 1.3686666666666667,
      "grad_norm": 0.3863372802734375,
      "learning_rate": 4.1445833333333336e-05,
      "loss": 0.0038,
      "step": 20530
    },
    {
      "epoch": 1.3693333333333333,
      "grad_norm": 0.21229872107505798,
      "learning_rate": 4.1441666666666667e-05,
      "loss": 0.0023,
      "step": 20540
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.8591098189353943,
      "learning_rate": 4.1437500000000004e-05,
      "loss": 0.0024,
      "step": 20550
    },
    {
      "epoch": 1.3706666666666667,
      "grad_norm": 1.266223669052124,
      "learning_rate": 4.1433333333333335e-05,
      "loss": 0.0022,
      "step": 20560
    },
    {
      "epoch": 1.3713333333333333,
      "grad_norm": 0.5344003438949585,
      "learning_rate": 4.1429166666666666e-05,
      "loss": 0.0031,
      "step": 20570
    },
    {
      "epoch": 1.3719999999999999,
      "grad_norm": 0.8203751444816589,
      "learning_rate": 4.1425000000000004e-05,
      "loss": 0.0034,
      "step": 20580
    },
    {
      "epoch": 1.3726666666666667,
      "grad_norm": 0.6961052417755127,
      "learning_rate": 4.1420833333333335e-05,
      "loss": 0.0021,
      "step": 20590
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.8903898596763611,
      "learning_rate": 4.141666666666667e-05,
      "loss": 0.0026,
      "step": 20600
    },
    {
      "epoch": 1.374,
      "grad_norm": 0.1328544318675995,
      "learning_rate": 4.1412500000000004e-05,
      "loss": 0.0018,
      "step": 20610
    },
    {
      "epoch": 1.3746666666666667,
      "grad_norm": 0.3643953204154968,
      "learning_rate": 4.1408333333333335e-05,
      "loss": 0.0025,
      "step": 20620
    },
    {
      "epoch": 1.3753333333333333,
      "grad_norm": 0.31183961033821106,
      "learning_rate": 4.1404166666666666e-05,
      "loss": 0.0029,
      "step": 20630
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.04674459248781204,
      "learning_rate": 4.14e-05,
      "loss": 0.0027,
      "step": 20640
    },
    {
      "epoch": 1.3766666666666667,
      "grad_norm": 0.18364733457565308,
      "learning_rate": 4.1395833333333334e-05,
      "loss": 0.0021,
      "step": 20650
    },
    {
      "epoch": 1.3773333333333333,
      "grad_norm": 0.5358255505561829,
      "learning_rate": 4.1391666666666665e-05,
      "loss": 0.003,
      "step": 20660
    },
    {
      "epoch": 1.3780000000000001,
      "grad_norm": 0.6593233346939087,
      "learning_rate": 4.13875e-05,
      "loss": 0.0029,
      "step": 20670
    },
    {
      "epoch": 1.3786666666666667,
      "grad_norm": 0.3479982912540436,
      "learning_rate": 4.1383333333333334e-05,
      "loss": 0.0028,
      "step": 20680
    },
    {
      "epoch": 1.3793333333333333,
      "grad_norm": 0.8470985293388367,
      "learning_rate": 4.137916666666667e-05,
      "loss": 0.0023,
      "step": 20690
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.9092565774917603,
      "learning_rate": 4.1375e-05,
      "loss": 0.0028,
      "step": 20700
    },
    {
      "epoch": 1.3806666666666667,
      "grad_norm": 0.4615734815597534,
      "learning_rate": 4.1370833333333334e-05,
      "loss": 0.0043,
      "step": 20710
    },
    {
      "epoch": 1.3813333333333333,
      "grad_norm": 0.2576292157173157,
      "learning_rate": 4.136666666666667e-05,
      "loss": 0.0017,
      "step": 20720
    },
    {
      "epoch": 1.3820000000000001,
      "grad_norm": 0.09737642109394073,
      "learning_rate": 4.13625e-05,
      "loss": 0.0035,
      "step": 20730
    },
    {
      "epoch": 1.3826666666666667,
      "grad_norm": 0.5343316197395325,
      "learning_rate": 4.135833333333334e-05,
      "loss": 0.0021,
      "step": 20740
    },
    {
      "epoch": 1.3833333333333333,
      "grad_norm": 0.5982210636138916,
      "learning_rate": 4.1354166666666664e-05,
      "loss": 0.0027,
      "step": 20750
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.6701486110687256,
      "learning_rate": 4.135e-05,
      "loss": 0.0027,
      "step": 20760
    },
    {
      "epoch": 1.3846666666666667,
      "grad_norm": 0.7968916893005371,
      "learning_rate": 4.134583333333333e-05,
      "loss": 0.0042,
      "step": 20770
    },
    {
      "epoch": 1.3853333333333333,
      "grad_norm": 0.745604395866394,
      "learning_rate": 4.1341666666666664e-05,
      "loss": 0.0034,
      "step": 20780
    },
    {
      "epoch": 1.3860000000000001,
      "grad_norm": 0.12439391016960144,
      "learning_rate": 4.13375e-05,
      "loss": 0.0028,
      "step": 20790
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.17640133202075958,
      "learning_rate": 4.133333333333333e-05,
      "loss": 0.0023,
      "step": 20800
    },
    {
      "epoch": 1.3873333333333333,
      "grad_norm": 0.8459435105323792,
      "learning_rate": 4.132916666666667e-05,
      "loss": 0.0025,
      "step": 20810
    },
    {
      "epoch": 1.388,
      "grad_norm": 1.3282837867736816,
      "learning_rate": 4.1325e-05,
      "loss": 0.0017,
      "step": 20820
    },
    {
      "epoch": 1.3886666666666667,
      "grad_norm": 0.31105345487594604,
      "learning_rate": 4.132083333333334e-05,
      "loss": 0.003,
      "step": 20830
    },
    {
      "epoch": 1.3893333333333333,
      "grad_norm": 0.43567073345184326,
      "learning_rate": 4.131666666666667e-05,
      "loss": 0.004,
      "step": 20840
    },
    {
      "epoch": 1.3900000000000001,
      "grad_norm": 0.1676032543182373,
      "learning_rate": 4.13125e-05,
      "loss": 0.003,
      "step": 20850
    },
    {
      "epoch": 1.3906666666666667,
      "grad_norm": 0.0634065642952919,
      "learning_rate": 4.130833333333334e-05,
      "loss": 0.0033,
      "step": 20860
    },
    {
      "epoch": 1.3913333333333333,
      "grad_norm": 1.1489602327346802,
      "learning_rate": 4.130416666666666e-05,
      "loss": 0.003,
      "step": 20870
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.7242273688316345,
      "learning_rate": 4.13e-05,
      "loss": 0.0032,
      "step": 20880
    },
    {
      "epoch": 1.3926666666666667,
      "grad_norm": 0.7375031113624573,
      "learning_rate": 4.129583333333333e-05,
      "loss": 0.0018,
      "step": 20890
    },
    {
      "epoch": 1.3933333333333333,
      "grad_norm": 0.26431283354759216,
      "learning_rate": 4.129166666666667e-05,
      "loss": 0.0026,
      "step": 20900
    },
    {
      "epoch": 1.3940000000000001,
      "grad_norm": 0.0407237708568573,
      "learning_rate": 4.12875e-05,
      "loss": 0.0029,
      "step": 20910
    },
    {
      "epoch": 1.3946666666666667,
      "grad_norm": 0.3099588453769684,
      "learning_rate": 4.128333333333333e-05,
      "loss": 0.0032,
      "step": 20920
    },
    {
      "epoch": 1.3953333333333333,
      "grad_norm": 0.34438955783843994,
      "learning_rate": 4.127916666666667e-05,
      "loss": 0.002,
      "step": 20930
    },
    {
      "epoch": 1.396,
      "grad_norm": 0.17395728826522827,
      "learning_rate": 4.1275e-05,
      "loss": 0.0024,
      "step": 20940
    },
    {
      "epoch": 1.3966666666666667,
      "grad_norm": 0.6378467679023743,
      "learning_rate": 4.127083333333334e-05,
      "loss": 0.0023,
      "step": 20950
    },
    {
      "epoch": 1.3973333333333333,
      "grad_norm": 1.126569390296936,
      "learning_rate": 4.126666666666667e-05,
      "loss": 0.0041,
      "step": 20960
    },
    {
      "epoch": 1.3980000000000001,
      "grad_norm": 1.1921943426132202,
      "learning_rate": 4.126250000000001e-05,
      "loss": 0.003,
      "step": 20970
    },
    {
      "epoch": 1.3986666666666667,
      "grad_norm": 0.8915072679519653,
      "learning_rate": 4.125833333333334e-05,
      "loss": 0.0025,
      "step": 20980
    },
    {
      "epoch": 1.3993333333333333,
      "grad_norm": 0.27943155169487,
      "learning_rate": 4.125416666666667e-05,
      "loss": 0.0024,
      "step": 20990
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.2256467193365097,
      "learning_rate": 4.125e-05,
      "loss": 0.0025,
      "step": 21000
    },
    {
      "epoch": 1.4006666666666667,
      "grad_norm": 0.12620270252227783,
      "learning_rate": 4.124583333333333e-05,
      "loss": 0.0032,
      "step": 21010
    },
    {
      "epoch": 1.4013333333333333,
      "grad_norm": 0.2117311805486679,
      "learning_rate": 4.124166666666667e-05,
      "loss": 0.0032,
      "step": 21020
    },
    {
      "epoch": 1.4020000000000001,
      "grad_norm": 0.5109361410140991,
      "learning_rate": 4.12375e-05,
      "loss": 0.0034,
      "step": 21030
    },
    {
      "epoch": 1.4026666666666667,
      "grad_norm": 0.8561180830001831,
      "learning_rate": 4.123333333333334e-05,
      "loss": 0.003,
      "step": 21040
    },
    {
      "epoch": 1.4033333333333333,
      "grad_norm": 0.25537148118019104,
      "learning_rate": 4.122916666666667e-05,
      "loss": 0.0025,
      "step": 21050
    },
    {
      "epoch": 1.404,
      "grad_norm": 1.1190626621246338,
      "learning_rate": 4.1225e-05,
      "loss": 0.0029,
      "step": 21060
    },
    {
      "epoch": 1.4046666666666667,
      "grad_norm": 0.503576397895813,
      "learning_rate": 4.122083333333334e-05,
      "loss": 0.0034,
      "step": 21070
    },
    {
      "epoch": 1.4053333333333333,
      "grad_norm": 0.441999614238739,
      "learning_rate": 4.121666666666667e-05,
      "loss": 0.0033,
      "step": 21080
    },
    {
      "epoch": 1.4060000000000001,
      "grad_norm": 0.9578282833099365,
      "learning_rate": 4.1212500000000006e-05,
      "loss": 0.0031,
      "step": 21090
    },
    {
      "epoch": 1.4066666666666667,
      "grad_norm": 0.9529359936714172,
      "learning_rate": 4.120833333333334e-05,
      "loss": 0.0033,
      "step": 21100
    },
    {
      "epoch": 1.4073333333333333,
      "grad_norm": 0.07338850945234299,
      "learning_rate": 4.120416666666667e-05,
      "loss": 0.0024,
      "step": 21110
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.49138903617858887,
      "learning_rate": 4.12e-05,
      "loss": 0.0019,
      "step": 21120
    },
    {
      "epoch": 1.4086666666666667,
      "grad_norm": 0.17481984198093414,
      "learning_rate": 4.1195833333333336e-05,
      "loss": 0.0027,
      "step": 21130
    },
    {
      "epoch": 1.4093333333333333,
      "grad_norm": 0.4408552944660187,
      "learning_rate": 4.119166666666667e-05,
      "loss": 0.0023,
      "step": 21140
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.3460536599159241,
      "learning_rate": 4.11875e-05,
      "loss": 0.0026,
      "step": 21150
    },
    {
      "epoch": 1.4106666666666667,
      "grad_norm": 1.0431872606277466,
      "learning_rate": 4.1183333333333336e-05,
      "loss": 0.0023,
      "step": 21160
    },
    {
      "epoch": 1.4113333333333333,
      "grad_norm": 0.5465039610862732,
      "learning_rate": 4.117916666666667e-05,
      "loss": 0.0026,
      "step": 21170
    },
    {
      "epoch": 1.412,
      "grad_norm": 0.8187592029571533,
      "learning_rate": 4.1175000000000005e-05,
      "loss": 0.0032,
      "step": 21180
    },
    {
      "epoch": 1.4126666666666667,
      "grad_norm": 0.2004980444908142,
      "learning_rate": 4.1170833333333336e-05,
      "loss": 0.0039,
      "step": 21190
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.3424781262874603,
      "learning_rate": 4.116666666666667e-05,
      "loss": 0.0021,
      "step": 21200
    },
    {
      "epoch": 1.414,
      "grad_norm": 0.44527092576026917,
      "learning_rate": 4.1162500000000004e-05,
      "loss": 0.0029,
      "step": 21210
    },
    {
      "epoch": 1.4146666666666667,
      "grad_norm": 0.06943570077419281,
      "learning_rate": 4.1158333333333335e-05,
      "loss": 0.0024,
      "step": 21220
    },
    {
      "epoch": 1.4153333333333333,
      "grad_norm": 0.7642005681991577,
      "learning_rate": 4.1154166666666666e-05,
      "loss": 0.0021,
      "step": 21230
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.9837644696235657,
      "learning_rate": 4.115e-05,
      "loss": 0.0029,
      "step": 21240
    },
    {
      "epoch": 1.4166666666666667,
      "grad_norm": 0.7253796458244324,
      "learning_rate": 4.1145833333333335e-05,
      "loss": 0.0042,
      "step": 21250
    },
    {
      "epoch": 1.4173333333333333,
      "grad_norm": 0.2081373780965805,
      "learning_rate": 4.1141666666666666e-05,
      "loss": 0.0036,
      "step": 21260
    },
    {
      "epoch": 1.418,
      "grad_norm": 0.4179301857948303,
      "learning_rate": 4.1137500000000004e-05,
      "loss": 0.0027,
      "step": 21270
    },
    {
      "epoch": 1.4186666666666667,
      "grad_norm": 0.12755538523197174,
      "learning_rate": 4.1133333333333335e-05,
      "loss": 0.0029,
      "step": 21280
    },
    {
      "epoch": 1.4193333333333333,
      "grad_norm": 0.13445346057415009,
      "learning_rate": 4.1129166666666666e-05,
      "loss": 0.0022,
      "step": 21290
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.24137988686561584,
      "learning_rate": 4.1125000000000004e-05,
      "loss": 0.0043,
      "step": 21300
    },
    {
      "epoch": 1.4206666666666667,
      "grad_norm": 0.44053903222084045,
      "learning_rate": 4.1120833333333334e-05,
      "loss": 0.0028,
      "step": 21310
    },
    {
      "epoch": 1.4213333333333333,
      "grad_norm": 0.37036576867103577,
      "learning_rate": 4.111666666666667e-05,
      "loss": 0.0024,
      "step": 21320
    },
    {
      "epoch": 1.422,
      "grad_norm": 0.8940213322639465,
      "learning_rate": 4.11125e-05,
      "loss": 0.0023,
      "step": 21330
    },
    {
      "epoch": 1.4226666666666667,
      "grad_norm": 0.3820987045764923,
      "learning_rate": 4.110833333333334e-05,
      "loss": 0.0024,
      "step": 21340
    },
    {
      "epoch": 1.4233333333333333,
      "grad_norm": 0.34477221965789795,
      "learning_rate": 4.110416666666667e-05,
      "loss": 0.002,
      "step": 21350
    },
    {
      "epoch": 1.424,
      "grad_norm": 1.0267112255096436,
      "learning_rate": 4.11e-05,
      "loss": 0.0025,
      "step": 21360
    },
    {
      "epoch": 1.4246666666666667,
      "grad_norm": 0.21961532533168793,
      "learning_rate": 4.1095833333333334e-05,
      "loss": 0.0026,
      "step": 21370
    },
    {
      "epoch": 1.4253333333333333,
      "grad_norm": 0.30995914340019226,
      "learning_rate": 4.1091666666666665e-05,
      "loss": 0.0024,
      "step": 21380
    },
    {
      "epoch": 1.426,
      "grad_norm": 0.44948774576187134,
      "learning_rate": 4.10875e-05,
      "loss": 0.0016,
      "step": 21390
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.6524138450622559,
      "learning_rate": 4.1083333333333334e-05,
      "loss": 0.0021,
      "step": 21400
    },
    {
      "epoch": 1.4273333333333333,
      "grad_norm": 0.17446497082710266,
      "learning_rate": 4.107916666666667e-05,
      "loss": 0.0031,
      "step": 21410
    },
    {
      "epoch": 1.428,
      "grad_norm": 0.19370877742767334,
      "learning_rate": 4.1075e-05,
      "loss": 0.0042,
      "step": 21420
    },
    {
      "epoch": 1.4286666666666665,
      "grad_norm": 0.26791131496429443,
      "learning_rate": 4.107083333333333e-05,
      "loss": 0.0032,
      "step": 21430
    },
    {
      "epoch": 1.4293333333333333,
      "grad_norm": 0.2498829960823059,
      "learning_rate": 4.106666666666667e-05,
      "loss": 0.0025,
      "step": 21440
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.1474936306476593,
      "learning_rate": 4.10625e-05,
      "loss": 0.0021,
      "step": 21450
    },
    {
      "epoch": 1.4306666666666668,
      "grad_norm": 0.16253800690174103,
      "learning_rate": 4.105833333333334e-05,
      "loss": 0.0019,
      "step": 21460
    },
    {
      "epoch": 1.4313333333333333,
      "grad_norm": 0.11928889155387878,
      "learning_rate": 4.105416666666667e-05,
      "loss": 0.0019,
      "step": 21470
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.6939035058021545,
      "learning_rate": 4.105e-05,
      "loss": 0.002,
      "step": 21480
    },
    {
      "epoch": 1.4326666666666665,
      "grad_norm": 0.41105329990386963,
      "learning_rate": 4.104583333333333e-05,
      "loss": 0.0019,
      "step": 21490
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 0.4281988739967346,
      "learning_rate": 4.104166666666667e-05,
      "loss": 0.002,
      "step": 21500
    },
    {
      "epoch": 1.434,
      "grad_norm": 0.21334628760814667,
      "learning_rate": 4.10375e-05,
      "loss": 0.0027,
      "step": 21510
    },
    {
      "epoch": 1.4346666666666668,
      "grad_norm": 0.2915416359901428,
      "learning_rate": 4.103333333333333e-05,
      "loss": 0.0023,
      "step": 21520
    },
    {
      "epoch": 1.4353333333333333,
      "grad_norm": 0.18936310708522797,
      "learning_rate": 4.102916666666667e-05,
      "loss": 0.0027,
      "step": 21530
    },
    {
      "epoch": 1.436,
      "grad_norm": 0.633004367351532,
      "learning_rate": 4.1025e-05,
      "loss": 0.0019,
      "step": 21540
    },
    {
      "epoch": 1.4366666666666665,
      "grad_norm": 0.5857294797897339,
      "learning_rate": 4.102083333333334e-05,
      "loss": 0.0015,
      "step": 21550
    },
    {
      "epoch": 1.4373333333333334,
      "grad_norm": 0.3246147334575653,
      "learning_rate": 4.101666666666667e-05,
      "loss": 0.0023,
      "step": 21560
    },
    {
      "epoch": 1.438,
      "grad_norm": 0.6757934093475342,
      "learning_rate": 4.10125e-05,
      "loss": 0.0037,
      "step": 21570
    },
    {
      "epoch": 1.4386666666666668,
      "grad_norm": 0.32087621092796326,
      "learning_rate": 4.100833333333334e-05,
      "loss": 0.0013,
      "step": 21580
    },
    {
      "epoch": 1.4393333333333334,
      "grad_norm": 0.4020286798477173,
      "learning_rate": 4.100416666666667e-05,
      "loss": 0.0027,
      "step": 21590
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.4133310914039612,
      "learning_rate": 4.1e-05,
      "loss": 0.0035,
      "step": 21600
    },
    {
      "epoch": 1.4406666666666665,
      "grad_norm": 0.8150439262390137,
      "learning_rate": 4.099583333333333e-05,
      "loss": 0.0024,
      "step": 21610
    },
    {
      "epoch": 1.4413333333333334,
      "grad_norm": 0.7992802858352661,
      "learning_rate": 4.099166666666667e-05,
      "loss": 0.0037,
      "step": 21620
    },
    {
      "epoch": 1.442,
      "grad_norm": 0.8706957697868347,
      "learning_rate": 4.09875e-05,
      "loss": 0.002,
      "step": 21630
    },
    {
      "epoch": 1.4426666666666668,
      "grad_norm": 0.8823243975639343,
      "learning_rate": 4.098333333333334e-05,
      "loss": 0.0025,
      "step": 21640
    },
    {
      "epoch": 1.4433333333333334,
      "grad_norm": 1.010103464126587,
      "learning_rate": 4.097916666666667e-05,
      "loss": 0.0039,
      "step": 21650
    },
    {
      "epoch": 1.444,
      "grad_norm": 0.3642112612724304,
      "learning_rate": 4.0975e-05,
      "loss": 0.0029,
      "step": 21660
    },
    {
      "epoch": 1.4446666666666665,
      "grad_norm": 0.7401876449584961,
      "learning_rate": 4.097083333333334e-05,
      "loss": 0.0024,
      "step": 21670
    },
    {
      "epoch": 1.4453333333333334,
      "grad_norm": 0.4680548906326294,
      "learning_rate": 4.096666666666667e-05,
      "loss": 0.0029,
      "step": 21680
    },
    {
      "epoch": 1.446,
      "grad_norm": 0.11645948141813278,
      "learning_rate": 4.0962500000000006e-05,
      "loss": 0.0025,
      "step": 21690
    },
    {
      "epoch": 1.4466666666666668,
      "grad_norm": 0.41669961810112,
      "learning_rate": 4.095833333333334e-05,
      "loss": 0.0025,
      "step": 21700
    },
    {
      "epoch": 1.4473333333333334,
      "grad_norm": 0.21641623973846436,
      "learning_rate": 4.095416666666667e-05,
      "loss": 0.0021,
      "step": 21710
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.10037290304899216,
      "learning_rate": 4.095e-05,
      "loss": 0.0038,
      "step": 21720
    },
    {
      "epoch": 1.4486666666666665,
      "grad_norm": 0.5773605108261108,
      "learning_rate": 4.094583333333333e-05,
      "loss": 0.0039,
      "step": 21730
    },
    {
      "epoch": 1.4493333333333334,
      "grad_norm": 0.9436445236206055,
      "learning_rate": 4.094166666666667e-05,
      "loss": 0.0027,
      "step": 21740
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.9661567807197571,
      "learning_rate": 4.09375e-05,
      "loss": 0.0026,
      "step": 21750
    },
    {
      "epoch": 1.4506666666666668,
      "grad_norm": 1.0275664329528809,
      "learning_rate": 4.093333333333334e-05,
      "loss": 0.0031,
      "step": 21760
    },
    {
      "epoch": 1.4513333333333334,
      "grad_norm": 0.6278252005577087,
      "learning_rate": 4.092916666666667e-05,
      "loss": 0.003,
      "step": 21770
    },
    {
      "epoch": 1.452,
      "grad_norm": 0.44133636355400085,
      "learning_rate": 4.0925000000000005e-05,
      "loss": 0.0027,
      "step": 21780
    },
    {
      "epoch": 1.4526666666666666,
      "grad_norm": 0.3337092697620392,
      "learning_rate": 4.0920833333333336e-05,
      "loss": 0.0029,
      "step": 21790
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.33351439237594604,
      "learning_rate": 4.091666666666667e-05,
      "loss": 0.0024,
      "step": 21800
    },
    {
      "epoch": 1.454,
      "grad_norm": 0.0513555072247982,
      "learning_rate": 4.0912500000000005e-05,
      "loss": 0.0019,
      "step": 21810
    },
    {
      "epoch": 1.4546666666666668,
      "grad_norm": 0.16135868430137634,
      "learning_rate": 4.0908333333333336e-05,
      "loss": 0.0037,
      "step": 21820
    },
    {
      "epoch": 1.4553333333333334,
      "grad_norm": 0.5554053783416748,
      "learning_rate": 4.0904166666666674e-05,
      "loss": 0.002,
      "step": 21830
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.09433802962303162,
      "learning_rate": 4.09e-05,
      "loss": 0.0028,
      "step": 21840
    },
    {
      "epoch": 1.4566666666666666,
      "grad_norm": 0.32504478096961975,
      "learning_rate": 4.0895833333333336e-05,
      "loss": 0.0016,
      "step": 21850
    },
    {
      "epoch": 1.4573333333333334,
      "grad_norm": 0.12323115766048431,
      "learning_rate": 4.089166666666667e-05,
      "loss": 0.0028,
      "step": 21860
    },
    {
      "epoch": 1.458,
      "grad_norm": 0.5186498165130615,
      "learning_rate": 4.08875e-05,
      "loss": 0.0022,
      "step": 21870
    },
    {
      "epoch": 1.4586666666666668,
      "grad_norm": 0.45803967118263245,
      "learning_rate": 4.0883333333333335e-05,
      "loss": 0.0042,
      "step": 21880
    },
    {
      "epoch": 1.4593333333333334,
      "grad_norm": 0.734078586101532,
      "learning_rate": 4.0879166666666666e-05,
      "loss": 0.0023,
      "step": 21890
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.3747337758541107,
      "learning_rate": 4.0875000000000004e-05,
      "loss": 0.003,
      "step": 21900
    },
    {
      "epoch": 1.4606666666666666,
      "grad_norm": 0.39028245210647583,
      "learning_rate": 4.0870833333333335e-05,
      "loss": 0.0019,
      "step": 21910
    },
    {
      "epoch": 1.4613333333333334,
      "grad_norm": 0.2869453728199005,
      "learning_rate": 4.086666666666667e-05,
      "loss": 0.0037,
      "step": 21920
    },
    {
      "epoch": 1.462,
      "grad_norm": 0.1970391571521759,
      "learning_rate": 4.0862500000000004e-05,
      "loss": 0.0023,
      "step": 21930
    },
    {
      "epoch": 1.4626666666666668,
      "grad_norm": 0.7144496440887451,
      "learning_rate": 4.0858333333333335e-05,
      "loss": 0.0026,
      "step": 21940
    },
    {
      "epoch": 1.4633333333333334,
      "grad_norm": 0.22453732788562775,
      "learning_rate": 4.085416666666667e-05,
      "loss": 0.0022,
      "step": 21950
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.27222928404808044,
      "learning_rate": 4.085e-05,
      "loss": 0.0026,
      "step": 21960
    },
    {
      "epoch": 1.4646666666666666,
      "grad_norm": 0.3970341980457306,
      "learning_rate": 4.0845833333333335e-05,
      "loss": 0.0031,
      "step": 21970
    },
    {
      "epoch": 1.4653333333333334,
      "grad_norm": 0.5334959030151367,
      "learning_rate": 4.0841666666666666e-05,
      "loss": 0.0027,
      "step": 21980
    },
    {
      "epoch": 1.466,
      "grad_norm": 0.5709512829780579,
      "learning_rate": 4.08375e-05,
      "loss": 0.0025,
      "step": 21990
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.5735976099967957,
      "learning_rate": 4.0833333333333334e-05,
      "loss": 0.0031,
      "step": 22000
    },
    {
      "epoch": 1.4673333333333334,
      "grad_norm": 0.9242192506790161,
      "learning_rate": 4.0829166666666665e-05,
      "loss": 0.0025,
      "step": 22010
    },
    {
      "epoch": 1.468,
      "grad_norm": 0.292859822511673,
      "learning_rate": 4.0825e-05,
      "loss": 0.0031,
      "step": 22020
    },
    {
      "epoch": 1.4686666666666666,
      "grad_norm": 0.8256627917289734,
      "learning_rate": 4.0820833333333334e-05,
      "loss": 0.0029,
      "step": 22030
    },
    {
      "epoch": 1.4693333333333334,
      "grad_norm": 0.20583103597164154,
      "learning_rate": 4.081666666666667e-05,
      "loss": 0.0026,
      "step": 22040
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.07938750833272934,
      "learning_rate": 4.08125e-05,
      "loss": 0.0023,
      "step": 22050
    },
    {
      "epoch": 1.4706666666666668,
      "grad_norm": 0.7154709100723267,
      "learning_rate": 4.080833333333334e-05,
      "loss": 0.0026,
      "step": 22060
    },
    {
      "epoch": 1.4713333333333334,
      "grad_norm": 0.5837453603744507,
      "learning_rate": 4.080416666666667e-05,
      "loss": 0.0026,
      "step": 22070
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.7267975807189941,
      "learning_rate": 4.08e-05,
      "loss": 0.0017,
      "step": 22080
    },
    {
      "epoch": 1.4726666666666666,
      "grad_norm": 0.29043927788734436,
      "learning_rate": 4.079583333333333e-05,
      "loss": 0.0021,
      "step": 22090
    },
    {
      "epoch": 1.4733333333333334,
      "grad_norm": 0.35484200716018677,
      "learning_rate": 4.0791666666666664e-05,
      "loss": 0.003,
      "step": 22100
    },
    {
      "epoch": 1.474,
      "grad_norm": 0.5362368226051331,
      "learning_rate": 4.07875e-05,
      "loss": 0.0026,
      "step": 22110
    },
    {
      "epoch": 1.4746666666666668,
      "grad_norm": 0.309068500995636,
      "learning_rate": 4.078333333333333e-05,
      "loss": 0.0025,
      "step": 22120
    },
    {
      "epoch": 1.4753333333333334,
      "grad_norm": 0.5467069149017334,
      "learning_rate": 4.077916666666667e-05,
      "loss": 0.0027,
      "step": 22130
    },
    {
      "epoch": 1.476,
      "grad_norm": 0.098948635160923,
      "learning_rate": 4.0775e-05,
      "loss": 0.003,
      "step": 22140
    },
    {
      "epoch": 1.4766666666666666,
      "grad_norm": 0.39638909697532654,
      "learning_rate": 4.077083333333333e-05,
      "loss": 0.002,
      "step": 22150
    },
    {
      "epoch": 1.4773333333333334,
      "grad_norm": 0.051542509347200394,
      "learning_rate": 4.076666666666667e-05,
      "loss": 0.0032,
      "step": 22160
    },
    {
      "epoch": 1.478,
      "grad_norm": 1.052506685256958,
      "learning_rate": 4.07625e-05,
      "loss": 0.0031,
      "step": 22170
    },
    {
      "epoch": 1.4786666666666668,
      "grad_norm": 0.7077842354774475,
      "learning_rate": 4.075833333333334e-05,
      "loss": 0.0034,
      "step": 22180
    },
    {
      "epoch": 1.4793333333333334,
      "grad_norm": 0.42377474904060364,
      "learning_rate": 4.075416666666667e-05,
      "loss": 0.0023,
      "step": 22190
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.32687264680862427,
      "learning_rate": 4.075e-05,
      "loss": 0.0023,
      "step": 22200
    },
    {
      "epoch": 1.4806666666666666,
      "grad_norm": 0.13480769097805023,
      "learning_rate": 4.074583333333333e-05,
      "loss": 0.0019,
      "step": 22210
    },
    {
      "epoch": 1.4813333333333334,
      "grad_norm": 0.3690189719200134,
      "learning_rate": 4.074166666666667e-05,
      "loss": 0.0026,
      "step": 22220
    },
    {
      "epoch": 1.482,
      "grad_norm": 0.9168546795845032,
      "learning_rate": 4.07375e-05,
      "loss": 0.0016,
      "step": 22230
    },
    {
      "epoch": 1.4826666666666668,
      "grad_norm": 0.698204517364502,
      "learning_rate": 4.073333333333333e-05,
      "loss": 0.0024,
      "step": 22240
    },
    {
      "epoch": 1.4833333333333334,
      "grad_norm": 0.2878173589706421,
      "learning_rate": 4.072916666666667e-05,
      "loss": 0.0024,
      "step": 22250
    },
    {
      "epoch": 1.484,
      "grad_norm": 0.4240482747554779,
      "learning_rate": 4.0725e-05,
      "loss": 0.0032,
      "step": 22260
    },
    {
      "epoch": 1.4846666666666666,
      "grad_norm": 0.6900939345359802,
      "learning_rate": 4.072083333333334e-05,
      "loss": 0.0022,
      "step": 22270
    },
    {
      "epoch": 1.4853333333333334,
      "grad_norm": 0.8642281889915466,
      "learning_rate": 4.071666666666667e-05,
      "loss": 0.0026,
      "step": 22280
    },
    {
      "epoch": 1.486,
      "grad_norm": 0.8873741626739502,
      "learning_rate": 4.07125e-05,
      "loss": 0.002,
      "step": 22290
    },
    {
      "epoch": 1.4866666666666668,
      "grad_norm": 0.08272123336791992,
      "learning_rate": 4.070833333333334e-05,
      "loss": 0.0024,
      "step": 22300
    },
    {
      "epoch": 1.4873333333333334,
      "grad_norm": 0.08979159593582153,
      "learning_rate": 4.070416666666667e-05,
      "loss": 0.0033,
      "step": 22310
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.17796236276626587,
      "learning_rate": 4.07e-05,
      "loss": 0.0023,
      "step": 22320
    },
    {
      "epoch": 1.4886666666666666,
      "grad_norm": 0.36063823103904724,
      "learning_rate": 4.069583333333333e-05,
      "loss": 0.0025,
      "step": 22330
    },
    {
      "epoch": 1.4893333333333334,
      "grad_norm": 0.15057362616062164,
      "learning_rate": 4.069166666666667e-05,
      "loss": 0.0039,
      "step": 22340
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.5077848434448242,
      "learning_rate": 4.06875e-05,
      "loss": 0.0024,
      "step": 22350
    },
    {
      "epoch": 1.4906666666666666,
      "grad_norm": 1.00753653049469,
      "learning_rate": 4.068333333333334e-05,
      "loss": 0.0022,
      "step": 22360
    },
    {
      "epoch": 1.4913333333333334,
      "grad_norm": 0.9025488495826721,
      "learning_rate": 4.067916666666667e-05,
      "loss": 0.0014,
      "step": 22370
    },
    {
      "epoch": 1.492,
      "grad_norm": 0.44170695543289185,
      "learning_rate": 4.0675e-05,
      "loss": 0.0027,
      "step": 22380
    },
    {
      "epoch": 1.4926666666666666,
      "grad_norm": 0.28185299038887024,
      "learning_rate": 4.067083333333334e-05,
      "loss": 0.0024,
      "step": 22390
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.20835891366004944,
      "learning_rate": 4.066666666666667e-05,
      "loss": 0.0028,
      "step": 22400
    },
    {
      "epoch": 1.494,
      "grad_norm": 0.24751058220863342,
      "learning_rate": 4.0662500000000006e-05,
      "loss": 0.0027,
      "step": 22410
    },
    {
      "epoch": 1.4946666666666666,
      "grad_norm": 0.0865318700671196,
      "learning_rate": 4.065833333333334e-05,
      "loss": 0.0018,
      "step": 22420
    },
    {
      "epoch": 1.4953333333333334,
      "grad_norm": 0.2842881977558136,
      "learning_rate": 4.065416666666667e-05,
      "loss": 0.0022,
      "step": 22430
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.31867271661758423,
      "learning_rate": 4.065e-05,
      "loss": 0.0018,
      "step": 22440
    },
    {
      "epoch": 1.4966666666666666,
      "grad_norm": 0.45934298634529114,
      "learning_rate": 4.064583333333333e-05,
      "loss": 0.0036,
      "step": 22450
    },
    {
      "epoch": 1.4973333333333334,
      "grad_norm": 0.11530698835849762,
      "learning_rate": 4.064166666666667e-05,
      "loss": 0.0032,
      "step": 22460
    },
    {
      "epoch": 1.498,
      "grad_norm": 0.2110515534877777,
      "learning_rate": 4.06375e-05,
      "loss": 0.0026,
      "step": 22470
    },
    {
      "epoch": 1.4986666666666666,
      "grad_norm": 0.20130081474781036,
      "learning_rate": 4.0633333333333336e-05,
      "loss": 0.0029,
      "step": 22480
    },
    {
      "epoch": 1.4993333333333334,
      "grad_norm": 0.5838696956634521,
      "learning_rate": 4.062916666666667e-05,
      "loss": 0.0034,
      "step": 22490
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.2859067916870117,
      "learning_rate": 4.0625000000000005e-05,
      "loss": 0.002,
      "step": 22500
    },
    {
      "epoch": 1.5006666666666666,
      "grad_norm": 0.09530913829803467,
      "learning_rate": 4.0620833333333336e-05,
      "loss": 0.0021,
      "step": 22510
    },
    {
      "epoch": 1.5013333333333332,
      "grad_norm": 1.0001016855239868,
      "learning_rate": 4.061666666666667e-05,
      "loss": 0.0026,
      "step": 22520
    },
    {
      "epoch": 1.502,
      "grad_norm": 0.479789674282074,
      "learning_rate": 4.0612500000000005e-05,
      "loss": 0.0022,
      "step": 22530
    },
    {
      "epoch": 1.5026666666666668,
      "grad_norm": 0.7304592728614807,
      "learning_rate": 4.0608333333333336e-05,
      "loss": 0.0028,
      "step": 22540
    },
    {
      "epoch": 1.5033333333333334,
      "grad_norm": 0.2898867726325989,
      "learning_rate": 4.060416666666667e-05,
      "loss": 0.0035,
      "step": 22550
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.20506185293197632,
      "learning_rate": 4.0600000000000004e-05,
      "loss": 0.0026,
      "step": 22560
    },
    {
      "epoch": 1.5046666666666666,
      "grad_norm": 0.1616661250591278,
      "learning_rate": 4.0595833333333335e-05,
      "loss": 0.0029,
      "step": 22570
    },
    {
      "epoch": 1.5053333333333332,
      "grad_norm": 1.0095224380493164,
      "learning_rate": 4.0591666666666666e-05,
      "loss": 0.0042,
      "step": 22580
    },
    {
      "epoch": 1.506,
      "grad_norm": 0.5195236206054688,
      "learning_rate": 4.05875e-05,
      "loss": 0.0022,
      "step": 22590
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.07880912721157074,
      "learning_rate": 4.0583333333333335e-05,
      "loss": 0.0027,
      "step": 22600
    },
    {
      "epoch": 1.5073333333333334,
      "grad_norm": 0.7748570442199707,
      "learning_rate": 4.0579166666666666e-05,
      "loss": 0.0033,
      "step": 22610
    },
    {
      "epoch": 1.508,
      "grad_norm": 0.6510517001152039,
      "learning_rate": 4.0575000000000004e-05,
      "loss": 0.0026,
      "step": 22620
    },
    {
      "epoch": 1.5086666666666666,
      "grad_norm": 0.46750110387802124,
      "learning_rate": 4.0570833333333335e-05,
      "loss": 0.0018,
      "step": 22630
    },
    {
      "epoch": 1.5093333333333332,
      "grad_norm": 0.08271615207195282,
      "learning_rate": 4.056666666666667e-05,
      "loss": 0.0016,
      "step": 22640
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.38167604804039,
      "learning_rate": 4.0562500000000003e-05,
      "loss": 0.0031,
      "step": 22650
    },
    {
      "epoch": 1.5106666666666668,
      "grad_norm": 0.48166319727897644,
      "learning_rate": 4.0558333333333334e-05,
      "loss": 0.0039,
      "step": 22660
    },
    {
      "epoch": 1.5113333333333334,
      "grad_norm": 1.3297046422958374,
      "learning_rate": 4.055416666666667e-05,
      "loss": 0.0041,
      "step": 22670
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.243392214179039,
      "learning_rate": 4.055e-05,
      "loss": 0.0023,
      "step": 22680
    },
    {
      "epoch": 1.5126666666666666,
      "grad_norm": 0.7824010848999023,
      "learning_rate": 4.0545833333333334e-05,
      "loss": 0.0027,
      "step": 22690
    },
    {
      "epoch": 1.5133333333333332,
      "grad_norm": 0.5606522560119629,
      "learning_rate": 4.0541666666666665e-05,
      "loss": 0.0023,
      "step": 22700
    },
    {
      "epoch": 1.514,
      "grad_norm": 0.1929042935371399,
      "learning_rate": 4.05375e-05,
      "loss": 0.0028,
      "step": 22710
    },
    {
      "epoch": 1.5146666666666668,
      "grad_norm": 0.614113450050354,
      "learning_rate": 4.0533333333333334e-05,
      "loss": 0.0032,
      "step": 22720
    },
    {
      "epoch": 1.5153333333333334,
      "grad_norm": 0.7505894899368286,
      "learning_rate": 4.0529166666666665e-05,
      "loss": 0.0033,
      "step": 22730
    },
    {
      "epoch": 1.516,
      "grad_norm": 0.864772379398346,
      "learning_rate": 4.0525e-05,
      "loss": 0.0044,
      "step": 22740
    },
    {
      "epoch": 1.5166666666666666,
      "grad_norm": 0.20872385799884796,
      "learning_rate": 4.0520833333333333e-05,
      "loss": 0.0026,
      "step": 22750
    },
    {
      "epoch": 1.5173333333333332,
      "grad_norm": 0.16785870492458344,
      "learning_rate": 4.051666666666667e-05,
      "loss": 0.0035,
      "step": 22760
    },
    {
      "epoch": 1.518,
      "grad_norm": 0.08666771650314331,
      "learning_rate": 4.05125e-05,
      "loss": 0.0025,
      "step": 22770
    },
    {
      "epoch": 1.5186666666666668,
      "grad_norm": 0.1027558222413063,
      "learning_rate": 4.050833333333334e-05,
      "loss": 0.0033,
      "step": 22780
    },
    {
      "epoch": 1.5193333333333334,
      "grad_norm": 0.09187962114810944,
      "learning_rate": 4.050416666666667e-05,
      "loss": 0.0024,
      "step": 22790
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.36856546998023987,
      "learning_rate": 4.05e-05,
      "loss": 0.0019,
      "step": 22800
    },
    {
      "epoch": 1.5206666666666666,
      "grad_norm": 0.09842873364686966,
      "learning_rate": 4.049583333333333e-05,
      "loss": 0.004,
      "step": 22810
    },
    {
      "epoch": 1.5213333333333332,
      "grad_norm": 0.30538395047187805,
      "learning_rate": 4.0491666666666664e-05,
      "loss": 0.0032,
      "step": 22820
    },
    {
      "epoch": 1.522,
      "grad_norm": 0.8391234278678894,
      "learning_rate": 4.04875e-05,
      "loss": 0.0036,
      "step": 22830
    },
    {
      "epoch": 1.5226666666666666,
      "grad_norm": 0.4293980002403259,
      "learning_rate": 4.048333333333333e-05,
      "loss": 0.0022,
      "step": 22840
    },
    {
      "epoch": 1.5233333333333334,
      "grad_norm": 0.19980213046073914,
      "learning_rate": 4.047916666666667e-05,
      "loss": 0.0025,
      "step": 22850
    },
    {
      "epoch": 1.524,
      "grad_norm": 0.03188460320234299,
      "learning_rate": 4.0475e-05,
      "loss": 0.0029,
      "step": 22860
    },
    {
      "epoch": 1.5246666666666666,
      "grad_norm": 0.47478392720222473,
      "learning_rate": 4.047083333333333e-05,
      "loss": 0.0035,
      "step": 22870
    },
    {
      "epoch": 1.5253333333333332,
      "grad_norm": 0.4227581024169922,
      "learning_rate": 4.046666666666667e-05,
      "loss": 0.0027,
      "step": 22880
    },
    {
      "epoch": 1.526,
      "grad_norm": 1.003430962562561,
      "learning_rate": 4.04625e-05,
      "loss": 0.004,
      "step": 22890
    },
    {
      "epoch": 1.5266666666666666,
      "grad_norm": 0.7285498380661011,
      "learning_rate": 4.045833333333334e-05,
      "loss": 0.0025,
      "step": 22900
    },
    {
      "epoch": 1.5273333333333334,
      "grad_norm": 0.44027531147003174,
      "learning_rate": 4.045416666666667e-05,
      "loss": 0.0035,
      "step": 22910
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.38233551383018494,
      "learning_rate": 4.045000000000001e-05,
      "loss": 0.0027,
      "step": 22920
    },
    {
      "epoch": 1.5286666666666666,
      "grad_norm": 0.19613482058048248,
      "learning_rate": 4.044583333333333e-05,
      "loss": 0.0024,
      "step": 22930
    },
    {
      "epoch": 1.5293333333333332,
      "grad_norm": 0.42434972524642944,
      "learning_rate": 4.044166666666667e-05,
      "loss": 0.0021,
      "step": 22940
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.05254817754030228,
      "learning_rate": 4.04375e-05,
      "loss": 0.0025,
      "step": 22950
    },
    {
      "epoch": 1.5306666666666666,
      "grad_norm": 0.559596598148346,
      "learning_rate": 4.043333333333333e-05,
      "loss": 0.0026,
      "step": 22960
    },
    {
      "epoch": 1.5313333333333334,
      "grad_norm": 0.7627626657485962,
      "learning_rate": 4.042916666666667e-05,
      "loss": 0.002,
      "step": 22970
    },
    {
      "epoch": 1.532,
      "grad_norm": 0.4334263801574707,
      "learning_rate": 4.0425e-05,
      "loss": 0.0043,
      "step": 22980
    },
    {
      "epoch": 1.5326666666666666,
      "grad_norm": 0.32608941197395325,
      "learning_rate": 4.042083333333334e-05,
      "loss": 0.0039,
      "step": 22990
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.33808332681655884,
      "learning_rate": 4.041666666666667e-05,
      "loss": 0.0025,
      "step": 23000
    },
    {
      "epoch": 1.534,
      "grad_norm": 0.11808726191520691,
      "learning_rate": 4.0412500000000007e-05,
      "loss": 0.003,
      "step": 23010
    },
    {
      "epoch": 1.5346666666666666,
      "grad_norm": 0.12715323269367218,
      "learning_rate": 4.040833333333334e-05,
      "loss": 0.0027,
      "step": 23020
    },
    {
      "epoch": 1.5353333333333334,
      "grad_norm": 0.055468060076236725,
      "learning_rate": 4.040416666666667e-05,
      "loss": 0.0025,
      "step": 23030
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.2701655924320221,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 0.0015,
      "step": 23040
    },
    {
      "epoch": 1.5366666666666666,
      "grad_norm": 0.624952495098114,
      "learning_rate": 4.039583333333333e-05,
      "loss": 0.0023,
      "step": 23050
    },
    {
      "epoch": 1.5373333333333332,
      "grad_norm": 0.9314624667167664,
      "learning_rate": 4.039166666666667e-05,
      "loss": 0.0018,
      "step": 23060
    },
    {
      "epoch": 1.538,
      "grad_norm": 0.6942979097366333,
      "learning_rate": 4.03875e-05,
      "loss": 0.0028,
      "step": 23070
    },
    {
      "epoch": 1.5386666666666666,
      "grad_norm": 0.7742076516151428,
      "learning_rate": 4.038333333333334e-05,
      "loss": 0.0027,
      "step": 23080
    },
    {
      "epoch": 1.5393333333333334,
      "grad_norm": 0.34818509221076965,
      "learning_rate": 4.037916666666667e-05,
      "loss": 0.0033,
      "step": 23090
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.6816876530647278,
      "learning_rate": 4.0375e-05,
      "loss": 0.0032,
      "step": 23100
    },
    {
      "epoch": 1.5406666666666666,
      "grad_norm": 1.0760579109191895,
      "learning_rate": 4.0370833333333337e-05,
      "loss": 0.0033,
      "step": 23110
    },
    {
      "epoch": 1.5413333333333332,
      "grad_norm": 0.12872318923473358,
      "learning_rate": 4.036666666666667e-05,
      "loss": 0.0033,
      "step": 23120
    },
    {
      "epoch": 1.542,
      "grad_norm": 0.13148431479930878,
      "learning_rate": 4.0362500000000005e-05,
      "loss": 0.002,
      "step": 23130
    },
    {
      "epoch": 1.5426666666666666,
      "grad_norm": 0.993354320526123,
      "learning_rate": 4.0358333333333336e-05,
      "loss": 0.0029,
      "step": 23140
    },
    {
      "epoch": 1.5433333333333334,
      "grad_norm": 1.0823458433151245,
      "learning_rate": 4.0354166666666674e-05,
      "loss": 0.0022,
      "step": 23150
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.6180091500282288,
      "learning_rate": 4.0350000000000005e-05,
      "loss": 0.003,
      "step": 23160
    },
    {
      "epoch": 1.5446666666666666,
      "grad_norm": 0.7768383026123047,
      "learning_rate": 4.0345833333333336e-05,
      "loss": 0.003,
      "step": 23170
    },
    {
      "epoch": 1.5453333333333332,
      "grad_norm": 0.3150985836982727,
      "learning_rate": 4.034166666666667e-05,
      "loss": 0.0038,
      "step": 23180
    },
    {
      "epoch": 1.546,
      "grad_norm": 0.34969353675842285,
      "learning_rate": 4.03375e-05,
      "loss": 0.0021,
      "step": 23190
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.16375640034675598,
      "learning_rate": 4.0333333333333336e-05,
      "loss": 0.0023,
      "step": 23200
    },
    {
      "epoch": 1.5473333333333334,
      "grad_norm": 0.3530873656272888,
      "learning_rate": 4.032916666666667e-05,
      "loss": 0.0018,
      "step": 23210
    },
    {
      "epoch": 1.548,
      "grad_norm": 0.16949982941150665,
      "learning_rate": 4.0325000000000004e-05,
      "loss": 0.0022,
      "step": 23220
    },
    {
      "epoch": 1.5486666666666666,
      "grad_norm": 0.5013180375099182,
      "learning_rate": 4.0320833333333335e-05,
      "loss": 0.003,
      "step": 23230
    },
    {
      "epoch": 1.5493333333333332,
      "grad_norm": 0.11739804595708847,
      "learning_rate": 4.0316666666666666e-05,
      "loss": 0.0024,
      "step": 23240
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.2638157606124878,
      "learning_rate": 4.0312500000000004e-05,
      "loss": 0.0025,
      "step": 23250
    },
    {
      "epoch": 1.5506666666666666,
      "grad_norm": 0.4946969449520111,
      "learning_rate": 4.0308333333333335e-05,
      "loss": 0.0018,
      "step": 23260
    },
    {
      "epoch": 1.5513333333333335,
      "grad_norm": 0.3933907151222229,
      "learning_rate": 4.030416666666667e-05,
      "loss": 0.0016,
      "step": 23270
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.16999652981758118,
      "learning_rate": 4.0300000000000004e-05,
      "loss": 0.0036,
      "step": 23280
    },
    {
      "epoch": 1.5526666666666666,
      "grad_norm": 0.1380273848772049,
      "learning_rate": 4.0295833333333335e-05,
      "loss": 0.0022,
      "step": 23290
    },
    {
      "epoch": 1.5533333333333332,
      "grad_norm": 0.12152210623025894,
      "learning_rate": 4.0291666666666666e-05,
      "loss": 0.0028,
      "step": 23300
    },
    {
      "epoch": 1.554,
      "grad_norm": 0.8241783380508423,
      "learning_rate": 4.0287500000000003e-05,
      "loss": 0.0026,
      "step": 23310
    },
    {
      "epoch": 1.5546666666666666,
      "grad_norm": 0.5331923365592957,
      "learning_rate": 4.0283333333333334e-05,
      "loss": 0.0023,
      "step": 23320
    },
    {
      "epoch": 1.5553333333333335,
      "grad_norm": 0.28015270829200745,
      "learning_rate": 4.0279166666666665e-05,
      "loss": 0.0023,
      "step": 23330
    },
    {
      "epoch": 1.556,
      "grad_norm": 0.2865453362464905,
      "learning_rate": 4.0275e-05,
      "loss": 0.0024,
      "step": 23340
    },
    {
      "epoch": 1.5566666666666666,
      "grad_norm": 0.5196732878684998,
      "learning_rate": 4.0270833333333334e-05,
      "loss": 0.0024,
      "step": 23350
    },
    {
      "epoch": 1.5573333333333332,
      "grad_norm": 0.653518557548523,
      "learning_rate": 4.026666666666667e-05,
      "loss": 0.0038,
      "step": 23360
    },
    {
      "epoch": 1.558,
      "grad_norm": 1.4029792547225952,
      "learning_rate": 4.02625e-05,
      "loss": 0.0023,
      "step": 23370
    },
    {
      "epoch": 1.5586666666666666,
      "grad_norm": 0.7584837675094604,
      "learning_rate": 4.0258333333333334e-05,
      "loss": 0.0038,
      "step": 23380
    },
    {
      "epoch": 1.5593333333333335,
      "grad_norm": 0.24235334992408752,
      "learning_rate": 4.025416666666667e-05,
      "loss": 0.0033,
      "step": 23390
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.17080844938755035,
      "learning_rate": 4.025e-05,
      "loss": 0.002,
      "step": 23400
    },
    {
      "epoch": 1.5606666666666666,
      "grad_norm": 0.8695517182350159,
      "learning_rate": 4.0245833333333334e-05,
      "loss": 0.003,
      "step": 23410
    },
    {
      "epoch": 1.5613333333333332,
      "grad_norm": 0.47613370418548584,
      "learning_rate": 4.0241666666666665e-05,
      "loss": 0.0027,
      "step": 23420
    },
    {
      "epoch": 1.562,
      "grad_norm": 0.40925389528274536,
      "learning_rate": 4.02375e-05,
      "loss": 0.0022,
      "step": 23430
    },
    {
      "epoch": 1.5626666666666666,
      "grad_norm": 0.3852322995662689,
      "learning_rate": 4.023333333333333e-05,
      "loss": 0.0025,
      "step": 23440
    },
    {
      "epoch": 1.5633333333333335,
      "grad_norm": 0.058132197707891464,
      "learning_rate": 4.022916666666667e-05,
      "loss": 0.0023,
      "step": 23450
    },
    {
      "epoch": 1.564,
      "grad_norm": 0.10523999482393265,
      "learning_rate": 4.0225e-05,
      "loss": 0.0028,
      "step": 23460
    },
    {
      "epoch": 1.5646666666666667,
      "grad_norm": 0.4689578711986542,
      "learning_rate": 4.022083333333333e-05,
      "loss": 0.0019,
      "step": 23470
    },
    {
      "epoch": 1.5653333333333332,
      "grad_norm": 0.45501193404197693,
      "learning_rate": 4.021666666666667e-05,
      "loss": 0.0033,
      "step": 23480
    },
    {
      "epoch": 1.5659999999999998,
      "grad_norm": 0.258914589881897,
      "learning_rate": 4.02125e-05,
      "loss": 0.0023,
      "step": 23490
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 1.114339828491211,
      "learning_rate": 4.020833333333334e-05,
      "loss": 0.0026,
      "step": 23500
    },
    {
      "epoch": 1.5673333333333335,
      "grad_norm": 0.3737528920173645,
      "learning_rate": 4.020416666666667e-05,
      "loss": 0.002,
      "step": 23510
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.7700586318969727,
      "learning_rate": 4.02e-05,
      "loss": 0.0028,
      "step": 23520
    },
    {
      "epoch": 1.5686666666666667,
      "grad_norm": 0.32803067564964294,
      "learning_rate": 4.019583333333333e-05,
      "loss": 0.0036,
      "step": 23530
    },
    {
      "epoch": 1.5693333333333332,
      "grad_norm": 1.1524732112884521,
      "learning_rate": 4.019166666666666e-05,
      "loss": 0.0036,
      "step": 23540
    },
    {
      "epoch": 1.5699999999999998,
      "grad_norm": 0.12158419191837311,
      "learning_rate": 4.01875e-05,
      "loss": 0.0036,
      "step": 23550
    },
    {
      "epoch": 1.5706666666666667,
      "grad_norm": 1.296542763710022,
      "learning_rate": 4.018333333333333e-05,
      "loss": 0.0025,
      "step": 23560
    },
    {
      "epoch": 1.5713333333333335,
      "grad_norm": 1.3032702207565308,
      "learning_rate": 4.017916666666667e-05,
      "loss": 0.004,
      "step": 23570
    },
    {
      "epoch": 1.572,
      "grad_norm": 0.7790342569351196,
      "learning_rate": 4.0175e-05,
      "loss": 0.0033,
      "step": 23580
    },
    {
      "epoch": 1.5726666666666667,
      "grad_norm": 0.8873734474182129,
      "learning_rate": 4.017083333333334e-05,
      "loss": 0.0033,
      "step": 23590
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.2621263563632965,
      "learning_rate": 4.016666666666667e-05,
      "loss": 0.0017,
      "step": 23600
    },
    {
      "epoch": 1.5739999999999998,
      "grad_norm": 0.05683434009552002,
      "learning_rate": 4.01625e-05,
      "loss": 0.0024,
      "step": 23610
    },
    {
      "epoch": 1.5746666666666667,
      "grad_norm": 0.5157490372657776,
      "learning_rate": 4.015833333333334e-05,
      "loss": 0.0032,
      "step": 23620
    },
    {
      "epoch": 1.5753333333333335,
      "grad_norm": 0.1776367425918579,
      "learning_rate": 4.015416666666667e-05,
      "loss": 0.0042,
      "step": 23630
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.7871776819229126,
      "learning_rate": 4.015000000000001e-05,
      "loss": 0.0022,
      "step": 23640
    },
    {
      "epoch": 1.5766666666666667,
      "grad_norm": 0.3473406434059143,
      "learning_rate": 4.014583333333333e-05,
      "loss": 0.0021,
      "step": 23650
    },
    {
      "epoch": 1.5773333333333333,
      "grad_norm": 0.3186165392398834,
      "learning_rate": 4.014166666666667e-05,
      "loss": 0.0022,
      "step": 23660
    },
    {
      "epoch": 1.5779999999999998,
      "grad_norm": 0.46413516998291016,
      "learning_rate": 4.01375e-05,
      "loss": 0.0025,
      "step": 23670
    },
    {
      "epoch": 1.5786666666666667,
      "grad_norm": 0.24113966524600983,
      "learning_rate": 4.013333333333333e-05,
      "loss": 0.0024,
      "step": 23680
    },
    {
      "epoch": 1.5793333333333335,
      "grad_norm": 0.1342954784631729,
      "learning_rate": 4.012916666666667e-05,
      "loss": 0.0014,
      "step": 23690
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.28131112456321716,
      "learning_rate": 4.0125e-05,
      "loss": 0.0026,
      "step": 23700
    },
    {
      "epoch": 1.5806666666666667,
      "grad_norm": 0.4721774756908417,
      "learning_rate": 4.012083333333334e-05,
      "loss": 0.0033,
      "step": 23710
    },
    {
      "epoch": 1.5813333333333333,
      "grad_norm": 0.2337532490491867,
      "learning_rate": 4.011666666666667e-05,
      "loss": 0.0029,
      "step": 23720
    },
    {
      "epoch": 1.5819999999999999,
      "grad_norm": 0.13249683380126953,
      "learning_rate": 4.0112500000000006e-05,
      "loss": 0.0024,
      "step": 23730
    },
    {
      "epoch": 1.5826666666666667,
      "grad_norm": 0.12036796659231186,
      "learning_rate": 4.010833333333334e-05,
      "loss": 0.0026,
      "step": 23740
    },
    {
      "epoch": 1.5833333333333335,
      "grad_norm": 0.566740870475769,
      "learning_rate": 4.010416666666667e-05,
      "loss": 0.004,
      "step": 23750
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.22782713174819946,
      "learning_rate": 4.0100000000000006e-05,
      "loss": 0.002,
      "step": 23760
    },
    {
      "epoch": 1.5846666666666667,
      "grad_norm": 0.38527050614356995,
      "learning_rate": 4.009583333333334e-05,
      "loss": 0.0017,
      "step": 23770
    },
    {
      "epoch": 1.5853333333333333,
      "grad_norm": 0.746850848197937,
      "learning_rate": 4.009166666666667e-05,
      "loss": 0.0028,
      "step": 23780
    },
    {
      "epoch": 1.5859999999999999,
      "grad_norm": 0.08203721046447754,
      "learning_rate": 4.00875e-05,
      "loss": 0.0027,
      "step": 23790
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 1.3673205375671387,
      "learning_rate": 4.0083333333333336e-05,
      "loss": 0.0031,
      "step": 23800
    },
    {
      "epoch": 1.5873333333333335,
      "grad_norm": 0.05554685369133949,
      "learning_rate": 4.007916666666667e-05,
      "loss": 0.0029,
      "step": 23810
    },
    {
      "epoch": 1.588,
      "grad_norm": 0.6697978973388672,
      "learning_rate": 4.0075e-05,
      "loss": 0.0029,
      "step": 23820
    },
    {
      "epoch": 1.5886666666666667,
      "grad_norm": 0.7787292003631592,
      "learning_rate": 4.0070833333333336e-05,
      "loss": 0.0034,
      "step": 23830
    },
    {
      "epoch": 1.5893333333333333,
      "grad_norm": 0.43865662813186646,
      "learning_rate": 4.006666666666667e-05,
      "loss": 0.0023,
      "step": 23840
    },
    {
      "epoch": 1.5899999999999999,
      "grad_norm": 0.6721833944320679,
      "learning_rate": 4.0062500000000005e-05,
      "loss": 0.0022,
      "step": 23850
    },
    {
      "epoch": 1.5906666666666667,
      "grad_norm": 0.27852705121040344,
      "learning_rate": 4.0058333333333336e-05,
      "loss": 0.0017,
      "step": 23860
    },
    {
      "epoch": 1.5913333333333335,
      "grad_norm": 1.3841564655303955,
      "learning_rate": 4.0054166666666674e-05,
      "loss": 0.0045,
      "step": 23870
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.42534032464027405,
      "learning_rate": 4.0050000000000004e-05,
      "loss": 0.0023,
      "step": 23880
    },
    {
      "epoch": 1.5926666666666667,
      "grad_norm": 0.15439625084400177,
      "learning_rate": 4.0045833333333335e-05,
      "loss": 0.0027,
      "step": 23890
    },
    {
      "epoch": 1.5933333333333333,
      "grad_norm": 0.06400389224290848,
      "learning_rate": 4.0041666666666666e-05,
      "loss": 0.0025,
      "step": 23900
    },
    {
      "epoch": 1.5939999999999999,
      "grad_norm": 0.7729862928390503,
      "learning_rate": 4.00375e-05,
      "loss": 0.0026,
      "step": 23910
    },
    {
      "epoch": 1.5946666666666667,
      "grad_norm": 1.2209892272949219,
      "learning_rate": 4.0033333333333335e-05,
      "loss": 0.0028,
      "step": 23920
    },
    {
      "epoch": 1.5953333333333335,
      "grad_norm": 0.4758095145225525,
      "learning_rate": 4.0029166666666666e-05,
      "loss": 0.0036,
      "step": 23930
    },
    {
      "epoch": 1.596,
      "grad_norm": 0.44267258048057556,
      "learning_rate": 4.0025000000000004e-05,
      "loss": 0.003,
      "step": 23940
    },
    {
      "epoch": 1.5966666666666667,
      "grad_norm": 0.7080391645431519,
      "learning_rate": 4.0020833333333335e-05,
      "loss": 0.0033,
      "step": 23950
    },
    {
      "epoch": 1.5973333333333333,
      "grad_norm": 0.28629234433174133,
      "learning_rate": 4.0016666666666666e-05,
      "loss": 0.0035,
      "step": 23960
    },
    {
      "epoch": 1.5979999999999999,
      "grad_norm": 0.3887186050415039,
      "learning_rate": 4.0012500000000004e-05,
      "loss": 0.0022,
      "step": 23970
    },
    {
      "epoch": 1.5986666666666667,
      "grad_norm": 0.7207039594650269,
      "learning_rate": 4.0008333333333335e-05,
      "loss": 0.0022,
      "step": 23980
    },
    {
      "epoch": 1.5993333333333335,
      "grad_norm": 0.21420878171920776,
      "learning_rate": 4.000416666666667e-05,
      "loss": 0.0036,
      "step": 23990
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.48850125074386597,
      "learning_rate": 4e-05,
      "loss": 0.0019,
      "step": 24000
    },
    {
      "epoch": 1.6006666666666667,
      "grad_norm": 0.06282410770654678,
      "learning_rate": 3.999583333333334e-05,
      "loss": 0.0035,
      "step": 24010
    },
    {
      "epoch": 1.6013333333333333,
      "grad_norm": 0.17540031671524048,
      "learning_rate": 3.9991666666666665e-05,
      "loss": 0.0025,
      "step": 24020
    },
    {
      "epoch": 1.6019999999999999,
      "grad_norm": 0.6619263291358948,
      "learning_rate": 3.99875e-05,
      "loss": 0.0026,
      "step": 24030
    },
    {
      "epoch": 1.6026666666666667,
      "grad_norm": 0.17208704352378845,
      "learning_rate": 3.9983333333333334e-05,
      "loss": 0.0036,
      "step": 24040
    },
    {
      "epoch": 1.6033333333333335,
      "grad_norm": 0.5583531260490417,
      "learning_rate": 3.9979166666666665e-05,
      "loss": 0.0023,
      "step": 24050
    },
    {
      "epoch": 1.604,
      "grad_norm": 0.9857080578804016,
      "learning_rate": 3.9975e-05,
      "loss": 0.0027,
      "step": 24060
    },
    {
      "epoch": 1.6046666666666667,
      "grad_norm": 0.39531755447387695,
      "learning_rate": 3.9970833333333334e-05,
      "loss": 0.0026,
      "step": 24070
    },
    {
      "epoch": 1.6053333333333333,
      "grad_norm": 0.32562315464019775,
      "learning_rate": 3.996666666666667e-05,
      "loss": 0.002,
      "step": 24080
    },
    {
      "epoch": 1.6059999999999999,
      "grad_norm": 0.8337164521217346,
      "learning_rate": 3.99625e-05,
      "loss": 0.0035,
      "step": 24090
    },
    {
      "epoch": 1.6066666666666667,
      "grad_norm": 0.3243434727191925,
      "learning_rate": 3.995833333333333e-05,
      "loss": 0.0039,
      "step": 24100
    },
    {
      "epoch": 1.6073333333333333,
      "grad_norm": 0.40574145317077637,
      "learning_rate": 3.995416666666667e-05,
      "loss": 0.0017,
      "step": 24110
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.9033150672912598,
      "learning_rate": 3.995e-05,
      "loss": 0.0023,
      "step": 24120
    },
    {
      "epoch": 1.6086666666666667,
      "grad_norm": 1.2921221256256104,
      "learning_rate": 3.994583333333334e-05,
      "loss": 0.0038,
      "step": 24130
    },
    {
      "epoch": 1.6093333333333333,
      "grad_norm": 0.36222076416015625,
      "learning_rate": 3.9941666666666664e-05,
      "loss": 0.0029,
      "step": 24140
    },
    {
      "epoch": 1.6099999999999999,
      "grad_norm": 0.5861894488334656,
      "learning_rate": 3.99375e-05,
      "loss": 0.0026,
      "step": 24150
    },
    {
      "epoch": 1.6106666666666667,
      "grad_norm": 0.6696480512619019,
      "learning_rate": 3.993333333333333e-05,
      "loss": 0.002,
      "step": 24160
    },
    {
      "epoch": 1.6113333333333333,
      "grad_norm": 0.9985445141792297,
      "learning_rate": 3.992916666666667e-05,
      "loss": 0.0023,
      "step": 24170
    },
    {
      "epoch": 1.612,
      "grad_norm": 0.09057127684354782,
      "learning_rate": 3.9925e-05,
      "loss": 0.0015,
      "step": 24180
    },
    {
      "epoch": 1.6126666666666667,
      "grad_norm": 0.14859941601753235,
      "learning_rate": 3.992083333333333e-05,
      "loss": 0.0032,
      "step": 24190
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.484948992729187,
      "learning_rate": 3.991666666666667e-05,
      "loss": 0.0029,
      "step": 24200
    },
    {
      "epoch": 1.6139999999999999,
      "grad_norm": 0.15486280620098114,
      "learning_rate": 3.99125e-05,
      "loss": 0.0022,
      "step": 24210
    },
    {
      "epoch": 1.6146666666666667,
      "grad_norm": 0.43851447105407715,
      "learning_rate": 3.990833333333334e-05,
      "loss": 0.0047,
      "step": 24220
    },
    {
      "epoch": 1.6153333333333333,
      "grad_norm": 0.44404932856559753,
      "learning_rate": 3.990416666666667e-05,
      "loss": 0.0029,
      "step": 24230
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.6816360950469971,
      "learning_rate": 3.99e-05,
      "loss": 0.0023,
      "step": 24240
    },
    {
      "epoch": 1.6166666666666667,
      "grad_norm": 0.04420170560479164,
      "learning_rate": 3.989583333333334e-05,
      "loss": 0.004,
      "step": 24250
    },
    {
      "epoch": 1.6173333333333333,
      "grad_norm": 0.2790897786617279,
      "learning_rate": 3.989166666666666e-05,
      "loss": 0.0019,
      "step": 24260
    },
    {
      "epoch": 1.6179999999999999,
      "grad_norm": 0.2749386727809906,
      "learning_rate": 3.98875e-05,
      "loss": 0.0031,
      "step": 24270
    },
    {
      "epoch": 1.6186666666666667,
      "grad_norm": 0.533519983291626,
      "learning_rate": 3.988333333333333e-05,
      "loss": 0.0023,
      "step": 24280
    },
    {
      "epoch": 1.6193333333333333,
      "grad_norm": 0.06915175914764404,
      "learning_rate": 3.987916666666667e-05,
      "loss": 0.0021,
      "step": 24290
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.7052954435348511,
      "learning_rate": 3.9875e-05,
      "loss": 0.002,
      "step": 24300
    },
    {
      "epoch": 1.6206666666666667,
      "grad_norm": 0.6859409809112549,
      "learning_rate": 3.987083333333334e-05,
      "loss": 0.0038,
      "step": 24310
    },
    {
      "epoch": 1.6213333333333333,
      "grad_norm": 0.3746488094329834,
      "learning_rate": 3.986666666666667e-05,
      "loss": 0.0023,
      "step": 24320
    },
    {
      "epoch": 1.6219999999999999,
      "grad_norm": 0.04001839831471443,
      "learning_rate": 3.98625e-05,
      "loss": 0.0031,
      "step": 24330
    },
    {
      "epoch": 1.6226666666666667,
      "grad_norm": 0.4078783392906189,
      "learning_rate": 3.985833333333334e-05,
      "loss": 0.0029,
      "step": 24340
    },
    {
      "epoch": 1.6233333333333333,
      "grad_norm": 0.5541219711303711,
      "learning_rate": 3.985416666666667e-05,
      "loss": 0.003,
      "step": 24350
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.10850179195404053,
      "learning_rate": 3.9850000000000006e-05,
      "loss": 0.0027,
      "step": 24360
    },
    {
      "epoch": 1.6246666666666667,
      "grad_norm": 0.2716982364654541,
      "learning_rate": 3.984583333333334e-05,
      "loss": 0.0019,
      "step": 24370
    },
    {
      "epoch": 1.6253333333333333,
      "grad_norm": 0.2808886766433716,
      "learning_rate": 3.984166666666667e-05,
      "loss": 0.0026,
      "step": 24380
    },
    {
      "epoch": 1.626,
      "grad_norm": 0.07456967234611511,
      "learning_rate": 3.98375e-05,
      "loss": 0.002,
      "step": 24390
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.7748794555664062,
      "learning_rate": 3.983333333333333e-05,
      "loss": 0.0028,
      "step": 24400
    },
    {
      "epoch": 1.6273333333333333,
      "grad_norm": 0.6549011468887329,
      "learning_rate": 3.982916666666667e-05,
      "loss": 0.0023,
      "step": 24410
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 0.6460959911346436,
      "learning_rate": 3.9825e-05,
      "loss": 0.002,
      "step": 24420
    },
    {
      "epoch": 1.6286666666666667,
      "grad_norm": 0.5189440846443176,
      "learning_rate": 3.982083333333334e-05,
      "loss": 0.0032,
      "step": 24430
    },
    {
      "epoch": 1.6293333333333333,
      "grad_norm": 0.5131881237030029,
      "learning_rate": 3.981666666666667e-05,
      "loss": 0.0035,
      "step": 24440
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.3851737678050995,
      "learning_rate": 3.9812500000000005e-05,
      "loss": 0.0017,
      "step": 24450
    },
    {
      "epoch": 1.6306666666666667,
      "grad_norm": 0.47387588024139404,
      "learning_rate": 3.9808333333333336e-05,
      "loss": 0.0032,
      "step": 24460
    },
    {
      "epoch": 1.6313333333333333,
      "grad_norm": 0.3506091833114624,
      "learning_rate": 3.980416666666667e-05,
      "loss": 0.0019,
      "step": 24470
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.1890156865119934,
      "learning_rate": 3.9800000000000005e-05,
      "loss": 0.0021,
      "step": 24480
    },
    {
      "epoch": 1.6326666666666667,
      "grad_norm": 0.23637019097805023,
      "learning_rate": 3.9795833333333336e-05,
      "loss": 0.0027,
      "step": 24490
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 0.06789056211709976,
      "learning_rate": 3.979166666666667e-05,
      "loss": 0.0023,
      "step": 24500
    },
    {
      "epoch": 1.634,
      "grad_norm": 0.4402431547641754,
      "learning_rate": 3.97875e-05,
      "loss": 0.0041,
      "step": 24510
    },
    {
      "epoch": 1.6346666666666667,
      "grad_norm": 0.29006046056747437,
      "learning_rate": 3.9783333333333336e-05,
      "loss": 0.002,
      "step": 24520
    },
    {
      "epoch": 1.6353333333333333,
      "grad_norm": 0.36881938576698303,
      "learning_rate": 3.977916666666667e-05,
      "loss": 0.003,
      "step": 24530
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 0.20292484760284424,
      "learning_rate": 3.9775e-05,
      "loss": 0.0028,
      "step": 24540
    },
    {
      "epoch": 1.6366666666666667,
      "grad_norm": 0.2410171627998352,
      "learning_rate": 3.9770833333333336e-05,
      "loss": 0.0023,
      "step": 24550
    },
    {
      "epoch": 1.6373333333333333,
      "grad_norm": 0.8576049208641052,
      "learning_rate": 3.9766666666666667e-05,
      "loss": 0.0033,
      "step": 24560
    },
    {
      "epoch": 1.638,
      "grad_norm": 0.1354210078716278,
      "learning_rate": 3.9762500000000004e-05,
      "loss": 0.0023,
      "step": 24570
    },
    {
      "epoch": 1.6386666666666667,
      "grad_norm": 0.33188173174858093,
      "learning_rate": 3.9758333333333335e-05,
      "loss": 0.0024,
      "step": 24580
    },
    {
      "epoch": 1.6393333333333333,
      "grad_norm": 0.352977991104126,
      "learning_rate": 3.975416666666667e-05,
      "loss": 0.0029,
      "step": 24590
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.2351883351802826,
      "learning_rate": 3.9750000000000004e-05,
      "loss": 0.0032,
      "step": 24600
    },
    {
      "epoch": 1.6406666666666667,
      "grad_norm": 0.42820847034454346,
      "learning_rate": 3.9745833333333335e-05,
      "loss": 0.0024,
      "step": 24610
    },
    {
      "epoch": 1.6413333333333333,
      "grad_norm": 1.211800217628479,
      "learning_rate": 3.9741666666666666e-05,
      "loss": 0.0025,
      "step": 24620
    },
    {
      "epoch": 1.642,
      "grad_norm": 1.3626545667648315,
      "learning_rate": 3.97375e-05,
      "loss": 0.0025,
      "step": 24630
    },
    {
      "epoch": 1.6426666666666667,
      "grad_norm": 0.4894922971725464,
      "learning_rate": 3.9733333333333335e-05,
      "loss": 0.0025,
      "step": 24640
    },
    {
      "epoch": 1.6433333333333333,
      "grad_norm": 0.3100893795490265,
      "learning_rate": 3.9729166666666666e-05,
      "loss": 0.0022,
      "step": 24650
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 0.09163913130760193,
      "learning_rate": 3.9725e-05,
      "loss": 0.0018,
      "step": 24660
    },
    {
      "epoch": 1.6446666666666667,
      "grad_norm": 1.4149162769317627,
      "learning_rate": 3.9720833333333334e-05,
      "loss": 0.003,
      "step": 24670
    },
    {
      "epoch": 1.6453333333333333,
      "grad_norm": 0.17328959703445435,
      "learning_rate": 3.9716666666666665e-05,
      "loss": 0.0034,
      "step": 24680
    },
    {
      "epoch": 1.646,
      "grad_norm": 0.19115273654460907,
      "learning_rate": 3.97125e-05,
      "loss": 0.0021,
      "step": 24690
    },
    {
      "epoch": 1.6466666666666665,
      "grad_norm": 1.0700095891952515,
      "learning_rate": 3.9708333333333334e-05,
      "loss": 0.0026,
      "step": 24700
    },
    {
      "epoch": 1.6473333333333333,
      "grad_norm": 0.203033909201622,
      "learning_rate": 3.970416666666667e-05,
      "loss": 0.0029,
      "step": 24710
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 1.074930191040039,
      "learning_rate": 3.97e-05,
      "loss": 0.0023,
      "step": 24720
    },
    {
      "epoch": 1.6486666666666667,
      "grad_norm": 0.8168829083442688,
      "learning_rate": 3.969583333333334e-05,
      "loss": 0.003,
      "step": 24730
    },
    {
      "epoch": 1.6493333333333333,
      "grad_norm": 0.8610550165176392,
      "learning_rate": 3.9691666666666665e-05,
      "loss": 0.0024,
      "step": 24740
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.43534401059150696,
      "learning_rate": 3.96875e-05,
      "loss": 0.0028,
      "step": 24750
    },
    {
      "epoch": 1.6506666666666665,
      "grad_norm": 1.0736160278320312,
      "learning_rate": 3.9683333333333333e-05,
      "loss": 0.0029,
      "step": 24760
    },
    {
      "epoch": 1.6513333333333333,
      "grad_norm": 1.0298010110855103,
      "learning_rate": 3.9679166666666664e-05,
      "loss": 0.0023,
      "step": 24770
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 0.24196550250053406,
      "learning_rate": 3.9675e-05,
      "loss": 0.0031,
      "step": 24780
    },
    {
      "epoch": 1.6526666666666667,
      "grad_norm": 0.5532404780387878,
      "learning_rate": 3.967083333333333e-05,
      "loss": 0.0025,
      "step": 24790
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.6522728800773621,
      "learning_rate": 3.966666666666667e-05,
      "loss": 0.0019,
      "step": 24800
    },
    {
      "epoch": 1.654,
      "grad_norm": 0.27545854449272156,
      "learning_rate": 3.96625e-05,
      "loss": 0.0025,
      "step": 24810
    },
    {
      "epoch": 1.6546666666666665,
      "grad_norm": 0.4219144582748413,
      "learning_rate": 3.965833333333334e-05,
      "loss": 0.0019,
      "step": 24820
    },
    {
      "epoch": 1.6553333333333333,
      "grad_norm": 0.23670072853565216,
      "learning_rate": 3.965416666666667e-05,
      "loss": 0.0021,
      "step": 24830
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.8431153297424316,
      "learning_rate": 3.965e-05,
      "loss": 0.0018,
      "step": 24840
    },
    {
      "epoch": 1.6566666666666667,
      "grad_norm": 0.7551458477973938,
      "learning_rate": 3.964583333333334e-05,
      "loss": 0.0025,
      "step": 24850
    },
    {
      "epoch": 1.6573333333333333,
      "grad_norm": 0.05423437058925629,
      "learning_rate": 3.9641666666666663e-05,
      "loss": 0.002,
      "step": 24860
    },
    {
      "epoch": 1.658,
      "grad_norm": 0.4981920123100281,
      "learning_rate": 3.96375e-05,
      "loss": 0.0019,
      "step": 24870
    },
    {
      "epoch": 1.6586666666666665,
      "grad_norm": 0.36631786823272705,
      "learning_rate": 3.963333333333333e-05,
      "loss": 0.0031,
      "step": 24880
    },
    {
      "epoch": 1.6593333333333333,
      "grad_norm": 0.36828121542930603,
      "learning_rate": 3.962916666666667e-05,
      "loss": 0.0024,
      "step": 24890
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 0.08486434817314148,
      "learning_rate": 3.9625e-05,
      "loss": 0.0021,
      "step": 24900
    },
    {
      "epoch": 1.6606666666666667,
      "grad_norm": 0.832685649394989,
      "learning_rate": 3.962083333333333e-05,
      "loss": 0.0028,
      "step": 24910
    },
    {
      "epoch": 1.6613333333333333,
      "grad_norm": 0.20322178304195404,
      "learning_rate": 3.961666666666667e-05,
      "loss": 0.003,
      "step": 24920
    },
    {
      "epoch": 1.662,
      "grad_norm": 0.5343401432037354,
      "learning_rate": 3.96125e-05,
      "loss": 0.0029,
      "step": 24930
    },
    {
      "epoch": 1.6626666666666665,
      "grad_norm": 0.3240778148174286,
      "learning_rate": 3.960833333333334e-05,
      "loss": 0.0023,
      "step": 24940
    },
    {
      "epoch": 1.6633333333333333,
      "grad_norm": 0.17759431898593903,
      "learning_rate": 3.960416666666667e-05,
      "loss": 0.0018,
      "step": 24950
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.44034090638160706,
      "learning_rate": 3.960000000000001e-05,
      "loss": 0.003,
      "step": 24960
    },
    {
      "epoch": 1.6646666666666667,
      "grad_norm": 0.9953340888023376,
      "learning_rate": 3.959583333333334e-05,
      "loss": 0.0022,
      "step": 24970
    },
    {
      "epoch": 1.6653333333333333,
      "grad_norm": 0.3256671130657196,
      "learning_rate": 3.959166666666667e-05,
      "loss": 0.0027,
      "step": 24980
    },
    {
      "epoch": 1.666,
      "grad_norm": 0.7668209075927734,
      "learning_rate": 3.95875e-05,
      "loss": 0.0024,
      "step": 24990
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.3716791570186615,
      "learning_rate": 3.958333333333333e-05,
      "loss": 0.0017,
      "step": 25000
    },
    {
      "epoch": 1.6673333333333333,
      "grad_norm": 0.36440208554267883,
      "learning_rate": 3.957916666666667e-05,
      "loss": 0.0025,
      "step": 25010
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 0.22982722520828247,
      "learning_rate": 3.9575e-05,
      "loss": 0.0015,
      "step": 25020
    },
    {
      "epoch": 1.6686666666666667,
      "grad_norm": 0.24758177995681763,
      "learning_rate": 3.957083333333334e-05,
      "loss": 0.002,
      "step": 25030
    },
    {
      "epoch": 1.6693333333333333,
      "grad_norm": 0.5126760005950928,
      "learning_rate": 3.956666666666667e-05,
      "loss": 0.0018,
      "step": 25040
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.5277827978134155,
      "learning_rate": 3.95625e-05,
      "loss": 0.0028,
      "step": 25050
    },
    {
      "epoch": 1.6706666666666665,
      "grad_norm": 0.8419413566589355,
      "learning_rate": 3.955833333333334e-05,
      "loss": 0.0016,
      "step": 25060
    },
    {
      "epoch": 1.6713333333333333,
      "grad_norm": 0.07935528457164764,
      "learning_rate": 3.955416666666667e-05,
      "loss": 0.0029,
      "step": 25070
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.9291000366210938,
      "learning_rate": 3.9550000000000006e-05,
      "loss": 0.0034,
      "step": 25080
    },
    {
      "epoch": 1.6726666666666667,
      "grad_norm": 0.7665714621543884,
      "learning_rate": 3.954583333333334e-05,
      "loss": 0.0029,
      "step": 25090
    },
    {
      "epoch": 1.6733333333333333,
      "grad_norm": 0.4518510401248932,
      "learning_rate": 3.9541666666666675e-05,
      "loss": 0.0028,
      "step": 25100
    },
    {
      "epoch": 1.674,
      "grad_norm": 0.21867534518241882,
      "learning_rate": 3.95375e-05,
      "loss": 0.0026,
      "step": 25110
    },
    {
      "epoch": 1.6746666666666665,
      "grad_norm": 0.6874381899833679,
      "learning_rate": 3.9533333333333337e-05,
      "loss": 0.0023,
      "step": 25120
    },
    {
      "epoch": 1.6753333333333333,
      "grad_norm": 0.7680776715278625,
      "learning_rate": 3.952916666666667e-05,
      "loss": 0.002,
      "step": 25130
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 0.1550808846950531,
      "learning_rate": 3.9525e-05,
      "loss": 0.0027,
      "step": 25140
    },
    {
      "epoch": 1.6766666666666667,
      "grad_norm": 0.280041366815567,
      "learning_rate": 3.9520833333333336e-05,
      "loss": 0.0029,
      "step": 25150
    },
    {
      "epoch": 1.6773333333333333,
      "grad_norm": 1.1020879745483398,
      "learning_rate": 3.951666666666667e-05,
      "loss": 0.0017,
      "step": 25160
    },
    {
      "epoch": 1.678,
      "grad_norm": 0.6689068078994751,
      "learning_rate": 3.9512500000000005e-05,
      "loss": 0.0039,
      "step": 25170
    },
    {
      "epoch": 1.6786666666666665,
      "grad_norm": 0.23807518184185028,
      "learning_rate": 3.9508333333333336e-05,
      "loss": 0.0044,
      "step": 25180
    },
    {
      "epoch": 1.6793333333333333,
      "grad_norm": 0.9709106683731079,
      "learning_rate": 3.950416666666667e-05,
      "loss": 0.0031,
      "step": 25190
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.9928411245346069,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 0.0034,
      "step": 25200
    },
    {
      "epoch": 1.6806666666666668,
      "grad_norm": 0.15950380265712738,
      "learning_rate": 3.9495833333333336e-05,
      "loss": 0.0035,
      "step": 25210
    },
    {
      "epoch": 1.6813333333333333,
      "grad_norm": 0.4660446345806122,
      "learning_rate": 3.9491666666666673e-05,
      "loss": 0.0027,
      "step": 25220
    },
    {
      "epoch": 1.682,
      "grad_norm": 0.4216170310974121,
      "learning_rate": 3.94875e-05,
      "loss": 0.0024,
      "step": 25230
    },
    {
      "epoch": 1.6826666666666665,
      "grad_norm": 0.05934230983257294,
      "learning_rate": 3.9483333333333335e-05,
      "loss": 0.0017,
      "step": 25240
    },
    {
      "epoch": 1.6833333333333333,
      "grad_norm": 0.30557432770729065,
      "learning_rate": 3.9479166666666666e-05,
      "loss": 0.0017,
      "step": 25250
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 0.34495291113853455,
      "learning_rate": 3.9475000000000004e-05,
      "loss": 0.0018,
      "step": 25260
    },
    {
      "epoch": 1.6846666666666668,
      "grad_norm": 0.41660696268081665,
      "learning_rate": 3.9470833333333335e-05,
      "loss": 0.0017,
      "step": 25270
    },
    {
      "epoch": 1.6853333333333333,
      "grad_norm": 0.5708158612251282,
      "learning_rate": 3.9466666666666666e-05,
      "loss": 0.0028,
      "step": 25280
    },
    {
      "epoch": 1.686,
      "grad_norm": 0.7770626544952393,
      "learning_rate": 3.9462500000000004e-05,
      "loss": 0.0022,
      "step": 25290
    },
    {
      "epoch": 1.6866666666666665,
      "grad_norm": 0.5007228851318359,
      "learning_rate": 3.9458333333333335e-05,
      "loss": 0.0027,
      "step": 25300
    },
    {
      "epoch": 1.6873333333333334,
      "grad_norm": 0.34874340891838074,
      "learning_rate": 3.945416666666667e-05,
      "loss": 0.0026,
      "step": 25310
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.3604185879230499,
      "learning_rate": 3.9450000000000003e-05,
      "loss": 0.0022,
      "step": 25320
    },
    {
      "epoch": 1.6886666666666668,
      "grad_norm": 0.4172418713569641,
      "learning_rate": 3.9445833333333334e-05,
      "loss": 0.0026,
      "step": 25330
    },
    {
      "epoch": 1.6893333333333334,
      "grad_norm": 0.2825094759464264,
      "learning_rate": 3.944166666666667e-05,
      "loss": 0.0034,
      "step": 25340
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.8680299520492554,
      "learning_rate": 3.9437499999999996e-05,
      "loss": 0.002,
      "step": 25350
    },
    {
      "epoch": 1.6906666666666665,
      "grad_norm": 0.8096643090248108,
      "learning_rate": 3.9433333333333334e-05,
      "loss": 0.0022,
      "step": 25360
    },
    {
      "epoch": 1.6913333333333334,
      "grad_norm": 0.786389946937561,
      "learning_rate": 3.9429166666666665e-05,
      "loss": 0.0034,
      "step": 25370
    },
    {
      "epoch": 1.692,
      "grad_norm": 0.7297549247741699,
      "learning_rate": 3.9425e-05,
      "loss": 0.0022,
      "step": 25380
    },
    {
      "epoch": 1.6926666666666668,
      "grad_norm": 0.31373703479766846,
      "learning_rate": 3.9420833333333334e-05,
      "loss": 0.0021,
      "step": 25390
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.8864315152168274,
      "learning_rate": 3.941666666666667e-05,
      "loss": 0.0033,
      "step": 25400
    },
    {
      "epoch": 1.694,
      "grad_norm": 0.48878833651542664,
      "learning_rate": 3.94125e-05,
      "loss": 0.0035,
      "step": 25410
    },
    {
      "epoch": 1.6946666666666665,
      "grad_norm": 0.16636274755001068,
      "learning_rate": 3.9408333333333334e-05,
      "loss": 0.0017,
      "step": 25420
    },
    {
      "epoch": 1.6953333333333334,
      "grad_norm": 0.6201583743095398,
      "learning_rate": 3.940416666666667e-05,
      "loss": 0.002,
      "step": 25430
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.31227540969848633,
      "learning_rate": 3.94e-05,
      "loss": 0.0022,
      "step": 25440
    },
    {
      "epoch": 1.6966666666666668,
      "grad_norm": 0.6222813129425049,
      "learning_rate": 3.939583333333334e-05,
      "loss": 0.002,
      "step": 25450
    },
    {
      "epoch": 1.6973333333333334,
      "grad_norm": 0.5770370960235596,
      "learning_rate": 3.939166666666667e-05,
      "loss": 0.0038,
      "step": 25460
    },
    {
      "epoch": 1.698,
      "grad_norm": 0.3659217059612274,
      "learning_rate": 3.93875e-05,
      "loss": 0.0033,
      "step": 25470
    },
    {
      "epoch": 1.6986666666666665,
      "grad_norm": 0.6268718242645264,
      "learning_rate": 3.938333333333333e-05,
      "loss": 0.0033,
      "step": 25480
    },
    {
      "epoch": 1.6993333333333334,
      "grad_norm": 0.8618601560592651,
      "learning_rate": 3.9379166666666664e-05,
      "loss": 0.0027,
      "step": 25490
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.16345027089118958,
      "learning_rate": 3.9375e-05,
      "loss": 0.0027,
      "step": 25500
    },
    {
      "epoch": 1.7006666666666668,
      "grad_norm": 0.2498033195734024,
      "learning_rate": 3.937083333333333e-05,
      "loss": 0.0017,
      "step": 25510
    },
    {
      "epoch": 1.7013333333333334,
      "grad_norm": 0.6643251180648804,
      "learning_rate": 3.936666666666667e-05,
      "loss": 0.003,
      "step": 25520
    },
    {
      "epoch": 1.702,
      "grad_norm": 0.9097817540168762,
      "learning_rate": 3.93625e-05,
      "loss": 0.0023,
      "step": 25530
    },
    {
      "epoch": 1.7026666666666666,
      "grad_norm": 0.3055444359779358,
      "learning_rate": 3.935833333333334e-05,
      "loss": 0.0031,
      "step": 25540
    },
    {
      "epoch": 1.7033333333333334,
      "grad_norm": 0.9616621136665344,
      "learning_rate": 3.935416666666667e-05,
      "loss": 0.0023,
      "step": 25550
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.5839217305183411,
      "learning_rate": 3.935e-05,
      "loss": 0.003,
      "step": 25560
    },
    {
      "epoch": 1.7046666666666668,
      "grad_norm": 0.4196002781391144,
      "learning_rate": 3.934583333333334e-05,
      "loss": 0.0025,
      "step": 25570
    },
    {
      "epoch": 1.7053333333333334,
      "grad_norm": 0.08982737362384796,
      "learning_rate": 3.934166666666667e-05,
      "loss": 0.0034,
      "step": 25580
    },
    {
      "epoch": 1.706,
      "grad_norm": 0.08808264881372452,
      "learning_rate": 3.93375e-05,
      "loss": 0.0026,
      "step": 25590
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.8682318329811096,
      "learning_rate": 3.933333333333333e-05,
      "loss": 0.0025,
      "step": 25600
    },
    {
      "epoch": 1.7073333333333334,
      "grad_norm": 0.48252472281455994,
      "learning_rate": 3.932916666666667e-05,
      "loss": 0.0029,
      "step": 25610
    },
    {
      "epoch": 1.708,
      "grad_norm": 0.3353409469127655,
      "learning_rate": 3.9325e-05,
      "loss": 0.0027,
      "step": 25620
    },
    {
      "epoch": 1.7086666666666668,
      "grad_norm": 0.34888511896133423,
      "learning_rate": 3.932083333333333e-05,
      "loss": 0.0033,
      "step": 25630
    },
    {
      "epoch": 1.7093333333333334,
      "grad_norm": 1.0393491983413696,
      "learning_rate": 3.931666666666667e-05,
      "loss": 0.0028,
      "step": 25640
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.34030264616012573,
      "learning_rate": 3.93125e-05,
      "loss": 0.003,
      "step": 25650
    },
    {
      "epoch": 1.7106666666666666,
      "grad_norm": 0.6926864981651306,
      "learning_rate": 3.930833333333334e-05,
      "loss": 0.0028,
      "step": 25660
    },
    {
      "epoch": 1.7113333333333334,
      "grad_norm": 0.37925300002098083,
      "learning_rate": 3.930416666666667e-05,
      "loss": 0.0026,
      "step": 25670
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.32462266087532043,
      "learning_rate": 3.9300000000000007e-05,
      "loss": 0.0023,
      "step": 25680
    },
    {
      "epoch": 1.7126666666666668,
      "grad_norm": 0.5596267580986023,
      "learning_rate": 3.929583333333334e-05,
      "loss": 0.0021,
      "step": 25690
    },
    {
      "epoch": 1.7133333333333334,
      "grad_norm": 0.6457023024559021,
      "learning_rate": 3.929166666666667e-05,
      "loss": 0.0021,
      "step": 25700
    },
    {
      "epoch": 1.714,
      "grad_norm": 0.29555878043174744,
      "learning_rate": 3.92875e-05,
      "loss": 0.0018,
      "step": 25710
    },
    {
      "epoch": 1.7146666666666666,
      "grad_norm": 0.2498026192188263,
      "learning_rate": 3.928333333333333e-05,
      "loss": 0.0022,
      "step": 25720
    },
    {
      "epoch": 1.7153333333333334,
      "grad_norm": 0.4675346314907074,
      "learning_rate": 3.927916666666667e-05,
      "loss": 0.003,
      "step": 25730
    },
    {
      "epoch": 1.716,
      "grad_norm": 0.2497205138206482,
      "learning_rate": 3.9275e-05,
      "loss": 0.0025,
      "step": 25740
    },
    {
      "epoch": 1.7166666666666668,
      "grad_norm": 0.633745551109314,
      "learning_rate": 3.927083333333334e-05,
      "loss": 0.0031,
      "step": 25750
    },
    {
      "epoch": 1.7173333333333334,
      "grad_norm": 0.397271066904068,
      "learning_rate": 3.926666666666667e-05,
      "loss": 0.0048,
      "step": 25760
    },
    {
      "epoch": 1.718,
      "grad_norm": 0.7574709057807922,
      "learning_rate": 3.92625e-05,
      "loss": 0.0032,
      "step": 25770
    },
    {
      "epoch": 1.7186666666666666,
      "grad_norm": 0.7455918192863464,
      "learning_rate": 3.925833333333334e-05,
      "loss": 0.0028,
      "step": 25780
    },
    {
      "epoch": 1.7193333333333334,
      "grad_norm": 0.17290250957012177,
      "learning_rate": 3.925416666666667e-05,
      "loss": 0.0028,
      "step": 25790
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.6030597686767578,
      "learning_rate": 3.9250000000000005e-05,
      "loss": 0.0024,
      "step": 25800
    },
    {
      "epoch": 1.7206666666666668,
      "grad_norm": 0.8493166565895081,
      "learning_rate": 3.9245833333333336e-05,
      "loss": 0.0019,
      "step": 25810
    },
    {
      "epoch": 1.7213333333333334,
      "grad_norm": 0.2042004019021988,
      "learning_rate": 3.9241666666666674e-05,
      "loss": 0.002,
      "step": 25820
    },
    {
      "epoch": 1.722,
      "grad_norm": 0.10135839879512787,
      "learning_rate": 3.92375e-05,
      "loss": 0.0034,
      "step": 25830
    },
    {
      "epoch": 1.7226666666666666,
      "grad_norm": 1.0724433660507202,
      "learning_rate": 3.9233333333333336e-05,
      "loss": 0.0027,
      "step": 25840
    },
    {
      "epoch": 1.7233333333333334,
      "grad_norm": 0.12740415334701538,
      "learning_rate": 3.922916666666667e-05,
      "loss": 0.0035,
      "step": 25850
    },
    {
      "epoch": 1.724,
      "grad_norm": 0.6367020606994629,
      "learning_rate": 3.9225e-05,
      "loss": 0.0023,
      "step": 25860
    },
    {
      "epoch": 1.7246666666666668,
      "grad_norm": 1.3064496517181396,
      "learning_rate": 3.9220833333333336e-05,
      "loss": 0.0021,
      "step": 25870
    },
    {
      "epoch": 1.7253333333333334,
      "grad_norm": 0.8493000864982605,
      "learning_rate": 3.921666666666667e-05,
      "loss": 0.0018,
      "step": 25880
    },
    {
      "epoch": 1.726,
      "grad_norm": 0.7229061126708984,
      "learning_rate": 3.9212500000000004e-05,
      "loss": 0.0033,
      "step": 25890
    },
    {
      "epoch": 1.7266666666666666,
      "grad_norm": 0.6566073894500732,
      "learning_rate": 3.9208333333333335e-05,
      "loss": 0.0023,
      "step": 25900
    },
    {
      "epoch": 1.7273333333333334,
      "grad_norm": 0.21026644110679626,
      "learning_rate": 3.9204166666666666e-05,
      "loss": 0.0021,
      "step": 25910
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.44041338562965393,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 0.0026,
      "step": 25920
    },
    {
      "epoch": 1.7286666666666668,
      "grad_norm": 0.7137769460678101,
      "learning_rate": 3.9195833333333335e-05,
      "loss": 0.0023,
      "step": 25930
    },
    {
      "epoch": 1.7293333333333334,
      "grad_norm": 0.25779128074645996,
      "learning_rate": 3.919166666666667e-05,
      "loss": 0.002,
      "step": 25940
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.29135340452194214,
      "learning_rate": 3.91875e-05,
      "loss": 0.0024,
      "step": 25950
    },
    {
      "epoch": 1.7306666666666666,
      "grad_norm": 0.5990191102027893,
      "learning_rate": 3.9183333333333335e-05,
      "loss": 0.0034,
      "step": 25960
    },
    {
      "epoch": 1.7313333333333332,
      "grad_norm": 0.7242286801338196,
      "learning_rate": 3.9179166666666666e-05,
      "loss": 0.0031,
      "step": 25970
    },
    {
      "epoch": 1.732,
      "grad_norm": 0.8429045081138611,
      "learning_rate": 3.9175000000000004e-05,
      "loss": 0.0025,
      "step": 25980
    },
    {
      "epoch": 1.7326666666666668,
      "grad_norm": 0.4352145195007324,
      "learning_rate": 3.9170833333333335e-05,
      "loss": 0.0031,
      "step": 25990
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.7553841471672058,
      "learning_rate": 3.9166666666666665e-05,
      "loss": 0.0021,
      "step": 26000
    },
    {
      "epoch": 1.734,
      "grad_norm": 0.86552494764328,
      "learning_rate": 3.91625e-05,
      "loss": 0.0029,
      "step": 26010
    },
    {
      "epoch": 1.7346666666666666,
      "grad_norm": 0.48625123500823975,
      "learning_rate": 3.9158333333333334e-05,
      "loss": 0.0018,
      "step": 26020
    },
    {
      "epoch": 1.7353333333333332,
      "grad_norm": 0.06613614410161972,
      "learning_rate": 3.915416666666667e-05,
      "loss": 0.0016,
      "step": 26030
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.1964849829673767,
      "learning_rate": 3.915e-05,
      "loss": 0.0023,
      "step": 26040
    },
    {
      "epoch": 1.7366666666666668,
      "grad_norm": 0.4069000482559204,
      "learning_rate": 3.9145833333333334e-05,
      "loss": 0.002,
      "step": 26050
    },
    {
      "epoch": 1.7373333333333334,
      "grad_norm": 0.07411254197359085,
      "learning_rate": 3.914166666666667e-05,
      "loss": 0.003,
      "step": 26060
    },
    {
      "epoch": 1.738,
      "grad_norm": 0.6539210081100464,
      "learning_rate": 3.9137499999999996e-05,
      "loss": 0.0026,
      "step": 26070
    },
    {
      "epoch": 1.7386666666666666,
      "grad_norm": 0.17888721823692322,
      "learning_rate": 3.9133333333333334e-05,
      "loss": 0.0021,
      "step": 26080
    },
    {
      "epoch": 1.7393333333333332,
      "grad_norm": 0.27174046635627747,
      "learning_rate": 3.9129166666666665e-05,
      "loss": 0.0025,
      "step": 26090
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.43816623091697693,
      "learning_rate": 3.9125e-05,
      "loss": 0.0022,
      "step": 26100
    },
    {
      "epoch": 1.7406666666666668,
      "grad_norm": 0.24541638791561127,
      "learning_rate": 3.912083333333333e-05,
      "loss": 0.0024,
      "step": 26110
    },
    {
      "epoch": 1.7413333333333334,
      "grad_norm": 0.22723768651485443,
      "learning_rate": 3.911666666666667e-05,
      "loss": 0.0017,
      "step": 26120
    },
    {
      "epoch": 1.742,
      "grad_norm": 0.2280600219964981,
      "learning_rate": 3.91125e-05,
      "loss": 0.003,
      "step": 26130
    },
    {
      "epoch": 1.7426666666666666,
      "grad_norm": 0.10428458452224731,
      "learning_rate": 3.910833333333333e-05,
      "loss": 0.0031,
      "step": 26140
    },
    {
      "epoch": 1.7433333333333332,
      "grad_norm": 0.41206857562065125,
      "learning_rate": 3.910416666666667e-05,
      "loss": 0.0032,
      "step": 26150
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.2846788763999939,
      "learning_rate": 3.91e-05,
      "loss": 0.0021,
      "step": 26160
    },
    {
      "epoch": 1.7446666666666668,
      "grad_norm": 0.7182213664054871,
      "learning_rate": 3.909583333333334e-05,
      "loss": 0.0028,
      "step": 26170
    },
    {
      "epoch": 1.7453333333333334,
      "grad_norm": 0.33537402749061584,
      "learning_rate": 3.909166666666667e-05,
      "loss": 0.0026,
      "step": 26180
    },
    {
      "epoch": 1.746,
      "grad_norm": 0.20851916074752808,
      "learning_rate": 3.90875e-05,
      "loss": 0.0019,
      "step": 26190
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.23445241153240204,
      "learning_rate": 3.908333333333333e-05,
      "loss": 0.0019,
      "step": 26200
    },
    {
      "epoch": 1.7473333333333332,
      "grad_norm": 0.6168071627616882,
      "learning_rate": 3.907916666666666e-05,
      "loss": 0.0026,
      "step": 26210
    },
    {
      "epoch": 1.748,
      "grad_norm": 0.16714592278003693,
      "learning_rate": 3.9075e-05,
      "loss": 0.0021,
      "step": 26220
    },
    {
      "epoch": 1.7486666666666668,
      "grad_norm": 0.551777720451355,
      "learning_rate": 3.907083333333333e-05,
      "loss": 0.002,
      "step": 26230
    },
    {
      "epoch": 1.7493333333333334,
      "grad_norm": 0.24453382194042206,
      "learning_rate": 3.906666666666667e-05,
      "loss": 0.002,
      "step": 26240
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.5965821743011475,
      "learning_rate": 3.90625e-05,
      "loss": 0.0025,
      "step": 26250
    },
    {
      "epoch": 1.7506666666666666,
      "grad_norm": 0.17743252217769623,
      "learning_rate": 3.905833333333334e-05,
      "loss": 0.0025,
      "step": 26260
    },
    {
      "epoch": 1.7513333333333332,
      "grad_norm": 0.381261944770813,
      "learning_rate": 3.905416666666667e-05,
      "loss": 0.0033,
      "step": 26270
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.164383664727211,
      "learning_rate": 3.905e-05,
      "loss": 0.0022,
      "step": 26280
    },
    {
      "epoch": 1.7526666666666668,
      "grad_norm": 0.7692646384239197,
      "learning_rate": 3.904583333333334e-05,
      "loss": 0.0028,
      "step": 26290
    },
    {
      "epoch": 1.7533333333333334,
      "grad_norm": 0.9396654367446899,
      "learning_rate": 3.904166666666667e-05,
      "loss": 0.0032,
      "step": 26300
    },
    {
      "epoch": 1.754,
      "grad_norm": 0.07380033284425735,
      "learning_rate": 3.903750000000001e-05,
      "loss": 0.0018,
      "step": 26310
    },
    {
      "epoch": 1.7546666666666666,
      "grad_norm": 0.9696553349494934,
      "learning_rate": 3.903333333333333e-05,
      "loss": 0.0024,
      "step": 26320
    },
    {
      "epoch": 1.7553333333333332,
      "grad_norm": 0.4168994724750519,
      "learning_rate": 3.902916666666667e-05,
      "loss": 0.0025,
      "step": 26330
    },
    {
      "epoch": 1.756,
      "grad_norm": 0.7500331997871399,
      "learning_rate": 3.9025e-05,
      "loss": 0.0025,
      "step": 26340
    },
    {
      "epoch": 1.7566666666666668,
      "grad_norm": 0.21216055750846863,
      "learning_rate": 3.902083333333333e-05,
      "loss": 0.0024,
      "step": 26350
    },
    {
      "epoch": 1.7573333333333334,
      "grad_norm": 0.788084089756012,
      "learning_rate": 3.901666666666667e-05,
      "loss": 0.003,
      "step": 26360
    },
    {
      "epoch": 1.758,
      "grad_norm": 0.32633697986602783,
      "learning_rate": 3.90125e-05,
      "loss": 0.0031,
      "step": 26370
    },
    {
      "epoch": 1.7586666666666666,
      "grad_norm": 0.07841019332408905,
      "learning_rate": 3.900833333333334e-05,
      "loss": 0.003,
      "step": 26380
    },
    {
      "epoch": 1.7593333333333332,
      "grad_norm": 1.041020154953003,
      "learning_rate": 3.900416666666667e-05,
      "loss": 0.0013,
      "step": 26390
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.696187436580658,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.0034,
      "step": 26400
    },
    {
      "epoch": 1.7606666666666668,
      "grad_norm": 0.4666512906551361,
      "learning_rate": 3.899583333333334e-05,
      "loss": 0.0019,
      "step": 26410
    },
    {
      "epoch": 1.7613333333333334,
      "grad_norm": 0.23613397777080536,
      "learning_rate": 3.899166666666667e-05,
      "loss": 0.0026,
      "step": 26420
    },
    {
      "epoch": 1.762,
      "grad_norm": 0.25766512751579285,
      "learning_rate": 3.8987500000000006e-05,
      "loss": 0.0019,
      "step": 26430
    },
    {
      "epoch": 1.7626666666666666,
      "grad_norm": 0.31850847601890564,
      "learning_rate": 3.898333333333333e-05,
      "loss": 0.0023,
      "step": 26440
    },
    {
      "epoch": 1.7633333333333332,
      "grad_norm": 0.1841357797384262,
      "learning_rate": 3.897916666666667e-05,
      "loss": 0.0026,
      "step": 26450
    },
    {
      "epoch": 1.764,
      "grad_norm": 0.11481815576553345,
      "learning_rate": 3.8975e-05,
      "loss": 0.0022,
      "step": 26460
    },
    {
      "epoch": 1.7646666666666668,
      "grad_norm": 0.29192519187927246,
      "learning_rate": 3.8970833333333336e-05,
      "loss": 0.002,
      "step": 26470
    },
    {
      "epoch": 1.7653333333333334,
      "grad_norm": 0.054561812430620193,
      "learning_rate": 3.896666666666667e-05,
      "loss": 0.0027,
      "step": 26480
    },
    {
      "epoch": 1.766,
      "grad_norm": 0.41513174772262573,
      "learning_rate": 3.8962500000000005e-05,
      "loss": 0.0029,
      "step": 26490
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 0.32778218388557434,
      "learning_rate": 3.8958333333333336e-05,
      "loss": 0.0024,
      "step": 26500
    },
    {
      "epoch": 1.7673333333333332,
      "grad_norm": 0.44122758507728577,
      "learning_rate": 3.895416666666667e-05,
      "loss": 0.0029,
      "step": 26510
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.7188869714736938,
      "learning_rate": 3.8950000000000005e-05,
      "loss": 0.0022,
      "step": 26520
    },
    {
      "epoch": 1.7686666666666668,
      "grad_norm": 0.19957570731639862,
      "learning_rate": 3.8945833333333336e-05,
      "loss": 0.0028,
      "step": 26530
    },
    {
      "epoch": 1.7693333333333334,
      "grad_norm": 0.39515694975852966,
      "learning_rate": 3.8941666666666674e-05,
      "loss": 0.0013,
      "step": 26540
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.49487292766571045,
      "learning_rate": 3.8937500000000005e-05,
      "loss": 0.0022,
      "step": 26550
    },
    {
      "epoch": 1.7706666666666666,
      "grad_norm": 0.21552807092666626,
      "learning_rate": 3.8933333333333336e-05,
      "loss": 0.0028,
      "step": 26560
    },
    {
      "epoch": 1.7713333333333332,
      "grad_norm": 0.31729748845100403,
      "learning_rate": 3.8929166666666666e-05,
      "loss": 0.0025,
      "step": 26570
    },
    {
      "epoch": 1.772,
      "grad_norm": 0.32192304730415344,
      "learning_rate": 3.8925e-05,
      "loss": 0.0032,
      "step": 26580
    },
    {
      "epoch": 1.7726666666666666,
      "grad_norm": 0.4891933500766754,
      "learning_rate": 3.8920833333333335e-05,
      "loss": 0.0027,
      "step": 26590
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.030036291107535362,
      "learning_rate": 3.8916666666666666e-05,
      "loss": 0.0033,
      "step": 26600
    },
    {
      "epoch": 1.774,
      "grad_norm": 0.45615220069885254,
      "learning_rate": 3.8912500000000004e-05,
      "loss": 0.0027,
      "step": 26610
    },
    {
      "epoch": 1.7746666666666666,
      "grad_norm": 0.35885143280029297,
      "learning_rate": 3.8908333333333335e-05,
      "loss": 0.0031,
      "step": 26620
    },
    {
      "epoch": 1.7753333333333332,
      "grad_norm": 0.42624208331108093,
      "learning_rate": 3.890416666666667e-05,
      "loss": 0.0017,
      "step": 26630
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.20996429026126862,
      "learning_rate": 3.8900000000000004e-05,
      "loss": 0.0031,
      "step": 26640
    },
    {
      "epoch": 1.7766666666666666,
      "grad_norm": 0.8944795727729797,
      "learning_rate": 3.8895833333333335e-05,
      "loss": 0.0022,
      "step": 26650
    },
    {
      "epoch": 1.7773333333333334,
      "grad_norm": 0.19290058314800262,
      "learning_rate": 3.889166666666667e-05,
      "loss": 0.0025,
      "step": 26660
    },
    {
      "epoch": 1.778,
      "grad_norm": 0.08635120838880539,
      "learning_rate": 3.88875e-05,
      "loss": 0.0028,
      "step": 26670
    },
    {
      "epoch": 1.7786666666666666,
      "grad_norm": 0.15765197575092316,
      "learning_rate": 3.8883333333333334e-05,
      "loss": 0.0024,
      "step": 26680
    },
    {
      "epoch": 1.7793333333333332,
      "grad_norm": 0.32924219965934753,
      "learning_rate": 3.8879166666666665e-05,
      "loss": 0.0017,
      "step": 26690
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.2685198485851288,
      "learning_rate": 3.8875e-05,
      "loss": 0.0018,
      "step": 26700
    },
    {
      "epoch": 1.7806666666666666,
      "grad_norm": 1.062066674232483,
      "learning_rate": 3.8870833333333334e-05,
      "loss": 0.002,
      "step": 26710
    },
    {
      "epoch": 1.7813333333333334,
      "grad_norm": 0.12623970210552216,
      "learning_rate": 3.8866666666666665e-05,
      "loss": 0.0024,
      "step": 26720
    },
    {
      "epoch": 1.782,
      "grad_norm": 0.8000769019126892,
      "learning_rate": 3.88625e-05,
      "loss": 0.0017,
      "step": 26730
    },
    {
      "epoch": 1.7826666666666666,
      "grad_norm": 0.09564810246229172,
      "learning_rate": 3.8858333333333334e-05,
      "loss": 0.0021,
      "step": 26740
    },
    {
      "epoch": 1.7833333333333332,
      "grad_norm": 0.13312043249607086,
      "learning_rate": 3.885416666666667e-05,
      "loss": 0.0027,
      "step": 26750
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.1948394924402237,
      "learning_rate": 3.885e-05,
      "loss": 0.0022,
      "step": 26760
    },
    {
      "epoch": 1.7846666666666666,
      "grad_norm": 0.6828614473342896,
      "learning_rate": 3.884583333333334e-05,
      "loss": 0.0023,
      "step": 26770
    },
    {
      "epoch": 1.7853333333333334,
      "grad_norm": 0.5324931740760803,
      "learning_rate": 3.884166666666667e-05,
      "loss": 0.0027,
      "step": 26780
    },
    {
      "epoch": 1.786,
      "grad_norm": 0.3941991627216339,
      "learning_rate": 3.88375e-05,
      "loss": 0.0025,
      "step": 26790
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 1.062006950378418,
      "learning_rate": 3.883333333333333e-05,
      "loss": 0.0025,
      "step": 26800
    },
    {
      "epoch": 1.7873333333333332,
      "grad_norm": 0.904609203338623,
      "learning_rate": 3.8829166666666664e-05,
      "loss": 0.0021,
      "step": 26810
    },
    {
      "epoch": 1.788,
      "grad_norm": 0.1442822366952896,
      "learning_rate": 3.8825e-05,
      "loss": 0.0025,
      "step": 26820
    },
    {
      "epoch": 1.7886666666666666,
      "grad_norm": 0.7580817341804504,
      "learning_rate": 3.882083333333333e-05,
      "loss": 0.0024,
      "step": 26830
    },
    {
      "epoch": 1.7893333333333334,
      "grad_norm": 0.15650621056556702,
      "learning_rate": 3.881666666666667e-05,
      "loss": 0.0025,
      "step": 26840
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.12215571105480194,
      "learning_rate": 3.88125e-05,
      "loss": 0.0022,
      "step": 26850
    },
    {
      "epoch": 1.7906666666666666,
      "grad_norm": 0.4468107521533966,
      "learning_rate": 3.880833333333333e-05,
      "loss": 0.0022,
      "step": 26860
    },
    {
      "epoch": 1.7913333333333332,
      "grad_norm": 0.6054049730300903,
      "learning_rate": 3.880416666666667e-05,
      "loss": 0.0019,
      "step": 26870
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.09750208258628845,
      "learning_rate": 3.88e-05,
      "loss": 0.0022,
      "step": 26880
    },
    {
      "epoch": 1.7926666666666666,
      "grad_norm": 0.23238272964954376,
      "learning_rate": 3.879583333333334e-05,
      "loss": 0.0021,
      "step": 26890
    },
    {
      "epoch": 1.7933333333333334,
      "grad_norm": 0.20007950067520142,
      "learning_rate": 3.879166666666667e-05,
      "loss": 0.0023,
      "step": 26900
    },
    {
      "epoch": 1.794,
      "grad_norm": 0.4956490397453308,
      "learning_rate": 3.878750000000001e-05,
      "loss": 0.0025,
      "step": 26910
    },
    {
      "epoch": 1.7946666666666666,
      "grad_norm": 0.08243144303560257,
      "learning_rate": 3.878333333333333e-05,
      "loss": 0.002,
      "step": 26920
    },
    {
      "epoch": 1.7953333333333332,
      "grad_norm": 0.18078961968421936,
      "learning_rate": 3.877916666666667e-05,
      "loss": 0.0037,
      "step": 26930
    },
    {
      "epoch": 1.796,
      "grad_norm": 0.3821382224559784,
      "learning_rate": 3.8775e-05,
      "loss": 0.0029,
      "step": 26940
    },
    {
      "epoch": 1.7966666666666666,
      "grad_norm": 0.2070668339729309,
      "learning_rate": 3.877083333333333e-05,
      "loss": 0.0029,
      "step": 26950
    },
    {
      "epoch": 1.7973333333333334,
      "grad_norm": 0.39818528294563293,
      "learning_rate": 3.876666666666667e-05,
      "loss": 0.0017,
      "step": 26960
    },
    {
      "epoch": 1.798,
      "grad_norm": 0.8659079670906067,
      "learning_rate": 3.87625e-05,
      "loss": 0.0026,
      "step": 26970
    },
    {
      "epoch": 1.7986666666666666,
      "grad_norm": 1.0707396268844604,
      "learning_rate": 3.875833333333334e-05,
      "loss": 0.0022,
      "step": 26980
    },
    {
      "epoch": 1.7993333333333332,
      "grad_norm": 0.365682452917099,
      "learning_rate": 3.875416666666667e-05,
      "loss": 0.0024,
      "step": 26990
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.45316606760025024,
      "learning_rate": 3.875e-05,
      "loss": 0.0026,
      "step": 27000
    },
    {
      "epoch": 1.8006666666666666,
      "grad_norm": 0.10264679789543152,
      "learning_rate": 3.874583333333334e-05,
      "loss": 0.0025,
      "step": 27010
    },
    {
      "epoch": 1.8013333333333335,
      "grad_norm": 0.2376437783241272,
      "learning_rate": 3.874166666666667e-05,
      "loss": 0.0031,
      "step": 27020
    },
    {
      "epoch": 1.802,
      "grad_norm": 0.16195040941238403,
      "learning_rate": 3.8737500000000006e-05,
      "loss": 0.0024,
      "step": 27030
    },
    {
      "epoch": 1.8026666666666666,
      "grad_norm": 0.48445361852645874,
      "learning_rate": 3.873333333333333e-05,
      "loss": 0.0028,
      "step": 27040
    },
    {
      "epoch": 1.8033333333333332,
      "grad_norm": 0.2793242037296295,
      "learning_rate": 3.872916666666667e-05,
      "loss": 0.0022,
      "step": 27050
    },
    {
      "epoch": 1.804,
      "grad_norm": 0.6258070468902588,
      "learning_rate": 3.8725e-05,
      "loss": 0.0022,
      "step": 27060
    },
    {
      "epoch": 1.8046666666666666,
      "grad_norm": 0.5595118403434753,
      "learning_rate": 3.872083333333334e-05,
      "loss": 0.0037,
      "step": 27070
    },
    {
      "epoch": 1.8053333333333335,
      "grad_norm": 0.8234226107597351,
      "learning_rate": 3.871666666666667e-05,
      "loss": 0.0021,
      "step": 27080
    },
    {
      "epoch": 1.806,
      "grad_norm": 0.2130257487297058,
      "learning_rate": 3.87125e-05,
      "loss": 0.0028,
      "step": 27090
    },
    {
      "epoch": 1.8066666666666666,
      "grad_norm": 0.6388428211212158,
      "learning_rate": 3.870833333333334e-05,
      "loss": 0.0028,
      "step": 27100
    },
    {
      "epoch": 1.8073333333333332,
      "grad_norm": 0.5519644021987915,
      "learning_rate": 3.870416666666667e-05,
      "loss": 0.0031,
      "step": 27110
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.21661414206027985,
      "learning_rate": 3.8700000000000006e-05,
      "loss": 0.0025,
      "step": 27120
    },
    {
      "epoch": 1.8086666666666666,
      "grad_norm": 0.25263479351997375,
      "learning_rate": 3.8695833333333337e-05,
      "loss": 0.0036,
      "step": 27130
    },
    {
      "epoch": 1.8093333333333335,
      "grad_norm": 0.7050550580024719,
      "learning_rate": 3.869166666666667e-05,
      "loss": 0.0041,
      "step": 27140
    },
    {
      "epoch": 1.81,
      "grad_norm": 1.1668399572372437,
      "learning_rate": 3.8687500000000005e-05,
      "loss": 0.0023,
      "step": 27150
    },
    {
      "epoch": 1.8106666666666666,
      "grad_norm": 0.3372742533683777,
      "learning_rate": 3.868333333333333e-05,
      "loss": 0.0027,
      "step": 27160
    },
    {
      "epoch": 1.8113333333333332,
      "grad_norm": 0.8879766464233398,
      "learning_rate": 3.867916666666667e-05,
      "loss": 0.0036,
      "step": 27170
    },
    {
      "epoch": 1.812,
      "grad_norm": 0.5558452010154724,
      "learning_rate": 3.8675e-05,
      "loss": 0.0023,
      "step": 27180
    },
    {
      "epoch": 1.8126666666666666,
      "grad_norm": 0.5410279631614685,
      "learning_rate": 3.8670833333333336e-05,
      "loss": 0.0029,
      "step": 27190
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.09772171080112457,
      "learning_rate": 3.866666666666667e-05,
      "loss": 0.0031,
      "step": 27200
    },
    {
      "epoch": 1.814,
      "grad_norm": 0.3553014397621155,
      "learning_rate": 3.8662500000000005e-05,
      "loss": 0.0028,
      "step": 27210
    },
    {
      "epoch": 1.8146666666666667,
      "grad_norm": 0.2123558670282364,
      "learning_rate": 3.8658333333333336e-05,
      "loss": 0.0018,
      "step": 27220
    },
    {
      "epoch": 1.8153333333333332,
      "grad_norm": 0.32113200426101685,
      "learning_rate": 3.8654166666666667e-05,
      "loss": 0.0033,
      "step": 27230
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.5636711120605469,
      "learning_rate": 3.8650000000000004e-05,
      "loss": 0.0029,
      "step": 27240
    },
    {
      "epoch": 1.8166666666666667,
      "grad_norm": 0.3737674057483673,
      "learning_rate": 3.8645833333333335e-05,
      "loss": 0.0027,
      "step": 27250
    },
    {
      "epoch": 1.8173333333333335,
      "grad_norm": 0.9007930755615234,
      "learning_rate": 3.864166666666667e-05,
      "loss": 0.003,
      "step": 27260
    },
    {
      "epoch": 1.818,
      "grad_norm": 0.5595963001251221,
      "learning_rate": 3.8637500000000004e-05,
      "loss": 0.0031,
      "step": 27270
    },
    {
      "epoch": 1.8186666666666667,
      "grad_norm": 0.4270504117012024,
      "learning_rate": 3.8633333333333335e-05,
      "loss": 0.0033,
      "step": 27280
    },
    {
      "epoch": 1.8193333333333332,
      "grad_norm": 0.8062865734100342,
      "learning_rate": 3.8629166666666666e-05,
      "loss": 0.0025,
      "step": 27290
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 0.04572350159287453,
      "learning_rate": 3.8625e-05,
      "loss": 0.0025,
      "step": 27300
    },
    {
      "epoch": 1.8206666666666667,
      "grad_norm": 0.1449657678604126,
      "learning_rate": 3.8620833333333335e-05,
      "loss": 0.0026,
      "step": 27310
    },
    {
      "epoch": 1.8213333333333335,
      "grad_norm": 0.6996849179267883,
      "learning_rate": 3.8616666666666666e-05,
      "loss": 0.0021,
      "step": 27320
    },
    {
      "epoch": 1.822,
      "grad_norm": 0.29522624611854553,
      "learning_rate": 3.8612500000000003e-05,
      "loss": 0.0024,
      "step": 27330
    },
    {
      "epoch": 1.8226666666666667,
      "grad_norm": 0.7343332171440125,
      "learning_rate": 3.8608333333333334e-05,
      "loss": 0.0022,
      "step": 27340
    },
    {
      "epoch": 1.8233333333333333,
      "grad_norm": 0.5666323304176331,
      "learning_rate": 3.860416666666667e-05,
      "loss": 0.0026,
      "step": 27350
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.5116698145866394,
      "learning_rate": 3.86e-05,
      "loss": 0.0022,
      "step": 27360
    },
    {
      "epoch": 1.8246666666666667,
      "grad_norm": 0.05894266813993454,
      "learning_rate": 3.8595833333333334e-05,
      "loss": 0.0027,
      "step": 27370
    },
    {
      "epoch": 1.8253333333333335,
      "grad_norm": 0.06490933895111084,
      "learning_rate": 3.859166666666667e-05,
      "loss": 0.0023,
      "step": 27380
    },
    {
      "epoch": 1.826,
      "grad_norm": 0.3309718072414398,
      "learning_rate": 3.85875e-05,
      "loss": 0.0019,
      "step": 27390
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.5066158175468445,
      "learning_rate": 3.8583333333333334e-05,
      "loss": 0.0027,
      "step": 27400
    },
    {
      "epoch": 1.8273333333333333,
      "grad_norm": 0.15557074546813965,
      "learning_rate": 3.8579166666666665e-05,
      "loss": 0.0023,
      "step": 27410
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 0.6241158246994019,
      "learning_rate": 3.8575e-05,
      "loss": 0.0031,
      "step": 27420
    },
    {
      "epoch": 1.8286666666666667,
      "grad_norm": 0.6727945804595947,
      "learning_rate": 3.8570833333333333e-05,
      "loss": 0.0017,
      "step": 27430
    },
    {
      "epoch": 1.8293333333333335,
      "grad_norm": 0.44508981704711914,
      "learning_rate": 3.8566666666666664e-05,
      "loss": 0.0019,
      "step": 27440
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.22727170586585999,
      "learning_rate": 3.85625e-05,
      "loss": 0.0029,
      "step": 27450
    },
    {
      "epoch": 1.8306666666666667,
      "grad_norm": 0.47669246792793274,
      "learning_rate": 3.855833333333333e-05,
      "loss": 0.0025,
      "step": 27460
    },
    {
      "epoch": 1.8313333333333333,
      "grad_norm": 0.1677996814250946,
      "learning_rate": 3.855416666666667e-05,
      "loss": 0.0018,
      "step": 27470
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.607684314250946,
      "learning_rate": 3.855e-05,
      "loss": 0.0029,
      "step": 27480
    },
    {
      "epoch": 1.8326666666666667,
      "grad_norm": 0.5328733921051025,
      "learning_rate": 3.854583333333334e-05,
      "loss": 0.0041,
      "step": 27490
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.07542811334133148,
      "learning_rate": 3.854166666666667e-05,
      "loss": 0.0021,
      "step": 27500
    },
    {
      "epoch": 1.834,
      "grad_norm": 0.19109761714935303,
      "learning_rate": 3.85375e-05,
      "loss": 0.0024,
      "step": 27510
    },
    {
      "epoch": 1.8346666666666667,
      "grad_norm": 0.5094313621520996,
      "learning_rate": 3.853333333333334e-05,
      "loss": 0.0028,
      "step": 27520
    },
    {
      "epoch": 1.8353333333333333,
      "grad_norm": 0.32901424169540405,
      "learning_rate": 3.8529166666666664e-05,
      "loss": 0.0018,
      "step": 27530
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 0.4075562059879303,
      "learning_rate": 3.8525e-05,
      "loss": 0.0025,
      "step": 27540
    },
    {
      "epoch": 1.8366666666666667,
      "grad_norm": 0.16567058861255646,
      "learning_rate": 3.852083333333333e-05,
      "loss": 0.0017,
      "step": 27550
    },
    {
      "epoch": 1.8373333333333335,
      "grad_norm": 0.2819863557815552,
      "learning_rate": 3.851666666666667e-05,
      "loss": 0.0024,
      "step": 27560
    },
    {
      "epoch": 1.838,
      "grad_norm": 0.5479186773300171,
      "learning_rate": 3.85125e-05,
      "loss": 0.0034,
      "step": 27570
    },
    {
      "epoch": 1.8386666666666667,
      "grad_norm": 0.5124157667160034,
      "learning_rate": 3.850833333333333e-05,
      "loss": 0.0022,
      "step": 27580
    },
    {
      "epoch": 1.8393333333333333,
      "grad_norm": 0.4300909638404846,
      "learning_rate": 3.850416666666667e-05,
      "loss": 0.002,
      "step": 27590
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.1348646581172943,
      "learning_rate": 3.85e-05,
      "loss": 0.0027,
      "step": 27600
    },
    {
      "epoch": 1.8406666666666667,
      "grad_norm": 0.37953534722328186,
      "learning_rate": 3.849583333333334e-05,
      "loss": 0.0023,
      "step": 27610
    },
    {
      "epoch": 1.8413333333333335,
      "grad_norm": 0.48242437839508057,
      "learning_rate": 3.849166666666667e-05,
      "loss": 0.0022,
      "step": 27620
    },
    {
      "epoch": 1.842,
      "grad_norm": 0.5006480813026428,
      "learning_rate": 3.848750000000001e-05,
      "loss": 0.0033,
      "step": 27630
    },
    {
      "epoch": 1.8426666666666667,
      "grad_norm": 0.5680106282234192,
      "learning_rate": 3.848333333333334e-05,
      "loss": 0.0024,
      "step": 27640
    },
    {
      "epoch": 1.8433333333333333,
      "grad_norm": 0.2997511327266693,
      "learning_rate": 3.847916666666667e-05,
      "loss": 0.0024,
      "step": 27650
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 0.20927588641643524,
      "learning_rate": 3.8475e-05,
      "loss": 0.0031,
      "step": 27660
    },
    {
      "epoch": 1.8446666666666667,
      "grad_norm": 0.5137502551078796,
      "learning_rate": 3.847083333333333e-05,
      "loss": 0.0028,
      "step": 27670
    },
    {
      "epoch": 1.8453333333333335,
      "grad_norm": 0.7986009120941162,
      "learning_rate": 3.846666666666667e-05,
      "loss": 0.0033,
      "step": 27680
    },
    {
      "epoch": 1.846,
      "grad_norm": 0.6520601511001587,
      "learning_rate": 3.84625e-05,
      "loss": 0.0027,
      "step": 27690
    },
    {
      "epoch": 1.8466666666666667,
      "grad_norm": 0.6035444140434265,
      "learning_rate": 3.845833333333334e-05,
      "loss": 0.0038,
      "step": 27700
    },
    {
      "epoch": 1.8473333333333333,
      "grad_norm": 0.17447341978549957,
      "learning_rate": 3.845416666666667e-05,
      "loss": 0.0019,
      "step": 27710
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.20893357694149017,
      "learning_rate": 3.845e-05,
      "loss": 0.0027,
      "step": 27720
    },
    {
      "epoch": 1.8486666666666667,
      "grad_norm": 0.056987032294273376,
      "learning_rate": 3.844583333333334e-05,
      "loss": 0.0017,
      "step": 27730
    },
    {
      "epoch": 1.8493333333333335,
      "grad_norm": 0.5178650617599487,
      "learning_rate": 3.844166666666667e-05,
      "loss": 0.0022,
      "step": 27740
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.9139026999473572,
      "learning_rate": 3.8437500000000006e-05,
      "loss": 0.0024,
      "step": 27750
    },
    {
      "epoch": 1.8506666666666667,
      "grad_norm": 0.7223072052001953,
      "learning_rate": 3.843333333333334e-05,
      "loss": 0.0036,
      "step": 27760
    },
    {
      "epoch": 1.8513333333333333,
      "grad_norm": 0.6886359453201294,
      "learning_rate": 3.842916666666667e-05,
      "loss": 0.0017,
      "step": 27770
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 0.7100803256034851,
      "learning_rate": 3.8425e-05,
      "loss": 0.0021,
      "step": 27780
    },
    {
      "epoch": 1.8526666666666667,
      "grad_norm": 0.3542778193950653,
      "learning_rate": 3.842083333333334e-05,
      "loss": 0.0043,
      "step": 27790
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.9624282121658325,
      "learning_rate": 3.841666666666667e-05,
      "loss": 0.0033,
      "step": 27800
    },
    {
      "epoch": 1.854,
      "grad_norm": 0.7501190900802612,
      "learning_rate": 3.84125e-05,
      "loss": 0.0026,
      "step": 27810
    },
    {
      "epoch": 1.8546666666666667,
      "grad_norm": 0.058706194162368774,
      "learning_rate": 3.8408333333333336e-05,
      "loss": 0.0021,
      "step": 27820
    },
    {
      "epoch": 1.8553333333333333,
      "grad_norm": 0.23621444404125214,
      "learning_rate": 3.840416666666667e-05,
      "loss": 0.003,
      "step": 27830
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.6227917671203613,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 0.0023,
      "step": 27840
    },
    {
      "epoch": 1.8566666666666667,
      "grad_norm": 0.4040904641151428,
      "learning_rate": 3.8395833333333336e-05,
      "loss": 0.0022,
      "step": 27850
    },
    {
      "epoch": 1.8573333333333333,
      "grad_norm": 0.8125553131103516,
      "learning_rate": 3.839166666666667e-05,
      "loss": 0.0022,
      "step": 27860
    },
    {
      "epoch": 1.858,
      "grad_norm": 0.13447509706020355,
      "learning_rate": 3.8387500000000005e-05,
      "loss": 0.002,
      "step": 27870
    },
    {
      "epoch": 1.8586666666666667,
      "grad_norm": 0.9412173628807068,
      "learning_rate": 3.8383333333333336e-05,
      "loss": 0.0027,
      "step": 27880
    },
    {
      "epoch": 1.8593333333333333,
      "grad_norm": 0.2216799557209015,
      "learning_rate": 3.837916666666667e-05,
      "loss": 0.0018,
      "step": 27890
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 0.35535532236099243,
      "learning_rate": 3.8375e-05,
      "loss": 0.0025,
      "step": 27900
    },
    {
      "epoch": 1.8606666666666667,
      "grad_norm": 0.28321635723114014,
      "learning_rate": 3.8370833333333335e-05,
      "loss": 0.0019,
      "step": 27910
    },
    {
      "epoch": 1.8613333333333333,
      "grad_norm": 0.04999709501862526,
      "learning_rate": 3.8366666666666666e-05,
      "loss": 0.0018,
      "step": 27920
    },
    {
      "epoch": 1.862,
      "grad_norm": 0.596649706363678,
      "learning_rate": 3.8362500000000004e-05,
      "loss": 0.0026,
      "step": 27930
    },
    {
      "epoch": 1.8626666666666667,
      "grad_norm": 0.489748477935791,
      "learning_rate": 3.8358333333333335e-05,
      "loss": 0.0024,
      "step": 27940
    },
    {
      "epoch": 1.8633333333333333,
      "grad_norm": 0.09388425946235657,
      "learning_rate": 3.8354166666666666e-05,
      "loss": 0.0016,
      "step": 27950
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.5410685539245605,
      "learning_rate": 3.8350000000000004e-05,
      "loss": 0.0022,
      "step": 27960
    },
    {
      "epoch": 1.8646666666666667,
      "grad_norm": 0.39449429512023926,
      "learning_rate": 3.8345833333333335e-05,
      "loss": 0.0026,
      "step": 27970
    },
    {
      "epoch": 1.8653333333333333,
      "grad_norm": 0.7309574484825134,
      "learning_rate": 3.834166666666667e-05,
      "loss": 0.0031,
      "step": 27980
    },
    {
      "epoch": 1.866,
      "grad_norm": 0.3548721969127655,
      "learning_rate": 3.8337500000000004e-05,
      "loss": 0.003,
      "step": 27990
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.9897531867027283,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 0.0031,
      "step": 28000
    },
    {
      "epoch": 1.8673333333333333,
      "grad_norm": 0.5974532961845398,
      "learning_rate": 3.8329166666666665e-05,
      "loss": 0.0027,
      "step": 28010
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 0.3637641370296478,
      "learning_rate": 3.8324999999999996e-05,
      "loss": 0.0023,
      "step": 28020
    },
    {
      "epoch": 1.8686666666666667,
      "grad_norm": 0.8657861948013306,
      "learning_rate": 3.8320833333333334e-05,
      "loss": 0.0033,
      "step": 28030
    },
    {
      "epoch": 1.8693333333333333,
      "grad_norm": 0.20450082421302795,
      "learning_rate": 3.8316666666666665e-05,
      "loss": 0.0026,
      "step": 28040
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.4771246612071991,
      "learning_rate": 3.83125e-05,
      "loss": 0.003,
      "step": 28050
    },
    {
      "epoch": 1.8706666666666667,
      "grad_norm": 0.557232677936554,
      "learning_rate": 3.8308333333333334e-05,
      "loss": 0.0022,
      "step": 28060
    },
    {
      "epoch": 1.8713333333333333,
      "grad_norm": 0.6959980130195618,
      "learning_rate": 3.830416666666667e-05,
      "loss": 0.003,
      "step": 28070
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.579001784324646,
      "learning_rate": 3.83e-05,
      "loss": 0.0022,
      "step": 28080
    },
    {
      "epoch": 1.8726666666666667,
      "grad_norm": 0.11948399245738983,
      "learning_rate": 3.8295833333333334e-05,
      "loss": 0.002,
      "step": 28090
    },
    {
      "epoch": 1.8733333333333333,
      "grad_norm": 0.3001165986061096,
      "learning_rate": 3.829166666666667e-05,
      "loss": 0.0018,
      "step": 28100
    },
    {
      "epoch": 1.874,
      "grad_norm": 0.5594066977500916,
      "learning_rate": 3.82875e-05,
      "loss": 0.0024,
      "step": 28110
    },
    {
      "epoch": 1.8746666666666667,
      "grad_norm": 0.2486095130443573,
      "learning_rate": 3.828333333333334e-05,
      "loss": 0.0021,
      "step": 28120
    },
    {
      "epoch": 1.8753333333333333,
      "grad_norm": 0.08009251952171326,
      "learning_rate": 3.8279166666666664e-05,
      "loss": 0.0026,
      "step": 28130
    },
    {
      "epoch": 1.876,
      "grad_norm": 0.2990782558917999,
      "learning_rate": 3.8275e-05,
      "loss": 0.0032,
      "step": 28140
    },
    {
      "epoch": 1.8766666666666667,
      "grad_norm": 0.2824981212615967,
      "learning_rate": 3.827083333333333e-05,
      "loss": 0.0021,
      "step": 28150
    },
    {
      "epoch": 1.8773333333333333,
      "grad_norm": 0.8221796154975891,
      "learning_rate": 3.8266666666666664e-05,
      "loss": 0.0022,
      "step": 28160
    },
    {
      "epoch": 1.8780000000000001,
      "grad_norm": 0.09512431919574738,
      "learning_rate": 3.82625e-05,
      "loss": 0.0026,
      "step": 28170
    },
    {
      "epoch": 1.8786666666666667,
      "grad_norm": 0.4363037943840027,
      "learning_rate": 3.825833333333333e-05,
      "loss": 0.0035,
      "step": 28180
    },
    {
      "epoch": 1.8793333333333333,
      "grad_norm": 0.30979692935943604,
      "learning_rate": 3.825416666666667e-05,
      "loss": 0.0027,
      "step": 28190
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.048398081213235855,
      "learning_rate": 3.825e-05,
      "loss": 0.0026,
      "step": 28200
    },
    {
      "epoch": 1.8806666666666667,
      "grad_norm": 0.14006875455379486,
      "learning_rate": 3.824583333333334e-05,
      "loss": 0.0019,
      "step": 28210
    },
    {
      "epoch": 1.8813333333333333,
      "grad_norm": 0.970460832118988,
      "learning_rate": 3.824166666666667e-05,
      "loss": 0.0014,
      "step": 28220
    },
    {
      "epoch": 1.8820000000000001,
      "grad_norm": 0.8680813312530518,
      "learning_rate": 3.82375e-05,
      "loss": 0.0026,
      "step": 28230
    },
    {
      "epoch": 1.8826666666666667,
      "grad_norm": 0.4400314390659332,
      "learning_rate": 3.823333333333334e-05,
      "loss": 0.0019,
      "step": 28240
    },
    {
      "epoch": 1.8833333333333333,
      "grad_norm": 0.9817326664924622,
      "learning_rate": 3.822916666666666e-05,
      "loss": 0.0023,
      "step": 28250
    },
    {
      "epoch": 1.884,
      "grad_norm": 0.7172625660896301,
      "learning_rate": 3.8225e-05,
      "loss": 0.0023,
      "step": 28260
    },
    {
      "epoch": 1.8846666666666667,
      "grad_norm": 0.5619585514068604,
      "learning_rate": 3.822083333333333e-05,
      "loss": 0.002,
      "step": 28270
    },
    {
      "epoch": 1.8853333333333333,
      "grad_norm": 0.45722290873527527,
      "learning_rate": 3.821666666666667e-05,
      "loss": 0.0024,
      "step": 28280
    },
    {
      "epoch": 1.8860000000000001,
      "grad_norm": 0.41700175404548645,
      "learning_rate": 3.82125e-05,
      "loss": 0.002,
      "step": 28290
    },
    {
      "epoch": 1.8866666666666667,
      "grad_norm": 0.42062637209892273,
      "learning_rate": 3.820833333333334e-05,
      "loss": 0.0027,
      "step": 28300
    },
    {
      "epoch": 1.8873333333333333,
      "grad_norm": 0.613987922668457,
      "learning_rate": 3.820416666666667e-05,
      "loss": 0.0025,
      "step": 28310
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.2949076294898987,
      "learning_rate": 3.82e-05,
      "loss": 0.0028,
      "step": 28320
    },
    {
      "epoch": 1.8886666666666667,
      "grad_norm": 0.7646782994270325,
      "learning_rate": 3.819583333333334e-05,
      "loss": 0.0027,
      "step": 28330
    },
    {
      "epoch": 1.8893333333333333,
      "grad_norm": 0.3984590172767639,
      "learning_rate": 3.819166666666667e-05,
      "loss": 0.0031,
      "step": 28340
    },
    {
      "epoch": 1.8900000000000001,
      "grad_norm": 0.8484988212585449,
      "learning_rate": 3.818750000000001e-05,
      "loss": 0.0025,
      "step": 28350
    },
    {
      "epoch": 1.8906666666666667,
      "grad_norm": 0.2774326205253601,
      "learning_rate": 3.818333333333334e-05,
      "loss": 0.0019,
      "step": 28360
    },
    {
      "epoch": 1.8913333333333333,
      "grad_norm": 0.35283228754997253,
      "learning_rate": 3.817916666666667e-05,
      "loss": 0.0031,
      "step": 28370
    },
    {
      "epoch": 1.892,
      "grad_norm": 0.7380397319793701,
      "learning_rate": 3.8175e-05,
      "loss": 0.0019,
      "step": 28380
    },
    {
      "epoch": 1.8926666666666667,
      "grad_norm": 0.5932804942131042,
      "learning_rate": 3.817083333333333e-05,
      "loss": 0.0024,
      "step": 28390
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.19337110221385956,
      "learning_rate": 3.816666666666667e-05,
      "loss": 0.0018,
      "step": 28400
    },
    {
      "epoch": 1.8940000000000001,
      "grad_norm": 0.6493966579437256,
      "learning_rate": 3.81625e-05,
      "loss": 0.0029,
      "step": 28410
    },
    {
      "epoch": 1.8946666666666667,
      "grad_norm": 0.45991411805152893,
      "learning_rate": 3.815833333333334e-05,
      "loss": 0.0026,
      "step": 28420
    },
    {
      "epoch": 1.8953333333333333,
      "grad_norm": 0.4222470223903656,
      "learning_rate": 3.815416666666667e-05,
      "loss": 0.0033,
      "step": 28430
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.4546002447605133,
      "learning_rate": 3.8150000000000006e-05,
      "loss": 0.0031,
      "step": 28440
    },
    {
      "epoch": 1.8966666666666665,
      "grad_norm": 0.35767659544944763,
      "learning_rate": 3.814583333333334e-05,
      "loss": 0.003,
      "step": 28450
    },
    {
      "epoch": 1.8973333333333333,
      "grad_norm": 0.7814659476280212,
      "learning_rate": 3.814166666666667e-05,
      "loss": 0.0021,
      "step": 28460
    },
    {
      "epoch": 1.8980000000000001,
      "grad_norm": 0.12075412273406982,
      "learning_rate": 3.8137500000000005e-05,
      "loss": 0.0022,
      "step": 28470
    },
    {
      "epoch": 1.8986666666666667,
      "grad_norm": 0.6412960290908813,
      "learning_rate": 3.8133333333333336e-05,
      "loss": 0.003,
      "step": 28480
    },
    {
      "epoch": 1.8993333333333333,
      "grad_norm": 0.2824527621269226,
      "learning_rate": 3.812916666666667e-05,
      "loss": 0.0029,
      "step": 28490
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.0617816224694252,
      "learning_rate": 3.8125e-05,
      "loss": 0.002,
      "step": 28500
    },
    {
      "epoch": 1.9006666666666665,
      "grad_norm": 0.08170520514249802,
      "learning_rate": 3.8120833333333336e-05,
      "loss": 0.0024,
      "step": 28510
    },
    {
      "epoch": 1.9013333333333333,
      "grad_norm": 0.06045572832226753,
      "learning_rate": 3.811666666666667e-05,
      "loss": 0.0027,
      "step": 28520
    },
    {
      "epoch": 1.9020000000000001,
      "grad_norm": 0.7531806826591492,
      "learning_rate": 3.81125e-05,
      "loss": 0.0027,
      "step": 28530
    },
    {
      "epoch": 1.9026666666666667,
      "grad_norm": 1.3639246225357056,
      "learning_rate": 3.8108333333333336e-05,
      "loss": 0.0017,
      "step": 28540
    },
    {
      "epoch": 1.9033333333333333,
      "grad_norm": 0.3621585965156555,
      "learning_rate": 3.810416666666667e-05,
      "loss": 0.0021,
      "step": 28550
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.41101765632629395,
      "learning_rate": 3.8100000000000005e-05,
      "loss": 0.0024,
      "step": 28560
    },
    {
      "epoch": 1.9046666666666665,
      "grad_norm": 0.5116313695907593,
      "learning_rate": 3.8095833333333335e-05,
      "loss": 0.0018,
      "step": 28570
    },
    {
      "epoch": 1.9053333333333333,
      "grad_norm": 0.48533064126968384,
      "learning_rate": 3.809166666666667e-05,
      "loss": 0.0025,
      "step": 28580
    },
    {
      "epoch": 1.9060000000000001,
      "grad_norm": 0.5367093086242676,
      "learning_rate": 3.8087500000000004e-05,
      "loss": 0.0031,
      "step": 28590
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.19813776016235352,
      "learning_rate": 3.8083333333333335e-05,
      "loss": 0.0019,
      "step": 28600
    },
    {
      "epoch": 1.9073333333333333,
      "grad_norm": 0.8310420513153076,
      "learning_rate": 3.8079166666666666e-05,
      "loss": 0.0024,
      "step": 28610
    },
    {
      "epoch": 1.908,
      "grad_norm": 0.46183034777641296,
      "learning_rate": 3.8075e-05,
      "loss": 0.0026,
      "step": 28620
    },
    {
      "epoch": 1.9086666666666665,
      "grad_norm": 0.38640642166137695,
      "learning_rate": 3.8070833333333335e-05,
      "loss": 0.0033,
      "step": 28630
    },
    {
      "epoch": 1.9093333333333333,
      "grad_norm": 0.23697200417518616,
      "learning_rate": 3.8066666666666666e-05,
      "loss": 0.0039,
      "step": 28640
    },
    {
      "epoch": 1.9100000000000001,
      "grad_norm": 0.44347119331359863,
      "learning_rate": 3.8062500000000004e-05,
      "loss": 0.0025,
      "step": 28650
    },
    {
      "epoch": 1.9106666666666667,
      "grad_norm": 0.26061391830444336,
      "learning_rate": 3.8058333333333335e-05,
      "loss": 0.0021,
      "step": 28660
    },
    {
      "epoch": 1.9113333333333333,
      "grad_norm": 0.6911202073097229,
      "learning_rate": 3.8054166666666666e-05,
      "loss": 0.0015,
      "step": 28670
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.44542497396469116,
      "learning_rate": 3.805e-05,
      "loss": 0.0037,
      "step": 28680
    },
    {
      "epoch": 1.9126666666666665,
      "grad_norm": 0.06201237440109253,
      "learning_rate": 3.8045833333333334e-05,
      "loss": 0.0026,
      "step": 28690
    },
    {
      "epoch": 1.9133333333333333,
      "grad_norm": 0.17172642052173615,
      "learning_rate": 3.804166666666667e-05,
      "loss": 0.0024,
      "step": 28700
    },
    {
      "epoch": 1.9140000000000001,
      "grad_norm": 0.49296241998672485,
      "learning_rate": 3.80375e-05,
      "loss": 0.0038,
      "step": 28710
    },
    {
      "epoch": 1.9146666666666667,
      "grad_norm": 0.38366764783859253,
      "learning_rate": 3.803333333333334e-05,
      "loss": 0.0034,
      "step": 28720
    },
    {
      "epoch": 1.9153333333333333,
      "grad_norm": 0.5315691232681274,
      "learning_rate": 3.8029166666666665e-05,
      "loss": 0.0016,
      "step": 28730
    },
    {
      "epoch": 1.916,
      "grad_norm": 0.6108524799346924,
      "learning_rate": 3.8025e-05,
      "loss": 0.0038,
      "step": 28740
    },
    {
      "epoch": 1.9166666666666665,
      "grad_norm": 0.09035707265138626,
      "learning_rate": 3.8020833333333334e-05,
      "loss": 0.0021,
      "step": 28750
    },
    {
      "epoch": 1.9173333333333333,
      "grad_norm": 0.8326107263565063,
      "learning_rate": 3.8016666666666665e-05,
      "loss": 0.0021,
      "step": 28760
    },
    {
      "epoch": 1.9180000000000001,
      "grad_norm": 0.1277940571308136,
      "learning_rate": 3.80125e-05,
      "loss": 0.0018,
      "step": 28770
    },
    {
      "epoch": 1.9186666666666667,
      "grad_norm": 0.40383976697921753,
      "learning_rate": 3.800833333333333e-05,
      "loss": 0.0027,
      "step": 28780
    },
    {
      "epoch": 1.9193333333333333,
      "grad_norm": 0.18838675320148468,
      "learning_rate": 3.800416666666667e-05,
      "loss": 0.002,
      "step": 28790
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.19941703975200653,
      "learning_rate": 3.8e-05,
      "loss": 0.0025,
      "step": 28800
    },
    {
      "epoch": 1.9206666666666665,
      "grad_norm": 0.5001472234725952,
      "learning_rate": 3.799583333333333e-05,
      "loss": 0.0023,
      "step": 28810
    },
    {
      "epoch": 1.9213333333333333,
      "grad_norm": 0.12341565638780594,
      "learning_rate": 3.799166666666667e-05,
      "loss": 0.0027,
      "step": 28820
    },
    {
      "epoch": 1.9220000000000002,
      "grad_norm": 0.38273417949676514,
      "learning_rate": 3.79875e-05,
      "loss": 0.0026,
      "step": 28830
    },
    {
      "epoch": 1.9226666666666667,
      "grad_norm": 0.1293499767780304,
      "learning_rate": 3.798333333333334e-05,
      "loss": 0.0025,
      "step": 28840
    },
    {
      "epoch": 1.9233333333333333,
      "grad_norm": 0.609502375125885,
      "learning_rate": 3.797916666666667e-05,
      "loss": 0.0025,
      "step": 28850
    },
    {
      "epoch": 1.924,
      "grad_norm": 0.04801972955465317,
      "learning_rate": 3.7975e-05,
      "loss": 0.0027,
      "step": 28860
    },
    {
      "epoch": 1.9246666666666665,
      "grad_norm": 0.12175247818231583,
      "learning_rate": 3.797083333333333e-05,
      "loss": 0.0027,
      "step": 28870
    },
    {
      "epoch": 1.9253333333333333,
      "grad_norm": 0.8606278300285339,
      "learning_rate": 3.796666666666667e-05,
      "loss": 0.002,
      "step": 28880
    },
    {
      "epoch": 1.9260000000000002,
      "grad_norm": 0.12834131717681885,
      "learning_rate": 3.79625e-05,
      "loss": 0.0028,
      "step": 28890
    },
    {
      "epoch": 1.9266666666666667,
      "grad_norm": 0.5707135796546936,
      "learning_rate": 3.795833333333333e-05,
      "loss": 0.0021,
      "step": 28900
    },
    {
      "epoch": 1.9273333333333333,
      "grad_norm": 0.3139572739601135,
      "learning_rate": 3.795416666666667e-05,
      "loss": 0.002,
      "step": 28910
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.6326785683631897,
      "learning_rate": 3.795e-05,
      "loss": 0.0026,
      "step": 28920
    },
    {
      "epoch": 1.9286666666666665,
      "grad_norm": 0.9034450650215149,
      "learning_rate": 3.794583333333334e-05,
      "loss": 0.0022,
      "step": 28930
    },
    {
      "epoch": 1.9293333333333333,
      "grad_norm": 0.08566641062498093,
      "learning_rate": 3.794166666666667e-05,
      "loss": 0.0016,
      "step": 28940
    },
    {
      "epoch": 1.9300000000000002,
      "grad_norm": 0.795588493347168,
      "learning_rate": 3.79375e-05,
      "loss": 0.0025,
      "step": 28950
    },
    {
      "epoch": 1.9306666666666668,
      "grad_norm": 0.5096858143806458,
      "learning_rate": 3.793333333333334e-05,
      "loss": 0.0032,
      "step": 28960
    },
    {
      "epoch": 1.9313333333333333,
      "grad_norm": 0.4713835120201111,
      "learning_rate": 3.792916666666667e-05,
      "loss": 0.002,
      "step": 28970
    },
    {
      "epoch": 1.932,
      "grad_norm": 0.12109997123479843,
      "learning_rate": 3.7925e-05,
      "loss": 0.0025,
      "step": 28980
    },
    {
      "epoch": 1.9326666666666665,
      "grad_norm": 0.859426736831665,
      "learning_rate": 3.792083333333333e-05,
      "loss": 0.0021,
      "step": 28990
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.4757271111011505,
      "learning_rate": 3.791666666666667e-05,
      "loss": 0.0029,
      "step": 29000
    },
    {
      "epoch": 1.9340000000000002,
      "grad_norm": 0.1758333444595337,
      "learning_rate": 3.79125e-05,
      "loss": 0.003,
      "step": 29010
    },
    {
      "epoch": 1.9346666666666668,
      "grad_norm": 0.3473324775695801,
      "learning_rate": 3.790833333333334e-05,
      "loss": 0.0037,
      "step": 29020
    },
    {
      "epoch": 1.9353333333333333,
      "grad_norm": 0.21318452060222626,
      "learning_rate": 3.790416666666667e-05,
      "loss": 0.0031,
      "step": 29030
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.24259702861309052,
      "learning_rate": 3.79e-05,
      "loss": 0.0027,
      "step": 29040
    },
    {
      "epoch": 1.9366666666666665,
      "grad_norm": 0.2279718667268753,
      "learning_rate": 3.789583333333334e-05,
      "loss": 0.0024,
      "step": 29050
    },
    {
      "epoch": 1.9373333333333334,
      "grad_norm": 0.06642955541610718,
      "learning_rate": 3.789166666666667e-05,
      "loss": 0.0025,
      "step": 29060
    },
    {
      "epoch": 1.938,
      "grad_norm": 0.62850421667099,
      "learning_rate": 3.7887500000000006e-05,
      "loss": 0.0031,
      "step": 29070
    },
    {
      "epoch": 1.9386666666666668,
      "grad_norm": 0.30540910363197327,
      "learning_rate": 3.788333333333334e-05,
      "loss": 0.0028,
      "step": 29080
    },
    {
      "epoch": 1.9393333333333334,
      "grad_norm": 0.48584258556365967,
      "learning_rate": 3.787916666666667e-05,
      "loss": 0.0027,
      "step": 29090
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.06004425510764122,
      "learning_rate": 3.7875e-05,
      "loss": 0.0031,
      "step": 29100
    },
    {
      "epoch": 1.9406666666666665,
      "grad_norm": 0.046044204384088516,
      "learning_rate": 3.787083333333333e-05,
      "loss": 0.0028,
      "step": 29110
    },
    {
      "epoch": 1.9413333333333334,
      "grad_norm": 0.31578218936920166,
      "learning_rate": 3.786666666666667e-05,
      "loss": 0.0025,
      "step": 29120
    },
    {
      "epoch": 1.942,
      "grad_norm": 0.14335505664348602,
      "learning_rate": 3.78625e-05,
      "loss": 0.003,
      "step": 29130
    },
    {
      "epoch": 1.9426666666666668,
      "grad_norm": 0.6580841541290283,
      "learning_rate": 3.7858333333333336e-05,
      "loss": 0.0022,
      "step": 29140
    },
    {
      "epoch": 1.9433333333333334,
      "grad_norm": 0.5155126452445984,
      "learning_rate": 3.785416666666667e-05,
      "loss": 0.0025,
      "step": 29150
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.8164697885513306,
      "learning_rate": 3.7850000000000005e-05,
      "loss": 0.0023,
      "step": 29160
    },
    {
      "epoch": 1.9446666666666665,
      "grad_norm": 0.6301010251045227,
      "learning_rate": 3.7845833333333336e-05,
      "loss": 0.0027,
      "step": 29170
    },
    {
      "epoch": 1.9453333333333334,
      "grad_norm": 0.7521503567695618,
      "learning_rate": 3.784166666666667e-05,
      "loss": 0.0019,
      "step": 29180
    },
    {
      "epoch": 1.946,
      "grad_norm": 0.4817618131637573,
      "learning_rate": 3.7837500000000005e-05,
      "loss": 0.0033,
      "step": 29190
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.20662342011928558,
      "learning_rate": 3.7833333333333336e-05,
      "loss": 0.003,
      "step": 29200
    },
    {
      "epoch": 1.9473333333333334,
      "grad_norm": 0.9102148413658142,
      "learning_rate": 3.7829166666666674e-05,
      "loss": 0.0024,
      "step": 29210
    },
    {
      "epoch": 1.948,
      "grad_norm": 0.7495293617248535,
      "learning_rate": 3.7825e-05,
      "loss": 0.0032,
      "step": 29220
    },
    {
      "epoch": 1.9486666666666665,
      "grad_norm": 0.15979444980621338,
      "learning_rate": 3.7820833333333336e-05,
      "loss": 0.0023,
      "step": 29230
    },
    {
      "epoch": 1.9493333333333334,
      "grad_norm": 0.4628688097000122,
      "learning_rate": 3.7816666666666667e-05,
      "loss": 0.0018,
      "step": 29240
    },
    {
      "epoch": 1.95,
      "grad_norm": 0.49881160259246826,
      "learning_rate": 3.78125e-05,
      "loss": 0.0018,
      "step": 29250
    },
    {
      "epoch": 1.9506666666666668,
      "grad_norm": 0.5594063401222229,
      "learning_rate": 3.7808333333333335e-05,
      "loss": 0.0024,
      "step": 29260
    },
    {
      "epoch": 1.9513333333333334,
      "grad_norm": 0.3278788626194,
      "learning_rate": 3.7804166666666666e-05,
      "loss": 0.0038,
      "step": 29270
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.09468968212604523,
      "learning_rate": 3.7800000000000004e-05,
      "loss": 0.0019,
      "step": 29280
    },
    {
      "epoch": 1.9526666666666666,
      "grad_norm": 0.7457286715507507,
      "learning_rate": 3.7795833333333335e-05,
      "loss": 0.0025,
      "step": 29290
    },
    {
      "epoch": 1.9533333333333334,
      "grad_norm": 1.0243712663650513,
      "learning_rate": 3.779166666666667e-05,
      "loss": 0.0019,
      "step": 29300
    },
    {
      "epoch": 1.954,
      "grad_norm": 0.0437350831925869,
      "learning_rate": 3.7787500000000004e-05,
      "loss": 0.0027,
      "step": 29310
    },
    {
      "epoch": 1.9546666666666668,
      "grad_norm": 0.5727301239967346,
      "learning_rate": 3.7783333333333335e-05,
      "loss": 0.0033,
      "step": 29320
    },
    {
      "epoch": 1.9553333333333334,
      "grad_norm": 0.7291285991668701,
      "learning_rate": 3.777916666666667e-05,
      "loss": 0.0028,
      "step": 29330
    },
    {
      "epoch": 1.956,
      "grad_norm": 0.6327577233314514,
      "learning_rate": 3.7775e-05,
      "loss": 0.0023,
      "step": 29340
    },
    {
      "epoch": 1.9566666666666666,
      "grad_norm": 0.7990520000457764,
      "learning_rate": 3.7770833333333334e-05,
      "loss": 0.0019,
      "step": 29350
    },
    {
      "epoch": 1.9573333333333334,
      "grad_norm": 0.056920699775218964,
      "learning_rate": 3.7766666666666665e-05,
      "loss": 0.0017,
      "step": 29360
    },
    {
      "epoch": 1.958,
      "grad_norm": 0.16247715055942535,
      "learning_rate": 3.77625e-05,
      "loss": 0.0021,
      "step": 29370
    },
    {
      "epoch": 1.9586666666666668,
      "grad_norm": 0.16137458384037018,
      "learning_rate": 3.7758333333333334e-05,
      "loss": 0.0022,
      "step": 29380
    },
    {
      "epoch": 1.9593333333333334,
      "grad_norm": 0.22204427421092987,
      "learning_rate": 3.7754166666666665e-05,
      "loss": 0.0024,
      "step": 29390
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.0850883424282074,
      "learning_rate": 3.775e-05,
      "loss": 0.0026,
      "step": 29400
    },
    {
      "epoch": 1.9606666666666666,
      "grad_norm": 0.4345994293689728,
      "learning_rate": 3.7745833333333334e-05,
      "loss": 0.0019,
      "step": 29410
    },
    {
      "epoch": 1.9613333333333334,
      "grad_norm": 0.4398026764392853,
      "learning_rate": 3.774166666666667e-05,
      "loss": 0.0014,
      "step": 29420
    },
    {
      "epoch": 1.962,
      "grad_norm": 0.26505374908447266,
      "learning_rate": 3.77375e-05,
      "loss": 0.0024,
      "step": 29430
    },
    {
      "epoch": 1.9626666666666668,
      "grad_norm": 0.34910568594932556,
      "learning_rate": 3.773333333333334e-05,
      "loss": 0.0015,
      "step": 29440
    },
    {
      "epoch": 1.9633333333333334,
      "grad_norm": 0.37781697511672974,
      "learning_rate": 3.772916666666667e-05,
      "loss": 0.0021,
      "step": 29450
    },
    {
      "epoch": 1.964,
      "grad_norm": 0.3268667459487915,
      "learning_rate": 3.7725e-05,
      "loss": 0.0029,
      "step": 29460
    },
    {
      "epoch": 1.9646666666666666,
      "grad_norm": 0.22737309336662292,
      "learning_rate": 3.772083333333333e-05,
      "loss": 0.0018,
      "step": 29470
    },
    {
      "epoch": 1.9653333333333334,
      "grad_norm": 0.15218421816825867,
      "learning_rate": 3.7716666666666664e-05,
      "loss": 0.002,
      "step": 29480
    },
    {
      "epoch": 1.966,
      "grad_norm": 0.24362623691558838,
      "learning_rate": 3.77125e-05,
      "loss": 0.0015,
      "step": 29490
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 0.0617278553545475,
      "learning_rate": 3.770833333333333e-05,
      "loss": 0.0019,
      "step": 29500
    },
    {
      "epoch": 1.9673333333333334,
      "grad_norm": 0.3027953803539276,
      "learning_rate": 3.770416666666667e-05,
      "loss": 0.0029,
      "step": 29510
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.5747984051704407,
      "learning_rate": 3.77e-05,
      "loss": 0.0021,
      "step": 29520
    },
    {
      "epoch": 1.9686666666666666,
      "grad_norm": 0.4758206307888031,
      "learning_rate": 3.769583333333333e-05,
      "loss": 0.0026,
      "step": 29530
    },
    {
      "epoch": 1.9693333333333334,
      "grad_norm": 0.5712505578994751,
      "learning_rate": 3.769166666666667e-05,
      "loss": 0.0027,
      "step": 29540
    },
    {
      "epoch": 1.97,
      "grad_norm": 0.6057475209236145,
      "learning_rate": 3.76875e-05,
      "loss": 0.003,
      "step": 29550
    },
    {
      "epoch": 1.9706666666666668,
      "grad_norm": 0.2230697125196457,
      "learning_rate": 3.768333333333334e-05,
      "loss": 0.0026,
      "step": 29560
    },
    {
      "epoch": 1.9713333333333334,
      "grad_norm": 0.8480091691017151,
      "learning_rate": 3.767916666666667e-05,
      "loss": 0.0029,
      "step": 29570
    },
    {
      "epoch": 1.972,
      "grad_norm": 0.2256600707769394,
      "learning_rate": 3.7675e-05,
      "loss": 0.0019,
      "step": 29580
    },
    {
      "epoch": 1.9726666666666666,
      "grad_norm": 0.45147189497947693,
      "learning_rate": 3.767083333333333e-05,
      "loss": 0.0012,
      "step": 29590
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.06482455134391785,
      "learning_rate": 3.766666666666667e-05,
      "loss": 0.0021,
      "step": 29600
    },
    {
      "epoch": 1.974,
      "grad_norm": 0.34386929869651794,
      "learning_rate": 3.76625e-05,
      "loss": 0.0032,
      "step": 29610
    },
    {
      "epoch": 1.9746666666666668,
      "grad_norm": 0.08501600474119186,
      "learning_rate": 3.765833333333333e-05,
      "loss": 0.0025,
      "step": 29620
    },
    {
      "epoch": 1.9753333333333334,
      "grad_norm": 0.6459311246871948,
      "learning_rate": 3.765416666666667e-05,
      "loss": 0.0022,
      "step": 29630
    },
    {
      "epoch": 1.976,
      "grad_norm": 0.19795051217079163,
      "learning_rate": 3.765e-05,
      "loss": 0.0024,
      "step": 29640
    },
    {
      "epoch": 1.9766666666666666,
      "grad_norm": 0.7020680904388428,
      "learning_rate": 3.764583333333334e-05,
      "loss": 0.0028,
      "step": 29650
    },
    {
      "epoch": 1.9773333333333334,
      "grad_norm": 0.4469146132469177,
      "learning_rate": 3.764166666666667e-05,
      "loss": 0.0029,
      "step": 29660
    },
    {
      "epoch": 1.978,
      "grad_norm": 0.23873084783554077,
      "learning_rate": 3.76375e-05,
      "loss": 0.0014,
      "step": 29670
    },
    {
      "epoch": 1.9786666666666668,
      "grad_norm": 0.4013995826244354,
      "learning_rate": 3.763333333333334e-05,
      "loss": 0.0032,
      "step": 29680
    },
    {
      "epoch": 1.9793333333333334,
      "grad_norm": 0.4013289511203766,
      "learning_rate": 3.762916666666667e-05,
      "loss": 0.0018,
      "step": 29690
    },
    {
      "epoch": 1.98,
      "grad_norm": 1.0793204307556152,
      "learning_rate": 3.7625e-05,
      "loss": 0.0024,
      "step": 29700
    },
    {
      "epoch": 1.9806666666666666,
      "grad_norm": 0.5501284599304199,
      "learning_rate": 3.762083333333333e-05,
      "loss": 0.0026,
      "step": 29710
    },
    {
      "epoch": 1.9813333333333332,
      "grad_norm": 0.3493656516075134,
      "learning_rate": 3.761666666666667e-05,
      "loss": 0.0032,
      "step": 29720
    },
    {
      "epoch": 1.982,
      "grad_norm": 0.3648200035095215,
      "learning_rate": 3.76125e-05,
      "loss": 0.0024,
      "step": 29730
    },
    {
      "epoch": 1.9826666666666668,
      "grad_norm": 0.40857994556427,
      "learning_rate": 3.760833333333334e-05,
      "loss": 0.002,
      "step": 29740
    },
    {
      "epoch": 1.9833333333333334,
      "grad_norm": 0.3108908236026764,
      "learning_rate": 3.760416666666667e-05,
      "loss": 0.0019,
      "step": 29750
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.43667492270469666,
      "learning_rate": 3.76e-05,
      "loss": 0.0018,
      "step": 29760
    },
    {
      "epoch": 1.9846666666666666,
      "grad_norm": 0.25804778933525085,
      "learning_rate": 3.759583333333334e-05,
      "loss": 0.0023,
      "step": 29770
    },
    {
      "epoch": 1.9853333333333332,
      "grad_norm": 0.17761607468128204,
      "learning_rate": 3.759166666666667e-05,
      "loss": 0.0017,
      "step": 29780
    },
    {
      "epoch": 1.986,
      "grad_norm": 0.9529135227203369,
      "learning_rate": 3.7587500000000006e-05,
      "loss": 0.0023,
      "step": 29790
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 0.6301303505897522,
      "learning_rate": 3.7583333333333337e-05,
      "loss": 0.0021,
      "step": 29800
    },
    {
      "epoch": 1.9873333333333334,
      "grad_norm": 0.2516917288303375,
      "learning_rate": 3.757916666666667e-05,
      "loss": 0.0025,
      "step": 29810
    },
    {
      "epoch": 1.988,
      "grad_norm": 0.3260403871536255,
      "learning_rate": 3.7575e-05,
      "loss": 0.0023,
      "step": 29820
    },
    {
      "epoch": 1.9886666666666666,
      "grad_norm": 0.05575714260339737,
      "learning_rate": 3.757083333333333e-05,
      "loss": 0.0035,
      "step": 29830
    },
    {
      "epoch": 1.9893333333333332,
      "grad_norm": 0.7082778811454773,
      "learning_rate": 3.756666666666667e-05,
      "loss": 0.0025,
      "step": 29840
    },
    {
      "epoch": 1.99,
      "grad_norm": 0.3873913884162903,
      "learning_rate": 3.75625e-05,
      "loss": 0.0035,
      "step": 29850
    },
    {
      "epoch": 1.9906666666666668,
      "grad_norm": 0.5160164833068848,
      "learning_rate": 3.7558333333333336e-05,
      "loss": 0.003,
      "step": 29860
    },
    {
      "epoch": 1.9913333333333334,
      "grad_norm": 0.1633058786392212,
      "learning_rate": 3.755416666666667e-05,
      "loss": 0.002,
      "step": 29870
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.6347034573554993,
      "learning_rate": 3.7550000000000005e-05,
      "loss": 0.0018,
      "step": 29880
    },
    {
      "epoch": 1.9926666666666666,
      "grad_norm": 0.5194934010505676,
      "learning_rate": 3.7545833333333336e-05,
      "loss": 0.0022,
      "step": 29890
    },
    {
      "epoch": 1.9933333333333332,
      "grad_norm": 0.6633479595184326,
      "learning_rate": 3.754166666666667e-05,
      "loss": 0.0032,
      "step": 29900
    },
    {
      "epoch": 1.994,
      "grad_norm": 1.0262516736984253,
      "learning_rate": 3.7537500000000004e-05,
      "loss": 0.0016,
      "step": 29910
    },
    {
      "epoch": 1.9946666666666668,
      "grad_norm": 0.09854263812303543,
      "learning_rate": 3.7533333333333335e-05,
      "loss": 0.0028,
      "step": 29920
    },
    {
      "epoch": 1.9953333333333334,
      "grad_norm": 0.8637401461601257,
      "learning_rate": 3.752916666666667e-05,
      "loss": 0.0027,
      "step": 29930
    },
    {
      "epoch": 1.996,
      "grad_norm": 0.477105975151062,
      "learning_rate": 3.7525e-05,
      "loss": 0.0034,
      "step": 29940
    },
    {
      "epoch": 1.9966666666666666,
      "grad_norm": 0.2540626525878906,
      "learning_rate": 3.7520833333333335e-05,
      "loss": 0.0027,
      "step": 29950
    },
    {
      "epoch": 1.9973333333333332,
      "grad_norm": 0.3162527084350586,
      "learning_rate": 3.7516666666666666e-05,
      "loss": 0.0035,
      "step": 29960
    },
    {
      "epoch": 1.998,
      "grad_norm": 0.5015859007835388,
      "learning_rate": 3.7512500000000004e-05,
      "loss": 0.0047,
      "step": 29970
    },
    {
      "epoch": 1.9986666666666668,
      "grad_norm": 0.05293567106127739,
      "learning_rate": 3.7508333333333335e-05,
      "loss": 0.0019,
      "step": 29980
    },
    {
      "epoch": 1.9993333333333334,
      "grad_norm": 0.476266473531723,
      "learning_rate": 3.7504166666666666e-05,
      "loss": 0.0025,
      "step": 29990
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.48305705189704895,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.0035,
      "step": 30000
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.002584896283224225,
      "eval_runtime": 157.6952,
      "eval_samples_per_second": 1268.27,
      "eval_steps_per_second": 31.707,
      "step": 30000
    },
    {
      "epoch": 2.0006666666666666,
      "grad_norm": 0.16397929191589355,
      "learning_rate": 3.7495833333333334e-05,
      "loss": 0.0026,
      "step": 30010
    },
    {
      "epoch": 2.001333333333333,
      "grad_norm": 0.5315370559692383,
      "learning_rate": 3.749166666666667e-05,
      "loss": 0.0024,
      "step": 30020
    },
    {
      "epoch": 2.002,
      "grad_norm": 0.8278684616088867,
      "learning_rate": 3.74875e-05,
      "loss": 0.0017,
      "step": 30030
    },
    {
      "epoch": 2.002666666666667,
      "grad_norm": 0.9578874111175537,
      "learning_rate": 3.7483333333333334e-05,
      "loss": 0.0021,
      "step": 30040
    },
    {
      "epoch": 2.0033333333333334,
      "grad_norm": 0.34230685234069824,
      "learning_rate": 3.747916666666667e-05,
      "loss": 0.0021,
      "step": 30050
    },
    {
      "epoch": 2.004,
      "grad_norm": 0.5037474632263184,
      "learning_rate": 3.7475e-05,
      "loss": 0.0033,
      "step": 30060
    },
    {
      "epoch": 2.0046666666666666,
      "grad_norm": 0.9180194139480591,
      "learning_rate": 3.7470833333333334e-05,
      "loss": 0.0028,
      "step": 30070
    },
    {
      "epoch": 2.005333333333333,
      "grad_norm": 0.6532093286514282,
      "learning_rate": 3.7466666666666665e-05,
      "loss": 0.002,
      "step": 30080
    },
    {
      "epoch": 2.006,
      "grad_norm": 0.29746225476264954,
      "learning_rate": 3.74625e-05,
      "loss": 0.0032,
      "step": 30090
    },
    {
      "epoch": 2.006666666666667,
      "grad_norm": 0.18001437187194824,
      "learning_rate": 3.7458333333333334e-05,
      "loss": 0.0023,
      "step": 30100
    },
    {
      "epoch": 2.0073333333333334,
      "grad_norm": 0.8294958472251892,
      "learning_rate": 3.745416666666667e-05,
      "loss": 0.0025,
      "step": 30110
    },
    {
      "epoch": 2.008,
      "grad_norm": 0.4727746248245239,
      "learning_rate": 3.745e-05,
      "loss": 0.0033,
      "step": 30120
    },
    {
      "epoch": 2.0086666666666666,
      "grad_norm": 0.37577125430107117,
      "learning_rate": 3.744583333333333e-05,
      "loss": 0.0021,
      "step": 30130
    },
    {
      "epoch": 2.009333333333333,
      "grad_norm": 0.7914257645606995,
      "learning_rate": 3.744166666666667e-05,
      "loss": 0.002,
      "step": 30140
    },
    {
      "epoch": 2.01,
      "grad_norm": 1.2786962985992432,
      "learning_rate": 3.74375e-05,
      "loss": 0.0032,
      "step": 30150
    },
    {
      "epoch": 2.010666666666667,
      "grad_norm": 0.7932385802268982,
      "learning_rate": 3.743333333333334e-05,
      "loss": 0.0023,
      "step": 30160
    },
    {
      "epoch": 2.0113333333333334,
      "grad_norm": 0.5254717469215393,
      "learning_rate": 3.742916666666667e-05,
      "loss": 0.0028,
      "step": 30170
    },
    {
      "epoch": 2.012,
      "grad_norm": 0.4901145100593567,
      "learning_rate": 3.7425e-05,
      "loss": 0.0027,
      "step": 30180
    },
    {
      "epoch": 2.0126666666666666,
      "grad_norm": 0.32939690351486206,
      "learning_rate": 3.742083333333333e-05,
      "loss": 0.0025,
      "step": 30190
    },
    {
      "epoch": 2.013333333333333,
      "grad_norm": 0.3127196729183197,
      "learning_rate": 3.7416666666666664e-05,
      "loss": 0.0022,
      "step": 30200
    },
    {
      "epoch": 2.014,
      "grad_norm": 0.2683500349521637,
      "learning_rate": 3.74125e-05,
      "loss": 0.002,
      "step": 30210
    },
    {
      "epoch": 2.014666666666667,
      "grad_norm": 0.4558480978012085,
      "learning_rate": 3.740833333333333e-05,
      "loss": 0.0013,
      "step": 30220
    },
    {
      "epoch": 2.0153333333333334,
      "grad_norm": 0.16080085933208466,
      "learning_rate": 3.740416666666667e-05,
      "loss": 0.0024,
      "step": 30230
    },
    {
      "epoch": 2.016,
      "grad_norm": 0.054428983479738235,
      "learning_rate": 3.74e-05,
      "loss": 0.0022,
      "step": 30240
    },
    {
      "epoch": 2.0166666666666666,
      "grad_norm": 0.8228318691253662,
      "learning_rate": 3.739583333333334e-05,
      "loss": 0.0037,
      "step": 30250
    },
    {
      "epoch": 2.017333333333333,
      "grad_norm": 0.4221019446849823,
      "learning_rate": 3.739166666666667e-05,
      "loss": 0.0032,
      "step": 30260
    },
    {
      "epoch": 2.018,
      "grad_norm": 0.34959474205970764,
      "learning_rate": 3.73875e-05,
      "loss": 0.0023,
      "step": 30270
    },
    {
      "epoch": 2.018666666666667,
      "grad_norm": 0.8677012920379639,
      "learning_rate": 3.738333333333334e-05,
      "loss": 0.002,
      "step": 30280
    },
    {
      "epoch": 2.0193333333333334,
      "grad_norm": 0.34478527307510376,
      "learning_rate": 3.737916666666667e-05,
      "loss": 0.0026,
      "step": 30290
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.7200069427490234,
      "learning_rate": 3.737500000000001e-05,
      "loss": 0.003,
      "step": 30300
    },
    {
      "epoch": 2.0206666666666666,
      "grad_norm": 0.7742915749549866,
      "learning_rate": 3.737083333333333e-05,
      "loss": 0.0015,
      "step": 30310
    },
    {
      "epoch": 2.021333333333333,
      "grad_norm": 0.532977283000946,
      "learning_rate": 3.736666666666667e-05,
      "loss": 0.0027,
      "step": 30320
    },
    {
      "epoch": 2.022,
      "grad_norm": 0.8263256549835205,
      "learning_rate": 3.73625e-05,
      "loss": 0.0018,
      "step": 30330
    },
    {
      "epoch": 2.022666666666667,
      "grad_norm": 0.08690214157104492,
      "learning_rate": 3.735833333333333e-05,
      "loss": 0.0023,
      "step": 30340
    },
    {
      "epoch": 2.0233333333333334,
      "grad_norm": 0.13923203945159912,
      "learning_rate": 3.735416666666667e-05,
      "loss": 0.002,
      "step": 30350
    },
    {
      "epoch": 2.024,
      "grad_norm": 0.6791432499885559,
      "learning_rate": 3.735e-05,
      "loss": 0.0017,
      "step": 30360
    },
    {
      "epoch": 2.0246666666666666,
      "grad_norm": 0.8195782899856567,
      "learning_rate": 3.734583333333334e-05,
      "loss": 0.0027,
      "step": 30370
    },
    {
      "epoch": 2.025333333333333,
      "grad_norm": 0.2838897407054901,
      "learning_rate": 3.734166666666667e-05,
      "loss": 0.0031,
      "step": 30380
    },
    {
      "epoch": 2.026,
      "grad_norm": 0.14326606690883636,
      "learning_rate": 3.7337500000000006e-05,
      "loss": 0.003,
      "step": 30390
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 0.43060535192489624,
      "learning_rate": 3.733333333333334e-05,
      "loss": 0.0029,
      "step": 30400
    },
    {
      "epoch": 2.0273333333333334,
      "grad_norm": 0.23888057470321655,
      "learning_rate": 3.732916666666667e-05,
      "loss": 0.0029,
      "step": 30410
    },
    {
      "epoch": 2.028,
      "grad_norm": 0.45783939957618713,
      "learning_rate": 3.7325000000000006e-05,
      "loss": 0.0023,
      "step": 30420
    },
    {
      "epoch": 2.0286666666666666,
      "grad_norm": 0.7134948372840881,
      "learning_rate": 3.732083333333333e-05,
      "loss": 0.0026,
      "step": 30430
    },
    {
      "epoch": 2.029333333333333,
      "grad_norm": 0.4381023645401001,
      "learning_rate": 3.731666666666667e-05,
      "loss": 0.0028,
      "step": 30440
    },
    {
      "epoch": 2.03,
      "grad_norm": 0.2840098440647125,
      "learning_rate": 3.73125e-05,
      "loss": 0.0018,
      "step": 30450
    },
    {
      "epoch": 2.030666666666667,
      "grad_norm": 0.5273054838180542,
      "learning_rate": 3.730833333333334e-05,
      "loss": 0.0021,
      "step": 30460
    },
    {
      "epoch": 2.0313333333333334,
      "grad_norm": 1.0127876996994019,
      "learning_rate": 3.730416666666667e-05,
      "loss": 0.0024,
      "step": 30470
    },
    {
      "epoch": 2.032,
      "grad_norm": 1.0583354234695435,
      "learning_rate": 3.73e-05,
      "loss": 0.0026,
      "step": 30480
    },
    {
      "epoch": 2.0326666666666666,
      "grad_norm": 1.5372521877288818,
      "learning_rate": 3.7295833333333336e-05,
      "loss": 0.0028,
      "step": 30490
    },
    {
      "epoch": 2.033333333333333,
      "grad_norm": 0.11492432653903961,
      "learning_rate": 3.729166666666667e-05,
      "loss": 0.0032,
      "step": 30500
    },
    {
      "epoch": 2.034,
      "grad_norm": 0.9298197031021118,
      "learning_rate": 3.7287500000000005e-05,
      "loss": 0.0035,
      "step": 30510
    },
    {
      "epoch": 2.034666666666667,
      "grad_norm": 0.6670718789100647,
      "learning_rate": 3.7283333333333336e-05,
      "loss": 0.005,
      "step": 30520
    },
    {
      "epoch": 2.0353333333333334,
      "grad_norm": 0.08552907407283783,
      "learning_rate": 3.7279166666666674e-05,
      "loss": 0.0021,
      "step": 30530
    },
    {
      "epoch": 2.036,
      "grad_norm": 0.6204840540885925,
      "learning_rate": 3.7275000000000005e-05,
      "loss": 0.0021,
      "step": 30540
    },
    {
      "epoch": 2.0366666666666666,
      "grad_norm": 0.05863786116242409,
      "learning_rate": 3.7270833333333336e-05,
      "loss": 0.0033,
      "step": 30550
    },
    {
      "epoch": 2.037333333333333,
      "grad_norm": 0.5086058974266052,
      "learning_rate": 3.726666666666667e-05,
      "loss": 0.0036,
      "step": 30560
    },
    {
      "epoch": 2.038,
      "grad_norm": 0.7054236531257629,
      "learning_rate": 3.72625e-05,
      "loss": 0.0024,
      "step": 30570
    },
    {
      "epoch": 2.038666666666667,
      "grad_norm": 0.7619335055351257,
      "learning_rate": 3.7258333333333335e-05,
      "loss": 0.0027,
      "step": 30580
    },
    {
      "epoch": 2.0393333333333334,
      "grad_norm": 0.45136821269989014,
      "learning_rate": 3.7254166666666666e-05,
      "loss": 0.0033,
      "step": 30590
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.42413610219955444,
      "learning_rate": 3.7250000000000004e-05,
      "loss": 0.0041,
      "step": 30600
    },
    {
      "epoch": 2.0406666666666666,
      "grad_norm": 0.15863679349422455,
      "learning_rate": 3.7245833333333335e-05,
      "loss": 0.0025,
      "step": 30610
    },
    {
      "epoch": 2.041333333333333,
      "grad_norm": 0.6541538834571838,
      "learning_rate": 3.7241666666666666e-05,
      "loss": 0.0019,
      "step": 30620
    },
    {
      "epoch": 2.042,
      "grad_norm": 0.13344579935073853,
      "learning_rate": 3.7237500000000004e-05,
      "loss": 0.0024,
      "step": 30630
    },
    {
      "epoch": 2.042666666666667,
      "grad_norm": 0.049975909292697906,
      "learning_rate": 3.7233333333333335e-05,
      "loss": 0.0026,
      "step": 30640
    },
    {
      "epoch": 2.0433333333333334,
      "grad_norm": 0.07688721269369125,
      "learning_rate": 3.722916666666667e-05,
      "loss": 0.0025,
      "step": 30650
    },
    {
      "epoch": 2.044,
      "grad_norm": 0.5835355520248413,
      "learning_rate": 3.7225000000000004e-05,
      "loss": 0.0026,
      "step": 30660
    },
    {
      "epoch": 2.0446666666666666,
      "grad_norm": 0.7037019729614258,
      "learning_rate": 3.7220833333333335e-05,
      "loss": 0.0019,
      "step": 30670
    },
    {
      "epoch": 2.0453333333333332,
      "grad_norm": 0.6037231087684631,
      "learning_rate": 3.7216666666666666e-05,
      "loss": 0.0033,
      "step": 30680
    },
    {
      "epoch": 2.046,
      "grad_norm": 0.31060105562210083,
      "learning_rate": 3.72125e-05,
      "loss": 0.0035,
      "step": 30690
    },
    {
      "epoch": 2.046666666666667,
      "grad_norm": 0.5894415974617004,
      "learning_rate": 3.7208333333333334e-05,
      "loss": 0.003,
      "step": 30700
    },
    {
      "epoch": 2.0473333333333334,
      "grad_norm": 0.3306839168071747,
      "learning_rate": 3.7204166666666665e-05,
      "loss": 0.0024,
      "step": 30710
    },
    {
      "epoch": 2.048,
      "grad_norm": 0.5069918036460876,
      "learning_rate": 3.72e-05,
      "loss": 0.0015,
      "step": 30720
    },
    {
      "epoch": 2.0486666666666666,
      "grad_norm": 0.2251659333705902,
      "learning_rate": 3.7195833333333334e-05,
      "loss": 0.0022,
      "step": 30730
    },
    {
      "epoch": 2.0493333333333332,
      "grad_norm": 0.8101402521133423,
      "learning_rate": 3.719166666666667e-05,
      "loss": 0.0021,
      "step": 30740
    },
    {
      "epoch": 2.05,
      "grad_norm": 0.27187350392341614,
      "learning_rate": 3.71875e-05,
      "loss": 0.002,
      "step": 30750
    },
    {
      "epoch": 2.050666666666667,
      "grad_norm": 1.0318868160247803,
      "learning_rate": 3.7183333333333334e-05,
      "loss": 0.002,
      "step": 30760
    },
    {
      "epoch": 2.0513333333333335,
      "grad_norm": 0.9139521718025208,
      "learning_rate": 3.717916666666667e-05,
      "loss": 0.0025,
      "step": 30770
    },
    {
      "epoch": 2.052,
      "grad_norm": 0.37741145491600037,
      "learning_rate": 3.7175e-05,
      "loss": 0.0025,
      "step": 30780
    },
    {
      "epoch": 2.0526666666666666,
      "grad_norm": 0.4171055257320404,
      "learning_rate": 3.717083333333333e-05,
      "loss": 0.0031,
      "step": 30790
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 0.2184123545885086,
      "learning_rate": 3.7166666666666664e-05,
      "loss": 0.0026,
      "step": 30800
    },
    {
      "epoch": 2.054,
      "grad_norm": 0.3959972560405731,
      "learning_rate": 3.71625e-05,
      "loss": 0.0017,
      "step": 30810
    },
    {
      "epoch": 2.054666666666667,
      "grad_norm": 0.2311609536409378,
      "learning_rate": 3.715833333333333e-05,
      "loss": 0.0022,
      "step": 30820
    },
    {
      "epoch": 2.0553333333333335,
      "grad_norm": 0.7050712704658508,
      "learning_rate": 3.715416666666667e-05,
      "loss": 0.0022,
      "step": 30830
    },
    {
      "epoch": 2.056,
      "grad_norm": 0.06557251513004303,
      "learning_rate": 3.715e-05,
      "loss": 0.0033,
      "step": 30840
    },
    {
      "epoch": 2.0566666666666666,
      "grad_norm": 0.23454798758029938,
      "learning_rate": 3.714583333333333e-05,
      "loss": 0.0024,
      "step": 30850
    },
    {
      "epoch": 2.0573333333333332,
      "grad_norm": 0.17052876949310303,
      "learning_rate": 3.714166666666667e-05,
      "loss": 0.0026,
      "step": 30860
    },
    {
      "epoch": 2.058,
      "grad_norm": 0.49085733294487,
      "learning_rate": 3.71375e-05,
      "loss": 0.0019,
      "step": 30870
    },
    {
      "epoch": 2.058666666666667,
      "grad_norm": 0.8947502374649048,
      "learning_rate": 3.713333333333334e-05,
      "loss": 0.0047,
      "step": 30880
    },
    {
      "epoch": 2.0593333333333335,
      "grad_norm": 0.6458309888839722,
      "learning_rate": 3.712916666666667e-05,
      "loss": 0.0023,
      "step": 30890
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.44327282905578613,
      "learning_rate": 3.7125e-05,
      "loss": 0.0021,
      "step": 30900
    },
    {
      "epoch": 2.0606666666666666,
      "grad_norm": 0.07047704607248306,
      "learning_rate": 3.712083333333333e-05,
      "loss": 0.0029,
      "step": 30910
    },
    {
      "epoch": 2.0613333333333332,
      "grad_norm": 0.4360774755477905,
      "learning_rate": 3.711666666666666e-05,
      "loss": 0.0028,
      "step": 30920
    },
    {
      "epoch": 2.062,
      "grad_norm": 0.2475004941225052,
      "learning_rate": 3.71125e-05,
      "loss": 0.002,
      "step": 30930
    },
    {
      "epoch": 2.062666666666667,
      "grad_norm": 0.057612545788288116,
      "learning_rate": 3.710833333333333e-05,
      "loss": 0.002,
      "step": 30940
    },
    {
      "epoch": 2.0633333333333335,
      "grad_norm": 1.2318514585494995,
      "learning_rate": 3.710416666666667e-05,
      "loss": 0.0027,
      "step": 30950
    },
    {
      "epoch": 2.064,
      "grad_norm": 0.6058511137962341,
      "learning_rate": 3.71e-05,
      "loss": 0.0024,
      "step": 30960
    },
    {
      "epoch": 2.0646666666666667,
      "grad_norm": 1.3175663948059082,
      "learning_rate": 3.709583333333334e-05,
      "loss": 0.0026,
      "step": 30970
    },
    {
      "epoch": 2.0653333333333332,
      "grad_norm": 0.2956904470920563,
      "learning_rate": 3.709166666666667e-05,
      "loss": 0.0027,
      "step": 30980
    },
    {
      "epoch": 2.066,
      "grad_norm": 1.0704083442687988,
      "learning_rate": 3.70875e-05,
      "loss": 0.0028,
      "step": 30990
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.7872604131698608,
      "learning_rate": 3.708333333333334e-05,
      "loss": 0.0027,
      "step": 31000
    },
    {
      "epoch": 2.0673333333333335,
      "grad_norm": 0.24895647168159485,
      "learning_rate": 3.707916666666667e-05,
      "loss": 0.0024,
      "step": 31010
    },
    {
      "epoch": 2.068,
      "grad_norm": 0.11767108738422394,
      "learning_rate": 3.707500000000001e-05,
      "loss": 0.0016,
      "step": 31020
    },
    {
      "epoch": 2.0686666666666667,
      "grad_norm": 0.3345724940299988,
      "learning_rate": 3.707083333333333e-05,
      "loss": 0.0026,
      "step": 31030
    },
    {
      "epoch": 2.0693333333333332,
      "grad_norm": 0.21161384880542755,
      "learning_rate": 3.706666666666667e-05,
      "loss": 0.0023,
      "step": 31040
    },
    {
      "epoch": 2.07,
      "grad_norm": 0.0790623277425766,
      "learning_rate": 3.70625e-05,
      "loss": 0.0023,
      "step": 31050
    },
    {
      "epoch": 2.070666666666667,
      "grad_norm": 0.7704787850379944,
      "learning_rate": 3.705833333333333e-05,
      "loss": 0.0024,
      "step": 31060
    },
    {
      "epoch": 2.0713333333333335,
      "grad_norm": 0.04928081855177879,
      "learning_rate": 3.705416666666667e-05,
      "loss": 0.0021,
      "step": 31070
    },
    {
      "epoch": 2.072,
      "grad_norm": 0.18582965433597565,
      "learning_rate": 3.705e-05,
      "loss": 0.0023,
      "step": 31080
    },
    {
      "epoch": 2.0726666666666667,
      "grad_norm": 0.2777382731437683,
      "learning_rate": 3.704583333333334e-05,
      "loss": 0.0026,
      "step": 31090
    },
    {
      "epoch": 2.0733333333333333,
      "grad_norm": 0.31720659136772156,
      "learning_rate": 3.704166666666667e-05,
      "loss": 0.0015,
      "step": 31100
    },
    {
      "epoch": 2.074,
      "grad_norm": 0.3452112078666687,
      "learning_rate": 3.7037500000000006e-05,
      "loss": 0.0024,
      "step": 31110
    },
    {
      "epoch": 2.074666666666667,
      "grad_norm": 0.5161382555961609,
      "learning_rate": 3.703333333333334e-05,
      "loss": 0.0029,
      "step": 31120
    },
    {
      "epoch": 2.0753333333333335,
      "grad_norm": 0.8150920867919922,
      "learning_rate": 3.702916666666667e-05,
      "loss": 0.0029,
      "step": 31130
    },
    {
      "epoch": 2.076,
      "grad_norm": 0.16860009729862213,
      "learning_rate": 3.7025000000000005e-05,
      "loss": 0.0031,
      "step": 31140
    },
    {
      "epoch": 2.0766666666666667,
      "grad_norm": 0.8140397071838379,
      "learning_rate": 3.702083333333333e-05,
      "loss": 0.0025,
      "step": 31150
    },
    {
      "epoch": 2.0773333333333333,
      "grad_norm": 0.5356423258781433,
      "learning_rate": 3.701666666666667e-05,
      "loss": 0.002,
      "step": 31160
    },
    {
      "epoch": 2.078,
      "grad_norm": 0.46358373761177063,
      "learning_rate": 3.70125e-05,
      "loss": 0.0033,
      "step": 31170
    },
    {
      "epoch": 2.078666666666667,
      "grad_norm": 0.2140740603208542,
      "learning_rate": 3.7008333333333336e-05,
      "loss": 0.0033,
      "step": 31180
    },
    {
      "epoch": 2.0793333333333335,
      "grad_norm": 0.6492676138877869,
      "learning_rate": 3.700416666666667e-05,
      "loss": 0.0024,
      "step": 31190
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.7152842283248901,
      "learning_rate": 3.7e-05,
      "loss": 0.0029,
      "step": 31200
    },
    {
      "epoch": 2.0806666666666667,
      "grad_norm": 0.3295847475528717,
      "learning_rate": 3.6995833333333336e-05,
      "loss": 0.0038,
      "step": 31210
    },
    {
      "epoch": 2.0813333333333333,
      "grad_norm": 0.8959659337997437,
      "learning_rate": 3.699166666666667e-05,
      "loss": 0.0021,
      "step": 31220
    },
    {
      "epoch": 2.082,
      "grad_norm": 0.9083738923072815,
      "learning_rate": 3.6987500000000005e-05,
      "loss": 0.0027,
      "step": 31230
    },
    {
      "epoch": 2.0826666666666664,
      "grad_norm": 0.43828263878822327,
      "learning_rate": 3.6983333333333336e-05,
      "loss": 0.0024,
      "step": 31240
    },
    {
      "epoch": 2.0833333333333335,
      "grad_norm": 0.3082842528820038,
      "learning_rate": 3.697916666666667e-05,
      "loss": 0.003,
      "step": 31250
    },
    {
      "epoch": 2.084,
      "grad_norm": 0.4317678213119507,
      "learning_rate": 3.6975000000000004e-05,
      "loss": 0.0026,
      "step": 31260
    },
    {
      "epoch": 2.0846666666666667,
      "grad_norm": 0.5987432599067688,
      "learning_rate": 3.6970833333333335e-05,
      "loss": 0.0032,
      "step": 31270
    },
    {
      "epoch": 2.0853333333333333,
      "grad_norm": 0.023696059361100197,
      "learning_rate": 3.6966666666666666e-05,
      "loss": 0.0035,
      "step": 31280
    },
    {
      "epoch": 2.086,
      "grad_norm": 0.3081451952457428,
      "learning_rate": 3.69625e-05,
      "loss": 0.0021,
      "step": 31290
    },
    {
      "epoch": 2.086666666666667,
      "grad_norm": 0.3363315165042877,
      "learning_rate": 3.6958333333333335e-05,
      "loss": 0.0026,
      "step": 31300
    },
    {
      "epoch": 2.0873333333333335,
      "grad_norm": 0.8127407431602478,
      "learning_rate": 3.6954166666666666e-05,
      "loss": 0.002,
      "step": 31310
    },
    {
      "epoch": 2.088,
      "grad_norm": 0.0782172754406929,
      "learning_rate": 3.6950000000000004e-05,
      "loss": 0.0026,
      "step": 31320
    },
    {
      "epoch": 2.0886666666666667,
      "grad_norm": 0.4543987512588501,
      "learning_rate": 3.6945833333333335e-05,
      "loss": 0.002,
      "step": 31330
    },
    {
      "epoch": 2.0893333333333333,
      "grad_norm": 0.5441620945930481,
      "learning_rate": 3.6941666666666666e-05,
      "loss": 0.003,
      "step": 31340
    },
    {
      "epoch": 2.09,
      "grad_norm": 0.6970025897026062,
      "learning_rate": 3.69375e-05,
      "loss": 0.0019,
      "step": 31350
    },
    {
      "epoch": 2.0906666666666665,
      "grad_norm": 0.4080004096031189,
      "learning_rate": 3.6933333333333334e-05,
      "loss": 0.0013,
      "step": 31360
    },
    {
      "epoch": 2.0913333333333335,
      "grad_norm": 0.7979048490524292,
      "learning_rate": 3.692916666666667e-05,
      "loss": 0.0029,
      "step": 31370
    },
    {
      "epoch": 2.092,
      "grad_norm": 0.8786678910255432,
      "learning_rate": 3.6925e-05,
      "loss": 0.0021,
      "step": 31380
    },
    {
      "epoch": 2.0926666666666667,
      "grad_norm": 0.12743280827999115,
      "learning_rate": 3.692083333333334e-05,
      "loss": 0.0024,
      "step": 31390
    },
    {
      "epoch": 2.0933333333333333,
      "grad_norm": 0.4930882155895233,
      "learning_rate": 3.6916666666666665e-05,
      "loss": 0.0032,
      "step": 31400
    },
    {
      "epoch": 2.094,
      "grad_norm": 0.16311268508434296,
      "learning_rate": 3.69125e-05,
      "loss": 0.0018,
      "step": 31410
    },
    {
      "epoch": 2.0946666666666665,
      "grad_norm": 0.5678921937942505,
      "learning_rate": 3.6908333333333334e-05,
      "loss": 0.0031,
      "step": 31420
    },
    {
      "epoch": 2.0953333333333335,
      "grad_norm": 0.565069317817688,
      "learning_rate": 3.6904166666666665e-05,
      "loss": 0.0024,
      "step": 31430
    },
    {
      "epoch": 2.096,
      "grad_norm": 0.658986508846283,
      "learning_rate": 3.69e-05,
      "loss": 0.0018,
      "step": 31440
    },
    {
      "epoch": 2.0966666666666667,
      "grad_norm": 0.6932136416435242,
      "learning_rate": 3.6895833333333333e-05,
      "loss": 0.0024,
      "step": 31450
    },
    {
      "epoch": 2.0973333333333333,
      "grad_norm": 0.5535260438919067,
      "learning_rate": 3.689166666666667e-05,
      "loss": 0.0027,
      "step": 31460
    },
    {
      "epoch": 2.098,
      "grad_norm": 0.0868845134973526,
      "learning_rate": 3.68875e-05,
      "loss": 0.0026,
      "step": 31470
    },
    {
      "epoch": 2.0986666666666665,
      "grad_norm": 0.26513656973838806,
      "learning_rate": 3.688333333333333e-05,
      "loss": 0.0017,
      "step": 31480
    },
    {
      "epoch": 2.0993333333333335,
      "grad_norm": 0.08572705835103989,
      "learning_rate": 3.687916666666667e-05,
      "loss": 0.002,
      "step": 31490
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.6288772225379944,
      "learning_rate": 3.6875e-05,
      "loss": 0.0035,
      "step": 31500
    },
    {
      "epoch": 2.1006666666666667,
      "grad_norm": 0.34947580099105835,
      "learning_rate": 3.687083333333334e-05,
      "loss": 0.0023,
      "step": 31510
    },
    {
      "epoch": 2.1013333333333333,
      "grad_norm": 0.2719343602657318,
      "learning_rate": 3.6866666666666664e-05,
      "loss": 0.0024,
      "step": 31520
    },
    {
      "epoch": 2.102,
      "grad_norm": 0.05891432240605354,
      "learning_rate": 3.68625e-05,
      "loss": 0.003,
      "step": 31530
    },
    {
      "epoch": 2.1026666666666665,
      "grad_norm": 0.6204150319099426,
      "learning_rate": 3.685833333333333e-05,
      "loss": 0.0021,
      "step": 31540
    },
    {
      "epoch": 2.1033333333333335,
      "grad_norm": 0.5426397919654846,
      "learning_rate": 3.685416666666667e-05,
      "loss": 0.0022,
      "step": 31550
    },
    {
      "epoch": 2.104,
      "grad_norm": 0.38589006662368774,
      "learning_rate": 3.685e-05,
      "loss": 0.0025,
      "step": 31560
    },
    {
      "epoch": 2.1046666666666667,
      "grad_norm": 0.8413629531860352,
      "learning_rate": 3.684583333333333e-05,
      "loss": 0.0022,
      "step": 31570
    },
    {
      "epoch": 2.1053333333333333,
      "grad_norm": 0.4667435884475708,
      "learning_rate": 3.684166666666667e-05,
      "loss": 0.0026,
      "step": 31580
    },
    {
      "epoch": 2.106,
      "grad_norm": 0.4879123568534851,
      "learning_rate": 3.68375e-05,
      "loss": 0.0025,
      "step": 31590
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 0.35488924384117126,
      "learning_rate": 3.683333333333334e-05,
      "loss": 0.0016,
      "step": 31600
    },
    {
      "epoch": 2.1073333333333335,
      "grad_norm": 0.24652515351772308,
      "learning_rate": 3.682916666666667e-05,
      "loss": 0.0025,
      "step": 31610
    },
    {
      "epoch": 2.108,
      "grad_norm": 0.17680920660495758,
      "learning_rate": 3.6825e-05,
      "loss": 0.0025,
      "step": 31620
    },
    {
      "epoch": 2.1086666666666667,
      "grad_norm": 0.1396339237689972,
      "learning_rate": 3.682083333333334e-05,
      "loss": 0.0023,
      "step": 31630
    },
    {
      "epoch": 2.1093333333333333,
      "grad_norm": 0.2624764144420624,
      "learning_rate": 3.681666666666667e-05,
      "loss": 0.0028,
      "step": 31640
    },
    {
      "epoch": 2.11,
      "grad_norm": 0.13108640909194946,
      "learning_rate": 3.68125e-05,
      "loss": 0.0018,
      "step": 31650
    },
    {
      "epoch": 2.1106666666666665,
      "grad_norm": 0.30307480692863464,
      "learning_rate": 3.680833333333333e-05,
      "loss": 0.0017,
      "step": 31660
    },
    {
      "epoch": 2.1113333333333335,
      "grad_norm": 0.06045684963464737,
      "learning_rate": 3.680416666666667e-05,
      "loss": 0.0019,
      "step": 31670
    },
    {
      "epoch": 2.112,
      "grad_norm": 0.4663565754890442,
      "learning_rate": 3.68e-05,
      "loss": 0.0017,
      "step": 31680
    },
    {
      "epoch": 2.1126666666666667,
      "grad_norm": 0.053839582949876785,
      "learning_rate": 3.679583333333334e-05,
      "loss": 0.0022,
      "step": 31690
    },
    {
      "epoch": 2.1133333333333333,
      "grad_norm": 0.22649776935577393,
      "learning_rate": 3.679166666666667e-05,
      "loss": 0.0031,
      "step": 31700
    },
    {
      "epoch": 2.114,
      "grad_norm": 0.04862329363822937,
      "learning_rate": 3.67875e-05,
      "loss": 0.0021,
      "step": 31710
    },
    {
      "epoch": 2.1146666666666665,
      "grad_norm": 0.22333240509033203,
      "learning_rate": 3.678333333333334e-05,
      "loss": 0.0017,
      "step": 31720
    },
    {
      "epoch": 2.1153333333333335,
      "grad_norm": 0.18611887097358704,
      "learning_rate": 3.677916666666667e-05,
      "loss": 0.004,
      "step": 31730
    },
    {
      "epoch": 2.116,
      "grad_norm": 0.1821553260087967,
      "learning_rate": 3.6775000000000006e-05,
      "loss": 0.0032,
      "step": 31740
    },
    {
      "epoch": 2.1166666666666667,
      "grad_norm": 0.24516548216342926,
      "learning_rate": 3.677083333333334e-05,
      "loss": 0.0029,
      "step": 31750
    },
    {
      "epoch": 2.1173333333333333,
      "grad_norm": 0.7847121357917786,
      "learning_rate": 3.676666666666667e-05,
      "loss": 0.0019,
      "step": 31760
    },
    {
      "epoch": 2.118,
      "grad_norm": 0.44487133622169495,
      "learning_rate": 3.67625e-05,
      "loss": 0.0025,
      "step": 31770
    },
    {
      "epoch": 2.1186666666666665,
      "grad_norm": 0.4129140079021454,
      "learning_rate": 3.675833333333334e-05,
      "loss": 0.0021,
      "step": 31780
    },
    {
      "epoch": 2.1193333333333335,
      "grad_norm": 0.16726797819137573,
      "learning_rate": 3.675416666666667e-05,
      "loss": 0.0021,
      "step": 31790
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.5098654627799988,
      "learning_rate": 3.675e-05,
      "loss": 0.0021,
      "step": 31800
    },
    {
      "epoch": 2.1206666666666667,
      "grad_norm": 0.26742085814476013,
      "learning_rate": 3.6745833333333337e-05,
      "loss": 0.0014,
      "step": 31810
    },
    {
      "epoch": 2.1213333333333333,
      "grad_norm": 0.3262517750263214,
      "learning_rate": 3.674166666666667e-05,
      "loss": 0.0023,
      "step": 31820
    },
    {
      "epoch": 2.122,
      "grad_norm": 0.48354119062423706,
      "learning_rate": 3.6737500000000005e-05,
      "loss": 0.0015,
      "step": 31830
    },
    {
      "epoch": 2.1226666666666665,
      "grad_norm": 0.3097864091396332,
      "learning_rate": 3.6733333333333336e-05,
      "loss": 0.0021,
      "step": 31840
    },
    {
      "epoch": 2.1233333333333335,
      "grad_norm": 0.6675893664360046,
      "learning_rate": 3.672916666666667e-05,
      "loss": 0.0018,
      "step": 31850
    },
    {
      "epoch": 2.124,
      "grad_norm": 0.21630799770355225,
      "learning_rate": 3.6725000000000005e-05,
      "loss": 0.0021,
      "step": 31860
    },
    {
      "epoch": 2.1246666666666667,
      "grad_norm": 0.685464084148407,
      "learning_rate": 3.6720833333333336e-05,
      "loss": 0.0021,
      "step": 31870
    },
    {
      "epoch": 2.1253333333333333,
      "grad_norm": 0.4659608006477356,
      "learning_rate": 3.671666666666667e-05,
      "loss": 0.002,
      "step": 31880
    },
    {
      "epoch": 2.126,
      "grad_norm": 0.5442287921905518,
      "learning_rate": 3.67125e-05,
      "loss": 0.0019,
      "step": 31890
    },
    {
      "epoch": 2.1266666666666665,
      "grad_norm": 0.7954556345939636,
      "learning_rate": 3.6708333333333336e-05,
      "loss": 0.0033,
      "step": 31900
    },
    {
      "epoch": 2.1273333333333335,
      "grad_norm": 0.6273508667945862,
      "learning_rate": 3.670416666666667e-05,
      "loss": 0.0017,
      "step": 31910
    },
    {
      "epoch": 2.128,
      "grad_norm": 0.17519086599349976,
      "learning_rate": 3.6700000000000004e-05,
      "loss": 0.0025,
      "step": 31920
    },
    {
      "epoch": 2.1286666666666667,
      "grad_norm": 0.3307065963745117,
      "learning_rate": 3.6695833333333335e-05,
      "loss": 0.0021,
      "step": 31930
    },
    {
      "epoch": 2.1293333333333333,
      "grad_norm": 0.6981773376464844,
      "learning_rate": 3.6691666666666666e-05,
      "loss": 0.0016,
      "step": 31940
    },
    {
      "epoch": 2.13,
      "grad_norm": 0.064447782933712,
      "learning_rate": 3.6687500000000004e-05,
      "loss": 0.0016,
      "step": 31950
    },
    {
      "epoch": 2.1306666666666665,
      "grad_norm": 0.31863898038864136,
      "learning_rate": 3.6683333333333335e-05,
      "loss": 0.002,
      "step": 31960
    },
    {
      "epoch": 2.1313333333333335,
      "grad_norm": 0.2485540360212326,
      "learning_rate": 3.667916666666667e-05,
      "loss": 0.0027,
      "step": 31970
    },
    {
      "epoch": 2.132,
      "grad_norm": 0.24194392561912537,
      "learning_rate": 3.6675000000000004e-05,
      "loss": 0.0017,
      "step": 31980
    },
    {
      "epoch": 2.1326666666666667,
      "grad_norm": 0.5449259281158447,
      "learning_rate": 3.6670833333333335e-05,
      "loss": 0.0023,
      "step": 31990
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.5738947987556458,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.0027,
      "step": 32000
    },
    {
      "epoch": 2.134,
      "grad_norm": 0.9823534488677979,
      "learning_rate": 3.66625e-05,
      "loss": 0.0022,
      "step": 32010
    },
    {
      "epoch": 2.1346666666666665,
      "grad_norm": 0.6130635738372803,
      "learning_rate": 3.6658333333333334e-05,
      "loss": 0.0034,
      "step": 32020
    },
    {
      "epoch": 2.1353333333333335,
      "grad_norm": 0.4747299253940582,
      "learning_rate": 3.6654166666666665e-05,
      "loss": 0.0031,
      "step": 32030
    },
    {
      "epoch": 2.136,
      "grad_norm": 0.5401285290718079,
      "learning_rate": 3.665e-05,
      "loss": 0.002,
      "step": 32040
    },
    {
      "epoch": 2.1366666666666667,
      "grad_norm": 0.3459891378879547,
      "learning_rate": 3.6645833333333334e-05,
      "loss": 0.003,
      "step": 32050
    },
    {
      "epoch": 2.1373333333333333,
      "grad_norm": 0.42345818877220154,
      "learning_rate": 3.664166666666667e-05,
      "loss": 0.0016,
      "step": 32060
    },
    {
      "epoch": 2.138,
      "grad_norm": 0.5066194534301758,
      "learning_rate": 3.66375e-05,
      "loss": 0.0025,
      "step": 32070
    },
    {
      "epoch": 2.1386666666666665,
      "grad_norm": 0.7227671146392822,
      "learning_rate": 3.6633333333333334e-05,
      "loss": 0.002,
      "step": 32080
    },
    {
      "epoch": 2.1393333333333335,
      "grad_norm": 0.6132657527923584,
      "learning_rate": 3.662916666666667e-05,
      "loss": 0.0021,
      "step": 32090
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.08994421362876892,
      "learning_rate": 3.6625e-05,
      "loss": 0.002,
      "step": 32100
    },
    {
      "epoch": 2.1406666666666667,
      "grad_norm": 0.3339230418205261,
      "learning_rate": 3.662083333333334e-05,
      "loss": 0.0022,
      "step": 32110
    },
    {
      "epoch": 2.1413333333333333,
      "grad_norm": 0.39200422167778015,
      "learning_rate": 3.6616666666666664e-05,
      "loss": 0.0021,
      "step": 32120
    },
    {
      "epoch": 2.142,
      "grad_norm": 0.1754407286643982,
      "learning_rate": 3.66125e-05,
      "loss": 0.0025,
      "step": 32130
    },
    {
      "epoch": 2.1426666666666665,
      "grad_norm": 0.6584570407867432,
      "learning_rate": 3.660833333333333e-05,
      "loss": 0.0034,
      "step": 32140
    },
    {
      "epoch": 2.1433333333333335,
      "grad_norm": 0.7186108231544495,
      "learning_rate": 3.6604166666666664e-05,
      "loss": 0.0028,
      "step": 32150
    },
    {
      "epoch": 2.144,
      "grad_norm": 0.5855106711387634,
      "learning_rate": 3.66e-05,
      "loss": 0.0016,
      "step": 32160
    },
    {
      "epoch": 2.1446666666666667,
      "grad_norm": 0.5731878280639648,
      "learning_rate": 3.659583333333333e-05,
      "loss": 0.0028,
      "step": 32170
    },
    {
      "epoch": 2.1453333333333333,
      "grad_norm": 0.22022858262062073,
      "learning_rate": 3.659166666666667e-05,
      "loss": 0.0027,
      "step": 32180
    },
    {
      "epoch": 2.146,
      "grad_norm": 0.11172972619533539,
      "learning_rate": 3.65875e-05,
      "loss": 0.0016,
      "step": 32190
    },
    {
      "epoch": 2.1466666666666665,
      "grad_norm": 0.3455006778240204,
      "learning_rate": 3.658333333333334e-05,
      "loss": 0.0027,
      "step": 32200
    },
    {
      "epoch": 2.1473333333333335,
      "grad_norm": 0.10881326347589493,
      "learning_rate": 3.657916666666667e-05,
      "loss": 0.0031,
      "step": 32210
    },
    {
      "epoch": 2.148,
      "grad_norm": 0.8006879687309265,
      "learning_rate": 3.6575e-05,
      "loss": 0.0031,
      "step": 32220
    },
    {
      "epoch": 2.1486666666666667,
      "grad_norm": 0.9491405487060547,
      "learning_rate": 3.657083333333334e-05,
      "loss": 0.0021,
      "step": 32230
    },
    {
      "epoch": 2.1493333333333333,
      "grad_norm": 0.3777521252632141,
      "learning_rate": 3.656666666666666e-05,
      "loss": 0.004,
      "step": 32240
    },
    {
      "epoch": 2.15,
      "grad_norm": 0.574836015701294,
      "learning_rate": 3.65625e-05,
      "loss": 0.0028,
      "step": 32250
    },
    {
      "epoch": 2.1506666666666665,
      "grad_norm": 0.5057017207145691,
      "learning_rate": 3.655833333333333e-05,
      "loss": 0.0027,
      "step": 32260
    },
    {
      "epoch": 2.1513333333333335,
      "grad_norm": 0.08086711168289185,
      "learning_rate": 3.655416666666667e-05,
      "loss": 0.0016,
      "step": 32270
    },
    {
      "epoch": 2.152,
      "grad_norm": 0.22847647964954376,
      "learning_rate": 3.655e-05,
      "loss": 0.0019,
      "step": 32280
    },
    {
      "epoch": 2.1526666666666667,
      "grad_norm": 0.27467694878578186,
      "learning_rate": 3.654583333333333e-05,
      "loss": 0.0034,
      "step": 32290
    },
    {
      "epoch": 2.1533333333333333,
      "grad_norm": 0.1439054012298584,
      "learning_rate": 3.654166666666667e-05,
      "loss": 0.0014,
      "step": 32300
    },
    {
      "epoch": 2.154,
      "grad_norm": 0.6567240357398987,
      "learning_rate": 3.65375e-05,
      "loss": 0.0029,
      "step": 32310
    },
    {
      "epoch": 2.1546666666666665,
      "grad_norm": 0.4966141879558563,
      "learning_rate": 3.653333333333334e-05,
      "loss": 0.0019,
      "step": 32320
    },
    {
      "epoch": 2.155333333333333,
      "grad_norm": 0.4777183532714844,
      "learning_rate": 3.652916666666667e-05,
      "loss": 0.0023,
      "step": 32330
    },
    {
      "epoch": 2.156,
      "grad_norm": 0.34569305181503296,
      "learning_rate": 3.652500000000001e-05,
      "loss": 0.0022,
      "step": 32340
    },
    {
      "epoch": 2.1566666666666667,
      "grad_norm": 0.27314937114715576,
      "learning_rate": 3.652083333333334e-05,
      "loss": 0.0024,
      "step": 32350
    },
    {
      "epoch": 2.1573333333333333,
      "grad_norm": 0.23275364935398102,
      "learning_rate": 3.651666666666667e-05,
      "loss": 0.0024,
      "step": 32360
    },
    {
      "epoch": 2.158,
      "grad_norm": 0.8379727602005005,
      "learning_rate": 3.65125e-05,
      "loss": 0.0024,
      "step": 32370
    },
    {
      "epoch": 2.1586666666666665,
      "grad_norm": 0.17181716859340668,
      "learning_rate": 3.650833333333333e-05,
      "loss": 0.0031,
      "step": 32380
    },
    {
      "epoch": 2.1593333333333335,
      "grad_norm": 0.7915701270103455,
      "learning_rate": 3.650416666666667e-05,
      "loss": 0.0024,
      "step": 32390
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.2728687822818756,
      "learning_rate": 3.65e-05,
      "loss": 0.0025,
      "step": 32400
    },
    {
      "epoch": 2.1606666666666667,
      "grad_norm": 0.19616852700710297,
      "learning_rate": 3.649583333333334e-05,
      "loss": 0.0018,
      "step": 32410
    },
    {
      "epoch": 2.1613333333333333,
      "grad_norm": 0.11816240102052689,
      "learning_rate": 3.649166666666667e-05,
      "loss": 0.0022,
      "step": 32420
    },
    {
      "epoch": 2.162,
      "grad_norm": 0.26368120312690735,
      "learning_rate": 3.64875e-05,
      "loss": 0.0022,
      "step": 32430
    },
    {
      "epoch": 2.1626666666666665,
      "grad_norm": 0.2510007619857788,
      "learning_rate": 3.648333333333334e-05,
      "loss": 0.0014,
      "step": 32440
    },
    {
      "epoch": 2.163333333333333,
      "grad_norm": 1.125920295715332,
      "learning_rate": 3.647916666666667e-05,
      "loss": 0.0023,
      "step": 32450
    },
    {
      "epoch": 2.164,
      "grad_norm": 1.1566498279571533,
      "learning_rate": 3.6475000000000006e-05,
      "loss": 0.0027,
      "step": 32460
    },
    {
      "epoch": 2.1646666666666667,
      "grad_norm": 0.2651233375072479,
      "learning_rate": 3.647083333333334e-05,
      "loss": 0.0021,
      "step": 32470
    },
    {
      "epoch": 2.1653333333333333,
      "grad_norm": 0.22074781358242035,
      "learning_rate": 3.646666666666667e-05,
      "loss": 0.0024,
      "step": 32480
    },
    {
      "epoch": 2.166,
      "grad_norm": 0.4960860311985016,
      "learning_rate": 3.64625e-05,
      "loss": 0.0027,
      "step": 32490
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 0.044675786048173904,
      "learning_rate": 3.6458333333333336e-05,
      "loss": 0.0028,
      "step": 32500
    },
    {
      "epoch": 2.1673333333333336,
      "grad_norm": 0.6060472130775452,
      "learning_rate": 3.645416666666667e-05,
      "loss": 0.0024,
      "step": 32510
    },
    {
      "epoch": 2.168,
      "grad_norm": 0.32415515184402466,
      "learning_rate": 3.645e-05,
      "loss": 0.0021,
      "step": 32520
    },
    {
      "epoch": 2.1686666666666667,
      "grad_norm": 0.5062006115913391,
      "learning_rate": 3.6445833333333336e-05,
      "loss": 0.0017,
      "step": 32530
    },
    {
      "epoch": 2.1693333333333333,
      "grad_norm": 0.42201313376426697,
      "learning_rate": 3.644166666666667e-05,
      "loss": 0.0034,
      "step": 32540
    },
    {
      "epoch": 2.17,
      "grad_norm": 0.27517223358154297,
      "learning_rate": 3.6437500000000005e-05,
      "loss": 0.0016,
      "step": 32550
    },
    {
      "epoch": 2.1706666666666665,
      "grad_norm": 0.44355764985084534,
      "learning_rate": 3.6433333333333336e-05,
      "loss": 0.0028,
      "step": 32560
    },
    {
      "epoch": 2.171333333333333,
      "grad_norm": 0.22099678218364716,
      "learning_rate": 3.642916666666667e-05,
      "loss": 0.0026,
      "step": 32570
    },
    {
      "epoch": 2.172,
      "grad_norm": 0.4566674828529358,
      "learning_rate": 3.6425000000000004e-05,
      "loss": 0.0035,
      "step": 32580
    },
    {
      "epoch": 2.1726666666666667,
      "grad_norm": 0.19353000819683075,
      "learning_rate": 3.6420833333333335e-05,
      "loss": 0.0035,
      "step": 32590
    },
    {
      "epoch": 2.1733333333333333,
      "grad_norm": 0.27227917313575745,
      "learning_rate": 3.641666666666667e-05,
      "loss": 0.0025,
      "step": 32600
    },
    {
      "epoch": 2.174,
      "grad_norm": 0.35342341661453247,
      "learning_rate": 3.64125e-05,
      "loss": 0.0031,
      "step": 32610
    },
    {
      "epoch": 2.1746666666666665,
      "grad_norm": 0.6262675523757935,
      "learning_rate": 3.6408333333333335e-05,
      "loss": 0.003,
      "step": 32620
    },
    {
      "epoch": 2.1753333333333336,
      "grad_norm": 0.6150290966033936,
      "learning_rate": 3.6404166666666666e-05,
      "loss": 0.0024,
      "step": 32630
    },
    {
      "epoch": 2.176,
      "grad_norm": 0.5570328235626221,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 0.0025,
      "step": 32640
    },
    {
      "epoch": 2.1766666666666667,
      "grad_norm": 0.4018138349056244,
      "learning_rate": 3.6395833333333335e-05,
      "loss": 0.0023,
      "step": 32650
    },
    {
      "epoch": 2.1773333333333333,
      "grad_norm": 0.7138878703117371,
      "learning_rate": 3.6391666666666666e-05,
      "loss": 0.0022,
      "step": 32660
    },
    {
      "epoch": 2.178,
      "grad_norm": 0.5869708061218262,
      "learning_rate": 3.6387500000000004e-05,
      "loss": 0.0023,
      "step": 32670
    },
    {
      "epoch": 2.1786666666666665,
      "grad_norm": 0.4921190142631531,
      "learning_rate": 3.6383333333333335e-05,
      "loss": 0.0032,
      "step": 32680
    },
    {
      "epoch": 2.179333333333333,
      "grad_norm": 0.09359975159168243,
      "learning_rate": 3.637916666666667e-05,
      "loss": 0.0023,
      "step": 32690
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.057032618671655655,
      "learning_rate": 3.6375e-05,
      "loss": 0.0035,
      "step": 32700
    },
    {
      "epoch": 2.1806666666666668,
      "grad_norm": 0.21456506848335266,
      "learning_rate": 3.6370833333333334e-05,
      "loss": 0.0022,
      "step": 32710
    },
    {
      "epoch": 2.1813333333333333,
      "grad_norm": 0.33507242798805237,
      "learning_rate": 3.636666666666667e-05,
      "loss": 0.0024,
      "step": 32720
    },
    {
      "epoch": 2.182,
      "grad_norm": 0.2398957759141922,
      "learning_rate": 3.6362499999999996e-05,
      "loss": 0.0019,
      "step": 32730
    },
    {
      "epoch": 2.1826666666666665,
      "grad_norm": 0.577980101108551,
      "learning_rate": 3.6358333333333334e-05,
      "loss": 0.0031,
      "step": 32740
    },
    {
      "epoch": 2.183333333333333,
      "grad_norm": 0.2847699224948883,
      "learning_rate": 3.6354166666666665e-05,
      "loss": 0.0016,
      "step": 32750
    },
    {
      "epoch": 2.184,
      "grad_norm": 0.11317440867424011,
      "learning_rate": 3.635e-05,
      "loss": 0.0014,
      "step": 32760
    },
    {
      "epoch": 2.1846666666666668,
      "grad_norm": 0.3101310729980469,
      "learning_rate": 3.6345833333333334e-05,
      "loss": 0.0026,
      "step": 32770
    },
    {
      "epoch": 2.1853333333333333,
      "grad_norm": 0.8954846858978271,
      "learning_rate": 3.634166666666667e-05,
      "loss": 0.0016,
      "step": 32780
    },
    {
      "epoch": 2.186,
      "grad_norm": 0.5808164477348328,
      "learning_rate": 3.63375e-05,
      "loss": 0.0024,
      "step": 32790
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 0.08749326318502426,
      "learning_rate": 3.633333333333333e-05,
      "loss": 0.0016,
      "step": 32800
    },
    {
      "epoch": 2.187333333333333,
      "grad_norm": 0.6834535002708435,
      "learning_rate": 3.632916666666667e-05,
      "loss": 0.0035,
      "step": 32810
    },
    {
      "epoch": 2.188,
      "grad_norm": 0.2927813231945038,
      "learning_rate": 3.6325e-05,
      "loss": 0.0044,
      "step": 32820
    },
    {
      "epoch": 2.1886666666666668,
      "grad_norm": 0.7399946451187134,
      "learning_rate": 3.632083333333334e-05,
      "loss": 0.0032,
      "step": 32830
    },
    {
      "epoch": 2.1893333333333334,
      "grad_norm": 0.15335826575756073,
      "learning_rate": 3.631666666666667e-05,
      "loss": 0.002,
      "step": 32840
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.9501102566719055,
      "learning_rate": 3.63125e-05,
      "loss": 0.0028,
      "step": 32850
    },
    {
      "epoch": 2.1906666666666665,
      "grad_norm": 0.24221022427082062,
      "learning_rate": 3.630833333333333e-05,
      "loss": 0.002,
      "step": 32860
    },
    {
      "epoch": 2.191333333333333,
      "grad_norm": 0.259975790977478,
      "learning_rate": 3.6304166666666664e-05,
      "loss": 0.0022,
      "step": 32870
    },
    {
      "epoch": 2.192,
      "grad_norm": 0.5097702741622925,
      "learning_rate": 3.63e-05,
      "loss": 0.0022,
      "step": 32880
    },
    {
      "epoch": 2.1926666666666668,
      "grad_norm": 0.24634940922260284,
      "learning_rate": 3.629583333333333e-05,
      "loss": 0.0021,
      "step": 32890
    },
    {
      "epoch": 2.1933333333333334,
      "grad_norm": 0.5311391949653625,
      "learning_rate": 3.629166666666667e-05,
      "loss": 0.0035,
      "step": 32900
    },
    {
      "epoch": 2.194,
      "grad_norm": 0.3949573338031769,
      "learning_rate": 3.62875e-05,
      "loss": 0.0023,
      "step": 32910
    },
    {
      "epoch": 2.1946666666666665,
      "grad_norm": 0.3225076496601105,
      "learning_rate": 3.628333333333334e-05,
      "loss": 0.0031,
      "step": 32920
    },
    {
      "epoch": 2.195333333333333,
      "grad_norm": 1.10291588306427,
      "learning_rate": 3.627916666666667e-05,
      "loss": 0.0023,
      "step": 32930
    },
    {
      "epoch": 2.196,
      "grad_norm": 0.6004117131233215,
      "learning_rate": 3.6275e-05,
      "loss": 0.0027,
      "step": 32940
    },
    {
      "epoch": 2.1966666666666668,
      "grad_norm": 0.2713029384613037,
      "learning_rate": 3.627083333333334e-05,
      "loss": 0.0022,
      "step": 32950
    },
    {
      "epoch": 2.1973333333333334,
      "grad_norm": 0.5031863451004028,
      "learning_rate": 3.626666666666667e-05,
      "loss": 0.0025,
      "step": 32960
    },
    {
      "epoch": 2.198,
      "grad_norm": 0.07962510734796524,
      "learning_rate": 3.62625e-05,
      "loss": 0.0028,
      "step": 32970
    },
    {
      "epoch": 2.1986666666666665,
      "grad_norm": 0.27054283022880554,
      "learning_rate": 3.625833333333333e-05,
      "loss": 0.0015,
      "step": 32980
    },
    {
      "epoch": 2.199333333333333,
      "grad_norm": 0.32963821291923523,
      "learning_rate": 3.625416666666667e-05,
      "loss": 0.0024,
      "step": 32990
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.13533969223499298,
      "learning_rate": 3.625e-05,
      "loss": 0.002,
      "step": 33000
    },
    {
      "epoch": 2.2006666666666668,
      "grad_norm": 0.46744197607040405,
      "learning_rate": 3.624583333333333e-05,
      "loss": 0.0022,
      "step": 33010
    },
    {
      "epoch": 2.2013333333333334,
      "grad_norm": 0.28072330355644226,
      "learning_rate": 3.624166666666667e-05,
      "loss": 0.0029,
      "step": 33020
    },
    {
      "epoch": 2.202,
      "grad_norm": 0.11317072063684464,
      "learning_rate": 3.62375e-05,
      "loss": 0.002,
      "step": 33030
    },
    {
      "epoch": 2.2026666666666666,
      "grad_norm": 0.3476315438747406,
      "learning_rate": 3.623333333333334e-05,
      "loss": 0.0024,
      "step": 33040
    },
    {
      "epoch": 2.203333333333333,
      "grad_norm": 0.5042304396629333,
      "learning_rate": 3.622916666666667e-05,
      "loss": 0.0027,
      "step": 33050
    },
    {
      "epoch": 2.204,
      "grad_norm": 0.5109084248542786,
      "learning_rate": 3.6225000000000006e-05,
      "loss": 0.0025,
      "step": 33060
    },
    {
      "epoch": 2.2046666666666668,
      "grad_norm": 0.28196048736572266,
      "learning_rate": 3.622083333333334e-05,
      "loss": 0.0026,
      "step": 33070
    },
    {
      "epoch": 2.2053333333333334,
      "grad_norm": 0.3376879096031189,
      "learning_rate": 3.621666666666667e-05,
      "loss": 0.0026,
      "step": 33080
    },
    {
      "epoch": 2.206,
      "grad_norm": 0.46645689010620117,
      "learning_rate": 3.62125e-05,
      "loss": 0.0021,
      "step": 33090
    },
    {
      "epoch": 2.2066666666666666,
      "grad_norm": 0.29466474056243896,
      "learning_rate": 3.620833333333333e-05,
      "loss": 0.0034,
      "step": 33100
    },
    {
      "epoch": 2.207333333333333,
      "grad_norm": 0.43280211091041565,
      "learning_rate": 3.620416666666667e-05,
      "loss": 0.0021,
      "step": 33110
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.13677982985973358,
      "learning_rate": 3.62e-05,
      "loss": 0.0018,
      "step": 33120
    },
    {
      "epoch": 2.208666666666667,
      "grad_norm": 0.6463130712509155,
      "learning_rate": 3.619583333333334e-05,
      "loss": 0.0026,
      "step": 33130
    },
    {
      "epoch": 2.2093333333333334,
      "grad_norm": 0.5951396822929382,
      "learning_rate": 3.619166666666667e-05,
      "loss": 0.002,
      "step": 33140
    },
    {
      "epoch": 2.21,
      "grad_norm": 0.5599589943885803,
      "learning_rate": 3.61875e-05,
      "loss": 0.0022,
      "step": 33150
    },
    {
      "epoch": 2.2106666666666666,
      "grad_norm": 0.556488037109375,
      "learning_rate": 3.6183333333333336e-05,
      "loss": 0.0035,
      "step": 33160
    },
    {
      "epoch": 2.211333333333333,
      "grad_norm": 0.5125572085380554,
      "learning_rate": 3.617916666666667e-05,
      "loss": 0.0015,
      "step": 33170
    },
    {
      "epoch": 2.212,
      "grad_norm": 0.11849144101142883,
      "learning_rate": 3.6175000000000005e-05,
      "loss": 0.0022,
      "step": 33180
    },
    {
      "epoch": 2.212666666666667,
      "grad_norm": 0.23680616915225983,
      "learning_rate": 3.6170833333333336e-05,
      "loss": 0.0013,
      "step": 33190
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 0.2067088782787323,
      "learning_rate": 3.6166666666666674e-05,
      "loss": 0.0015,
      "step": 33200
    },
    {
      "epoch": 2.214,
      "grad_norm": 0.42120182514190674,
      "learning_rate": 3.61625e-05,
      "loss": 0.0025,
      "step": 33210
    },
    {
      "epoch": 2.2146666666666666,
      "grad_norm": 0.23198142647743225,
      "learning_rate": 3.6158333333333336e-05,
      "loss": 0.002,
      "step": 33220
    },
    {
      "epoch": 2.215333333333333,
      "grad_norm": 0.27533987164497375,
      "learning_rate": 3.615416666666667e-05,
      "loss": 0.0022,
      "step": 33230
    },
    {
      "epoch": 2.216,
      "grad_norm": 0.23007521033287048,
      "learning_rate": 3.615e-05,
      "loss": 0.003,
      "step": 33240
    },
    {
      "epoch": 2.216666666666667,
      "grad_norm": 0.19280537962913513,
      "learning_rate": 3.6145833333333336e-05,
      "loss": 0.0027,
      "step": 33250
    },
    {
      "epoch": 2.2173333333333334,
      "grad_norm": 0.4222376048564911,
      "learning_rate": 3.6141666666666667e-05,
      "loss": 0.002,
      "step": 33260
    },
    {
      "epoch": 2.218,
      "grad_norm": 0.4312111735343933,
      "learning_rate": 3.6137500000000004e-05,
      "loss": 0.0024,
      "step": 33270
    },
    {
      "epoch": 2.2186666666666666,
      "grad_norm": 0.5003790259361267,
      "learning_rate": 3.6133333333333335e-05,
      "loss": 0.0019,
      "step": 33280
    },
    {
      "epoch": 2.219333333333333,
      "grad_norm": 0.062457818537950516,
      "learning_rate": 3.6129166666666666e-05,
      "loss": 0.003,
      "step": 33290
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.40033799409866333,
      "learning_rate": 3.6125000000000004e-05,
      "loss": 0.0028,
      "step": 33300
    },
    {
      "epoch": 2.220666666666667,
      "grad_norm": 0.3823149502277374,
      "learning_rate": 3.6120833333333335e-05,
      "loss": 0.0022,
      "step": 33310
    },
    {
      "epoch": 2.2213333333333334,
      "grad_norm": 0.28213077783584595,
      "learning_rate": 3.611666666666667e-05,
      "loss": 0.0026,
      "step": 33320
    },
    {
      "epoch": 2.222,
      "grad_norm": 0.29478970170021057,
      "learning_rate": 3.61125e-05,
      "loss": 0.0029,
      "step": 33330
    },
    {
      "epoch": 2.2226666666666666,
      "grad_norm": 1.1523154973983765,
      "learning_rate": 3.6108333333333335e-05,
      "loss": 0.0025,
      "step": 33340
    },
    {
      "epoch": 2.223333333333333,
      "grad_norm": 0.15085208415985107,
      "learning_rate": 3.6104166666666666e-05,
      "loss": 0.0031,
      "step": 33350
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.7694917917251587,
      "learning_rate": 3.61e-05,
      "loss": 0.0025,
      "step": 33360
    },
    {
      "epoch": 2.224666666666667,
      "grad_norm": 0.5111077427864075,
      "learning_rate": 3.6095833333333334e-05,
      "loss": 0.0033,
      "step": 33370
    },
    {
      "epoch": 2.2253333333333334,
      "grad_norm": 0.27172166109085083,
      "learning_rate": 3.6091666666666665e-05,
      "loss": 0.0023,
      "step": 33380
    },
    {
      "epoch": 2.226,
      "grad_norm": 0.5146915316581726,
      "learning_rate": 3.60875e-05,
      "loss": 0.0029,
      "step": 33390
    },
    {
      "epoch": 2.2266666666666666,
      "grad_norm": 0.2439885437488556,
      "learning_rate": 3.6083333333333334e-05,
      "loss": 0.0026,
      "step": 33400
    },
    {
      "epoch": 2.227333333333333,
      "grad_norm": 0.23724015057086945,
      "learning_rate": 3.607916666666667e-05,
      "loss": 0.0029,
      "step": 33410
    },
    {
      "epoch": 2.228,
      "grad_norm": 0.15359994769096375,
      "learning_rate": 3.6075e-05,
      "loss": 0.003,
      "step": 33420
    },
    {
      "epoch": 2.228666666666667,
      "grad_norm": 0.12025205045938492,
      "learning_rate": 3.6070833333333334e-05,
      "loss": 0.0025,
      "step": 33430
    },
    {
      "epoch": 2.2293333333333334,
      "grad_norm": 0.3496265709400177,
      "learning_rate": 3.606666666666667e-05,
      "loss": 0.0026,
      "step": 33440
    },
    {
      "epoch": 2.23,
      "grad_norm": 0.0498758964240551,
      "learning_rate": 3.60625e-05,
      "loss": 0.002,
      "step": 33450
    },
    {
      "epoch": 2.2306666666666666,
      "grad_norm": 0.9846974611282349,
      "learning_rate": 3.6058333333333333e-05,
      "loss": 0.0027,
      "step": 33460
    },
    {
      "epoch": 2.231333333333333,
      "grad_norm": 0.49397706985473633,
      "learning_rate": 3.6054166666666664e-05,
      "loss": 0.0018,
      "step": 33470
    },
    {
      "epoch": 2.232,
      "grad_norm": 0.7910025119781494,
      "learning_rate": 3.605e-05,
      "loss": 0.0027,
      "step": 33480
    },
    {
      "epoch": 2.232666666666667,
      "grad_norm": 0.14897695183753967,
      "learning_rate": 3.604583333333333e-05,
      "loss": 0.0026,
      "step": 33490
    },
    {
      "epoch": 2.2333333333333334,
      "grad_norm": 0.5874235033988953,
      "learning_rate": 3.604166666666667e-05,
      "loss": 0.0015,
      "step": 33500
    },
    {
      "epoch": 2.234,
      "grad_norm": 0.4269556999206543,
      "learning_rate": 3.60375e-05,
      "loss": 0.0026,
      "step": 33510
    },
    {
      "epoch": 2.2346666666666666,
      "grad_norm": 0.1241692453622818,
      "learning_rate": 3.603333333333333e-05,
      "loss": 0.0019,
      "step": 33520
    },
    {
      "epoch": 2.235333333333333,
      "grad_norm": 0.24177619814872742,
      "learning_rate": 3.602916666666667e-05,
      "loss": 0.0019,
      "step": 33530
    },
    {
      "epoch": 2.2359999999999998,
      "grad_norm": 0.47479018568992615,
      "learning_rate": 3.6025e-05,
      "loss": 0.0023,
      "step": 33540
    },
    {
      "epoch": 2.236666666666667,
      "grad_norm": 0.4633784294128418,
      "learning_rate": 3.602083333333334e-05,
      "loss": 0.0021,
      "step": 33550
    },
    {
      "epoch": 2.2373333333333334,
      "grad_norm": 0.906938374042511,
      "learning_rate": 3.601666666666667e-05,
      "loss": 0.002,
      "step": 33560
    },
    {
      "epoch": 2.238,
      "grad_norm": 0.906440258026123,
      "learning_rate": 3.60125e-05,
      "loss": 0.0025,
      "step": 33570
    },
    {
      "epoch": 2.2386666666666666,
      "grad_norm": 0.9053689241409302,
      "learning_rate": 3.600833333333333e-05,
      "loss": 0.0025,
      "step": 33580
    },
    {
      "epoch": 2.239333333333333,
      "grad_norm": 0.4021035134792328,
      "learning_rate": 3.600416666666667e-05,
      "loss": 0.0038,
      "step": 33590
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.15499676764011383,
      "learning_rate": 3.6e-05,
      "loss": 0.0025,
      "step": 33600
    },
    {
      "epoch": 2.240666666666667,
      "grad_norm": 0.7241674661636353,
      "learning_rate": 3.599583333333333e-05,
      "loss": 0.0018,
      "step": 33610
    },
    {
      "epoch": 2.2413333333333334,
      "grad_norm": 0.29368525743484497,
      "learning_rate": 3.599166666666667e-05,
      "loss": 0.003,
      "step": 33620
    },
    {
      "epoch": 2.242,
      "grad_norm": 0.4994797110557556,
      "learning_rate": 3.59875e-05,
      "loss": 0.0015,
      "step": 33630
    },
    {
      "epoch": 2.2426666666666666,
      "grad_norm": 0.2478896826505661,
      "learning_rate": 3.598333333333334e-05,
      "loss": 0.0021,
      "step": 33640
    },
    {
      "epoch": 2.243333333333333,
      "grad_norm": 0.2477634847164154,
      "learning_rate": 3.597916666666667e-05,
      "loss": 0.002,
      "step": 33650
    },
    {
      "epoch": 2.2439999999999998,
      "grad_norm": 0.3919324278831482,
      "learning_rate": 3.5975e-05,
      "loss": 0.0021,
      "step": 33660
    },
    {
      "epoch": 2.244666666666667,
      "grad_norm": 0.1883387565612793,
      "learning_rate": 3.597083333333334e-05,
      "loss": 0.0028,
      "step": 33670
    },
    {
      "epoch": 2.2453333333333334,
      "grad_norm": 0.446076363325119,
      "learning_rate": 3.596666666666667e-05,
      "loss": 0.0031,
      "step": 33680
    },
    {
      "epoch": 2.246,
      "grad_norm": 0.19347353279590607,
      "learning_rate": 3.59625e-05,
      "loss": 0.0021,
      "step": 33690
    },
    {
      "epoch": 2.2466666666666666,
      "grad_norm": 0.39879608154296875,
      "learning_rate": 3.595833333333333e-05,
      "loss": 0.0016,
      "step": 33700
    },
    {
      "epoch": 2.247333333333333,
      "grad_norm": 0.5151295065879822,
      "learning_rate": 3.595416666666667e-05,
      "loss": 0.0019,
      "step": 33710
    },
    {
      "epoch": 2.248,
      "grad_norm": 0.3659210801124573,
      "learning_rate": 3.595e-05,
      "loss": 0.0029,
      "step": 33720
    },
    {
      "epoch": 2.248666666666667,
      "grad_norm": 0.5763517618179321,
      "learning_rate": 3.594583333333334e-05,
      "loss": 0.0027,
      "step": 33730
    },
    {
      "epoch": 2.2493333333333334,
      "grad_norm": 0.5330579280853271,
      "learning_rate": 3.594166666666667e-05,
      "loss": 0.0031,
      "step": 33740
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.1995001882314682,
      "learning_rate": 3.59375e-05,
      "loss": 0.0021,
      "step": 33750
    },
    {
      "epoch": 2.2506666666666666,
      "grad_norm": 0.49019572138786316,
      "learning_rate": 3.593333333333334e-05,
      "loss": 0.0025,
      "step": 33760
    },
    {
      "epoch": 2.251333333333333,
      "grad_norm": 0.3229024112224579,
      "learning_rate": 3.592916666666667e-05,
      "loss": 0.0024,
      "step": 33770
    },
    {
      "epoch": 2.252,
      "grad_norm": 0.8631366491317749,
      "learning_rate": 3.5925000000000006e-05,
      "loss": 0.0024,
      "step": 33780
    },
    {
      "epoch": 2.252666666666667,
      "grad_norm": 0.06005578860640526,
      "learning_rate": 3.592083333333334e-05,
      "loss": 0.0028,
      "step": 33790
    },
    {
      "epoch": 2.2533333333333334,
      "grad_norm": 0.6140061616897583,
      "learning_rate": 3.591666666666667e-05,
      "loss": 0.0023,
      "step": 33800
    },
    {
      "epoch": 2.254,
      "grad_norm": 0.1753062605857849,
      "learning_rate": 3.5912500000000006e-05,
      "loss": 0.0028,
      "step": 33810
    },
    {
      "epoch": 2.2546666666666666,
      "grad_norm": 0.7818279266357422,
      "learning_rate": 3.590833333333333e-05,
      "loss": 0.0033,
      "step": 33820
    },
    {
      "epoch": 2.255333333333333,
      "grad_norm": 0.2658126950263977,
      "learning_rate": 3.590416666666667e-05,
      "loss": 0.0035,
      "step": 33830
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.28592807054519653,
      "learning_rate": 3.59e-05,
      "loss": 0.0017,
      "step": 33840
    },
    {
      "epoch": 2.256666666666667,
      "grad_norm": 0.19358965754508972,
      "learning_rate": 3.5895833333333336e-05,
      "loss": 0.002,
      "step": 33850
    },
    {
      "epoch": 2.2573333333333334,
      "grad_norm": 0.32457229495048523,
      "learning_rate": 3.589166666666667e-05,
      "loss": 0.0018,
      "step": 33860
    },
    {
      "epoch": 2.258,
      "grad_norm": 1.061588168144226,
      "learning_rate": 3.5887500000000005e-05,
      "loss": 0.0028,
      "step": 33870
    },
    {
      "epoch": 2.2586666666666666,
      "grad_norm": 0.6058723330497742,
      "learning_rate": 3.5883333333333336e-05,
      "loss": 0.0025,
      "step": 33880
    },
    {
      "epoch": 2.259333333333333,
      "grad_norm": 0.47619959712028503,
      "learning_rate": 3.587916666666667e-05,
      "loss": 0.0019,
      "step": 33890
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.638458788394928,
      "learning_rate": 3.5875000000000005e-05,
      "loss": 0.0018,
      "step": 33900
    },
    {
      "epoch": 2.260666666666667,
      "grad_norm": 0.9217308759689331,
      "learning_rate": 3.5870833333333336e-05,
      "loss": 0.0028,
      "step": 33910
    },
    {
      "epoch": 2.2613333333333334,
      "grad_norm": 0.31973761320114136,
      "learning_rate": 3.586666666666667e-05,
      "loss": 0.0029,
      "step": 33920
    },
    {
      "epoch": 2.262,
      "grad_norm": 0.9535794258117676,
      "learning_rate": 3.5862500000000004e-05,
      "loss": 0.0032,
      "step": 33930
    },
    {
      "epoch": 2.2626666666666666,
      "grad_norm": 0.2605435252189636,
      "learning_rate": 3.5858333333333335e-05,
      "loss": 0.002,
      "step": 33940
    },
    {
      "epoch": 2.263333333333333,
      "grad_norm": 0.9030385613441467,
      "learning_rate": 3.5854166666666666e-05,
      "loss": 0.0017,
      "step": 33950
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 0.39133647084236145,
      "learning_rate": 3.585e-05,
      "loss": 0.002,
      "step": 33960
    },
    {
      "epoch": 2.264666666666667,
      "grad_norm": 0.262540727853775,
      "learning_rate": 3.5845833333333335e-05,
      "loss": 0.0029,
      "step": 33970
    },
    {
      "epoch": 2.2653333333333334,
      "grad_norm": 0.6136764883995056,
      "learning_rate": 3.5841666666666666e-05,
      "loss": 0.0029,
      "step": 33980
    },
    {
      "epoch": 2.266,
      "grad_norm": 0.15090076625347137,
      "learning_rate": 3.5837500000000004e-05,
      "loss": 0.0039,
      "step": 33990
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.7111231088638306,
      "learning_rate": 3.5833333333333335e-05,
      "loss": 0.0021,
      "step": 34000
    },
    {
      "epoch": 2.267333333333333,
      "grad_norm": 0.0992211103439331,
      "learning_rate": 3.582916666666667e-05,
      "loss": 0.0022,
      "step": 34010
    },
    {
      "epoch": 2.268,
      "grad_norm": 0.20556828379631042,
      "learning_rate": 3.5825000000000003e-05,
      "loss": 0.0026,
      "step": 34020
    },
    {
      "epoch": 2.268666666666667,
      "grad_norm": 0.5639288425445557,
      "learning_rate": 3.5820833333333334e-05,
      "loss": 0.0028,
      "step": 34030
    },
    {
      "epoch": 2.2693333333333334,
      "grad_norm": 0.6346699595451355,
      "learning_rate": 3.581666666666667e-05,
      "loss": 0.0024,
      "step": 34040
    },
    {
      "epoch": 2.27,
      "grad_norm": 0.4887145459651947,
      "learning_rate": 3.58125e-05,
      "loss": 0.0023,
      "step": 34050
    },
    {
      "epoch": 2.2706666666666666,
      "grad_norm": 0.4688054919242859,
      "learning_rate": 3.5808333333333334e-05,
      "loss": 0.0013,
      "step": 34060
    },
    {
      "epoch": 2.271333333333333,
      "grad_norm": 0.23247766494750977,
      "learning_rate": 3.5804166666666665e-05,
      "loss": 0.0032,
      "step": 34070
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 0.15690162777900696,
      "learning_rate": 3.58e-05,
      "loss": 0.0037,
      "step": 34080
    },
    {
      "epoch": 2.272666666666667,
      "grad_norm": 0.32784321904182434,
      "learning_rate": 3.5795833333333334e-05,
      "loss": 0.0033,
      "step": 34090
    },
    {
      "epoch": 2.2733333333333334,
      "grad_norm": 0.45901769399642944,
      "learning_rate": 3.5791666666666665e-05,
      "loss": 0.0025,
      "step": 34100
    },
    {
      "epoch": 2.274,
      "grad_norm": 0.305907279253006,
      "learning_rate": 3.57875e-05,
      "loss": 0.0023,
      "step": 34110
    },
    {
      "epoch": 2.2746666666666666,
      "grad_norm": 0.547083854675293,
      "learning_rate": 3.5783333333333333e-05,
      "loss": 0.0016,
      "step": 34120
    },
    {
      "epoch": 2.275333333333333,
      "grad_norm": 0.23031017184257507,
      "learning_rate": 3.577916666666667e-05,
      "loss": 0.0017,
      "step": 34130
    },
    {
      "epoch": 2.276,
      "grad_norm": 0.7568621039390564,
      "learning_rate": 3.5775e-05,
      "loss": 0.0034,
      "step": 34140
    },
    {
      "epoch": 2.276666666666667,
      "grad_norm": 0.8245847821235657,
      "learning_rate": 3.577083333333334e-05,
      "loss": 0.0022,
      "step": 34150
    },
    {
      "epoch": 2.2773333333333334,
      "grad_norm": 0.38714486360549927,
      "learning_rate": 3.576666666666667e-05,
      "loss": 0.0017,
      "step": 34160
    },
    {
      "epoch": 2.278,
      "grad_norm": 0.5715139508247375,
      "learning_rate": 3.57625e-05,
      "loss": 0.0022,
      "step": 34170
    },
    {
      "epoch": 2.2786666666666666,
      "grad_norm": 0.24535468220710754,
      "learning_rate": 3.575833333333333e-05,
      "loss": 0.0024,
      "step": 34180
    },
    {
      "epoch": 2.279333333333333,
      "grad_norm": 0.15604014694690704,
      "learning_rate": 3.5754166666666664e-05,
      "loss": 0.0029,
      "step": 34190
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.5473805665969849,
      "learning_rate": 3.575e-05,
      "loss": 0.0024,
      "step": 34200
    },
    {
      "epoch": 2.280666666666667,
      "grad_norm": 0.20308825373649597,
      "learning_rate": 3.574583333333333e-05,
      "loss": 0.0023,
      "step": 34210
    },
    {
      "epoch": 2.2813333333333334,
      "grad_norm": 0.7532167434692383,
      "learning_rate": 3.574166666666667e-05,
      "loss": 0.0015,
      "step": 34220
    },
    {
      "epoch": 2.282,
      "grad_norm": 0.13182656466960907,
      "learning_rate": 3.57375e-05,
      "loss": 0.0022,
      "step": 34230
    },
    {
      "epoch": 2.2826666666666666,
      "grad_norm": 0.10296551883220673,
      "learning_rate": 3.573333333333333e-05,
      "loss": 0.0027,
      "step": 34240
    },
    {
      "epoch": 2.283333333333333,
      "grad_norm": 0.3585743010044098,
      "learning_rate": 3.572916666666667e-05,
      "loss": 0.0015,
      "step": 34250
    },
    {
      "epoch": 2.284,
      "grad_norm": 0.5689421892166138,
      "learning_rate": 3.5725e-05,
      "loss": 0.0026,
      "step": 34260
    },
    {
      "epoch": 2.284666666666667,
      "grad_norm": 0.49409180879592896,
      "learning_rate": 3.572083333333334e-05,
      "loss": 0.0027,
      "step": 34270
    },
    {
      "epoch": 2.2853333333333334,
      "grad_norm": 0.4958898425102234,
      "learning_rate": 3.571666666666667e-05,
      "loss": 0.0021,
      "step": 34280
    },
    {
      "epoch": 2.286,
      "grad_norm": 0.07953819632530212,
      "learning_rate": 3.571250000000001e-05,
      "loss": 0.002,
      "step": 34290
    },
    {
      "epoch": 2.2866666666666666,
      "grad_norm": 0.08130060136318207,
      "learning_rate": 3.570833333333333e-05,
      "loss": 0.0017,
      "step": 34300
    },
    {
      "epoch": 2.287333333333333,
      "grad_norm": 0.1301741600036621,
      "learning_rate": 3.570416666666667e-05,
      "loss": 0.0027,
      "step": 34310
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.5518369674682617,
      "learning_rate": 3.57e-05,
      "loss": 0.0025,
      "step": 34320
    },
    {
      "epoch": 2.288666666666667,
      "grad_norm": 0.8300293684005737,
      "learning_rate": 3.569583333333333e-05,
      "loss": 0.0032,
      "step": 34330
    },
    {
      "epoch": 2.2893333333333334,
      "grad_norm": 0.6802248358726501,
      "learning_rate": 3.569166666666667e-05,
      "loss": 0.0023,
      "step": 34340
    },
    {
      "epoch": 2.29,
      "grad_norm": 0.39644283056259155,
      "learning_rate": 3.56875e-05,
      "loss": 0.0036,
      "step": 34350
    },
    {
      "epoch": 2.2906666666666666,
      "grad_norm": 0.5453606843948364,
      "learning_rate": 3.568333333333334e-05,
      "loss": 0.0022,
      "step": 34360
    },
    {
      "epoch": 2.291333333333333,
      "grad_norm": 0.09098663926124573,
      "learning_rate": 3.567916666666667e-05,
      "loss": 0.0014,
      "step": 34370
    },
    {
      "epoch": 2.292,
      "grad_norm": 0.5100805759429932,
      "learning_rate": 3.5675e-05,
      "loss": 0.0027,
      "step": 34380
    },
    {
      "epoch": 2.292666666666667,
      "grad_norm": 0.08563496917486191,
      "learning_rate": 3.567083333333334e-05,
      "loss": 0.0026,
      "step": 34390
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 0.0732218325138092,
      "learning_rate": 3.566666666666667e-05,
      "loss": 0.0018,
      "step": 34400
    },
    {
      "epoch": 2.294,
      "grad_norm": 0.2164747416973114,
      "learning_rate": 3.5662500000000006e-05,
      "loss": 0.0023,
      "step": 34410
    },
    {
      "epoch": 2.2946666666666666,
      "grad_norm": 0.6120136976242065,
      "learning_rate": 3.565833333333333e-05,
      "loss": 0.0013,
      "step": 34420
    },
    {
      "epoch": 2.2953333333333332,
      "grad_norm": 0.7489691376686096,
      "learning_rate": 3.565416666666667e-05,
      "loss": 0.0022,
      "step": 34430
    },
    {
      "epoch": 2.296,
      "grad_norm": 0.3889595866203308,
      "learning_rate": 3.565e-05,
      "loss": 0.0029,
      "step": 34440
    },
    {
      "epoch": 2.296666666666667,
      "grad_norm": 0.6761428117752075,
      "learning_rate": 3.564583333333334e-05,
      "loss": 0.0032,
      "step": 34450
    },
    {
      "epoch": 2.2973333333333334,
      "grad_norm": 0.08931738138198853,
      "learning_rate": 3.564166666666667e-05,
      "loss": 0.0026,
      "step": 34460
    },
    {
      "epoch": 2.298,
      "grad_norm": 0.1171465665102005,
      "learning_rate": 3.56375e-05,
      "loss": 0.0029,
      "step": 34470
    },
    {
      "epoch": 2.2986666666666666,
      "grad_norm": 0.5546360015869141,
      "learning_rate": 3.563333333333334e-05,
      "loss": 0.0023,
      "step": 34480
    },
    {
      "epoch": 2.2993333333333332,
      "grad_norm": 0.2393847554922104,
      "learning_rate": 3.562916666666667e-05,
      "loss": 0.0019,
      "step": 34490
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.48265933990478516,
      "learning_rate": 3.5625000000000005e-05,
      "loss": 0.0029,
      "step": 34500
    },
    {
      "epoch": 2.300666666666667,
      "grad_norm": 0.6044543981552124,
      "learning_rate": 3.5620833333333336e-05,
      "loss": 0.0014,
      "step": 34510
    },
    {
      "epoch": 2.3013333333333335,
      "grad_norm": 0.8594886064529419,
      "learning_rate": 3.561666666666667e-05,
      "loss": 0.0035,
      "step": 34520
    },
    {
      "epoch": 2.302,
      "grad_norm": 0.32454296946525574,
      "learning_rate": 3.5612500000000005e-05,
      "loss": 0.0026,
      "step": 34530
    },
    {
      "epoch": 2.3026666666666666,
      "grad_norm": 0.825706422328949,
      "learning_rate": 3.560833333333333e-05,
      "loss": 0.0026,
      "step": 34540
    },
    {
      "epoch": 2.3033333333333332,
      "grad_norm": 0.35561102628707886,
      "learning_rate": 3.560416666666667e-05,
      "loss": 0.0023,
      "step": 34550
    },
    {
      "epoch": 2.304,
      "grad_norm": 0.6844339966773987,
      "learning_rate": 3.56e-05,
      "loss": 0.0026,
      "step": 34560
    },
    {
      "epoch": 2.304666666666667,
      "grad_norm": 0.3507183790206909,
      "learning_rate": 3.5595833333333336e-05,
      "loss": 0.0038,
      "step": 34570
    },
    {
      "epoch": 2.3053333333333335,
      "grad_norm": 0.04529055207967758,
      "learning_rate": 3.559166666666667e-05,
      "loss": 0.0029,
      "step": 34580
    },
    {
      "epoch": 2.306,
      "grad_norm": 0.6235560774803162,
      "learning_rate": 3.5587500000000004e-05,
      "loss": 0.0019,
      "step": 34590
    },
    {
      "epoch": 2.3066666666666666,
      "grad_norm": 0.37161722779273987,
      "learning_rate": 3.5583333333333335e-05,
      "loss": 0.0034,
      "step": 34600
    },
    {
      "epoch": 2.3073333333333332,
      "grad_norm": 0.7688451409339905,
      "learning_rate": 3.5579166666666666e-05,
      "loss": 0.0022,
      "step": 34610
    },
    {
      "epoch": 2.308,
      "grad_norm": 0.13761959969997406,
      "learning_rate": 3.5575000000000004e-05,
      "loss": 0.0021,
      "step": 34620
    },
    {
      "epoch": 2.3086666666666664,
      "grad_norm": 0.042996328324079514,
      "learning_rate": 3.5570833333333335e-05,
      "loss": 0.0022,
      "step": 34630
    },
    {
      "epoch": 2.3093333333333335,
      "grad_norm": 0.8910713195800781,
      "learning_rate": 3.556666666666667e-05,
      "loss": 0.0021,
      "step": 34640
    },
    {
      "epoch": 2.31,
      "grad_norm": 0.4627207815647125,
      "learning_rate": 3.5562500000000004e-05,
      "loss": 0.0025,
      "step": 34650
    },
    {
      "epoch": 2.3106666666666666,
      "grad_norm": 0.9190138578414917,
      "learning_rate": 3.5558333333333335e-05,
      "loss": 0.0018,
      "step": 34660
    },
    {
      "epoch": 2.3113333333333332,
      "grad_norm": 0.6828109622001648,
      "learning_rate": 3.5554166666666666e-05,
      "loss": 0.0026,
      "step": 34670
    },
    {
      "epoch": 2.312,
      "grad_norm": 0.46947020292282104,
      "learning_rate": 3.555e-05,
      "loss": 0.0018,
      "step": 34680
    },
    {
      "epoch": 2.312666666666667,
      "grad_norm": 0.5831744074821472,
      "learning_rate": 3.5545833333333334e-05,
      "loss": 0.0029,
      "step": 34690
    },
    {
      "epoch": 2.3133333333333335,
      "grad_norm": 0.787067174911499,
      "learning_rate": 3.5541666666666665e-05,
      "loss": 0.0028,
      "step": 34700
    },
    {
      "epoch": 2.314,
      "grad_norm": 0.23372814059257507,
      "learning_rate": 3.55375e-05,
      "loss": 0.0032,
      "step": 34710
    },
    {
      "epoch": 2.3146666666666667,
      "grad_norm": 1.00558340549469,
      "learning_rate": 3.5533333333333334e-05,
      "loss": 0.0034,
      "step": 34720
    },
    {
      "epoch": 2.3153333333333332,
      "grad_norm": 0.1606551706790924,
      "learning_rate": 3.552916666666667e-05,
      "loss": 0.0022,
      "step": 34730
    },
    {
      "epoch": 2.316,
      "grad_norm": 0.3774943947792053,
      "learning_rate": 3.5525e-05,
      "loss": 0.0037,
      "step": 34740
    },
    {
      "epoch": 2.3166666666666664,
      "grad_norm": 0.6807674169540405,
      "learning_rate": 3.5520833333333334e-05,
      "loss": 0.0018,
      "step": 34750
    },
    {
      "epoch": 2.3173333333333335,
      "grad_norm": 0.05664097145199776,
      "learning_rate": 3.551666666666667e-05,
      "loss": 0.003,
      "step": 34760
    },
    {
      "epoch": 2.318,
      "grad_norm": 0.6504949927330017,
      "learning_rate": 3.55125e-05,
      "loss": 0.0027,
      "step": 34770
    },
    {
      "epoch": 2.3186666666666667,
      "grad_norm": 0.5182164907455444,
      "learning_rate": 3.5508333333333334e-05,
      "loss": 0.0017,
      "step": 34780
    },
    {
      "epoch": 2.3193333333333332,
      "grad_norm": 0.9058358073234558,
      "learning_rate": 3.5504166666666665e-05,
      "loss": 0.0027,
      "step": 34790
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.1268446147441864,
      "learning_rate": 3.55e-05,
      "loss": 0.0029,
      "step": 34800
    },
    {
      "epoch": 2.320666666666667,
      "grad_norm": 0.8961594700813293,
      "learning_rate": 3.549583333333333e-05,
      "loss": 0.0036,
      "step": 34810
    },
    {
      "epoch": 2.3213333333333335,
      "grad_norm": 0.2738659381866455,
      "learning_rate": 3.5491666666666664e-05,
      "loss": 0.0025,
      "step": 34820
    },
    {
      "epoch": 2.322,
      "grad_norm": 0.44049781560897827,
      "learning_rate": 3.54875e-05,
      "loss": 0.0022,
      "step": 34830
    },
    {
      "epoch": 2.3226666666666667,
      "grad_norm": 0.25166988372802734,
      "learning_rate": 3.548333333333333e-05,
      "loss": 0.0026,
      "step": 34840
    },
    {
      "epoch": 2.3233333333333333,
      "grad_norm": 0.2842924892902374,
      "learning_rate": 3.547916666666667e-05,
      "loss": 0.0032,
      "step": 34850
    },
    {
      "epoch": 2.324,
      "grad_norm": 0.8532469868659973,
      "learning_rate": 3.5475e-05,
      "loss": 0.0023,
      "step": 34860
    },
    {
      "epoch": 2.3246666666666664,
      "grad_norm": 0.27103543281555176,
      "learning_rate": 3.547083333333334e-05,
      "loss": 0.0038,
      "step": 34870
    },
    {
      "epoch": 2.3253333333333335,
      "grad_norm": 0.3484737277030945,
      "learning_rate": 3.546666666666667e-05,
      "loss": 0.0023,
      "step": 34880
    },
    {
      "epoch": 2.326,
      "grad_norm": 0.20876435935497284,
      "learning_rate": 3.54625e-05,
      "loss": 0.0027,
      "step": 34890
    },
    {
      "epoch": 2.3266666666666667,
      "grad_norm": 0.44464758038520813,
      "learning_rate": 3.545833333333333e-05,
      "loss": 0.004,
      "step": 34900
    },
    {
      "epoch": 2.3273333333333333,
      "grad_norm": 0.5101150274276733,
      "learning_rate": 3.545416666666666e-05,
      "loss": 0.0032,
      "step": 34910
    },
    {
      "epoch": 2.328,
      "grad_norm": 0.7515746355056763,
      "learning_rate": 3.545e-05,
      "loss": 0.0042,
      "step": 34920
    },
    {
      "epoch": 2.328666666666667,
      "grad_norm": 0.2625657618045807,
      "learning_rate": 3.544583333333333e-05,
      "loss": 0.0022,
      "step": 34930
    },
    {
      "epoch": 2.3293333333333335,
      "grad_norm": 0.27160903811454773,
      "learning_rate": 3.544166666666667e-05,
      "loss": 0.0022,
      "step": 34940
    },
    {
      "epoch": 2.33,
      "grad_norm": 0.17195062339305878,
      "learning_rate": 3.54375e-05,
      "loss": 0.0024,
      "step": 34950
    },
    {
      "epoch": 2.3306666666666667,
      "grad_norm": 0.04356234893202782,
      "learning_rate": 3.543333333333333e-05,
      "loss": 0.002,
      "step": 34960
    },
    {
      "epoch": 2.3313333333333333,
      "grad_norm": 0.3071202039718628,
      "learning_rate": 3.542916666666667e-05,
      "loss": 0.0016,
      "step": 34970
    },
    {
      "epoch": 2.332,
      "grad_norm": 0.17364870011806488,
      "learning_rate": 3.5425e-05,
      "loss": 0.0015,
      "step": 34980
    },
    {
      "epoch": 2.3326666666666664,
      "grad_norm": 0.09916204959154129,
      "learning_rate": 3.542083333333334e-05,
      "loss": 0.0022,
      "step": 34990
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.7353923320770264,
      "learning_rate": 3.541666666666667e-05,
      "loss": 0.0029,
      "step": 35000
    },
    {
      "epoch": 2.334,
      "grad_norm": 0.5264559388160706,
      "learning_rate": 3.541250000000001e-05,
      "loss": 0.0023,
      "step": 35010
    },
    {
      "epoch": 2.3346666666666667,
      "grad_norm": 0.3616064786911011,
      "learning_rate": 3.540833333333334e-05,
      "loss": 0.0016,
      "step": 35020
    },
    {
      "epoch": 2.3353333333333333,
      "grad_norm": 0.3584188222885132,
      "learning_rate": 3.540416666666667e-05,
      "loss": 0.0021,
      "step": 35030
    },
    {
      "epoch": 2.336,
      "grad_norm": 0.14958502352237701,
      "learning_rate": 3.54e-05,
      "loss": 0.003,
      "step": 35040
    },
    {
      "epoch": 2.336666666666667,
      "grad_norm": 0.5846105813980103,
      "learning_rate": 3.539583333333333e-05,
      "loss": 0.004,
      "step": 35050
    },
    {
      "epoch": 2.3373333333333335,
      "grad_norm": 0.35568249225616455,
      "learning_rate": 3.539166666666667e-05,
      "loss": 0.0028,
      "step": 35060
    },
    {
      "epoch": 2.338,
      "grad_norm": 0.09422560036182404,
      "learning_rate": 3.53875e-05,
      "loss": 0.0027,
      "step": 35070
    },
    {
      "epoch": 2.3386666666666667,
      "grad_norm": 0.19998110830783844,
      "learning_rate": 3.538333333333334e-05,
      "loss": 0.0013,
      "step": 35080
    },
    {
      "epoch": 2.3393333333333333,
      "grad_norm": 0.30964022874832153,
      "learning_rate": 3.537916666666667e-05,
      "loss": 0.002,
      "step": 35090
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.5101279020309448,
      "learning_rate": 3.5375e-05,
      "loss": 0.0025,
      "step": 35100
    },
    {
      "epoch": 2.3406666666666665,
      "grad_norm": 0.5959398150444031,
      "learning_rate": 3.537083333333334e-05,
      "loss": 0.0021,
      "step": 35110
    },
    {
      "epoch": 2.3413333333333335,
      "grad_norm": 0.48381152749061584,
      "learning_rate": 3.536666666666667e-05,
      "loss": 0.0025,
      "step": 35120
    },
    {
      "epoch": 2.342,
      "grad_norm": 0.04778626933693886,
      "learning_rate": 3.5362500000000006e-05,
      "loss": 0.0025,
      "step": 35130
    },
    {
      "epoch": 2.3426666666666667,
      "grad_norm": 0.08492077887058258,
      "learning_rate": 3.535833333333334e-05,
      "loss": 0.0026,
      "step": 35140
    },
    {
      "epoch": 2.3433333333333333,
      "grad_norm": 0.5016598105430603,
      "learning_rate": 3.535416666666667e-05,
      "loss": 0.0021,
      "step": 35150
    },
    {
      "epoch": 2.344,
      "grad_norm": 0.5996476411819458,
      "learning_rate": 3.535e-05,
      "loss": 0.0024,
      "step": 35160
    },
    {
      "epoch": 2.344666666666667,
      "grad_norm": 0.06755883246660233,
      "learning_rate": 3.5345833333333336e-05,
      "loss": 0.0024,
      "step": 35170
    },
    {
      "epoch": 2.3453333333333335,
      "grad_norm": 0.7406607866287231,
      "learning_rate": 3.534166666666667e-05,
      "loss": 0.0018,
      "step": 35180
    },
    {
      "epoch": 2.346,
      "grad_norm": 0.668834388256073,
      "learning_rate": 3.53375e-05,
      "loss": 0.003,
      "step": 35190
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 0.7172471284866333,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 0.003,
      "step": 35200
    },
    {
      "epoch": 2.3473333333333333,
      "grad_norm": 0.2832857072353363,
      "learning_rate": 3.532916666666667e-05,
      "loss": 0.0022,
      "step": 35210
    },
    {
      "epoch": 2.348,
      "grad_norm": 0.7491344213485718,
      "learning_rate": 3.5325000000000005e-05,
      "loss": 0.0018,
      "step": 35220
    },
    {
      "epoch": 2.3486666666666665,
      "grad_norm": 0.6122206449508667,
      "learning_rate": 3.5320833333333336e-05,
      "loss": 0.0016,
      "step": 35230
    },
    {
      "epoch": 2.3493333333333335,
      "grad_norm": 0.7905141115188599,
      "learning_rate": 3.531666666666667e-05,
      "loss": 0.0025,
      "step": 35240
    },
    {
      "epoch": 2.35,
      "grad_norm": 0.637826144695282,
      "learning_rate": 3.5312500000000005e-05,
      "loss": 0.0026,
      "step": 35250
    },
    {
      "epoch": 2.3506666666666667,
      "grad_norm": 0.13603627681732178,
      "learning_rate": 3.5308333333333336e-05,
      "loss": 0.0016,
      "step": 35260
    },
    {
      "epoch": 2.3513333333333333,
      "grad_norm": 0.35178354382514954,
      "learning_rate": 3.5304166666666666e-05,
      "loss": 0.0017,
      "step": 35270
    },
    {
      "epoch": 2.352,
      "grad_norm": 0.3622223138809204,
      "learning_rate": 3.53e-05,
      "loss": 0.0024,
      "step": 35280
    },
    {
      "epoch": 2.352666666666667,
      "grad_norm": 0.15972036123275757,
      "learning_rate": 3.5295833333333335e-05,
      "loss": 0.0022,
      "step": 35290
    },
    {
      "epoch": 2.3533333333333335,
      "grad_norm": 0.3122945725917816,
      "learning_rate": 3.5291666666666666e-05,
      "loss": 0.0029,
      "step": 35300
    },
    {
      "epoch": 2.354,
      "grad_norm": 0.6301915049552917,
      "learning_rate": 3.5287500000000004e-05,
      "loss": 0.003,
      "step": 35310
    },
    {
      "epoch": 2.3546666666666667,
      "grad_norm": 0.38481900095939636,
      "learning_rate": 3.5283333333333335e-05,
      "loss": 0.0036,
      "step": 35320
    },
    {
      "epoch": 2.3553333333333333,
      "grad_norm": 0.9354325532913208,
      "learning_rate": 3.5279166666666666e-05,
      "loss": 0.0023,
      "step": 35330
    },
    {
      "epoch": 2.356,
      "grad_norm": 0.2391478717327118,
      "learning_rate": 3.5275000000000004e-05,
      "loss": 0.0017,
      "step": 35340
    },
    {
      "epoch": 2.3566666666666665,
      "grad_norm": 0.9747793674468994,
      "learning_rate": 3.5270833333333335e-05,
      "loss": 0.0029,
      "step": 35350
    },
    {
      "epoch": 2.3573333333333335,
      "grad_norm": 0.3642825484275818,
      "learning_rate": 3.526666666666667e-05,
      "loss": 0.0019,
      "step": 35360
    },
    {
      "epoch": 2.358,
      "grad_norm": 0.7722926139831543,
      "learning_rate": 3.52625e-05,
      "loss": 0.0019,
      "step": 35370
    },
    {
      "epoch": 2.3586666666666667,
      "grad_norm": 0.24190683662891388,
      "learning_rate": 3.5258333333333334e-05,
      "loss": 0.0024,
      "step": 35380
    },
    {
      "epoch": 2.3593333333333333,
      "grad_norm": 0.23562900722026825,
      "learning_rate": 3.5254166666666665e-05,
      "loss": 0.002,
      "step": 35390
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.4090963304042816,
      "learning_rate": 3.525e-05,
      "loss": 0.0019,
      "step": 35400
    },
    {
      "epoch": 2.360666666666667,
      "grad_norm": 0.23456743359565735,
      "learning_rate": 3.5245833333333334e-05,
      "loss": 0.0022,
      "step": 35410
    },
    {
      "epoch": 2.3613333333333335,
      "grad_norm": 0.4671136438846588,
      "learning_rate": 3.5241666666666665e-05,
      "loss": 0.0029,
      "step": 35420
    },
    {
      "epoch": 2.362,
      "grad_norm": 0.1616460680961609,
      "learning_rate": 3.52375e-05,
      "loss": 0.0024,
      "step": 35430
    },
    {
      "epoch": 2.3626666666666667,
      "grad_norm": 0.12340027093887329,
      "learning_rate": 3.5233333333333334e-05,
      "loss": 0.002,
      "step": 35440
    },
    {
      "epoch": 2.3633333333333333,
      "grad_norm": 0.8246593475341797,
      "learning_rate": 3.522916666666667e-05,
      "loss": 0.0018,
      "step": 35450
    },
    {
      "epoch": 2.364,
      "grad_norm": 0.15396319329738617,
      "learning_rate": 3.5225e-05,
      "loss": 0.0018,
      "step": 35460
    },
    {
      "epoch": 2.3646666666666665,
      "grad_norm": 0.30924105644226074,
      "learning_rate": 3.522083333333333e-05,
      "loss": 0.0013,
      "step": 35470
    },
    {
      "epoch": 2.3653333333333335,
      "grad_norm": 0.07517319172620773,
      "learning_rate": 3.521666666666667e-05,
      "loss": 0.0026,
      "step": 35480
    },
    {
      "epoch": 2.366,
      "grad_norm": 0.7601889371871948,
      "learning_rate": 3.52125e-05,
      "loss": 0.0018,
      "step": 35490
    },
    {
      "epoch": 2.3666666666666667,
      "grad_norm": 0.3834913671016693,
      "learning_rate": 3.520833333333334e-05,
      "loss": 0.0022,
      "step": 35500
    },
    {
      "epoch": 2.3673333333333333,
      "grad_norm": 0.6835098266601562,
      "learning_rate": 3.5204166666666664e-05,
      "loss": 0.0018,
      "step": 35510
    },
    {
      "epoch": 2.368,
      "grad_norm": 0.2790641784667969,
      "learning_rate": 3.52e-05,
      "loss": 0.0024,
      "step": 35520
    },
    {
      "epoch": 2.3686666666666665,
      "grad_norm": 0.12200049310922623,
      "learning_rate": 3.519583333333333e-05,
      "loss": 0.0012,
      "step": 35530
    },
    {
      "epoch": 2.3693333333333335,
      "grad_norm": 1.2364097833633423,
      "learning_rate": 3.519166666666667e-05,
      "loss": 0.0041,
      "step": 35540
    },
    {
      "epoch": 2.37,
      "grad_norm": 0.3922302722930908,
      "learning_rate": 3.51875e-05,
      "loss": 0.0021,
      "step": 35550
    },
    {
      "epoch": 2.3706666666666667,
      "grad_norm": 0.4958149492740631,
      "learning_rate": 3.518333333333333e-05,
      "loss": 0.0024,
      "step": 35560
    },
    {
      "epoch": 2.3713333333333333,
      "grad_norm": 0.3501502275466919,
      "learning_rate": 3.517916666666667e-05,
      "loss": 0.0029,
      "step": 35570
    },
    {
      "epoch": 2.372,
      "grad_norm": 0.5748058557510376,
      "learning_rate": 3.5175e-05,
      "loss": 0.0021,
      "step": 35580
    },
    {
      "epoch": 2.3726666666666665,
      "grad_norm": 0.24644745886325836,
      "learning_rate": 3.517083333333334e-05,
      "loss": 0.0028,
      "step": 35590
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 0.10611995309591293,
      "learning_rate": 3.516666666666667e-05,
      "loss": 0.0021,
      "step": 35600
    },
    {
      "epoch": 2.374,
      "grad_norm": 0.024484628811478615,
      "learning_rate": 3.51625e-05,
      "loss": 0.0022,
      "step": 35610
    },
    {
      "epoch": 2.3746666666666667,
      "grad_norm": 0.6406220197677612,
      "learning_rate": 3.515833333333334e-05,
      "loss": 0.002,
      "step": 35620
    },
    {
      "epoch": 2.3753333333333333,
      "grad_norm": 0.09167144447565079,
      "learning_rate": 3.515416666666666e-05,
      "loss": 0.0017,
      "step": 35630
    },
    {
      "epoch": 2.376,
      "grad_norm": 0.37382128834724426,
      "learning_rate": 3.515e-05,
      "loss": 0.0029,
      "step": 35640
    },
    {
      "epoch": 2.3766666666666665,
      "grad_norm": 0.05591597408056259,
      "learning_rate": 3.514583333333333e-05,
      "loss": 0.0027,
      "step": 35650
    },
    {
      "epoch": 2.3773333333333335,
      "grad_norm": 0.1574607491493225,
      "learning_rate": 3.514166666666667e-05,
      "loss": 0.0017,
      "step": 35660
    },
    {
      "epoch": 2.378,
      "grad_norm": 0.5605809092521667,
      "learning_rate": 3.51375e-05,
      "loss": 0.0024,
      "step": 35670
    },
    {
      "epoch": 2.3786666666666667,
      "grad_norm": 0.5349953770637512,
      "learning_rate": 3.513333333333334e-05,
      "loss": 0.002,
      "step": 35680
    },
    {
      "epoch": 2.3793333333333333,
      "grad_norm": 0.05445542931556702,
      "learning_rate": 3.512916666666667e-05,
      "loss": 0.0033,
      "step": 35690
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.4003235697746277,
      "learning_rate": 3.5125e-05,
      "loss": 0.0024,
      "step": 35700
    },
    {
      "epoch": 2.3806666666666665,
      "grad_norm": 0.379238486289978,
      "learning_rate": 3.512083333333334e-05,
      "loss": 0.0026,
      "step": 35710
    },
    {
      "epoch": 2.3813333333333335,
      "grad_norm": 0.5821328163146973,
      "learning_rate": 3.511666666666667e-05,
      "loss": 0.0032,
      "step": 35720
    },
    {
      "epoch": 2.382,
      "grad_norm": 0.05111543834209442,
      "learning_rate": 3.5112500000000006e-05,
      "loss": 0.0025,
      "step": 35730
    },
    {
      "epoch": 2.3826666666666667,
      "grad_norm": 0.1170610785484314,
      "learning_rate": 3.510833333333334e-05,
      "loss": 0.0029,
      "step": 35740
    },
    {
      "epoch": 2.3833333333333333,
      "grad_norm": 0.07844045013189316,
      "learning_rate": 3.510416666666667e-05,
      "loss": 0.0028,
      "step": 35750
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.6123406887054443,
      "learning_rate": 3.51e-05,
      "loss": 0.0029,
      "step": 35760
    },
    {
      "epoch": 2.3846666666666665,
      "grad_norm": 0.0776141956448555,
      "learning_rate": 3.509583333333333e-05,
      "loss": 0.0017,
      "step": 35770
    },
    {
      "epoch": 2.3853333333333335,
      "grad_norm": 0.2824383080005646,
      "learning_rate": 3.509166666666667e-05,
      "loss": 0.0016,
      "step": 35780
    },
    {
      "epoch": 2.386,
      "grad_norm": 1.0195902585983276,
      "learning_rate": 3.50875e-05,
      "loss": 0.0034,
      "step": 35790
    },
    {
      "epoch": 2.3866666666666667,
      "grad_norm": 0.08985768258571625,
      "learning_rate": 3.508333333333334e-05,
      "loss": 0.003,
      "step": 35800
    },
    {
      "epoch": 2.3873333333333333,
      "grad_norm": 0.8184340000152588,
      "learning_rate": 3.507916666666667e-05,
      "loss": 0.002,
      "step": 35810
    },
    {
      "epoch": 2.388,
      "grad_norm": 0.6525068283081055,
      "learning_rate": 3.5075000000000006e-05,
      "loss": 0.0035,
      "step": 35820
    },
    {
      "epoch": 2.3886666666666665,
      "grad_norm": 0.0633355900645256,
      "learning_rate": 3.5070833333333337e-05,
      "loss": 0.0028,
      "step": 35830
    },
    {
      "epoch": 2.389333333333333,
      "grad_norm": 0.4758768379688263,
      "learning_rate": 3.506666666666667e-05,
      "loss": 0.0025,
      "step": 35840
    },
    {
      "epoch": 2.39,
      "grad_norm": 0.23097877204418182,
      "learning_rate": 3.5062500000000005e-05,
      "loss": 0.0029,
      "step": 35850
    },
    {
      "epoch": 2.3906666666666667,
      "grad_norm": 0.11823456734418869,
      "learning_rate": 3.5058333333333336e-05,
      "loss": 0.0019,
      "step": 35860
    },
    {
      "epoch": 2.3913333333333333,
      "grad_norm": 0.25723904371261597,
      "learning_rate": 3.505416666666667e-05,
      "loss": 0.002,
      "step": 35870
    },
    {
      "epoch": 2.392,
      "grad_norm": 0.5300003290176392,
      "learning_rate": 3.505e-05,
      "loss": 0.0028,
      "step": 35880
    },
    {
      "epoch": 2.3926666666666665,
      "grad_norm": 0.46532291173934937,
      "learning_rate": 3.5045833333333336e-05,
      "loss": 0.0021,
      "step": 35890
    },
    {
      "epoch": 2.3933333333333335,
      "grad_norm": 0.3448233902454376,
      "learning_rate": 3.504166666666667e-05,
      "loss": 0.0017,
      "step": 35900
    },
    {
      "epoch": 2.394,
      "grad_norm": 0.2038126140832901,
      "learning_rate": 3.50375e-05,
      "loss": 0.0031,
      "step": 35910
    },
    {
      "epoch": 2.3946666666666667,
      "grad_norm": 0.4806114733219147,
      "learning_rate": 3.5033333333333336e-05,
      "loss": 0.002,
      "step": 35920
    },
    {
      "epoch": 2.3953333333333333,
      "grad_norm": 0.15250985324382782,
      "learning_rate": 3.5029166666666667e-05,
      "loss": 0.0022,
      "step": 35930
    },
    {
      "epoch": 2.396,
      "grad_norm": 0.22317591309547424,
      "learning_rate": 3.5025000000000004e-05,
      "loss": 0.0018,
      "step": 35940
    },
    {
      "epoch": 2.3966666666666665,
      "grad_norm": 0.10315264016389847,
      "learning_rate": 3.5020833333333335e-05,
      "loss": 0.0015,
      "step": 35950
    },
    {
      "epoch": 2.397333333333333,
      "grad_norm": 0.2685336470603943,
      "learning_rate": 3.501666666666667e-05,
      "loss": 0.0025,
      "step": 35960
    },
    {
      "epoch": 2.398,
      "grad_norm": 0.7373304963111877,
      "learning_rate": 3.5012500000000004e-05,
      "loss": 0.0021,
      "step": 35970
    },
    {
      "epoch": 2.3986666666666667,
      "grad_norm": 0.47751763463020325,
      "learning_rate": 3.5008333333333335e-05,
      "loss": 0.0019,
      "step": 35980
    },
    {
      "epoch": 2.3993333333333333,
      "grad_norm": 0.057102322578430176,
      "learning_rate": 3.5004166666666666e-05,
      "loss": 0.0017,
      "step": 35990
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.1850496381521225,
      "learning_rate": 3.5e-05,
      "loss": 0.0021,
      "step": 36000
    },
    {
      "epoch": 2.4006666666666665,
      "grad_norm": 0.5417062640190125,
      "learning_rate": 3.4995833333333335e-05,
      "loss": 0.0028,
      "step": 36010
    },
    {
      "epoch": 2.4013333333333335,
      "grad_norm": 0.855022668838501,
      "learning_rate": 3.4991666666666666e-05,
      "loss": 0.002,
      "step": 36020
    },
    {
      "epoch": 2.402,
      "grad_norm": 0.8335151672363281,
      "learning_rate": 3.4987500000000003e-05,
      "loss": 0.0032,
      "step": 36030
    },
    {
      "epoch": 2.4026666666666667,
      "grad_norm": 0.2551669776439667,
      "learning_rate": 3.4983333333333334e-05,
      "loss": 0.0025,
      "step": 36040
    },
    {
      "epoch": 2.4033333333333333,
      "grad_norm": 0.8521153926849365,
      "learning_rate": 3.4979166666666665e-05,
      "loss": 0.003,
      "step": 36050
    },
    {
      "epoch": 2.404,
      "grad_norm": 0.2267766296863556,
      "learning_rate": 3.4975e-05,
      "loss": 0.0032,
      "step": 36060
    },
    {
      "epoch": 2.4046666666666665,
      "grad_norm": 0.08799919486045837,
      "learning_rate": 3.4970833333333334e-05,
      "loss": 0.0023,
      "step": 36070
    },
    {
      "epoch": 2.405333333333333,
      "grad_norm": 0.27042481303215027,
      "learning_rate": 3.496666666666667e-05,
      "loss": 0.0035,
      "step": 36080
    },
    {
      "epoch": 2.406,
      "grad_norm": 0.4253125786781311,
      "learning_rate": 3.49625e-05,
      "loss": 0.0036,
      "step": 36090
    },
    {
      "epoch": 2.4066666666666667,
      "grad_norm": 0.3543647229671478,
      "learning_rate": 3.495833333333334e-05,
      "loss": 0.0023,
      "step": 36100
    },
    {
      "epoch": 2.4073333333333333,
      "grad_norm": 0.8542614579200745,
      "learning_rate": 3.4954166666666665e-05,
      "loss": 0.0032,
      "step": 36110
    },
    {
      "epoch": 2.408,
      "grad_norm": 0.07715336978435516,
      "learning_rate": 3.495e-05,
      "loss": 0.0025,
      "step": 36120
    },
    {
      "epoch": 2.4086666666666665,
      "grad_norm": 0.5846720933914185,
      "learning_rate": 3.4945833333333333e-05,
      "loss": 0.0027,
      "step": 36130
    },
    {
      "epoch": 2.4093333333333335,
      "grad_norm": 0.05359463021159172,
      "learning_rate": 3.4941666666666664e-05,
      "loss": 0.0032,
      "step": 36140
    },
    {
      "epoch": 2.41,
      "grad_norm": 0.39407607913017273,
      "learning_rate": 3.49375e-05,
      "loss": 0.0023,
      "step": 36150
    },
    {
      "epoch": 2.4106666666666667,
      "grad_norm": 0.35656318068504333,
      "learning_rate": 3.493333333333333e-05,
      "loss": 0.0029,
      "step": 36160
    },
    {
      "epoch": 2.4113333333333333,
      "grad_norm": 0.7419555187225342,
      "learning_rate": 3.492916666666667e-05,
      "loss": 0.0025,
      "step": 36170
    },
    {
      "epoch": 2.412,
      "grad_norm": 0.3820410966873169,
      "learning_rate": 3.4925e-05,
      "loss": 0.0025,
      "step": 36180
    },
    {
      "epoch": 2.4126666666666665,
      "grad_norm": 0.4518872797489166,
      "learning_rate": 3.492083333333333e-05,
      "loss": 0.0025,
      "step": 36190
    },
    {
      "epoch": 2.413333333333333,
      "grad_norm": 0.1908327043056488,
      "learning_rate": 3.491666666666667e-05,
      "loss": 0.0017,
      "step": 36200
    },
    {
      "epoch": 2.414,
      "grad_norm": 0.49724912643432617,
      "learning_rate": 3.49125e-05,
      "loss": 0.0026,
      "step": 36210
    },
    {
      "epoch": 2.4146666666666667,
      "grad_norm": 0.3767381012439728,
      "learning_rate": 3.490833333333334e-05,
      "loss": 0.0024,
      "step": 36220
    },
    {
      "epoch": 2.4153333333333333,
      "grad_norm": 0.09154604375362396,
      "learning_rate": 3.4904166666666664e-05,
      "loss": 0.0018,
      "step": 36230
    },
    {
      "epoch": 2.416,
      "grad_norm": 0.5530876517295837,
      "learning_rate": 3.49e-05,
      "loss": 0.003,
      "step": 36240
    },
    {
      "epoch": 2.4166666666666665,
      "grad_norm": 0.04047233611345291,
      "learning_rate": 3.489583333333333e-05,
      "loss": 0.0032,
      "step": 36250
    },
    {
      "epoch": 2.4173333333333336,
      "grad_norm": 0.20076371729373932,
      "learning_rate": 3.489166666666667e-05,
      "loss": 0.0019,
      "step": 36260
    },
    {
      "epoch": 2.418,
      "grad_norm": 0.4620446264743805,
      "learning_rate": 3.48875e-05,
      "loss": 0.0019,
      "step": 36270
    },
    {
      "epoch": 2.4186666666666667,
      "grad_norm": 0.5498698353767395,
      "learning_rate": 3.488333333333333e-05,
      "loss": 0.0022,
      "step": 36280
    },
    {
      "epoch": 2.4193333333333333,
      "grad_norm": 0.27434003353118896,
      "learning_rate": 3.487916666666667e-05,
      "loss": 0.0018,
      "step": 36290
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.1669730842113495,
      "learning_rate": 3.4875e-05,
      "loss": 0.0021,
      "step": 36300
    },
    {
      "epoch": 2.4206666666666665,
      "grad_norm": 0.44605553150177,
      "learning_rate": 3.487083333333334e-05,
      "loss": 0.0028,
      "step": 36310
    },
    {
      "epoch": 2.421333333333333,
      "grad_norm": 0.2424493283033371,
      "learning_rate": 3.486666666666667e-05,
      "loss": 0.0022,
      "step": 36320
    },
    {
      "epoch": 2.422,
      "grad_norm": 0.7639760971069336,
      "learning_rate": 3.48625e-05,
      "loss": 0.0023,
      "step": 36330
    },
    {
      "epoch": 2.4226666666666667,
      "grad_norm": 0.20556248724460602,
      "learning_rate": 3.485833333333334e-05,
      "loss": 0.0017,
      "step": 36340
    },
    {
      "epoch": 2.4233333333333333,
      "grad_norm": 0.4342198669910431,
      "learning_rate": 3.485416666666667e-05,
      "loss": 0.0021,
      "step": 36350
    },
    {
      "epoch": 2.424,
      "grad_norm": 0.7182506322860718,
      "learning_rate": 3.485e-05,
      "loss": 0.0026,
      "step": 36360
    },
    {
      "epoch": 2.4246666666666665,
      "grad_norm": 0.0892447978258133,
      "learning_rate": 3.484583333333333e-05,
      "loss": 0.0022,
      "step": 36370
    },
    {
      "epoch": 2.4253333333333336,
      "grad_norm": 0.15609949827194214,
      "learning_rate": 3.484166666666667e-05,
      "loss": 0.0016,
      "step": 36380
    },
    {
      "epoch": 2.426,
      "grad_norm": 0.0893445536494255,
      "learning_rate": 3.48375e-05,
      "loss": 0.0027,
      "step": 36390
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 0.6126381754875183,
      "learning_rate": 3.483333333333334e-05,
      "loss": 0.0029,
      "step": 36400
    },
    {
      "epoch": 2.4273333333333333,
      "grad_norm": 0.5602201223373413,
      "learning_rate": 3.482916666666667e-05,
      "loss": 0.0023,
      "step": 36410
    },
    {
      "epoch": 2.428,
      "grad_norm": 0.29895180463790894,
      "learning_rate": 3.4825e-05,
      "loss": 0.0023,
      "step": 36420
    },
    {
      "epoch": 2.4286666666666665,
      "grad_norm": 0.05043996870517731,
      "learning_rate": 3.482083333333334e-05,
      "loss": 0.0016,
      "step": 36430
    },
    {
      "epoch": 2.429333333333333,
      "grad_norm": 0.4537930190563202,
      "learning_rate": 3.481666666666667e-05,
      "loss": 0.0018,
      "step": 36440
    },
    {
      "epoch": 2.43,
      "grad_norm": 0.3941343128681183,
      "learning_rate": 3.4812500000000006e-05,
      "loss": 0.0022,
      "step": 36450
    },
    {
      "epoch": 2.4306666666666668,
      "grad_norm": 0.841717541217804,
      "learning_rate": 3.480833333333334e-05,
      "loss": 0.0024,
      "step": 36460
    },
    {
      "epoch": 2.4313333333333333,
      "grad_norm": 0.05510895699262619,
      "learning_rate": 3.480416666666667e-05,
      "loss": 0.0016,
      "step": 36470
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.11965695023536682,
      "learning_rate": 3.48e-05,
      "loss": 0.0026,
      "step": 36480
    },
    {
      "epoch": 2.4326666666666665,
      "grad_norm": 0.5099150538444519,
      "learning_rate": 3.479583333333333e-05,
      "loss": 0.002,
      "step": 36490
    },
    {
      "epoch": 2.4333333333333336,
      "grad_norm": 0.1125885397195816,
      "learning_rate": 3.479166666666667e-05,
      "loss": 0.002,
      "step": 36500
    },
    {
      "epoch": 2.434,
      "grad_norm": 0.49025478959083557,
      "learning_rate": 3.47875e-05,
      "loss": 0.0019,
      "step": 36510
    },
    {
      "epoch": 2.4346666666666668,
      "grad_norm": 0.12321505695581436,
      "learning_rate": 3.4783333333333336e-05,
      "loss": 0.0025,
      "step": 36520
    },
    {
      "epoch": 2.4353333333333333,
      "grad_norm": 0.4584117531776428,
      "learning_rate": 3.477916666666667e-05,
      "loss": 0.0028,
      "step": 36530
    },
    {
      "epoch": 2.436,
      "grad_norm": 0.5318207144737244,
      "learning_rate": 3.4775000000000005e-05,
      "loss": 0.0024,
      "step": 36540
    },
    {
      "epoch": 2.4366666666666665,
      "grad_norm": 0.4230593144893646,
      "learning_rate": 3.4770833333333336e-05,
      "loss": 0.0025,
      "step": 36550
    },
    {
      "epoch": 2.437333333333333,
      "grad_norm": 0.3190728724002838,
      "learning_rate": 3.476666666666667e-05,
      "loss": 0.0024,
      "step": 36560
    },
    {
      "epoch": 2.438,
      "grad_norm": 0.6163355708122253,
      "learning_rate": 3.4762500000000005e-05,
      "loss": 0.0019,
      "step": 36570
    },
    {
      "epoch": 2.4386666666666668,
      "grad_norm": 0.6472652554512024,
      "learning_rate": 3.4758333333333336e-05,
      "loss": 0.0028,
      "step": 36580
    },
    {
      "epoch": 2.4393333333333334,
      "grad_norm": 0.2809496819972992,
      "learning_rate": 3.4754166666666673e-05,
      "loss": 0.0018,
      "step": 36590
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.2967897355556488,
      "learning_rate": 3.475e-05,
      "loss": 0.0037,
      "step": 36600
    },
    {
      "epoch": 2.4406666666666665,
      "grad_norm": 0.7425248622894287,
      "learning_rate": 3.4745833333333335e-05,
      "loss": 0.0022,
      "step": 36610
    },
    {
      "epoch": 2.4413333333333336,
      "grad_norm": 0.8519296646118164,
      "learning_rate": 3.4741666666666666e-05,
      "loss": 0.0016,
      "step": 36620
    },
    {
      "epoch": 2.442,
      "grad_norm": 0.09116007387638092,
      "learning_rate": 3.47375e-05,
      "loss": 0.0021,
      "step": 36630
    },
    {
      "epoch": 2.4426666666666668,
      "grad_norm": 0.1607842743396759,
      "learning_rate": 3.4733333333333335e-05,
      "loss": 0.0016,
      "step": 36640
    },
    {
      "epoch": 2.4433333333333334,
      "grad_norm": 0.48874837160110474,
      "learning_rate": 3.4729166666666666e-05,
      "loss": 0.0015,
      "step": 36650
    },
    {
      "epoch": 2.444,
      "grad_norm": 0.20896854996681213,
      "learning_rate": 3.4725000000000004e-05,
      "loss": 0.0013,
      "step": 36660
    },
    {
      "epoch": 2.4446666666666665,
      "grad_norm": 0.26359298825263977,
      "learning_rate": 3.4720833333333335e-05,
      "loss": 0.0015,
      "step": 36670
    },
    {
      "epoch": 2.445333333333333,
      "grad_norm": 0.7701197862625122,
      "learning_rate": 3.471666666666667e-05,
      "loss": 0.0022,
      "step": 36680
    },
    {
      "epoch": 2.446,
      "grad_norm": 0.2001761496067047,
      "learning_rate": 3.4712500000000003e-05,
      "loss": 0.0022,
      "step": 36690
    },
    {
      "epoch": 2.4466666666666668,
      "grad_norm": 0.6966806650161743,
      "learning_rate": 3.4708333333333334e-05,
      "loss": 0.002,
      "step": 36700
    },
    {
      "epoch": 2.4473333333333334,
      "grad_norm": 0.4610849916934967,
      "learning_rate": 3.470416666666667e-05,
      "loss": 0.0022,
      "step": 36710
    },
    {
      "epoch": 2.448,
      "grad_norm": 0.7994750142097473,
      "learning_rate": 3.4699999999999996e-05,
      "loss": 0.0025,
      "step": 36720
    },
    {
      "epoch": 2.4486666666666665,
      "grad_norm": 0.2704702913761139,
      "learning_rate": 3.4695833333333334e-05,
      "loss": 0.0018,
      "step": 36730
    },
    {
      "epoch": 2.449333333333333,
      "grad_norm": 0.44019615650177,
      "learning_rate": 3.4691666666666665e-05,
      "loss": 0.0019,
      "step": 36740
    },
    {
      "epoch": 2.45,
      "grad_norm": 1.0584290027618408,
      "learning_rate": 3.46875e-05,
      "loss": 0.0019,
      "step": 36750
    },
    {
      "epoch": 2.4506666666666668,
      "grad_norm": 0.2446506917476654,
      "learning_rate": 3.4683333333333334e-05,
      "loss": 0.0021,
      "step": 36760
    },
    {
      "epoch": 2.4513333333333334,
      "grad_norm": 0.3149245083332062,
      "learning_rate": 3.4679166666666665e-05,
      "loss": 0.003,
      "step": 36770
    },
    {
      "epoch": 2.452,
      "grad_norm": 0.6051302552223206,
      "learning_rate": 3.4675e-05,
      "loss": 0.0025,
      "step": 36780
    },
    {
      "epoch": 2.4526666666666666,
      "grad_norm": 0.5704408288002014,
      "learning_rate": 3.4670833333333334e-05,
      "loss": 0.0021,
      "step": 36790
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 0.2979021966457367,
      "learning_rate": 3.466666666666667e-05,
      "loss": 0.0025,
      "step": 36800
    },
    {
      "epoch": 2.454,
      "grad_norm": 0.6920629143714905,
      "learning_rate": 3.46625e-05,
      "loss": 0.0023,
      "step": 36810
    },
    {
      "epoch": 2.4546666666666668,
      "grad_norm": 0.3175460994243622,
      "learning_rate": 3.465833333333334e-05,
      "loss": 0.0016,
      "step": 36820
    },
    {
      "epoch": 2.4553333333333334,
      "grad_norm": 0.5642074942588806,
      "learning_rate": 3.465416666666667e-05,
      "loss": 0.0028,
      "step": 36830
    },
    {
      "epoch": 2.456,
      "grad_norm": 0.420550674200058,
      "learning_rate": 3.465e-05,
      "loss": 0.0018,
      "step": 36840
    },
    {
      "epoch": 2.4566666666666666,
      "grad_norm": 0.3332120180130005,
      "learning_rate": 3.464583333333333e-05,
      "loss": 0.0017,
      "step": 36850
    },
    {
      "epoch": 2.457333333333333,
      "grad_norm": 0.9059508442878723,
      "learning_rate": 3.4641666666666664e-05,
      "loss": 0.0026,
      "step": 36860
    },
    {
      "epoch": 2.458,
      "grad_norm": 0.7233245372772217,
      "learning_rate": 3.46375e-05,
      "loss": 0.0037,
      "step": 36870
    },
    {
      "epoch": 2.458666666666667,
      "grad_norm": 0.847983181476593,
      "learning_rate": 3.463333333333333e-05,
      "loss": 0.0021,
      "step": 36880
    },
    {
      "epoch": 2.4593333333333334,
      "grad_norm": 0.08664675801992416,
      "learning_rate": 3.462916666666667e-05,
      "loss": 0.002,
      "step": 36890
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.30865707993507385,
      "learning_rate": 3.4625e-05,
      "loss": 0.0018,
      "step": 36900
    },
    {
      "epoch": 2.4606666666666666,
      "grad_norm": 0.12870189547538757,
      "learning_rate": 3.462083333333333e-05,
      "loss": 0.0023,
      "step": 36910
    },
    {
      "epoch": 2.461333333333333,
      "grad_norm": 0.34920233488082886,
      "learning_rate": 3.461666666666667e-05,
      "loss": 0.002,
      "step": 36920
    },
    {
      "epoch": 2.462,
      "grad_norm": 0.7614660263061523,
      "learning_rate": 3.46125e-05,
      "loss": 0.0025,
      "step": 36930
    },
    {
      "epoch": 2.462666666666667,
      "grad_norm": 0.3453119099140167,
      "learning_rate": 3.460833333333334e-05,
      "loss": 0.0029,
      "step": 36940
    },
    {
      "epoch": 2.4633333333333334,
      "grad_norm": 0.2793477773666382,
      "learning_rate": 3.460416666666667e-05,
      "loss": 0.0025,
      "step": 36950
    },
    {
      "epoch": 2.464,
      "grad_norm": 0.15130507946014404,
      "learning_rate": 3.46e-05,
      "loss": 0.002,
      "step": 36960
    },
    {
      "epoch": 2.4646666666666666,
      "grad_norm": 0.10130132734775543,
      "learning_rate": 3.459583333333333e-05,
      "loss": 0.002,
      "step": 36970
    },
    {
      "epoch": 2.465333333333333,
      "grad_norm": 0.5391833782196045,
      "learning_rate": 3.459166666666667e-05,
      "loss": 0.0026,
      "step": 36980
    },
    {
      "epoch": 2.466,
      "grad_norm": 0.5806901454925537,
      "learning_rate": 3.45875e-05,
      "loss": 0.002,
      "step": 36990
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 0.21970918774604797,
      "learning_rate": 3.458333333333333e-05,
      "loss": 0.0022,
      "step": 37000
    },
    {
      "epoch": 2.4673333333333334,
      "grad_norm": 0.32512739300727844,
      "learning_rate": 3.457916666666667e-05,
      "loss": 0.0021,
      "step": 37010
    },
    {
      "epoch": 2.468,
      "grad_norm": 0.22120870649814606,
      "learning_rate": 3.4575e-05,
      "loss": 0.0021,
      "step": 37020
    },
    {
      "epoch": 2.4686666666666666,
      "grad_norm": 0.30253103375434875,
      "learning_rate": 3.457083333333334e-05,
      "loss": 0.0031,
      "step": 37030
    },
    {
      "epoch": 2.469333333333333,
      "grad_norm": 1.125080943107605,
      "learning_rate": 3.456666666666667e-05,
      "loss": 0.0025,
      "step": 37040
    },
    {
      "epoch": 2.4699999999999998,
      "grad_norm": 0.11152856051921844,
      "learning_rate": 3.45625e-05,
      "loss": 0.0026,
      "step": 37050
    },
    {
      "epoch": 2.470666666666667,
      "grad_norm": 0.16414682567119598,
      "learning_rate": 3.455833333333334e-05,
      "loss": 0.0018,
      "step": 37060
    },
    {
      "epoch": 2.4713333333333334,
      "grad_norm": 0.7658061981201172,
      "learning_rate": 3.455416666666667e-05,
      "loss": 0.0023,
      "step": 37070
    },
    {
      "epoch": 2.472,
      "grad_norm": 0.048445191234350204,
      "learning_rate": 3.455e-05,
      "loss": 0.0019,
      "step": 37080
    },
    {
      "epoch": 2.4726666666666666,
      "grad_norm": 0.09586130082607269,
      "learning_rate": 3.454583333333333e-05,
      "loss": 0.0015,
      "step": 37090
    },
    {
      "epoch": 2.473333333333333,
      "grad_norm": 0.5777697563171387,
      "learning_rate": 3.454166666666667e-05,
      "loss": 0.0029,
      "step": 37100
    },
    {
      "epoch": 2.474,
      "grad_norm": 0.48126381635665894,
      "learning_rate": 3.45375e-05,
      "loss": 0.0027,
      "step": 37110
    },
    {
      "epoch": 2.474666666666667,
      "grad_norm": 0.2193564623594284,
      "learning_rate": 3.453333333333334e-05,
      "loss": 0.0018,
      "step": 37120
    },
    {
      "epoch": 2.4753333333333334,
      "grad_norm": 0.19513344764709473,
      "learning_rate": 3.452916666666667e-05,
      "loss": 0.0023,
      "step": 37130
    },
    {
      "epoch": 2.476,
      "grad_norm": 0.10480615496635437,
      "learning_rate": 3.4525e-05,
      "loss": 0.0021,
      "step": 37140
    },
    {
      "epoch": 2.4766666666666666,
      "grad_norm": 0.303055077791214,
      "learning_rate": 3.452083333333334e-05,
      "loss": 0.0024,
      "step": 37150
    },
    {
      "epoch": 2.477333333333333,
      "grad_norm": 0.49579352140426636,
      "learning_rate": 3.451666666666667e-05,
      "loss": 0.0021,
      "step": 37160
    },
    {
      "epoch": 2.4779999999999998,
      "grad_norm": 0.08637702465057373,
      "learning_rate": 3.4512500000000005e-05,
      "loss": 0.002,
      "step": 37170
    },
    {
      "epoch": 2.478666666666667,
      "grad_norm": 0.23382222652435303,
      "learning_rate": 3.4508333333333336e-05,
      "loss": 0.0022,
      "step": 37180
    },
    {
      "epoch": 2.4793333333333334,
      "grad_norm": 0.4334127604961395,
      "learning_rate": 3.4504166666666674e-05,
      "loss": 0.002,
      "step": 37190
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.6318567395210266,
      "learning_rate": 3.45e-05,
      "loss": 0.0035,
      "step": 37200
    },
    {
      "epoch": 2.4806666666666666,
      "grad_norm": 0.5169150233268738,
      "learning_rate": 3.4495833333333336e-05,
      "loss": 0.003,
      "step": 37210
    },
    {
      "epoch": 2.481333333333333,
      "grad_norm": 0.14558924734592438,
      "learning_rate": 3.449166666666667e-05,
      "loss": 0.0028,
      "step": 37220
    },
    {
      "epoch": 2.482,
      "grad_norm": 0.3747519850730896,
      "learning_rate": 3.44875e-05,
      "loss": 0.0019,
      "step": 37230
    },
    {
      "epoch": 2.482666666666667,
      "grad_norm": 0.6466686725616455,
      "learning_rate": 3.4483333333333336e-05,
      "loss": 0.0022,
      "step": 37240
    },
    {
      "epoch": 2.4833333333333334,
      "grad_norm": 0.19433455169200897,
      "learning_rate": 3.447916666666667e-05,
      "loss": 0.0027,
      "step": 37250
    },
    {
      "epoch": 2.484,
      "grad_norm": 0.3672159016132355,
      "learning_rate": 3.4475000000000005e-05,
      "loss": 0.0021,
      "step": 37260
    },
    {
      "epoch": 2.4846666666666666,
      "grad_norm": 0.4156661629676819,
      "learning_rate": 3.4470833333333335e-05,
      "loss": 0.0023,
      "step": 37270
    },
    {
      "epoch": 2.485333333333333,
      "grad_norm": 0.37333422899246216,
      "learning_rate": 3.4466666666666666e-05,
      "loss": 0.0023,
      "step": 37280
    },
    {
      "epoch": 2.4859999999999998,
      "grad_norm": 0.0814109519124031,
      "learning_rate": 3.4462500000000004e-05,
      "loss": 0.0023,
      "step": 37290
    },
    {
      "epoch": 2.486666666666667,
      "grad_norm": 0.6484255194664001,
      "learning_rate": 3.4458333333333335e-05,
      "loss": 0.0019,
      "step": 37300
    },
    {
      "epoch": 2.4873333333333334,
      "grad_norm": 0.10952090471982956,
      "learning_rate": 3.445416666666667e-05,
      "loss": 0.0015,
      "step": 37310
    },
    {
      "epoch": 2.488,
      "grad_norm": 0.2925907075405121,
      "learning_rate": 3.445e-05,
      "loss": 0.0016,
      "step": 37320
    },
    {
      "epoch": 2.4886666666666666,
      "grad_norm": 0.11399363726377487,
      "learning_rate": 3.4445833333333335e-05,
      "loss": 0.0024,
      "step": 37330
    },
    {
      "epoch": 2.489333333333333,
      "grad_norm": 0.5639620423316956,
      "learning_rate": 3.4441666666666666e-05,
      "loss": 0.0013,
      "step": 37340
    },
    {
      "epoch": 2.49,
      "grad_norm": 0.4944480061531067,
      "learning_rate": 3.4437500000000004e-05,
      "loss": 0.0023,
      "step": 37350
    },
    {
      "epoch": 2.490666666666667,
      "grad_norm": 0.3397994935512543,
      "learning_rate": 3.4433333333333335e-05,
      "loss": 0.0031,
      "step": 37360
    },
    {
      "epoch": 2.4913333333333334,
      "grad_norm": 0.7673540115356445,
      "learning_rate": 3.4429166666666666e-05,
      "loss": 0.003,
      "step": 37370
    },
    {
      "epoch": 2.492,
      "grad_norm": 0.058076173067092896,
      "learning_rate": 3.4425e-05,
      "loss": 0.0027,
      "step": 37380
    },
    {
      "epoch": 2.4926666666666666,
      "grad_norm": 0.9945045113563538,
      "learning_rate": 3.4420833333333334e-05,
      "loss": 0.0024,
      "step": 37390
    },
    {
      "epoch": 2.493333333333333,
      "grad_norm": 0.6489936113357544,
      "learning_rate": 3.441666666666667e-05,
      "loss": 0.002,
      "step": 37400
    },
    {
      "epoch": 2.4939999999999998,
      "grad_norm": 0.35014331340789795,
      "learning_rate": 3.44125e-05,
      "loss": 0.0033,
      "step": 37410
    },
    {
      "epoch": 2.494666666666667,
      "grad_norm": 0.18849995732307434,
      "learning_rate": 3.4408333333333334e-05,
      "loss": 0.0018,
      "step": 37420
    },
    {
      "epoch": 2.4953333333333334,
      "grad_norm": 0.5295605659484863,
      "learning_rate": 3.440416666666667e-05,
      "loss": 0.0021,
      "step": 37430
    },
    {
      "epoch": 2.496,
      "grad_norm": 0.3891046345233917,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 0.002,
      "step": 37440
    },
    {
      "epoch": 2.4966666666666666,
      "grad_norm": 0.35771629214286804,
      "learning_rate": 3.4395833333333334e-05,
      "loss": 0.0018,
      "step": 37450
    },
    {
      "epoch": 2.497333333333333,
      "grad_norm": 0.38129082322120667,
      "learning_rate": 3.4391666666666665e-05,
      "loss": 0.0025,
      "step": 37460
    },
    {
      "epoch": 2.498,
      "grad_norm": 0.8722718954086304,
      "learning_rate": 3.43875e-05,
      "loss": 0.0031,
      "step": 37470
    },
    {
      "epoch": 2.498666666666667,
      "grad_norm": 0.2381248027086258,
      "learning_rate": 3.438333333333333e-05,
      "loss": 0.0022,
      "step": 37480
    },
    {
      "epoch": 2.4993333333333334,
      "grad_norm": 0.46791449189186096,
      "learning_rate": 3.437916666666667e-05,
      "loss": 0.0015,
      "step": 37490
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.6392080187797546,
      "learning_rate": 3.4375e-05,
      "loss": 0.0021,
      "step": 37500
    },
    {
      "epoch": 2.5006666666666666,
      "grad_norm": 0.34210044145584106,
      "learning_rate": 3.437083333333333e-05,
      "loss": 0.0031,
      "step": 37510
    },
    {
      "epoch": 2.501333333333333,
      "grad_norm": 0.6680587530136108,
      "learning_rate": 3.436666666666667e-05,
      "loss": 0.0023,
      "step": 37520
    },
    {
      "epoch": 2.502,
      "grad_norm": 0.636996328830719,
      "learning_rate": 3.43625e-05,
      "loss": 0.0024,
      "step": 37530
    },
    {
      "epoch": 2.502666666666667,
      "grad_norm": 0.0734519213438034,
      "learning_rate": 3.435833333333334e-05,
      "loss": 0.0019,
      "step": 37540
    },
    {
      "epoch": 2.5033333333333334,
      "grad_norm": 0.21017788350582123,
      "learning_rate": 3.435416666666667e-05,
      "loss": 0.0028,
      "step": 37550
    },
    {
      "epoch": 2.504,
      "grad_norm": 0.1815803050994873,
      "learning_rate": 3.435e-05,
      "loss": 0.0016,
      "step": 37560
    },
    {
      "epoch": 2.5046666666666666,
      "grad_norm": 0.3186192512512207,
      "learning_rate": 3.434583333333333e-05,
      "loss": 0.0038,
      "step": 37570
    },
    {
      "epoch": 2.505333333333333,
      "grad_norm": 0.1568811535835266,
      "learning_rate": 3.4341666666666663e-05,
      "loss": 0.0022,
      "step": 37580
    },
    {
      "epoch": 2.5060000000000002,
      "grad_norm": 0.4156593084335327,
      "learning_rate": 3.43375e-05,
      "loss": 0.0016,
      "step": 37590
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 0.05029352381825447,
      "learning_rate": 3.433333333333333e-05,
      "loss": 0.0018,
      "step": 37600
    },
    {
      "epoch": 2.5073333333333334,
      "grad_norm": 0.46360114216804504,
      "learning_rate": 3.432916666666667e-05,
      "loss": 0.0024,
      "step": 37610
    },
    {
      "epoch": 2.508,
      "grad_norm": 0.22846560180187225,
      "learning_rate": 3.4325e-05,
      "loss": 0.0024,
      "step": 37620
    },
    {
      "epoch": 2.5086666666666666,
      "grad_norm": 0.6785711050033569,
      "learning_rate": 3.432083333333334e-05,
      "loss": 0.0018,
      "step": 37630
    },
    {
      "epoch": 2.509333333333333,
      "grad_norm": 0.3547513484954834,
      "learning_rate": 3.431666666666667e-05,
      "loss": 0.0021,
      "step": 37640
    },
    {
      "epoch": 2.51,
      "grad_norm": 0.463705450296402,
      "learning_rate": 3.43125e-05,
      "loss": 0.003,
      "step": 37650
    },
    {
      "epoch": 2.510666666666667,
      "grad_norm": 0.3512214124202728,
      "learning_rate": 3.430833333333334e-05,
      "loss": 0.0019,
      "step": 37660
    },
    {
      "epoch": 2.5113333333333334,
      "grad_norm": 0.12887461483478546,
      "learning_rate": 3.430416666666667e-05,
      "loss": 0.0022,
      "step": 37670
    },
    {
      "epoch": 2.512,
      "grad_norm": 0.7974669337272644,
      "learning_rate": 3.430000000000001e-05,
      "loss": 0.0022,
      "step": 37680
    },
    {
      "epoch": 2.5126666666666666,
      "grad_norm": 0.787473738193512,
      "learning_rate": 3.429583333333333e-05,
      "loss": 0.0031,
      "step": 37690
    },
    {
      "epoch": 2.513333333333333,
      "grad_norm": 0.3451524078845978,
      "learning_rate": 3.429166666666667e-05,
      "loss": 0.0027,
      "step": 37700
    },
    {
      "epoch": 2.5140000000000002,
      "grad_norm": 0.3655049204826355,
      "learning_rate": 3.42875e-05,
      "loss": 0.0024,
      "step": 37710
    },
    {
      "epoch": 2.514666666666667,
      "grad_norm": 0.6024777889251709,
      "learning_rate": 3.428333333333333e-05,
      "loss": 0.0016,
      "step": 37720
    },
    {
      "epoch": 2.5153333333333334,
      "grad_norm": 0.14474664628505707,
      "learning_rate": 3.427916666666667e-05,
      "loss": 0.0015,
      "step": 37730
    },
    {
      "epoch": 2.516,
      "grad_norm": 0.3345494866371155,
      "learning_rate": 3.4275e-05,
      "loss": 0.0015,
      "step": 37740
    },
    {
      "epoch": 2.5166666666666666,
      "grad_norm": 0.11346899718046188,
      "learning_rate": 3.427083333333334e-05,
      "loss": 0.0016,
      "step": 37750
    },
    {
      "epoch": 2.517333333333333,
      "grad_norm": 0.3544006943702698,
      "learning_rate": 3.426666666666667e-05,
      "loss": 0.0019,
      "step": 37760
    },
    {
      "epoch": 2.518,
      "grad_norm": 0.24058790504932404,
      "learning_rate": 3.4262500000000006e-05,
      "loss": 0.0018,
      "step": 37770
    },
    {
      "epoch": 2.518666666666667,
      "grad_norm": 0.15503756701946259,
      "learning_rate": 3.425833333333334e-05,
      "loss": 0.0031,
      "step": 37780
    },
    {
      "epoch": 2.5193333333333334,
      "grad_norm": 0.7706987261772156,
      "learning_rate": 3.425416666666667e-05,
      "loss": 0.0031,
      "step": 37790
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.30185285210609436,
      "learning_rate": 3.4250000000000006e-05,
      "loss": 0.0026,
      "step": 37800
    },
    {
      "epoch": 2.5206666666666666,
      "grad_norm": 0.14383654296398163,
      "learning_rate": 3.424583333333333e-05,
      "loss": 0.0024,
      "step": 37810
    },
    {
      "epoch": 2.521333333333333,
      "grad_norm": 0.3754529654979706,
      "learning_rate": 3.424166666666667e-05,
      "loss": 0.0023,
      "step": 37820
    },
    {
      "epoch": 2.5220000000000002,
      "grad_norm": 0.04229164123535156,
      "learning_rate": 3.42375e-05,
      "loss": 0.0021,
      "step": 37830
    },
    {
      "epoch": 2.522666666666667,
      "grad_norm": 0.0771610289812088,
      "learning_rate": 3.4233333333333336e-05,
      "loss": 0.0029,
      "step": 37840
    },
    {
      "epoch": 2.5233333333333334,
      "grad_norm": 0.20151403546333313,
      "learning_rate": 3.422916666666667e-05,
      "loss": 0.0018,
      "step": 37850
    },
    {
      "epoch": 2.524,
      "grad_norm": 0.17107968032360077,
      "learning_rate": 3.4225e-05,
      "loss": 0.0021,
      "step": 37860
    },
    {
      "epoch": 2.5246666666666666,
      "grad_norm": 0.15680716931819916,
      "learning_rate": 3.4220833333333336e-05,
      "loss": 0.0024,
      "step": 37870
    },
    {
      "epoch": 2.525333333333333,
      "grad_norm": 0.15043045580387115,
      "learning_rate": 3.421666666666667e-05,
      "loss": 0.0018,
      "step": 37880
    },
    {
      "epoch": 2.526,
      "grad_norm": 0.4308185279369354,
      "learning_rate": 3.4212500000000005e-05,
      "loss": 0.0018,
      "step": 37890
    },
    {
      "epoch": 2.5266666666666664,
      "grad_norm": 0.22428888082504272,
      "learning_rate": 3.4208333333333336e-05,
      "loss": 0.002,
      "step": 37900
    },
    {
      "epoch": 2.5273333333333334,
      "grad_norm": 0.3088287115097046,
      "learning_rate": 3.4204166666666674e-05,
      "loss": 0.0032,
      "step": 37910
    },
    {
      "epoch": 2.528,
      "grad_norm": 1.0828301906585693,
      "learning_rate": 3.4200000000000005e-05,
      "loss": 0.0028,
      "step": 37920
    },
    {
      "epoch": 2.5286666666666666,
      "grad_norm": 0.05614352226257324,
      "learning_rate": 3.4195833333333336e-05,
      "loss": 0.0027,
      "step": 37930
    },
    {
      "epoch": 2.529333333333333,
      "grad_norm": 0.5543704032897949,
      "learning_rate": 3.4191666666666667e-05,
      "loss": 0.0028,
      "step": 37940
    },
    {
      "epoch": 2.5300000000000002,
      "grad_norm": 0.3502567708492279,
      "learning_rate": 3.41875e-05,
      "loss": 0.0027,
      "step": 37950
    },
    {
      "epoch": 2.530666666666667,
      "grad_norm": 0.1252070814371109,
      "learning_rate": 3.4183333333333335e-05,
      "loss": 0.0024,
      "step": 37960
    },
    {
      "epoch": 2.5313333333333334,
      "grad_norm": 0.3811993896961212,
      "learning_rate": 3.4179166666666666e-05,
      "loss": 0.002,
      "step": 37970
    },
    {
      "epoch": 2.532,
      "grad_norm": 1.1161234378814697,
      "learning_rate": 3.4175000000000004e-05,
      "loss": 0.0022,
      "step": 37980
    },
    {
      "epoch": 2.5326666666666666,
      "grad_norm": 0.19766785204410553,
      "learning_rate": 3.4170833333333335e-05,
      "loss": 0.0016,
      "step": 37990
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.6239672303199768,
      "learning_rate": 3.4166666666666666e-05,
      "loss": 0.003,
      "step": 38000
    },
    {
      "epoch": 2.534,
      "grad_norm": 1.1055669784545898,
      "learning_rate": 3.4162500000000004e-05,
      "loss": 0.0015,
      "step": 38010
    },
    {
      "epoch": 2.5346666666666664,
      "grad_norm": 0.1454298049211502,
      "learning_rate": 3.4158333333333335e-05,
      "loss": 0.0018,
      "step": 38020
    },
    {
      "epoch": 2.5353333333333334,
      "grad_norm": 0.3037377595901489,
      "learning_rate": 3.415416666666667e-05,
      "loss": 0.0026,
      "step": 38030
    },
    {
      "epoch": 2.536,
      "grad_norm": 0.15243366360664368,
      "learning_rate": 3.415e-05,
      "loss": 0.0014,
      "step": 38040
    },
    {
      "epoch": 2.5366666666666666,
      "grad_norm": 0.18613678216934204,
      "learning_rate": 3.4145833333333334e-05,
      "loss": 0.0021,
      "step": 38050
    },
    {
      "epoch": 2.537333333333333,
      "grad_norm": 0.3526535928249359,
      "learning_rate": 3.4141666666666665e-05,
      "loss": 0.0016,
      "step": 38060
    },
    {
      "epoch": 2.5380000000000003,
      "grad_norm": 0.8360634446144104,
      "learning_rate": 3.41375e-05,
      "loss": 0.0025,
      "step": 38070
    },
    {
      "epoch": 2.538666666666667,
      "grad_norm": 0.28644540905952454,
      "learning_rate": 3.4133333333333334e-05,
      "loss": 0.0029,
      "step": 38080
    },
    {
      "epoch": 2.5393333333333334,
      "grad_norm": 0.801048219203949,
      "learning_rate": 3.4129166666666665e-05,
      "loss": 0.0013,
      "step": 38090
    },
    {
      "epoch": 2.54,
      "grad_norm": 0.09076396375894547,
      "learning_rate": 3.4125e-05,
      "loss": 0.002,
      "step": 38100
    },
    {
      "epoch": 2.5406666666666666,
      "grad_norm": 0.22830750048160553,
      "learning_rate": 3.4120833333333334e-05,
      "loss": 0.0021,
      "step": 38110
    },
    {
      "epoch": 2.541333333333333,
      "grad_norm": 0.26696765422821045,
      "learning_rate": 3.411666666666667e-05,
      "loss": 0.0018,
      "step": 38120
    },
    {
      "epoch": 2.542,
      "grad_norm": 0.39878159761428833,
      "learning_rate": 3.41125e-05,
      "loss": 0.0031,
      "step": 38130
    },
    {
      "epoch": 2.5426666666666664,
      "grad_norm": 0.5668426752090454,
      "learning_rate": 3.4108333333333333e-05,
      "loss": 0.0027,
      "step": 38140
    },
    {
      "epoch": 2.5433333333333334,
      "grad_norm": 0.461445152759552,
      "learning_rate": 3.410416666666667e-05,
      "loss": 0.0013,
      "step": 38150
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.12764598429203033,
      "learning_rate": 3.41e-05,
      "loss": 0.0013,
      "step": 38160
    },
    {
      "epoch": 2.5446666666666666,
      "grad_norm": 0.12479343265295029,
      "learning_rate": 3.409583333333333e-05,
      "loss": 0.002,
      "step": 38170
    },
    {
      "epoch": 2.5453333333333332,
      "grad_norm": 0.9891151189804077,
      "learning_rate": 3.4091666666666664e-05,
      "loss": 0.0024,
      "step": 38180
    },
    {
      "epoch": 2.5460000000000003,
      "grad_norm": 0.6495085954666138,
      "learning_rate": 3.40875e-05,
      "loss": 0.0022,
      "step": 38190
    },
    {
      "epoch": 2.546666666666667,
      "grad_norm": 0.6514760851860046,
      "learning_rate": 3.408333333333333e-05,
      "loss": 0.0031,
      "step": 38200
    },
    {
      "epoch": 2.5473333333333334,
      "grad_norm": 0.26831603050231934,
      "learning_rate": 3.407916666666667e-05,
      "loss": 0.0017,
      "step": 38210
    },
    {
      "epoch": 2.548,
      "grad_norm": 0.12914195656776428,
      "learning_rate": 3.4075e-05,
      "loss": 0.003,
      "step": 38220
    },
    {
      "epoch": 2.5486666666666666,
      "grad_norm": 0.2959878444671631,
      "learning_rate": 3.407083333333333e-05,
      "loss": 0.0017,
      "step": 38230
    },
    {
      "epoch": 2.5493333333333332,
      "grad_norm": 0.7488080859184265,
      "learning_rate": 3.406666666666667e-05,
      "loss": 0.0024,
      "step": 38240
    },
    {
      "epoch": 2.55,
      "grad_norm": 0.6921879649162292,
      "learning_rate": 3.40625e-05,
      "loss": 0.0019,
      "step": 38250
    },
    {
      "epoch": 2.5506666666666664,
      "grad_norm": 0.2876913547515869,
      "learning_rate": 3.405833333333334e-05,
      "loss": 0.0022,
      "step": 38260
    },
    {
      "epoch": 2.5513333333333335,
      "grad_norm": 0.05270546302199364,
      "learning_rate": 3.405416666666667e-05,
      "loss": 0.0022,
      "step": 38270
    },
    {
      "epoch": 2.552,
      "grad_norm": 0.3656136393547058,
      "learning_rate": 3.405e-05,
      "loss": 0.002,
      "step": 38280
    },
    {
      "epoch": 2.5526666666666666,
      "grad_norm": 0.6415225863456726,
      "learning_rate": 3.404583333333333e-05,
      "loss": 0.0025,
      "step": 38290
    },
    {
      "epoch": 2.5533333333333332,
      "grad_norm": 0.41913002729415894,
      "learning_rate": 3.404166666666666e-05,
      "loss": 0.0021,
      "step": 38300
    },
    {
      "epoch": 2.5540000000000003,
      "grad_norm": 0.09132151305675507,
      "learning_rate": 3.40375e-05,
      "loss": 0.0018,
      "step": 38310
    },
    {
      "epoch": 2.554666666666667,
      "grad_norm": 0.2524380385875702,
      "learning_rate": 3.403333333333333e-05,
      "loss": 0.0013,
      "step": 38320
    },
    {
      "epoch": 2.5553333333333335,
      "grad_norm": 0.43347489833831787,
      "learning_rate": 3.402916666666667e-05,
      "loss": 0.0026,
      "step": 38330
    },
    {
      "epoch": 2.556,
      "grad_norm": 0.20898863673210144,
      "learning_rate": 3.4025e-05,
      "loss": 0.0024,
      "step": 38340
    },
    {
      "epoch": 2.5566666666666666,
      "grad_norm": 0.04248802736401558,
      "learning_rate": 3.402083333333334e-05,
      "loss": 0.0022,
      "step": 38350
    },
    {
      "epoch": 2.5573333333333332,
      "grad_norm": 0.31661486625671387,
      "learning_rate": 3.401666666666667e-05,
      "loss": 0.0022,
      "step": 38360
    },
    {
      "epoch": 2.558,
      "grad_norm": 0.3452360928058624,
      "learning_rate": 3.40125e-05,
      "loss": 0.0013,
      "step": 38370
    },
    {
      "epoch": 2.5586666666666664,
      "grad_norm": 0.4155409336090088,
      "learning_rate": 3.400833333333334e-05,
      "loss": 0.002,
      "step": 38380
    },
    {
      "epoch": 2.5593333333333335,
      "grad_norm": 0.352105975151062,
      "learning_rate": 3.400416666666667e-05,
      "loss": 0.0021,
      "step": 38390
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.32213491201400757,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.0023,
      "step": 38400
    },
    {
      "epoch": 2.5606666666666666,
      "grad_norm": 0.5487841367721558,
      "learning_rate": 3.399583333333333e-05,
      "loss": 0.0014,
      "step": 38410
    },
    {
      "epoch": 2.5613333333333332,
      "grad_norm": 0.8169993162155151,
      "learning_rate": 3.399166666666667e-05,
      "loss": 0.0019,
      "step": 38420
    },
    {
      "epoch": 2.5620000000000003,
      "grad_norm": 0.4760259687900543,
      "learning_rate": 3.39875e-05,
      "loss": 0.0022,
      "step": 38430
    },
    {
      "epoch": 2.562666666666667,
      "grad_norm": 0.283351331949234,
      "learning_rate": 3.398333333333333e-05,
      "loss": 0.0022,
      "step": 38440
    },
    {
      "epoch": 2.5633333333333335,
      "grad_norm": 0.31990548968315125,
      "learning_rate": 3.397916666666667e-05,
      "loss": 0.0029,
      "step": 38450
    },
    {
      "epoch": 2.564,
      "grad_norm": 0.19477568566799164,
      "learning_rate": 3.3975e-05,
      "loss": 0.0026,
      "step": 38460
    },
    {
      "epoch": 2.5646666666666667,
      "grad_norm": 0.7385607957839966,
      "learning_rate": 3.397083333333334e-05,
      "loss": 0.0025,
      "step": 38470
    },
    {
      "epoch": 2.5653333333333332,
      "grad_norm": 0.24201956391334534,
      "learning_rate": 3.396666666666667e-05,
      "loss": 0.0021,
      "step": 38480
    },
    {
      "epoch": 2.566,
      "grad_norm": 0.3694794178009033,
      "learning_rate": 3.3962500000000006e-05,
      "loss": 0.0025,
      "step": 38490
    },
    {
      "epoch": 2.5666666666666664,
      "grad_norm": 0.7172806262969971,
      "learning_rate": 3.3958333333333337e-05,
      "loss": 0.0019,
      "step": 38500
    },
    {
      "epoch": 2.5673333333333335,
      "grad_norm": 0.39323684573173523,
      "learning_rate": 3.395416666666667e-05,
      "loss": 0.0014,
      "step": 38510
    },
    {
      "epoch": 2.568,
      "grad_norm": 0.28250303864479065,
      "learning_rate": 3.3950000000000005e-05,
      "loss": 0.002,
      "step": 38520
    },
    {
      "epoch": 2.5686666666666667,
      "grad_norm": 0.9321190714836121,
      "learning_rate": 3.394583333333333e-05,
      "loss": 0.0029,
      "step": 38530
    },
    {
      "epoch": 2.5693333333333332,
      "grad_norm": 0.8015320897102356,
      "learning_rate": 3.394166666666667e-05,
      "loss": 0.002,
      "step": 38540
    },
    {
      "epoch": 2.57,
      "grad_norm": 0.44762906432151794,
      "learning_rate": 3.39375e-05,
      "loss": 0.0035,
      "step": 38550
    },
    {
      "epoch": 2.570666666666667,
      "grad_norm": 0.24562694132328033,
      "learning_rate": 3.3933333333333336e-05,
      "loss": 0.0021,
      "step": 38560
    },
    {
      "epoch": 2.5713333333333335,
      "grad_norm": 0.05028397589921951,
      "learning_rate": 3.392916666666667e-05,
      "loss": 0.0026,
      "step": 38570
    },
    {
      "epoch": 2.572,
      "grad_norm": 0.4110625684261322,
      "learning_rate": 3.3925e-05,
      "loss": 0.0027,
      "step": 38580
    },
    {
      "epoch": 2.5726666666666667,
      "grad_norm": 0.17782166600227356,
      "learning_rate": 3.3920833333333336e-05,
      "loss": 0.002,
      "step": 38590
    },
    {
      "epoch": 2.5733333333333333,
      "grad_norm": 0.5037124752998352,
      "learning_rate": 3.391666666666667e-05,
      "loss": 0.002,
      "step": 38600
    },
    {
      "epoch": 2.574,
      "grad_norm": 0.389405757188797,
      "learning_rate": 3.3912500000000004e-05,
      "loss": 0.0029,
      "step": 38610
    },
    {
      "epoch": 2.5746666666666664,
      "grad_norm": 0.4006877541542053,
      "learning_rate": 3.3908333333333335e-05,
      "loss": 0.0024,
      "step": 38620
    },
    {
      "epoch": 2.5753333333333335,
      "grad_norm": 0.2843211591243744,
      "learning_rate": 3.390416666666667e-05,
      "loss": 0.0021,
      "step": 38630
    },
    {
      "epoch": 2.576,
      "grad_norm": 0.24040193855762482,
      "learning_rate": 3.3900000000000004e-05,
      "loss": 0.0016,
      "step": 38640
    },
    {
      "epoch": 2.5766666666666667,
      "grad_norm": 0.19016261398792267,
      "learning_rate": 3.3895833333333335e-05,
      "loss": 0.0022,
      "step": 38650
    },
    {
      "epoch": 2.5773333333333333,
      "grad_norm": 0.12391965091228485,
      "learning_rate": 3.3891666666666666e-05,
      "loss": 0.0018,
      "step": 38660
    },
    {
      "epoch": 2.578,
      "grad_norm": 0.6403319835662842,
      "learning_rate": 3.38875e-05,
      "loss": 0.002,
      "step": 38670
    },
    {
      "epoch": 2.578666666666667,
      "grad_norm": 0.7700067758560181,
      "learning_rate": 3.3883333333333335e-05,
      "loss": 0.0021,
      "step": 38680
    },
    {
      "epoch": 2.5793333333333335,
      "grad_norm": 0.09939965605735779,
      "learning_rate": 3.3879166666666666e-05,
      "loss": 0.0014,
      "step": 38690
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.104454405605793,
      "learning_rate": 3.3875000000000003e-05,
      "loss": 0.0025,
      "step": 38700
    },
    {
      "epoch": 2.5806666666666667,
      "grad_norm": 0.39840319752693176,
      "learning_rate": 3.3870833333333334e-05,
      "loss": 0.0021,
      "step": 38710
    },
    {
      "epoch": 2.5813333333333333,
      "grad_norm": 0.3471699655056,
      "learning_rate": 3.3866666666666665e-05,
      "loss": 0.0031,
      "step": 38720
    },
    {
      "epoch": 2.582,
      "grad_norm": 0.614806592464447,
      "learning_rate": 3.38625e-05,
      "loss": 0.0025,
      "step": 38730
    },
    {
      "epoch": 2.5826666666666664,
      "grad_norm": 0.6669778823852539,
      "learning_rate": 3.3858333333333334e-05,
      "loss": 0.0017,
      "step": 38740
    },
    {
      "epoch": 2.5833333333333335,
      "grad_norm": 0.07763785868883133,
      "learning_rate": 3.385416666666667e-05,
      "loss": 0.0025,
      "step": 38750
    },
    {
      "epoch": 2.584,
      "grad_norm": 0.1976795196533203,
      "learning_rate": 3.385e-05,
      "loss": 0.0024,
      "step": 38760
    },
    {
      "epoch": 2.5846666666666667,
      "grad_norm": 0.6602902412414551,
      "learning_rate": 3.384583333333334e-05,
      "loss": 0.0023,
      "step": 38770
    },
    {
      "epoch": 2.5853333333333333,
      "grad_norm": 0.7963746190071106,
      "learning_rate": 3.3841666666666665e-05,
      "loss": 0.0019,
      "step": 38780
    },
    {
      "epoch": 2.586,
      "grad_norm": 0.4297560453414917,
      "learning_rate": 3.38375e-05,
      "loss": 0.002,
      "step": 38790
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 0.04686453938484192,
      "learning_rate": 3.3833333333333334e-05,
      "loss": 0.0028,
      "step": 38800
    },
    {
      "epoch": 2.5873333333333335,
      "grad_norm": 0.14923948049545288,
      "learning_rate": 3.3829166666666665e-05,
      "loss": 0.002,
      "step": 38810
    },
    {
      "epoch": 2.588,
      "grad_norm": 0.3752850294113159,
      "learning_rate": 3.3825e-05,
      "loss": 0.0023,
      "step": 38820
    },
    {
      "epoch": 2.5886666666666667,
      "grad_norm": 0.5729219913482666,
      "learning_rate": 3.382083333333333e-05,
      "loss": 0.0024,
      "step": 38830
    },
    {
      "epoch": 2.5893333333333333,
      "grad_norm": 0.8610241413116455,
      "learning_rate": 3.381666666666667e-05,
      "loss": 0.0022,
      "step": 38840
    },
    {
      "epoch": 2.59,
      "grad_norm": 0.496542364358902,
      "learning_rate": 3.38125e-05,
      "loss": 0.0017,
      "step": 38850
    },
    {
      "epoch": 2.5906666666666665,
      "grad_norm": 0.0542631596326828,
      "learning_rate": 3.380833333333333e-05,
      "loss": 0.0023,
      "step": 38860
    },
    {
      "epoch": 2.5913333333333335,
      "grad_norm": 0.1482139378786087,
      "learning_rate": 3.380416666666667e-05,
      "loss": 0.0022,
      "step": 38870
    },
    {
      "epoch": 2.592,
      "grad_norm": 0.954736053943634,
      "learning_rate": 3.38e-05,
      "loss": 0.0029,
      "step": 38880
    },
    {
      "epoch": 2.5926666666666667,
      "grad_norm": 0.111254021525383,
      "learning_rate": 3.379583333333334e-05,
      "loss": 0.004,
      "step": 38890
    },
    {
      "epoch": 2.5933333333333333,
      "grad_norm": 0.9066899418830872,
      "learning_rate": 3.3791666666666664e-05,
      "loss": 0.0021,
      "step": 38900
    },
    {
      "epoch": 2.594,
      "grad_norm": 0.35360464453697205,
      "learning_rate": 3.37875e-05,
      "loss": 0.0034,
      "step": 38910
    },
    {
      "epoch": 2.594666666666667,
      "grad_norm": 0.4804205000400543,
      "learning_rate": 3.378333333333333e-05,
      "loss": 0.0024,
      "step": 38920
    },
    {
      "epoch": 2.5953333333333335,
      "grad_norm": 0.23533490300178528,
      "learning_rate": 3.377916666666667e-05,
      "loss": 0.0025,
      "step": 38930
    },
    {
      "epoch": 2.596,
      "grad_norm": 0.5822045803070068,
      "learning_rate": 3.3775e-05,
      "loss": 0.0025,
      "step": 38940
    },
    {
      "epoch": 2.5966666666666667,
      "grad_norm": 0.4163431227207184,
      "learning_rate": 3.377083333333333e-05,
      "loss": 0.0028,
      "step": 38950
    },
    {
      "epoch": 2.5973333333333333,
      "grad_norm": 0.03147601708769798,
      "learning_rate": 3.376666666666667e-05,
      "loss": 0.0025,
      "step": 38960
    },
    {
      "epoch": 2.598,
      "grad_norm": 0.04447447136044502,
      "learning_rate": 3.37625e-05,
      "loss": 0.0019,
      "step": 38970
    },
    {
      "epoch": 2.5986666666666665,
      "grad_norm": 0.17116130888462067,
      "learning_rate": 3.375833333333334e-05,
      "loss": 0.0027,
      "step": 38980
    },
    {
      "epoch": 2.5993333333333335,
      "grad_norm": 0.15114519000053406,
      "learning_rate": 3.375416666666667e-05,
      "loss": 0.0022,
      "step": 38990
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.5931397676467896,
      "learning_rate": 3.375000000000001e-05,
      "loss": 0.0027,
      "step": 39000
    },
    {
      "epoch": 2.6006666666666667,
      "grad_norm": 0.1490887999534607,
      "learning_rate": 3.374583333333334e-05,
      "loss": 0.0027,
      "step": 39010
    },
    {
      "epoch": 2.6013333333333333,
      "grad_norm": 0.26551553606987,
      "learning_rate": 3.374166666666667e-05,
      "loss": 0.0025,
      "step": 39020
    },
    {
      "epoch": 2.602,
      "grad_norm": 0.4953993260860443,
      "learning_rate": 3.37375e-05,
      "loss": 0.0023,
      "step": 39030
    },
    {
      "epoch": 2.602666666666667,
      "grad_norm": 0.09790831059217453,
      "learning_rate": 3.373333333333333e-05,
      "loss": 0.002,
      "step": 39040
    },
    {
      "epoch": 2.6033333333333335,
      "grad_norm": 0.11921615153551102,
      "learning_rate": 3.372916666666667e-05,
      "loss": 0.002,
      "step": 39050
    },
    {
      "epoch": 2.604,
      "grad_norm": 0.822887659072876,
      "learning_rate": 3.3725e-05,
      "loss": 0.0024,
      "step": 39060
    },
    {
      "epoch": 2.6046666666666667,
      "grad_norm": 0.7989768981933594,
      "learning_rate": 3.372083333333334e-05,
      "loss": 0.0017,
      "step": 39070
    },
    {
      "epoch": 2.6053333333333333,
      "grad_norm": 0.6512326002120972,
      "learning_rate": 3.371666666666667e-05,
      "loss": 0.0031,
      "step": 39080
    },
    {
      "epoch": 2.606,
      "grad_norm": 0.5482643246650696,
      "learning_rate": 3.37125e-05,
      "loss": 0.002,
      "step": 39090
    },
    {
      "epoch": 2.6066666666666665,
      "grad_norm": 0.6482656002044678,
      "learning_rate": 3.370833333333334e-05,
      "loss": 0.002,
      "step": 39100
    },
    {
      "epoch": 2.607333333333333,
      "grad_norm": 0.4420532286167145,
      "learning_rate": 3.370416666666667e-05,
      "loss": 0.0019,
      "step": 39110
    },
    {
      "epoch": 2.608,
      "grad_norm": 0.08589158207178116,
      "learning_rate": 3.3700000000000006e-05,
      "loss": 0.0019,
      "step": 39120
    },
    {
      "epoch": 2.6086666666666667,
      "grad_norm": 0.42436936497688293,
      "learning_rate": 3.369583333333334e-05,
      "loss": 0.0016,
      "step": 39130
    },
    {
      "epoch": 2.6093333333333333,
      "grad_norm": 0.33859601616859436,
      "learning_rate": 3.369166666666667e-05,
      "loss": 0.003,
      "step": 39140
    },
    {
      "epoch": 2.61,
      "grad_norm": 0.4213757812976837,
      "learning_rate": 3.36875e-05,
      "loss": 0.0022,
      "step": 39150
    },
    {
      "epoch": 2.610666666666667,
      "grad_norm": 0.5712443590164185,
      "learning_rate": 3.368333333333334e-05,
      "loss": 0.0021,
      "step": 39160
    },
    {
      "epoch": 2.6113333333333335,
      "grad_norm": 0.6819218397140503,
      "learning_rate": 3.367916666666667e-05,
      "loss": 0.0027,
      "step": 39170
    },
    {
      "epoch": 2.612,
      "grad_norm": 0.7442302107810974,
      "learning_rate": 3.3675e-05,
      "loss": 0.0025,
      "step": 39180
    },
    {
      "epoch": 2.6126666666666667,
      "grad_norm": 0.9373354911804199,
      "learning_rate": 3.3670833333333336e-05,
      "loss": 0.0016,
      "step": 39190
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 0.22072553634643555,
      "learning_rate": 3.366666666666667e-05,
      "loss": 0.0024,
      "step": 39200
    },
    {
      "epoch": 2.614,
      "grad_norm": 0.5670120716094971,
      "learning_rate": 3.3662500000000005e-05,
      "loss": 0.0023,
      "step": 39210
    },
    {
      "epoch": 2.6146666666666665,
      "grad_norm": 0.4036421775817871,
      "learning_rate": 3.3658333333333336e-05,
      "loss": 0.0023,
      "step": 39220
    },
    {
      "epoch": 2.615333333333333,
      "grad_norm": 0.641842246055603,
      "learning_rate": 3.365416666666667e-05,
      "loss": 0.0026,
      "step": 39230
    },
    {
      "epoch": 2.616,
      "grad_norm": 0.723220705986023,
      "learning_rate": 3.3650000000000005e-05,
      "loss": 0.0018,
      "step": 39240
    },
    {
      "epoch": 2.6166666666666667,
      "grad_norm": 0.3107214868068695,
      "learning_rate": 3.3645833333333336e-05,
      "loss": 0.0025,
      "step": 39250
    },
    {
      "epoch": 2.6173333333333333,
      "grad_norm": 0.28505727648735046,
      "learning_rate": 3.364166666666667e-05,
      "loss": 0.0019,
      "step": 39260
    },
    {
      "epoch": 2.618,
      "grad_norm": 0.3741546869277954,
      "learning_rate": 3.36375e-05,
      "loss": 0.003,
      "step": 39270
    },
    {
      "epoch": 2.618666666666667,
      "grad_norm": 0.22390973567962646,
      "learning_rate": 3.3633333333333335e-05,
      "loss": 0.0024,
      "step": 39280
    },
    {
      "epoch": 2.6193333333333335,
      "grad_norm": 0.3931742012500763,
      "learning_rate": 3.3629166666666666e-05,
      "loss": 0.0012,
      "step": 39290
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.06342089176177979,
      "learning_rate": 3.3625000000000004e-05,
      "loss": 0.0023,
      "step": 39300
    },
    {
      "epoch": 2.6206666666666667,
      "grad_norm": 0.14501865208148956,
      "learning_rate": 3.3620833333333335e-05,
      "loss": 0.0018,
      "step": 39310
    },
    {
      "epoch": 2.6213333333333333,
      "grad_norm": 0.3708474636077881,
      "learning_rate": 3.3616666666666666e-05,
      "loss": 0.002,
      "step": 39320
    },
    {
      "epoch": 2.622,
      "grad_norm": 0.3817223906517029,
      "learning_rate": 3.3612500000000004e-05,
      "loss": 0.0022,
      "step": 39330
    },
    {
      "epoch": 2.6226666666666665,
      "grad_norm": 0.45229262113571167,
      "learning_rate": 3.3608333333333335e-05,
      "loss": 0.0018,
      "step": 39340
    },
    {
      "epoch": 2.623333333333333,
      "grad_norm": 0.2042199969291687,
      "learning_rate": 3.360416666666667e-05,
      "loss": 0.0023,
      "step": 39350
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.05511455982923508,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 0.0022,
      "step": 39360
    },
    {
      "epoch": 2.6246666666666667,
      "grad_norm": 0.14448340237140656,
      "learning_rate": 3.3595833333333335e-05,
      "loss": 0.0019,
      "step": 39370
    },
    {
      "epoch": 2.6253333333333333,
      "grad_norm": 0.05793174356222153,
      "learning_rate": 3.3591666666666666e-05,
      "loss": 0.0021,
      "step": 39380
    },
    {
      "epoch": 2.626,
      "grad_norm": 0.1900687962770462,
      "learning_rate": 3.3587499999999996e-05,
      "loss": 0.0024,
      "step": 39390
    },
    {
      "epoch": 2.626666666666667,
      "grad_norm": 0.7647036910057068,
      "learning_rate": 3.3583333333333334e-05,
      "loss": 0.0022,
      "step": 39400
    },
    {
      "epoch": 2.6273333333333335,
      "grad_norm": 0.42992013692855835,
      "learning_rate": 3.3579166666666665e-05,
      "loss": 0.0019,
      "step": 39410
    },
    {
      "epoch": 2.628,
      "grad_norm": 0.23401211202144623,
      "learning_rate": 3.3575e-05,
      "loss": 0.0017,
      "step": 39420
    },
    {
      "epoch": 2.6286666666666667,
      "grad_norm": 0.4959293007850647,
      "learning_rate": 3.3570833333333334e-05,
      "loss": 0.0023,
      "step": 39430
    },
    {
      "epoch": 2.6293333333333333,
      "grad_norm": 0.22154100239276886,
      "learning_rate": 3.356666666666667e-05,
      "loss": 0.0028,
      "step": 39440
    },
    {
      "epoch": 2.63,
      "grad_norm": 0.3931969106197357,
      "learning_rate": 3.35625e-05,
      "loss": 0.0016,
      "step": 39450
    },
    {
      "epoch": 2.6306666666666665,
      "grad_norm": 0.02888457663357258,
      "learning_rate": 3.3558333333333334e-05,
      "loss": 0.0025,
      "step": 39460
    },
    {
      "epoch": 2.631333333333333,
      "grad_norm": 0.5339109897613525,
      "learning_rate": 3.355416666666667e-05,
      "loss": 0.0024,
      "step": 39470
    },
    {
      "epoch": 2.632,
      "grad_norm": 0.45639604330062866,
      "learning_rate": 3.355e-05,
      "loss": 0.0029,
      "step": 39480
    },
    {
      "epoch": 2.6326666666666667,
      "grad_norm": 0.3493328094482422,
      "learning_rate": 3.354583333333334e-05,
      "loss": 0.002,
      "step": 39490
    },
    {
      "epoch": 2.6333333333333333,
      "grad_norm": 0.05238652601838112,
      "learning_rate": 3.3541666666666664e-05,
      "loss": 0.0024,
      "step": 39500
    },
    {
      "epoch": 2.634,
      "grad_norm": 0.2456299215555191,
      "learning_rate": 3.35375e-05,
      "loss": 0.0024,
      "step": 39510
    },
    {
      "epoch": 2.634666666666667,
      "grad_norm": 0.2680678367614746,
      "learning_rate": 3.353333333333333e-05,
      "loss": 0.0019,
      "step": 39520
    },
    {
      "epoch": 2.6353333333333335,
      "grad_norm": 0.254319429397583,
      "learning_rate": 3.3529166666666664e-05,
      "loss": 0.0024,
      "step": 39530
    },
    {
      "epoch": 2.636,
      "grad_norm": 0.0466192252933979,
      "learning_rate": 3.3525e-05,
      "loss": 0.0015,
      "step": 39540
    },
    {
      "epoch": 2.6366666666666667,
      "grad_norm": 0.29078882932662964,
      "learning_rate": 3.352083333333333e-05,
      "loss": 0.0025,
      "step": 39550
    },
    {
      "epoch": 2.6373333333333333,
      "grad_norm": 0.13539551198482513,
      "learning_rate": 3.351666666666667e-05,
      "loss": 0.002,
      "step": 39560
    },
    {
      "epoch": 2.638,
      "grad_norm": 0.4054776728153229,
      "learning_rate": 3.35125e-05,
      "loss": 0.0024,
      "step": 39570
    },
    {
      "epoch": 2.6386666666666665,
      "grad_norm": 0.5084819793701172,
      "learning_rate": 3.350833333333334e-05,
      "loss": 0.0021,
      "step": 39580
    },
    {
      "epoch": 2.639333333333333,
      "grad_norm": 0.9778376221656799,
      "learning_rate": 3.350416666666667e-05,
      "loss": 0.002,
      "step": 39590
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.1858316957950592,
      "learning_rate": 3.35e-05,
      "loss": 0.0018,
      "step": 39600
    },
    {
      "epoch": 2.6406666666666667,
      "grad_norm": 0.6250472664833069,
      "learning_rate": 3.349583333333334e-05,
      "loss": 0.0026,
      "step": 39610
    },
    {
      "epoch": 2.6413333333333333,
      "grad_norm": 0.4131610095500946,
      "learning_rate": 3.349166666666666e-05,
      "loss": 0.003,
      "step": 39620
    },
    {
      "epoch": 2.642,
      "grad_norm": 0.3863151967525482,
      "learning_rate": 3.34875e-05,
      "loss": 0.0025,
      "step": 39630
    },
    {
      "epoch": 2.642666666666667,
      "grad_norm": 0.37899717688560486,
      "learning_rate": 3.348333333333333e-05,
      "loss": 0.0019,
      "step": 39640
    },
    {
      "epoch": 2.6433333333333335,
      "grad_norm": 0.13225407898426056,
      "learning_rate": 3.347916666666667e-05,
      "loss": 0.0024,
      "step": 39650
    },
    {
      "epoch": 2.644,
      "grad_norm": 0.18537944555282593,
      "learning_rate": 3.3475e-05,
      "loss": 0.0025,
      "step": 39660
    },
    {
      "epoch": 2.6446666666666667,
      "grad_norm": 0.4247226119041443,
      "learning_rate": 3.347083333333333e-05,
      "loss": 0.0024,
      "step": 39670
    },
    {
      "epoch": 2.6453333333333333,
      "grad_norm": 0.5643874406814575,
      "learning_rate": 3.346666666666667e-05,
      "loss": 0.0025,
      "step": 39680
    },
    {
      "epoch": 2.646,
      "grad_norm": 0.11509409546852112,
      "learning_rate": 3.34625e-05,
      "loss": 0.0024,
      "step": 39690
    },
    {
      "epoch": 2.6466666666666665,
      "grad_norm": 0.7505669593811035,
      "learning_rate": 3.345833333333334e-05,
      "loss": 0.0029,
      "step": 39700
    },
    {
      "epoch": 2.647333333333333,
      "grad_norm": 0.4472161829471588,
      "learning_rate": 3.345416666666667e-05,
      "loss": 0.0014,
      "step": 39710
    },
    {
      "epoch": 2.648,
      "grad_norm": 0.6483436822891235,
      "learning_rate": 3.345000000000001e-05,
      "loss": 0.0016,
      "step": 39720
    },
    {
      "epoch": 2.6486666666666667,
      "grad_norm": 0.11463987827301025,
      "learning_rate": 3.344583333333334e-05,
      "loss": 0.0031,
      "step": 39730
    },
    {
      "epoch": 2.6493333333333333,
      "grad_norm": 0.12380918860435486,
      "learning_rate": 3.344166666666667e-05,
      "loss": 0.0023,
      "step": 39740
    },
    {
      "epoch": 2.65,
      "grad_norm": 0.38251304626464844,
      "learning_rate": 3.34375e-05,
      "loss": 0.0023,
      "step": 39750
    },
    {
      "epoch": 2.6506666666666665,
      "grad_norm": 0.5552302002906799,
      "learning_rate": 3.343333333333333e-05,
      "loss": 0.0016,
      "step": 39760
    },
    {
      "epoch": 2.6513333333333335,
      "grad_norm": 1.0473201274871826,
      "learning_rate": 3.342916666666667e-05,
      "loss": 0.0026,
      "step": 39770
    },
    {
      "epoch": 2.652,
      "grad_norm": 0.6034982204437256,
      "learning_rate": 3.3425e-05,
      "loss": 0.0021,
      "step": 39780
    },
    {
      "epoch": 2.6526666666666667,
      "grad_norm": 0.21093016862869263,
      "learning_rate": 3.342083333333334e-05,
      "loss": 0.0018,
      "step": 39790
    },
    {
      "epoch": 2.6533333333333333,
      "grad_norm": 0.38083791732788086,
      "learning_rate": 3.341666666666667e-05,
      "loss": 0.0018,
      "step": 39800
    },
    {
      "epoch": 2.654,
      "grad_norm": 0.43038204312324524,
      "learning_rate": 3.34125e-05,
      "loss": 0.0028,
      "step": 39810
    },
    {
      "epoch": 2.6546666666666665,
      "grad_norm": 0.3182651996612549,
      "learning_rate": 3.340833333333334e-05,
      "loss": 0.0017,
      "step": 39820
    },
    {
      "epoch": 2.655333333333333,
      "grad_norm": 0.05436385050415993,
      "learning_rate": 3.340416666666667e-05,
      "loss": 0.0024,
      "step": 39830
    },
    {
      "epoch": 2.656,
      "grad_norm": 0.26841533184051514,
      "learning_rate": 3.3400000000000005e-05,
      "loss": 0.0029,
      "step": 39840
    },
    {
      "epoch": 2.6566666666666667,
      "grad_norm": 0.11455134302377701,
      "learning_rate": 3.3395833333333336e-05,
      "loss": 0.0025,
      "step": 39850
    },
    {
      "epoch": 2.6573333333333333,
      "grad_norm": 0.9081094264984131,
      "learning_rate": 3.339166666666667e-05,
      "loss": 0.0016,
      "step": 39860
    },
    {
      "epoch": 2.658,
      "grad_norm": 0.07125003635883331,
      "learning_rate": 3.33875e-05,
      "loss": 0.0016,
      "step": 39870
    },
    {
      "epoch": 2.6586666666666665,
      "grad_norm": 0.4427018463611603,
      "learning_rate": 3.3383333333333336e-05,
      "loss": 0.0019,
      "step": 39880
    },
    {
      "epoch": 2.6593333333333335,
      "grad_norm": 0.33870047330856323,
      "learning_rate": 3.337916666666667e-05,
      "loss": 0.0025,
      "step": 39890
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.7327855229377747,
      "learning_rate": 3.3375e-05,
      "loss": 0.0025,
      "step": 39900
    },
    {
      "epoch": 2.6606666666666667,
      "grad_norm": 0.09183495491743088,
      "learning_rate": 3.3370833333333336e-05,
      "loss": 0.0017,
      "step": 39910
    },
    {
      "epoch": 2.6613333333333333,
      "grad_norm": 0.15950943529605865,
      "learning_rate": 3.336666666666667e-05,
      "loss": 0.0017,
      "step": 39920
    },
    {
      "epoch": 2.662,
      "grad_norm": 0.3996625244617462,
      "learning_rate": 3.3362500000000005e-05,
      "loss": 0.0021,
      "step": 39930
    },
    {
      "epoch": 2.6626666666666665,
      "grad_norm": 0.9016313552856445,
      "learning_rate": 3.3358333333333336e-05,
      "loss": 0.0022,
      "step": 39940
    },
    {
      "epoch": 2.663333333333333,
      "grad_norm": 0.2409307062625885,
      "learning_rate": 3.3354166666666667e-05,
      "loss": 0.0027,
      "step": 39950
    },
    {
      "epoch": 2.664,
      "grad_norm": 0.3301810026168823,
      "learning_rate": 3.3350000000000004e-05,
      "loss": 0.0024,
      "step": 39960
    },
    {
      "epoch": 2.6646666666666667,
      "grad_norm": 0.6825792789459229,
      "learning_rate": 3.3345833333333335e-05,
      "loss": 0.0024,
      "step": 39970
    },
    {
      "epoch": 2.6653333333333333,
      "grad_norm": 0.37397128343582153,
      "learning_rate": 3.3341666666666666e-05,
      "loss": 0.0039,
      "step": 39980
    },
    {
      "epoch": 2.666,
      "grad_norm": 0.4868101477622986,
      "learning_rate": 3.33375e-05,
      "loss": 0.0015,
      "step": 39990
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.6618501543998718,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.0021,
      "step": 40000
    },
    {
      "epoch": 2.6673333333333336,
      "grad_norm": 0.1248059794306755,
      "learning_rate": 3.3329166666666666e-05,
      "loss": 0.0024,
      "step": 40010
    },
    {
      "epoch": 2.668,
      "grad_norm": 0.41602396965026855,
      "learning_rate": 3.3325000000000004e-05,
      "loss": 0.0025,
      "step": 40020
    },
    {
      "epoch": 2.6686666666666667,
      "grad_norm": 0.23476573824882507,
      "learning_rate": 3.3320833333333335e-05,
      "loss": 0.003,
      "step": 40030
    },
    {
      "epoch": 2.6693333333333333,
      "grad_norm": 0.26539385318756104,
      "learning_rate": 3.3316666666666666e-05,
      "loss": 0.0021,
      "step": 40040
    },
    {
      "epoch": 2.67,
      "grad_norm": 0.37656864523887634,
      "learning_rate": 3.33125e-05,
      "loss": 0.0024,
      "step": 40050
    },
    {
      "epoch": 2.6706666666666665,
      "grad_norm": 0.6571652293205261,
      "learning_rate": 3.3308333333333334e-05,
      "loss": 0.0039,
      "step": 40060
    },
    {
      "epoch": 2.671333333333333,
      "grad_norm": 0.3696521520614624,
      "learning_rate": 3.330416666666667e-05,
      "loss": 0.0035,
      "step": 40070
    },
    {
      "epoch": 2.672,
      "grad_norm": 0.3756289482116699,
      "learning_rate": 3.33e-05,
      "loss": 0.0018,
      "step": 40080
    },
    {
      "epoch": 2.6726666666666667,
      "grad_norm": 0.0813206136226654,
      "learning_rate": 3.3295833333333334e-05,
      "loss": 0.0026,
      "step": 40090
    },
    {
      "epoch": 2.6733333333333333,
      "grad_norm": 0.2628992795944214,
      "learning_rate": 3.329166666666667e-05,
      "loss": 0.0017,
      "step": 40100
    },
    {
      "epoch": 2.674,
      "grad_norm": 0.3449110686779022,
      "learning_rate": 3.3287499999999996e-05,
      "loss": 0.0022,
      "step": 40110
    },
    {
      "epoch": 2.6746666666666665,
      "grad_norm": 0.1955895572900772,
      "learning_rate": 3.3283333333333334e-05,
      "loss": 0.0026,
      "step": 40120
    },
    {
      "epoch": 2.6753333333333336,
      "grad_norm": 0.46783310174942017,
      "learning_rate": 3.3279166666666665e-05,
      "loss": 0.0031,
      "step": 40130
    },
    {
      "epoch": 2.676,
      "grad_norm": 0.26750683784484863,
      "learning_rate": 3.3275e-05,
      "loss": 0.0019,
      "step": 40140
    },
    {
      "epoch": 2.6766666666666667,
      "grad_norm": 0.11857162415981293,
      "learning_rate": 3.3270833333333333e-05,
      "loss": 0.0023,
      "step": 40150
    },
    {
      "epoch": 2.6773333333333333,
      "grad_norm": 0.1481332927942276,
      "learning_rate": 3.326666666666667e-05,
      "loss": 0.0028,
      "step": 40160
    },
    {
      "epoch": 2.678,
      "grad_norm": 0.13094060122966766,
      "learning_rate": 3.32625e-05,
      "loss": 0.0028,
      "step": 40170
    },
    {
      "epoch": 2.6786666666666665,
      "grad_norm": 0.4815300703048706,
      "learning_rate": 3.325833333333333e-05,
      "loss": 0.0018,
      "step": 40180
    },
    {
      "epoch": 2.679333333333333,
      "grad_norm": 0.7450370192527771,
      "learning_rate": 3.325416666666667e-05,
      "loss": 0.0026,
      "step": 40190
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.5522945523262024,
      "learning_rate": 3.325e-05,
      "loss": 0.0026,
      "step": 40200
    },
    {
      "epoch": 2.6806666666666668,
      "grad_norm": 0.04166775941848755,
      "learning_rate": 3.324583333333334e-05,
      "loss": 0.0018,
      "step": 40210
    },
    {
      "epoch": 2.6813333333333333,
      "grad_norm": 0.03963005542755127,
      "learning_rate": 3.324166666666667e-05,
      "loss": 0.0015,
      "step": 40220
    },
    {
      "epoch": 2.682,
      "grad_norm": 0.33116620779037476,
      "learning_rate": 3.32375e-05,
      "loss": 0.0017,
      "step": 40230
    },
    {
      "epoch": 2.6826666666666665,
      "grad_norm": 0.7794815897941589,
      "learning_rate": 3.323333333333333e-05,
      "loss": 0.003,
      "step": 40240
    },
    {
      "epoch": 2.6833333333333336,
      "grad_norm": 0.17987048625946045,
      "learning_rate": 3.3229166666666663e-05,
      "loss": 0.0022,
      "step": 40250
    },
    {
      "epoch": 2.684,
      "grad_norm": 0.47262847423553467,
      "learning_rate": 3.3225e-05,
      "loss": 0.0021,
      "step": 40260
    },
    {
      "epoch": 2.6846666666666668,
      "grad_norm": 0.18352054059505463,
      "learning_rate": 3.322083333333333e-05,
      "loss": 0.0019,
      "step": 40270
    },
    {
      "epoch": 2.6853333333333333,
      "grad_norm": 0.2615845203399658,
      "learning_rate": 3.321666666666667e-05,
      "loss": 0.0031,
      "step": 40280
    },
    {
      "epoch": 2.686,
      "grad_norm": 0.5652338266372681,
      "learning_rate": 3.32125e-05,
      "loss": 0.0031,
      "step": 40290
    },
    {
      "epoch": 2.6866666666666665,
      "grad_norm": 0.063930943608284,
      "learning_rate": 3.320833333333334e-05,
      "loss": 0.0018,
      "step": 40300
    },
    {
      "epoch": 2.687333333333333,
      "grad_norm": 0.15338720381259918,
      "learning_rate": 3.320416666666667e-05,
      "loss": 0.002,
      "step": 40310
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 0.8871645331382751,
      "learning_rate": 3.32e-05,
      "loss": 0.0026,
      "step": 40320
    },
    {
      "epoch": 2.6886666666666668,
      "grad_norm": 0.33882665634155273,
      "learning_rate": 3.319583333333334e-05,
      "loss": 0.0022,
      "step": 40330
    },
    {
      "epoch": 2.6893333333333334,
      "grad_norm": 0.8691903352737427,
      "learning_rate": 3.319166666666667e-05,
      "loss": 0.0021,
      "step": 40340
    },
    {
      "epoch": 2.69,
      "grad_norm": 0.5059344172477722,
      "learning_rate": 3.31875e-05,
      "loss": 0.0024,
      "step": 40350
    },
    {
      "epoch": 2.6906666666666665,
      "grad_norm": 0.6014447212219238,
      "learning_rate": 3.318333333333333e-05,
      "loss": 0.0024,
      "step": 40360
    },
    {
      "epoch": 2.6913333333333336,
      "grad_norm": 0.1563066840171814,
      "learning_rate": 3.317916666666667e-05,
      "loss": 0.0019,
      "step": 40370
    },
    {
      "epoch": 2.692,
      "grad_norm": 0.6893832087516785,
      "learning_rate": 3.3175e-05,
      "loss": 0.0016,
      "step": 40380
    },
    {
      "epoch": 2.6926666666666668,
      "grad_norm": 0.18777576088905334,
      "learning_rate": 3.317083333333333e-05,
      "loss": 0.0025,
      "step": 40390
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 0.7643935084342957,
      "learning_rate": 3.316666666666667e-05,
      "loss": 0.0025,
      "step": 40400
    },
    {
      "epoch": 2.694,
      "grad_norm": 0.9648799300193787,
      "learning_rate": 3.31625e-05,
      "loss": 0.0022,
      "step": 40410
    },
    {
      "epoch": 2.6946666666666665,
      "grad_norm": 0.15446272492408752,
      "learning_rate": 3.315833333333334e-05,
      "loss": 0.0025,
      "step": 40420
    },
    {
      "epoch": 2.695333333333333,
      "grad_norm": 0.16111530363559723,
      "learning_rate": 3.315416666666667e-05,
      "loss": 0.0019,
      "step": 40430
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 0.4216461181640625,
      "learning_rate": 3.3150000000000006e-05,
      "loss": 0.0029,
      "step": 40440
    },
    {
      "epoch": 2.6966666666666668,
      "grad_norm": 0.8358415961265564,
      "learning_rate": 3.314583333333334e-05,
      "loss": 0.0021,
      "step": 40450
    },
    {
      "epoch": 2.6973333333333334,
      "grad_norm": 0.5239542126655579,
      "learning_rate": 3.314166666666667e-05,
      "loss": 0.0018,
      "step": 40460
    },
    {
      "epoch": 2.698,
      "grad_norm": 0.23901845514774323,
      "learning_rate": 3.31375e-05,
      "loss": 0.002,
      "step": 40470
    },
    {
      "epoch": 2.6986666666666665,
      "grad_norm": 0.45629021525382996,
      "learning_rate": 3.313333333333333e-05,
      "loss": 0.0019,
      "step": 40480
    },
    {
      "epoch": 2.6993333333333336,
      "grad_norm": 1.209743618965149,
      "learning_rate": 3.312916666666667e-05,
      "loss": 0.0017,
      "step": 40490
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.12304934114217758,
      "learning_rate": 3.3125e-05,
      "loss": 0.0012,
      "step": 40500
    },
    {
      "epoch": 2.7006666666666668,
      "grad_norm": 0.4511335790157318,
      "learning_rate": 3.3120833333333337e-05,
      "loss": 0.002,
      "step": 40510
    },
    {
      "epoch": 2.7013333333333334,
      "grad_norm": 0.05334116145968437,
      "learning_rate": 3.311666666666667e-05,
      "loss": 0.0022,
      "step": 40520
    },
    {
      "epoch": 2.702,
      "grad_norm": 0.7237421870231628,
      "learning_rate": 3.31125e-05,
      "loss": 0.0023,
      "step": 40530
    },
    {
      "epoch": 2.7026666666666666,
      "grad_norm": 0.3818010985851288,
      "learning_rate": 3.3108333333333336e-05,
      "loss": 0.0015,
      "step": 40540
    },
    {
      "epoch": 2.703333333333333,
      "grad_norm": 0.36843809485435486,
      "learning_rate": 3.310416666666667e-05,
      "loss": 0.0033,
      "step": 40550
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.048186045140028,
      "learning_rate": 3.3100000000000005e-05,
      "loss": 0.0032,
      "step": 40560
    },
    {
      "epoch": 2.7046666666666668,
      "grad_norm": 0.17821453511714935,
      "learning_rate": 3.3095833333333336e-05,
      "loss": 0.0026,
      "step": 40570
    },
    {
      "epoch": 2.7053333333333334,
      "grad_norm": 0.30243024230003357,
      "learning_rate": 3.3091666666666674e-05,
      "loss": 0.0026,
      "step": 40580
    },
    {
      "epoch": 2.706,
      "grad_norm": 0.7040023803710938,
      "learning_rate": 3.30875e-05,
      "loss": 0.0023,
      "step": 40590
    },
    {
      "epoch": 2.7066666666666666,
      "grad_norm": 0.06328174471855164,
      "learning_rate": 3.3083333333333336e-05,
      "loss": 0.002,
      "step": 40600
    },
    {
      "epoch": 2.7073333333333336,
      "grad_norm": 0.7954360246658325,
      "learning_rate": 3.3079166666666667e-05,
      "loss": 0.0022,
      "step": 40610
    },
    {
      "epoch": 2.708,
      "grad_norm": 0.4631683826446533,
      "learning_rate": 3.3075e-05,
      "loss": 0.0023,
      "step": 40620
    },
    {
      "epoch": 2.708666666666667,
      "grad_norm": 0.3145907521247864,
      "learning_rate": 3.3070833333333335e-05,
      "loss": 0.0022,
      "step": 40630
    },
    {
      "epoch": 2.7093333333333334,
      "grad_norm": 0.5935567021369934,
      "learning_rate": 3.3066666666666666e-05,
      "loss": 0.002,
      "step": 40640
    },
    {
      "epoch": 2.71,
      "grad_norm": 0.5672016143798828,
      "learning_rate": 3.3062500000000004e-05,
      "loss": 0.0016,
      "step": 40650
    },
    {
      "epoch": 2.7106666666666666,
      "grad_norm": 0.06426158547401428,
      "learning_rate": 3.3058333333333335e-05,
      "loss": 0.003,
      "step": 40660
    },
    {
      "epoch": 2.711333333333333,
      "grad_norm": 0.04412953183054924,
      "learning_rate": 3.305416666666667e-05,
      "loss": 0.0022,
      "step": 40670
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 0.031788814812898636,
      "learning_rate": 3.3050000000000004e-05,
      "loss": 0.0025,
      "step": 40680
    },
    {
      "epoch": 2.712666666666667,
      "grad_norm": 0.1674325317144394,
      "learning_rate": 3.3045833333333335e-05,
      "loss": 0.0022,
      "step": 40690
    },
    {
      "epoch": 2.7133333333333334,
      "grad_norm": 0.37334421277046204,
      "learning_rate": 3.304166666666667e-05,
      "loss": 0.0019,
      "step": 40700
    },
    {
      "epoch": 2.714,
      "grad_norm": 0.03656771034002304,
      "learning_rate": 3.30375e-05,
      "loss": 0.0021,
      "step": 40710
    },
    {
      "epoch": 2.7146666666666666,
      "grad_norm": 0.8264545202255249,
      "learning_rate": 3.3033333333333334e-05,
      "loss": 0.0022,
      "step": 40720
    },
    {
      "epoch": 2.7153333333333336,
      "grad_norm": 0.3947441577911377,
      "learning_rate": 3.3029166666666665e-05,
      "loss": 0.002,
      "step": 40730
    },
    {
      "epoch": 2.716,
      "grad_norm": 0.18168427050113678,
      "learning_rate": 3.3025e-05,
      "loss": 0.0021,
      "step": 40740
    },
    {
      "epoch": 2.716666666666667,
      "grad_norm": 0.18468761444091797,
      "learning_rate": 3.3020833333333334e-05,
      "loss": 0.0017,
      "step": 40750
    },
    {
      "epoch": 2.7173333333333334,
      "grad_norm": 0.23895514011383057,
      "learning_rate": 3.3016666666666665e-05,
      "loss": 0.002,
      "step": 40760
    },
    {
      "epoch": 2.718,
      "grad_norm": 0.45336082577705383,
      "learning_rate": 3.30125e-05,
      "loss": 0.0023,
      "step": 40770
    },
    {
      "epoch": 2.7186666666666666,
      "grad_norm": 0.28153225779533386,
      "learning_rate": 3.3008333333333334e-05,
      "loss": 0.0022,
      "step": 40780
    },
    {
      "epoch": 2.719333333333333,
      "grad_norm": 0.14949500560760498,
      "learning_rate": 3.300416666666667e-05,
      "loss": 0.0033,
      "step": 40790
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.7130101919174194,
      "learning_rate": 3.3e-05,
      "loss": 0.002,
      "step": 40800
    },
    {
      "epoch": 2.720666666666667,
      "grad_norm": 0.2089034914970398,
      "learning_rate": 3.299583333333334e-05,
      "loss": 0.0018,
      "step": 40810
    },
    {
      "epoch": 2.7213333333333334,
      "grad_norm": 0.6666410565376282,
      "learning_rate": 3.299166666666667e-05,
      "loss": 0.0022,
      "step": 40820
    },
    {
      "epoch": 2.722,
      "grad_norm": 0.05079290643334389,
      "learning_rate": 3.29875e-05,
      "loss": 0.0016,
      "step": 40830
    },
    {
      "epoch": 2.7226666666666666,
      "grad_norm": 0.5717142820358276,
      "learning_rate": 3.298333333333333e-05,
      "loss": 0.0029,
      "step": 40840
    },
    {
      "epoch": 2.7233333333333336,
      "grad_norm": 0.3459777534008026,
      "learning_rate": 3.2979166666666664e-05,
      "loss": 0.0018,
      "step": 40850
    },
    {
      "epoch": 2.724,
      "grad_norm": 0.5903334617614746,
      "learning_rate": 3.2975e-05,
      "loss": 0.0019,
      "step": 40860
    },
    {
      "epoch": 2.724666666666667,
      "grad_norm": 0.7325797080993652,
      "learning_rate": 3.297083333333333e-05,
      "loss": 0.0023,
      "step": 40870
    },
    {
      "epoch": 2.7253333333333334,
      "grad_norm": 0.37270858883857727,
      "learning_rate": 3.296666666666667e-05,
      "loss": 0.0022,
      "step": 40880
    },
    {
      "epoch": 2.726,
      "grad_norm": 0.4584774672985077,
      "learning_rate": 3.29625e-05,
      "loss": 0.0017,
      "step": 40890
    },
    {
      "epoch": 2.7266666666666666,
      "grad_norm": 0.33907511830329895,
      "learning_rate": 3.295833333333333e-05,
      "loss": 0.003,
      "step": 40900
    },
    {
      "epoch": 2.727333333333333,
      "grad_norm": 0.27439674735069275,
      "learning_rate": 3.295416666666667e-05,
      "loss": 0.0019,
      "step": 40910
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 0.2789178788661957,
      "learning_rate": 3.295e-05,
      "loss": 0.0031,
      "step": 40920
    },
    {
      "epoch": 2.728666666666667,
      "grad_norm": 0.509007453918457,
      "learning_rate": 3.294583333333334e-05,
      "loss": 0.0021,
      "step": 40930
    },
    {
      "epoch": 2.7293333333333334,
      "grad_norm": 0.567746639251709,
      "learning_rate": 3.294166666666667e-05,
      "loss": 0.002,
      "step": 40940
    },
    {
      "epoch": 2.73,
      "grad_norm": 0.40975573658943176,
      "learning_rate": 3.29375e-05,
      "loss": 0.0031,
      "step": 40950
    },
    {
      "epoch": 2.7306666666666666,
      "grad_norm": 0.23694613575935364,
      "learning_rate": 3.293333333333333e-05,
      "loss": 0.0027,
      "step": 40960
    },
    {
      "epoch": 2.731333333333333,
      "grad_norm": 0.4536300301551819,
      "learning_rate": 3.292916666666667e-05,
      "loss": 0.002,
      "step": 40970
    },
    {
      "epoch": 2.732,
      "grad_norm": 0.03153350576758385,
      "learning_rate": 3.2925e-05,
      "loss": 0.0035,
      "step": 40980
    },
    {
      "epoch": 2.732666666666667,
      "grad_norm": 0.39144933223724365,
      "learning_rate": 3.292083333333333e-05,
      "loss": 0.0027,
      "step": 40990
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 0.1480686217546463,
      "learning_rate": 3.291666666666667e-05,
      "loss": 0.0023,
      "step": 41000
    },
    {
      "epoch": 2.734,
      "grad_norm": 0.684563934803009,
      "learning_rate": 3.29125e-05,
      "loss": 0.0025,
      "step": 41010
    },
    {
      "epoch": 2.7346666666666666,
      "grad_norm": 0.3639145791530609,
      "learning_rate": 3.290833333333334e-05,
      "loss": 0.0025,
      "step": 41020
    },
    {
      "epoch": 2.735333333333333,
      "grad_norm": 0.581393837928772,
      "learning_rate": 3.290416666666667e-05,
      "loss": 0.0021,
      "step": 41030
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 0.4086572229862213,
      "learning_rate": 3.29e-05,
      "loss": 0.0029,
      "step": 41040
    },
    {
      "epoch": 2.736666666666667,
      "grad_norm": 0.49768394231796265,
      "learning_rate": 3.289583333333334e-05,
      "loss": 0.0014,
      "step": 41050
    },
    {
      "epoch": 2.7373333333333334,
      "grad_norm": 0.6310592889785767,
      "learning_rate": 3.289166666666667e-05,
      "loss": 0.0026,
      "step": 41060
    },
    {
      "epoch": 2.738,
      "grad_norm": 0.34996187686920166,
      "learning_rate": 3.28875e-05,
      "loss": 0.0021,
      "step": 41070
    },
    {
      "epoch": 2.7386666666666666,
      "grad_norm": 0.44017136096954346,
      "learning_rate": 3.288333333333333e-05,
      "loss": 0.0018,
      "step": 41080
    },
    {
      "epoch": 2.739333333333333,
      "grad_norm": 0.11569171398878098,
      "learning_rate": 3.287916666666667e-05,
      "loss": 0.002,
      "step": 41090
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.06685752421617508,
      "learning_rate": 3.2875e-05,
      "loss": 0.0029,
      "step": 41100
    },
    {
      "epoch": 2.740666666666667,
      "grad_norm": 0.05475408583879471,
      "learning_rate": 3.287083333333334e-05,
      "loss": 0.0018,
      "step": 41110
    },
    {
      "epoch": 2.7413333333333334,
      "grad_norm": 0.5949037075042725,
      "learning_rate": 3.286666666666667e-05,
      "loss": 0.0016,
      "step": 41120
    },
    {
      "epoch": 2.742,
      "grad_norm": 0.2683766186237335,
      "learning_rate": 3.28625e-05,
      "loss": 0.0017,
      "step": 41130
    },
    {
      "epoch": 2.7426666666666666,
      "grad_norm": 0.961236298084259,
      "learning_rate": 3.285833333333334e-05,
      "loss": 0.0015,
      "step": 41140
    },
    {
      "epoch": 2.743333333333333,
      "grad_norm": 0.5278878808021545,
      "learning_rate": 3.285416666666667e-05,
      "loss": 0.0027,
      "step": 41150
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 0.3856258690357208,
      "learning_rate": 3.2850000000000006e-05,
      "loss": 0.0021,
      "step": 41160
    },
    {
      "epoch": 2.744666666666667,
      "grad_norm": 0.25889483094215393,
      "learning_rate": 3.284583333333334e-05,
      "loss": 0.0017,
      "step": 41170
    },
    {
      "epoch": 2.7453333333333334,
      "grad_norm": 0.4869965612888336,
      "learning_rate": 3.284166666666667e-05,
      "loss": 0.0026,
      "step": 41180
    },
    {
      "epoch": 2.746,
      "grad_norm": 0.4017970561981201,
      "learning_rate": 3.28375e-05,
      "loss": 0.0018,
      "step": 41190
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 0.18659541010856628,
      "learning_rate": 3.283333333333333e-05,
      "loss": 0.0016,
      "step": 41200
    },
    {
      "epoch": 2.747333333333333,
      "grad_norm": 0.551527738571167,
      "learning_rate": 3.282916666666667e-05,
      "loss": 0.0016,
      "step": 41210
    },
    {
      "epoch": 2.748,
      "grad_norm": 0.3760819435119629,
      "learning_rate": 3.2825e-05,
      "loss": 0.003,
      "step": 41220
    },
    {
      "epoch": 2.748666666666667,
      "grad_norm": 0.18951578438282013,
      "learning_rate": 3.2820833333333336e-05,
      "loss": 0.002,
      "step": 41230
    },
    {
      "epoch": 2.7493333333333334,
      "grad_norm": 0.4548763036727905,
      "learning_rate": 3.281666666666667e-05,
      "loss": 0.0023,
      "step": 41240
    },
    {
      "epoch": 2.75,
      "grad_norm": 0.055934809148311615,
      "learning_rate": 3.2812500000000005e-05,
      "loss": 0.0013,
      "step": 41250
    },
    {
      "epoch": 2.7506666666666666,
      "grad_norm": 0.27104100584983826,
      "learning_rate": 3.2808333333333336e-05,
      "loss": 0.0018,
      "step": 41260
    },
    {
      "epoch": 2.751333333333333,
      "grad_norm": 0.9509298205375671,
      "learning_rate": 3.280416666666667e-05,
      "loss": 0.0028,
      "step": 41270
    },
    {
      "epoch": 2.752,
      "grad_norm": 0.8446675539016724,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 0.0023,
      "step": 41280
    },
    {
      "epoch": 2.752666666666667,
      "grad_norm": 0.4068654477596283,
      "learning_rate": 3.2795833333333335e-05,
      "loss": 0.0021,
      "step": 41290
    },
    {
      "epoch": 2.7533333333333334,
      "grad_norm": 0.9897050857543945,
      "learning_rate": 3.279166666666667e-05,
      "loss": 0.0018,
      "step": 41300
    },
    {
      "epoch": 2.754,
      "grad_norm": 0.8813216686248779,
      "learning_rate": 3.2787500000000004e-05,
      "loss": 0.0019,
      "step": 41310
    },
    {
      "epoch": 2.7546666666666666,
      "grad_norm": 0.6217968463897705,
      "learning_rate": 3.2783333333333335e-05,
      "loss": 0.0031,
      "step": 41320
    },
    {
      "epoch": 2.755333333333333,
      "grad_norm": 0.3969660699367523,
      "learning_rate": 3.2779166666666666e-05,
      "loss": 0.0025,
      "step": 41330
    },
    {
      "epoch": 2.7560000000000002,
      "grad_norm": 0.4763089418411255,
      "learning_rate": 3.2775e-05,
      "loss": 0.0022,
      "step": 41340
    },
    {
      "epoch": 2.756666666666667,
      "grad_norm": 0.8745440244674683,
      "learning_rate": 3.2770833333333335e-05,
      "loss": 0.0031,
      "step": 41350
    },
    {
      "epoch": 2.7573333333333334,
      "grad_norm": 0.48054036498069763,
      "learning_rate": 3.2766666666666666e-05,
      "loss": 0.0024,
      "step": 41360
    },
    {
      "epoch": 2.758,
      "grad_norm": 0.30374521017074585,
      "learning_rate": 3.2762500000000004e-05,
      "loss": 0.0029,
      "step": 41370
    },
    {
      "epoch": 2.7586666666666666,
      "grad_norm": 0.5937651991844177,
      "learning_rate": 3.2758333333333335e-05,
      "loss": 0.0018,
      "step": 41380
    },
    {
      "epoch": 2.759333333333333,
      "grad_norm": 0.8030225038528442,
      "learning_rate": 3.275416666666667e-05,
      "loss": 0.003,
      "step": 41390
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.8530703783035278,
      "learning_rate": 3.275e-05,
      "loss": 0.0028,
      "step": 41400
    },
    {
      "epoch": 2.760666666666667,
      "grad_norm": 0.9207127690315247,
      "learning_rate": 3.2745833333333334e-05,
      "loss": 0.0025,
      "step": 41410
    },
    {
      "epoch": 2.7613333333333334,
      "grad_norm": 0.5577203035354614,
      "learning_rate": 3.274166666666667e-05,
      "loss": 0.0027,
      "step": 41420
    },
    {
      "epoch": 2.762,
      "grad_norm": 0.6533398032188416,
      "learning_rate": 3.27375e-05,
      "loss": 0.0017,
      "step": 41430
    },
    {
      "epoch": 2.7626666666666666,
      "grad_norm": 0.6347814202308655,
      "learning_rate": 3.2733333333333334e-05,
      "loss": 0.0019,
      "step": 41440
    },
    {
      "epoch": 2.763333333333333,
      "grad_norm": 0.8600483536720276,
      "learning_rate": 3.2729166666666665e-05,
      "loss": 0.0028,
      "step": 41450
    },
    {
      "epoch": 2.7640000000000002,
      "grad_norm": 0.8431035280227661,
      "learning_rate": 3.2725e-05,
      "loss": 0.0033,
      "step": 41460
    },
    {
      "epoch": 2.764666666666667,
      "grad_norm": 0.3682137131690979,
      "learning_rate": 3.2720833333333334e-05,
      "loss": 0.0024,
      "step": 41470
    },
    {
      "epoch": 2.7653333333333334,
      "grad_norm": 0.4897051751613617,
      "learning_rate": 3.2716666666666665e-05,
      "loss": 0.0029,
      "step": 41480
    },
    {
      "epoch": 2.766,
      "grad_norm": 0.4072927236557007,
      "learning_rate": 3.27125e-05,
      "loss": 0.0029,
      "step": 41490
    },
    {
      "epoch": 2.7666666666666666,
      "grad_norm": 0.14920645952224731,
      "learning_rate": 3.270833333333333e-05,
      "loss": 0.0034,
      "step": 41500
    },
    {
      "epoch": 2.767333333333333,
      "grad_norm": 0.09330867975950241,
      "learning_rate": 3.270416666666667e-05,
      "loss": 0.0026,
      "step": 41510
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.681489884853363,
      "learning_rate": 3.27e-05,
      "loss": 0.0018,
      "step": 41520
    },
    {
      "epoch": 2.768666666666667,
      "grad_norm": 0.25436410307884216,
      "learning_rate": 3.269583333333334e-05,
      "loss": 0.0018,
      "step": 41530
    },
    {
      "epoch": 2.7693333333333334,
      "grad_norm": 0.23058292269706726,
      "learning_rate": 3.269166666666667e-05,
      "loss": 0.002,
      "step": 41540
    },
    {
      "epoch": 2.77,
      "grad_norm": 0.04240471497178078,
      "learning_rate": 3.26875e-05,
      "loss": 0.0016,
      "step": 41550
    },
    {
      "epoch": 2.7706666666666666,
      "grad_norm": 0.110863596200943,
      "learning_rate": 3.268333333333333e-05,
      "loss": 0.0023,
      "step": 41560
    },
    {
      "epoch": 2.771333333333333,
      "grad_norm": 0.5141744017601013,
      "learning_rate": 3.2679166666666664e-05,
      "loss": 0.002,
      "step": 41570
    },
    {
      "epoch": 2.7720000000000002,
      "grad_norm": 0.1671254187822342,
      "learning_rate": 3.2675e-05,
      "loss": 0.0022,
      "step": 41580
    },
    {
      "epoch": 2.772666666666667,
      "grad_norm": 0.22427898645401,
      "learning_rate": 3.267083333333333e-05,
      "loss": 0.0013,
      "step": 41590
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 0.11978039890527725,
      "learning_rate": 3.266666666666667e-05,
      "loss": 0.0026,
      "step": 41600
    },
    {
      "epoch": 2.774,
      "grad_norm": 0.1836114227771759,
      "learning_rate": 3.26625e-05,
      "loss": 0.0023,
      "step": 41610
    },
    {
      "epoch": 2.7746666666666666,
      "grad_norm": 0.4484216868877411,
      "learning_rate": 3.265833333333333e-05,
      "loss": 0.0012,
      "step": 41620
    },
    {
      "epoch": 2.775333333333333,
      "grad_norm": 0.1874830275774002,
      "learning_rate": 3.265416666666667e-05,
      "loss": 0.002,
      "step": 41630
    },
    {
      "epoch": 2.776,
      "grad_norm": 0.35862496495246887,
      "learning_rate": 3.265e-05,
      "loss": 0.0017,
      "step": 41640
    },
    {
      "epoch": 2.7766666666666664,
      "grad_norm": 1.0038366317749023,
      "learning_rate": 3.264583333333334e-05,
      "loss": 0.0019,
      "step": 41650
    },
    {
      "epoch": 2.7773333333333334,
      "grad_norm": 0.39777177572250366,
      "learning_rate": 3.264166666666667e-05,
      "loss": 0.002,
      "step": 41660
    },
    {
      "epoch": 2.778,
      "grad_norm": 0.26748645305633545,
      "learning_rate": 3.263750000000001e-05,
      "loss": 0.0024,
      "step": 41670
    },
    {
      "epoch": 2.7786666666666666,
      "grad_norm": 0.46204832196235657,
      "learning_rate": 3.263333333333333e-05,
      "loss": 0.0018,
      "step": 41680
    },
    {
      "epoch": 2.779333333333333,
      "grad_norm": 0.22317707538604736,
      "learning_rate": 3.262916666666667e-05,
      "loss": 0.0025,
      "step": 41690
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 0.045067667961120605,
      "learning_rate": 3.2625e-05,
      "loss": 0.0026,
      "step": 41700
    },
    {
      "epoch": 2.780666666666667,
      "grad_norm": 0.07358377426862717,
      "learning_rate": 3.262083333333333e-05,
      "loss": 0.002,
      "step": 41710
    },
    {
      "epoch": 2.7813333333333334,
      "grad_norm": 0.2766926884651184,
      "learning_rate": 3.261666666666667e-05,
      "loss": 0.0016,
      "step": 41720
    },
    {
      "epoch": 2.782,
      "grad_norm": 0.07123984396457672,
      "learning_rate": 3.26125e-05,
      "loss": 0.0032,
      "step": 41730
    },
    {
      "epoch": 2.7826666666666666,
      "grad_norm": 0.41962990164756775,
      "learning_rate": 3.260833333333334e-05,
      "loss": 0.0034,
      "step": 41740
    },
    {
      "epoch": 2.783333333333333,
      "grad_norm": 0.08729296177625656,
      "learning_rate": 3.260416666666667e-05,
      "loss": 0.0022,
      "step": 41750
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.43746310472488403,
      "learning_rate": 3.26e-05,
      "loss": 0.0035,
      "step": 41760
    },
    {
      "epoch": 2.7846666666666664,
      "grad_norm": 0.03134514391422272,
      "learning_rate": 3.259583333333334e-05,
      "loss": 0.0023,
      "step": 41770
    },
    {
      "epoch": 2.7853333333333334,
      "grad_norm": 0.36975106596946716,
      "learning_rate": 3.259166666666667e-05,
      "loss": 0.0028,
      "step": 41780
    },
    {
      "epoch": 2.786,
      "grad_norm": 0.2327285259962082,
      "learning_rate": 3.2587500000000006e-05,
      "loss": 0.0019,
      "step": 41790
    },
    {
      "epoch": 2.7866666666666666,
      "grad_norm": 0.1626468002796173,
      "learning_rate": 3.258333333333333e-05,
      "loss": 0.0035,
      "step": 41800
    },
    {
      "epoch": 2.787333333333333,
      "grad_norm": 0.03569331765174866,
      "learning_rate": 3.257916666666667e-05,
      "loss": 0.0019,
      "step": 41810
    },
    {
      "epoch": 2.7880000000000003,
      "grad_norm": 0.4482688009738922,
      "learning_rate": 3.2575e-05,
      "loss": 0.0025,
      "step": 41820
    },
    {
      "epoch": 2.788666666666667,
      "grad_norm": 0.9951381087303162,
      "learning_rate": 3.257083333333334e-05,
      "loss": 0.0021,
      "step": 41830
    },
    {
      "epoch": 2.7893333333333334,
      "grad_norm": 0.4690035581588745,
      "learning_rate": 3.256666666666667e-05,
      "loss": 0.0019,
      "step": 41840
    },
    {
      "epoch": 2.79,
      "grad_norm": 0.14392924308776855,
      "learning_rate": 3.25625e-05,
      "loss": 0.002,
      "step": 41850
    },
    {
      "epoch": 2.7906666666666666,
      "grad_norm": 0.046274106949567795,
      "learning_rate": 3.2558333333333336e-05,
      "loss": 0.0031,
      "step": 41860
    },
    {
      "epoch": 2.791333333333333,
      "grad_norm": 0.4290200471878052,
      "learning_rate": 3.255416666666667e-05,
      "loss": 0.0034,
      "step": 41870
    },
    {
      "epoch": 2.792,
      "grad_norm": 0.30685240030288696,
      "learning_rate": 3.2550000000000005e-05,
      "loss": 0.0026,
      "step": 41880
    },
    {
      "epoch": 2.7926666666666664,
      "grad_norm": 0.38520997762680054,
      "learning_rate": 3.2545833333333336e-05,
      "loss": 0.0022,
      "step": 41890
    },
    {
      "epoch": 2.7933333333333334,
      "grad_norm": 0.7783359289169312,
      "learning_rate": 3.254166666666667e-05,
      "loss": 0.0025,
      "step": 41900
    },
    {
      "epoch": 2.794,
      "grad_norm": 0.2713839113712311,
      "learning_rate": 3.2537500000000005e-05,
      "loss": 0.0018,
      "step": 41910
    },
    {
      "epoch": 2.7946666666666666,
      "grad_norm": 0.22666560113430023,
      "learning_rate": 3.253333333333333e-05,
      "loss": 0.0023,
      "step": 41920
    },
    {
      "epoch": 2.7953333333333332,
      "grad_norm": 0.4469330310821533,
      "learning_rate": 3.252916666666667e-05,
      "loss": 0.0032,
      "step": 41930
    },
    {
      "epoch": 2.7960000000000003,
      "grad_norm": 0.1444236785173416,
      "learning_rate": 3.2525e-05,
      "loss": 0.0017,
      "step": 41940
    },
    {
      "epoch": 2.796666666666667,
      "grad_norm": 0.6803916096687317,
      "learning_rate": 3.2520833333333336e-05,
      "loss": 0.0017,
      "step": 41950
    },
    {
      "epoch": 2.7973333333333334,
      "grad_norm": 0.13375598192214966,
      "learning_rate": 3.2516666666666666e-05,
      "loss": 0.0018,
      "step": 41960
    },
    {
      "epoch": 2.798,
      "grad_norm": 0.6762252449989319,
      "learning_rate": 3.2512500000000004e-05,
      "loss": 0.0017,
      "step": 41970
    },
    {
      "epoch": 2.7986666666666666,
      "grad_norm": 0.45790034532546997,
      "learning_rate": 3.2508333333333335e-05,
      "loss": 0.0025,
      "step": 41980
    },
    {
      "epoch": 2.7993333333333332,
      "grad_norm": 0.6441309452056885,
      "learning_rate": 3.2504166666666666e-05,
      "loss": 0.0022,
      "step": 41990
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.6434664130210876,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 0.0019,
      "step": 42000
    },
    {
      "epoch": 2.8006666666666664,
      "grad_norm": 0.14705830812454224,
      "learning_rate": 3.2495833333333335e-05,
      "loss": 0.0016,
      "step": 42010
    },
    {
      "epoch": 2.8013333333333335,
      "grad_norm": 0.1537749022245407,
      "learning_rate": 3.249166666666667e-05,
      "loss": 0.0015,
      "step": 42020
    },
    {
      "epoch": 2.802,
      "grad_norm": 0.3666006028652191,
      "learning_rate": 3.2487500000000004e-05,
      "loss": 0.0032,
      "step": 42030
    },
    {
      "epoch": 2.8026666666666666,
      "grad_norm": 0.6286078691482544,
      "learning_rate": 3.2483333333333335e-05,
      "loss": 0.0015,
      "step": 42040
    },
    {
      "epoch": 2.8033333333333332,
      "grad_norm": 0.36597782373428345,
      "learning_rate": 3.2479166666666666e-05,
      "loss": 0.0031,
      "step": 42050
    },
    {
      "epoch": 2.8040000000000003,
      "grad_norm": 0.22882747650146484,
      "learning_rate": 3.2474999999999997e-05,
      "loss": 0.0016,
      "step": 42060
    },
    {
      "epoch": 2.804666666666667,
      "grad_norm": 0.38887476921081543,
      "learning_rate": 3.2470833333333334e-05,
      "loss": 0.003,
      "step": 42070
    },
    {
      "epoch": 2.8053333333333335,
      "grad_norm": 0.7713925242424011,
      "learning_rate": 3.2466666666666665e-05,
      "loss": 0.0021,
      "step": 42080
    },
    {
      "epoch": 2.806,
      "grad_norm": 0.44076329469680786,
      "learning_rate": 3.24625e-05,
      "loss": 0.0029,
      "step": 42090
    },
    {
      "epoch": 2.8066666666666666,
      "grad_norm": 0.30344387888908386,
      "learning_rate": 3.2458333333333334e-05,
      "loss": 0.0017,
      "step": 42100
    },
    {
      "epoch": 2.8073333333333332,
      "grad_norm": 0.19531956315040588,
      "learning_rate": 3.245416666666667e-05,
      "loss": 0.0014,
      "step": 42110
    },
    {
      "epoch": 2.808,
      "grad_norm": 0.22817689180374146,
      "learning_rate": 3.245e-05,
      "loss": 0.0026,
      "step": 42120
    },
    {
      "epoch": 2.8086666666666664,
      "grad_norm": 0.4791575074195862,
      "learning_rate": 3.2445833333333334e-05,
      "loss": 0.0014,
      "step": 42130
    },
    {
      "epoch": 2.8093333333333335,
      "grad_norm": 0.7462943196296692,
      "learning_rate": 3.244166666666667e-05,
      "loss": 0.0027,
      "step": 42140
    },
    {
      "epoch": 2.81,
      "grad_norm": 0.8360211253166199,
      "learning_rate": 3.24375e-05,
      "loss": 0.0019,
      "step": 42150
    },
    {
      "epoch": 2.8106666666666666,
      "grad_norm": 0.07943098247051239,
      "learning_rate": 3.243333333333333e-05,
      "loss": 0.0015,
      "step": 42160
    },
    {
      "epoch": 2.8113333333333332,
      "grad_norm": 0.6257185935974121,
      "learning_rate": 3.2429166666666664e-05,
      "loss": 0.0019,
      "step": 42170
    },
    {
      "epoch": 2.8120000000000003,
      "grad_norm": 0.488416850566864,
      "learning_rate": 3.2425e-05,
      "loss": 0.0018,
      "step": 42180
    },
    {
      "epoch": 2.812666666666667,
      "grad_norm": 0.2550347149372101,
      "learning_rate": 3.242083333333333e-05,
      "loss": 0.0014,
      "step": 42190
    },
    {
      "epoch": 2.8133333333333335,
      "grad_norm": 0.18224380910396576,
      "learning_rate": 3.2416666666666664e-05,
      "loss": 0.0019,
      "step": 42200
    },
    {
      "epoch": 2.814,
      "grad_norm": 0.2997909486293793,
      "learning_rate": 3.24125e-05,
      "loss": 0.0018,
      "step": 42210
    },
    {
      "epoch": 2.8146666666666667,
      "grad_norm": 0.6018239855766296,
      "learning_rate": 3.240833333333333e-05,
      "loss": 0.0036,
      "step": 42220
    },
    {
      "epoch": 2.8153333333333332,
      "grad_norm": 0.11913862824440002,
      "learning_rate": 3.240416666666667e-05,
      "loss": 0.0031,
      "step": 42230
    },
    {
      "epoch": 2.816,
      "grad_norm": 0.32071492075920105,
      "learning_rate": 3.24e-05,
      "loss": 0.0022,
      "step": 42240
    },
    {
      "epoch": 2.8166666666666664,
      "grad_norm": 0.2704734802246094,
      "learning_rate": 3.239583333333334e-05,
      "loss": 0.003,
      "step": 42250
    },
    {
      "epoch": 2.8173333333333335,
      "grad_norm": 0.1166609674692154,
      "learning_rate": 3.239166666666667e-05,
      "loss": 0.0022,
      "step": 42260
    },
    {
      "epoch": 2.818,
      "grad_norm": 0.7294671535491943,
      "learning_rate": 3.23875e-05,
      "loss": 0.0015,
      "step": 42270
    },
    {
      "epoch": 2.8186666666666667,
      "grad_norm": 0.36040443181991577,
      "learning_rate": 3.238333333333333e-05,
      "loss": 0.0019,
      "step": 42280
    },
    {
      "epoch": 2.8193333333333332,
      "grad_norm": 0.1555458903312683,
      "learning_rate": 3.237916666666666e-05,
      "loss": 0.0015,
      "step": 42290
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.1649322807788849,
      "learning_rate": 3.2375e-05,
      "loss": 0.0025,
      "step": 42300
    },
    {
      "epoch": 2.820666666666667,
      "grad_norm": 0.19057896733283997,
      "learning_rate": 3.237083333333333e-05,
      "loss": 0.0024,
      "step": 42310
    },
    {
      "epoch": 2.8213333333333335,
      "grad_norm": 0.6628713607788086,
      "learning_rate": 3.236666666666667e-05,
      "loss": 0.0016,
      "step": 42320
    },
    {
      "epoch": 2.822,
      "grad_norm": 0.19470442831516266,
      "learning_rate": 3.23625e-05,
      "loss": 0.0016,
      "step": 42330
    },
    {
      "epoch": 2.8226666666666667,
      "grad_norm": 0.6610196232795715,
      "learning_rate": 3.235833333333333e-05,
      "loss": 0.0018,
      "step": 42340
    },
    {
      "epoch": 2.8233333333333333,
      "grad_norm": 0.3939497470855713,
      "learning_rate": 3.235416666666667e-05,
      "loss": 0.0022,
      "step": 42350
    },
    {
      "epoch": 2.824,
      "grad_norm": 0.4069526791572571,
      "learning_rate": 3.235e-05,
      "loss": 0.002,
      "step": 42360
    },
    {
      "epoch": 2.8246666666666664,
      "grad_norm": 0.10868164151906967,
      "learning_rate": 3.234583333333334e-05,
      "loss": 0.0021,
      "step": 42370
    },
    {
      "epoch": 2.8253333333333335,
      "grad_norm": 0.2225663810968399,
      "learning_rate": 3.234166666666667e-05,
      "loss": 0.0014,
      "step": 42380
    },
    {
      "epoch": 2.826,
      "grad_norm": 0.09967140853404999,
      "learning_rate": 3.233750000000001e-05,
      "loss": 0.0021,
      "step": 42390
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 0.6482926607131958,
      "learning_rate": 3.233333333333333e-05,
      "loss": 0.0015,
      "step": 42400
    },
    {
      "epoch": 2.8273333333333333,
      "grad_norm": 0.061458609998226166,
      "learning_rate": 3.232916666666667e-05,
      "loss": 0.003,
      "step": 42410
    },
    {
      "epoch": 2.828,
      "grad_norm": 0.12122306227684021,
      "learning_rate": 3.2325e-05,
      "loss": 0.0013,
      "step": 42420
    },
    {
      "epoch": 2.828666666666667,
      "grad_norm": 0.6315228343009949,
      "learning_rate": 3.232083333333333e-05,
      "loss": 0.0023,
      "step": 42430
    },
    {
      "epoch": 2.8293333333333335,
      "grad_norm": 0.7643763422966003,
      "learning_rate": 3.231666666666667e-05,
      "loss": 0.0019,
      "step": 42440
    },
    {
      "epoch": 2.83,
      "grad_norm": 0.036996692419052124,
      "learning_rate": 3.23125e-05,
      "loss": 0.0015,
      "step": 42450
    },
    {
      "epoch": 2.8306666666666667,
      "grad_norm": 0.32980409264564514,
      "learning_rate": 3.230833333333334e-05,
      "loss": 0.003,
      "step": 42460
    },
    {
      "epoch": 2.8313333333333333,
      "grad_norm": 0.23184214532375336,
      "learning_rate": 3.230416666666667e-05,
      "loss": 0.0016,
      "step": 42470
    },
    {
      "epoch": 2.832,
      "grad_norm": 0.35128241777420044,
      "learning_rate": 3.2300000000000006e-05,
      "loss": 0.004,
      "step": 42480
    },
    {
      "epoch": 2.8326666666666664,
      "grad_norm": 0.5288267135620117,
      "learning_rate": 3.229583333333334e-05,
      "loss": 0.0028,
      "step": 42490
    },
    {
      "epoch": 2.8333333333333335,
      "grad_norm": 0.6785536408424377,
      "learning_rate": 3.229166666666667e-05,
      "loss": 0.002,
      "step": 42500
    },
    {
      "epoch": 2.834,
      "grad_norm": 0.07748769223690033,
      "learning_rate": 3.2287500000000006e-05,
      "loss": 0.0018,
      "step": 42510
    },
    {
      "epoch": 2.8346666666666667,
      "grad_norm": 0.21130788326263428,
      "learning_rate": 3.2283333333333337e-05,
      "loss": 0.0023,
      "step": 42520
    },
    {
      "epoch": 2.8353333333333333,
      "grad_norm": 1.052440881729126,
      "learning_rate": 3.227916666666667e-05,
      "loss": 0.002,
      "step": 42530
    },
    {
      "epoch": 2.836,
      "grad_norm": 0.4433210492134094,
      "learning_rate": 3.2275e-05,
      "loss": 0.0031,
      "step": 42540
    },
    {
      "epoch": 2.836666666666667,
      "grad_norm": 0.1135815754532814,
      "learning_rate": 3.2270833333333336e-05,
      "loss": 0.0016,
      "step": 42550
    },
    {
      "epoch": 2.8373333333333335,
      "grad_norm": 0.42872992157936096,
      "learning_rate": 3.226666666666667e-05,
      "loss": 0.0019,
      "step": 42560
    },
    {
      "epoch": 2.838,
      "grad_norm": 0.3086528778076172,
      "learning_rate": 3.22625e-05,
      "loss": 0.0023,
      "step": 42570
    },
    {
      "epoch": 2.8386666666666667,
      "grad_norm": 0.33798709511756897,
      "learning_rate": 3.2258333333333336e-05,
      "loss": 0.0022,
      "step": 42580
    },
    {
      "epoch": 2.8393333333333333,
      "grad_norm": 0.21391016244888306,
      "learning_rate": 3.225416666666667e-05,
      "loss": 0.0024,
      "step": 42590
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.10823935270309448,
      "learning_rate": 3.2250000000000005e-05,
      "loss": 0.0021,
      "step": 42600
    },
    {
      "epoch": 2.8406666666666665,
      "grad_norm": 0.4625557065010071,
      "learning_rate": 3.2245833333333336e-05,
      "loss": 0.0034,
      "step": 42610
    },
    {
      "epoch": 2.8413333333333335,
      "grad_norm": 0.1834861934185028,
      "learning_rate": 3.224166666666667e-05,
      "loss": 0.0017,
      "step": 42620
    },
    {
      "epoch": 2.842,
      "grad_norm": 0.047986309975385666,
      "learning_rate": 3.2237500000000004e-05,
      "loss": 0.0027,
      "step": 42630
    },
    {
      "epoch": 2.8426666666666667,
      "grad_norm": 0.2325703352689743,
      "learning_rate": 3.2233333333333335e-05,
      "loss": 0.0026,
      "step": 42640
    },
    {
      "epoch": 2.8433333333333333,
      "grad_norm": 0.6475822329521179,
      "learning_rate": 3.2229166666666666e-05,
      "loss": 0.0021,
      "step": 42650
    },
    {
      "epoch": 2.844,
      "grad_norm": 0.5994831323623657,
      "learning_rate": 3.2225e-05,
      "loss": 0.003,
      "step": 42660
    },
    {
      "epoch": 2.844666666666667,
      "grad_norm": 0.7124294638633728,
      "learning_rate": 3.2220833333333335e-05,
      "loss": 0.0021,
      "step": 42670
    },
    {
      "epoch": 2.8453333333333335,
      "grad_norm": 0.1130778044462204,
      "learning_rate": 3.2216666666666666e-05,
      "loss": 0.0021,
      "step": 42680
    },
    {
      "epoch": 2.846,
      "grad_norm": 0.1913333237171173,
      "learning_rate": 3.2212500000000004e-05,
      "loss": 0.0025,
      "step": 42690
    },
    {
      "epoch": 2.8466666666666667,
      "grad_norm": 0.3437506854534149,
      "learning_rate": 3.2208333333333335e-05,
      "loss": 0.0033,
      "step": 42700
    },
    {
      "epoch": 2.8473333333333333,
      "grad_norm": 0.09331446886062622,
      "learning_rate": 3.2204166666666666e-05,
      "loss": 0.0018,
      "step": 42710
    },
    {
      "epoch": 2.848,
      "grad_norm": 0.2640396058559418,
      "learning_rate": 3.2200000000000003e-05,
      "loss": 0.0024,
      "step": 42720
    },
    {
      "epoch": 2.8486666666666665,
      "grad_norm": 0.2645183801651001,
      "learning_rate": 3.2195833333333334e-05,
      "loss": 0.0021,
      "step": 42730
    },
    {
      "epoch": 2.8493333333333335,
      "grad_norm": 0.6388124823570251,
      "learning_rate": 3.219166666666667e-05,
      "loss": 0.0019,
      "step": 42740
    },
    {
      "epoch": 2.85,
      "grad_norm": 0.14539837837219238,
      "learning_rate": 3.21875e-05,
      "loss": 0.0025,
      "step": 42750
    },
    {
      "epoch": 2.8506666666666667,
      "grad_norm": 0.22692377865314484,
      "learning_rate": 3.218333333333334e-05,
      "loss": 0.0027,
      "step": 42760
    },
    {
      "epoch": 2.8513333333333333,
      "grad_norm": 0.8459457755088806,
      "learning_rate": 3.2179166666666665e-05,
      "loss": 0.0029,
      "step": 42770
    },
    {
      "epoch": 2.852,
      "grad_norm": 0.22649137675762177,
      "learning_rate": 3.2175e-05,
      "loss": 0.0027,
      "step": 42780
    },
    {
      "epoch": 2.852666666666667,
      "grad_norm": 0.08232797682285309,
      "learning_rate": 3.2170833333333334e-05,
      "loss": 0.0029,
      "step": 42790
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 0.5177240371704102,
      "learning_rate": 3.2166666666666665e-05,
      "loss": 0.0017,
      "step": 42800
    },
    {
      "epoch": 2.854,
      "grad_norm": 0.5206912755966187,
      "learning_rate": 3.21625e-05,
      "loss": 0.0034,
      "step": 42810
    },
    {
      "epoch": 2.8546666666666667,
      "grad_norm": 0.22304750978946686,
      "learning_rate": 3.2158333333333333e-05,
      "loss": 0.0012,
      "step": 42820
    },
    {
      "epoch": 2.8553333333333333,
      "grad_norm": 0.05556610971689224,
      "learning_rate": 3.215416666666667e-05,
      "loss": 0.003,
      "step": 42830
    },
    {
      "epoch": 2.856,
      "grad_norm": 0.3580516278743744,
      "learning_rate": 3.215e-05,
      "loss": 0.0024,
      "step": 42840
    },
    {
      "epoch": 2.8566666666666665,
      "grad_norm": 0.35622939467430115,
      "learning_rate": 3.214583333333333e-05,
      "loss": 0.0019,
      "step": 42850
    },
    {
      "epoch": 2.857333333333333,
      "grad_norm": 0.4198882579803467,
      "learning_rate": 3.214166666666667e-05,
      "loss": 0.0024,
      "step": 42860
    },
    {
      "epoch": 2.858,
      "grad_norm": 0.19258491694927216,
      "learning_rate": 3.21375e-05,
      "loss": 0.0019,
      "step": 42870
    },
    {
      "epoch": 2.8586666666666667,
      "grad_norm": 0.1832585483789444,
      "learning_rate": 3.213333333333334e-05,
      "loss": 0.0015,
      "step": 42880
    },
    {
      "epoch": 2.8593333333333333,
      "grad_norm": 0.20493152737617493,
      "learning_rate": 3.2129166666666664e-05,
      "loss": 0.0019,
      "step": 42890
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.18567726016044617,
      "learning_rate": 3.2125e-05,
      "loss": 0.0023,
      "step": 42900
    },
    {
      "epoch": 2.860666666666667,
      "grad_norm": 0.384235680103302,
      "learning_rate": 3.212083333333333e-05,
      "loss": 0.0023,
      "step": 42910
    },
    {
      "epoch": 2.8613333333333335,
      "grad_norm": 0.6236652731895447,
      "learning_rate": 3.211666666666667e-05,
      "loss": 0.0024,
      "step": 42920
    },
    {
      "epoch": 2.862,
      "grad_norm": 0.1997375339269638,
      "learning_rate": 3.21125e-05,
      "loss": 0.0015,
      "step": 42930
    },
    {
      "epoch": 2.8626666666666667,
      "grad_norm": 0.6209093332290649,
      "learning_rate": 3.210833333333333e-05,
      "loss": 0.0017,
      "step": 42940
    },
    {
      "epoch": 2.8633333333333333,
      "grad_norm": 0.07171235233545303,
      "learning_rate": 3.210416666666667e-05,
      "loss": 0.0027,
      "step": 42950
    },
    {
      "epoch": 2.864,
      "grad_norm": 0.16455216705799103,
      "learning_rate": 3.21e-05,
      "loss": 0.0026,
      "step": 42960
    },
    {
      "epoch": 2.8646666666666665,
      "grad_norm": 0.7559261918067932,
      "learning_rate": 3.209583333333334e-05,
      "loss": 0.0017,
      "step": 42970
    },
    {
      "epoch": 2.865333333333333,
      "grad_norm": 0.5068577527999878,
      "learning_rate": 3.209166666666667e-05,
      "loss": 0.0019,
      "step": 42980
    },
    {
      "epoch": 2.866,
      "grad_norm": 0.15451088547706604,
      "learning_rate": 3.20875e-05,
      "loss": 0.0024,
      "step": 42990
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 0.7012469172477722,
      "learning_rate": 3.208333333333334e-05,
      "loss": 0.0027,
      "step": 43000
    },
    {
      "epoch": 2.8673333333333333,
      "grad_norm": 0.2663310766220093,
      "learning_rate": 3.207916666666666e-05,
      "loss": 0.0011,
      "step": 43010
    },
    {
      "epoch": 2.868,
      "grad_norm": 0.04420949146151543,
      "learning_rate": 3.2075e-05,
      "loss": 0.002,
      "step": 43020
    },
    {
      "epoch": 2.868666666666667,
      "grad_norm": 0.07542889565229416,
      "learning_rate": 3.207083333333333e-05,
      "loss": 0.002,
      "step": 43030
    },
    {
      "epoch": 2.8693333333333335,
      "grad_norm": 0.6754747033119202,
      "learning_rate": 3.206666666666667e-05,
      "loss": 0.0018,
      "step": 43040
    },
    {
      "epoch": 2.87,
      "grad_norm": 1.1249024868011475,
      "learning_rate": 3.20625e-05,
      "loss": 0.0016,
      "step": 43050
    },
    {
      "epoch": 2.8706666666666667,
      "grad_norm": 0.37012678384780884,
      "learning_rate": 3.205833333333334e-05,
      "loss": 0.0021,
      "step": 43060
    },
    {
      "epoch": 2.8713333333333333,
      "grad_norm": 1.0431170463562012,
      "learning_rate": 3.205416666666667e-05,
      "loss": 0.0012,
      "step": 43070
    },
    {
      "epoch": 2.872,
      "grad_norm": 0.12126059085130692,
      "learning_rate": 3.205e-05,
      "loss": 0.0033,
      "step": 43080
    },
    {
      "epoch": 2.8726666666666665,
      "grad_norm": 0.7080467939376831,
      "learning_rate": 3.204583333333334e-05,
      "loss": 0.0021,
      "step": 43090
    },
    {
      "epoch": 2.873333333333333,
      "grad_norm": 0.15935018658638,
      "learning_rate": 3.204166666666667e-05,
      "loss": 0.0028,
      "step": 43100
    },
    {
      "epoch": 2.874,
      "grad_norm": 0.0768609568476677,
      "learning_rate": 3.2037500000000006e-05,
      "loss": 0.0021,
      "step": 43110
    },
    {
      "epoch": 2.8746666666666667,
      "grad_norm": 0.5155206918716431,
      "learning_rate": 3.203333333333334e-05,
      "loss": 0.0023,
      "step": 43120
    },
    {
      "epoch": 2.8753333333333333,
      "grad_norm": 0.32895612716674805,
      "learning_rate": 3.202916666666667e-05,
      "loss": 0.0032,
      "step": 43130
    },
    {
      "epoch": 2.876,
      "grad_norm": 0.5963144898414612,
      "learning_rate": 3.2025e-05,
      "loss": 0.0021,
      "step": 43140
    },
    {
      "epoch": 2.876666666666667,
      "grad_norm": 0.5613356232643127,
      "learning_rate": 3.202083333333333e-05,
      "loss": 0.0024,
      "step": 43150
    },
    {
      "epoch": 2.8773333333333335,
      "grad_norm": 1.1396152973175049,
      "learning_rate": 3.201666666666667e-05,
      "loss": 0.0018,
      "step": 43160
    },
    {
      "epoch": 2.878,
      "grad_norm": 0.8895727396011353,
      "learning_rate": 3.20125e-05,
      "loss": 0.0034,
      "step": 43170
    },
    {
      "epoch": 2.8786666666666667,
      "grad_norm": 0.7654352784156799,
      "learning_rate": 3.2008333333333337e-05,
      "loss": 0.0024,
      "step": 43180
    },
    {
      "epoch": 2.8793333333333333,
      "grad_norm": 0.7256985306739807,
      "learning_rate": 3.200416666666667e-05,
      "loss": 0.0025,
      "step": 43190
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.13039757311344147,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.0026,
      "step": 43200
    },
    {
      "epoch": 2.8806666666666665,
      "grad_norm": 0.05598822236061096,
      "learning_rate": 3.1995833333333336e-05,
      "loss": 0.0037,
      "step": 43210
    },
    {
      "epoch": 2.881333333333333,
      "grad_norm": 0.07370471209287643,
      "learning_rate": 3.199166666666667e-05,
      "loss": 0.0023,
      "step": 43220
    },
    {
      "epoch": 2.882,
      "grad_norm": 0.4294130802154541,
      "learning_rate": 3.1987500000000005e-05,
      "loss": 0.0023,
      "step": 43230
    },
    {
      "epoch": 2.8826666666666667,
      "grad_norm": 0.13589884340763092,
      "learning_rate": 3.1983333333333336e-05,
      "loss": 0.0015,
      "step": 43240
    },
    {
      "epoch": 2.8833333333333333,
      "grad_norm": 0.6260533332824707,
      "learning_rate": 3.197916666666667e-05,
      "loss": 0.0017,
      "step": 43250
    },
    {
      "epoch": 2.884,
      "grad_norm": 0.44826772809028625,
      "learning_rate": 3.1975e-05,
      "loss": 0.0021,
      "step": 43260
    },
    {
      "epoch": 2.884666666666667,
      "grad_norm": 0.6367394328117371,
      "learning_rate": 3.1970833333333336e-05,
      "loss": 0.0029,
      "step": 43270
    },
    {
      "epoch": 2.8853333333333335,
      "grad_norm": 0.3328740894794464,
      "learning_rate": 3.196666666666667e-05,
      "loss": 0.0024,
      "step": 43280
    },
    {
      "epoch": 2.886,
      "grad_norm": 0.2351277619600296,
      "learning_rate": 3.19625e-05,
      "loss": 0.0019,
      "step": 43290
    },
    {
      "epoch": 2.8866666666666667,
      "grad_norm": 0.6752275228500366,
      "learning_rate": 3.1958333333333335e-05,
      "loss": 0.0014,
      "step": 43300
    },
    {
      "epoch": 2.8873333333333333,
      "grad_norm": 1.0414878129959106,
      "learning_rate": 3.1954166666666666e-05,
      "loss": 0.0022,
      "step": 43310
    },
    {
      "epoch": 2.888,
      "grad_norm": 0.4472544193267822,
      "learning_rate": 3.1950000000000004e-05,
      "loss": 0.002,
      "step": 43320
    },
    {
      "epoch": 2.8886666666666665,
      "grad_norm": 0.030247660353779793,
      "learning_rate": 3.1945833333333335e-05,
      "loss": 0.0018,
      "step": 43330
    },
    {
      "epoch": 2.889333333333333,
      "grad_norm": 0.04778634011745453,
      "learning_rate": 3.194166666666667e-05,
      "loss": 0.0024,
      "step": 43340
    },
    {
      "epoch": 2.89,
      "grad_norm": 0.47220417857170105,
      "learning_rate": 3.1937500000000004e-05,
      "loss": 0.0017,
      "step": 43350
    },
    {
      "epoch": 2.8906666666666667,
      "grad_norm": 0.4361531138420105,
      "learning_rate": 3.1933333333333335e-05,
      "loss": 0.0022,
      "step": 43360
    },
    {
      "epoch": 2.8913333333333333,
      "grad_norm": 0.22996771335601807,
      "learning_rate": 3.1929166666666666e-05,
      "loss": 0.0049,
      "step": 43370
    },
    {
      "epoch": 2.892,
      "grad_norm": 0.5296840071678162,
      "learning_rate": 3.1925e-05,
      "loss": 0.0027,
      "step": 43380
    },
    {
      "epoch": 2.892666666666667,
      "grad_norm": 0.08711639046669006,
      "learning_rate": 3.1920833333333334e-05,
      "loss": 0.0024,
      "step": 43390
    },
    {
      "epoch": 2.8933333333333335,
      "grad_norm": 0.055279895663261414,
      "learning_rate": 3.1916666666666665e-05,
      "loss": 0.0031,
      "step": 43400
    },
    {
      "epoch": 2.894,
      "grad_norm": 0.6133958697319031,
      "learning_rate": 3.19125e-05,
      "loss": 0.0028,
      "step": 43410
    },
    {
      "epoch": 2.8946666666666667,
      "grad_norm": 0.22615736722946167,
      "learning_rate": 3.1908333333333334e-05,
      "loss": 0.002,
      "step": 43420
    },
    {
      "epoch": 2.8953333333333333,
      "grad_norm": 0.2639533281326294,
      "learning_rate": 3.1904166666666665e-05,
      "loss": 0.0028,
      "step": 43430
    },
    {
      "epoch": 2.896,
      "grad_norm": 0.1578693985939026,
      "learning_rate": 3.19e-05,
      "loss": 0.0018,
      "step": 43440
    },
    {
      "epoch": 2.8966666666666665,
      "grad_norm": 0.8584810495376587,
      "learning_rate": 3.1895833333333334e-05,
      "loss": 0.0024,
      "step": 43450
    },
    {
      "epoch": 2.897333333333333,
      "grad_norm": 0.2643825113773346,
      "learning_rate": 3.189166666666667e-05,
      "loss": 0.0022,
      "step": 43460
    },
    {
      "epoch": 2.898,
      "grad_norm": 0.2529900074005127,
      "learning_rate": 3.18875e-05,
      "loss": 0.0017,
      "step": 43470
    },
    {
      "epoch": 2.8986666666666667,
      "grad_norm": 0.4421508312225342,
      "learning_rate": 3.188333333333334e-05,
      "loss": 0.0018,
      "step": 43480
    },
    {
      "epoch": 2.8993333333333333,
      "grad_norm": 0.144248828291893,
      "learning_rate": 3.1879166666666665e-05,
      "loss": 0.0017,
      "step": 43490
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.4038203954696655,
      "learning_rate": 3.1875e-05,
      "loss": 0.0021,
      "step": 43500
    },
    {
      "epoch": 2.9006666666666665,
      "grad_norm": 0.8178604245185852,
      "learning_rate": 3.187083333333333e-05,
      "loss": 0.0016,
      "step": 43510
    },
    {
      "epoch": 2.9013333333333335,
      "grad_norm": 0.4960462152957916,
      "learning_rate": 3.1866666666666664e-05,
      "loss": 0.0019,
      "step": 43520
    },
    {
      "epoch": 2.902,
      "grad_norm": 0.11337937414646149,
      "learning_rate": 3.18625e-05,
      "loss": 0.0038,
      "step": 43530
    },
    {
      "epoch": 2.9026666666666667,
      "grad_norm": 0.445200115442276,
      "learning_rate": 3.185833333333333e-05,
      "loss": 0.0026,
      "step": 43540
    },
    {
      "epoch": 2.9033333333333333,
      "grad_norm": 0.04611461982131004,
      "learning_rate": 3.185416666666667e-05,
      "loss": 0.002,
      "step": 43550
    },
    {
      "epoch": 2.904,
      "grad_norm": 0.2237425595521927,
      "learning_rate": 3.185e-05,
      "loss": 0.0021,
      "step": 43560
    },
    {
      "epoch": 2.9046666666666665,
      "grad_norm": 0.07508045434951782,
      "learning_rate": 3.184583333333333e-05,
      "loss": 0.0021,
      "step": 43570
    },
    {
      "epoch": 2.905333333333333,
      "grad_norm": 0.6592945456504822,
      "learning_rate": 3.184166666666667e-05,
      "loss": 0.002,
      "step": 43580
    },
    {
      "epoch": 2.906,
      "grad_norm": 0.5832246541976929,
      "learning_rate": 3.18375e-05,
      "loss": 0.0015,
      "step": 43590
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 0.1505698710680008,
      "learning_rate": 3.183333333333334e-05,
      "loss": 0.0027,
      "step": 43600
    },
    {
      "epoch": 2.9073333333333333,
      "grad_norm": 0.6758555173873901,
      "learning_rate": 3.182916666666666e-05,
      "loss": 0.0019,
      "step": 43610
    },
    {
      "epoch": 2.908,
      "grad_norm": 0.26957058906555176,
      "learning_rate": 3.1825e-05,
      "loss": 0.0026,
      "step": 43620
    },
    {
      "epoch": 2.9086666666666665,
      "grad_norm": 0.5985391736030579,
      "learning_rate": 3.182083333333333e-05,
      "loss": 0.002,
      "step": 43630
    },
    {
      "epoch": 2.9093333333333335,
      "grad_norm": 0.19061221182346344,
      "learning_rate": 3.181666666666667e-05,
      "loss": 0.0018,
      "step": 43640
    },
    {
      "epoch": 2.91,
      "grad_norm": 0.20945505797863007,
      "learning_rate": 3.18125e-05,
      "loss": 0.0023,
      "step": 43650
    },
    {
      "epoch": 2.9106666666666667,
      "grad_norm": 0.7866327166557312,
      "learning_rate": 3.180833333333333e-05,
      "loss": 0.0029,
      "step": 43660
    },
    {
      "epoch": 2.9113333333333333,
      "grad_norm": 0.403780460357666,
      "learning_rate": 3.180416666666667e-05,
      "loss": 0.0019,
      "step": 43670
    },
    {
      "epoch": 2.912,
      "grad_norm": 0.34120500087738037,
      "learning_rate": 3.18e-05,
      "loss": 0.0033,
      "step": 43680
    },
    {
      "epoch": 2.9126666666666665,
      "grad_norm": 0.6594022512435913,
      "learning_rate": 3.179583333333334e-05,
      "loss": 0.0035,
      "step": 43690
    },
    {
      "epoch": 2.913333333333333,
      "grad_norm": 0.1715932935476303,
      "learning_rate": 3.179166666666667e-05,
      "loss": 0.0026,
      "step": 43700
    },
    {
      "epoch": 2.914,
      "grad_norm": 0.11119546741247177,
      "learning_rate": 3.17875e-05,
      "loss": 0.002,
      "step": 43710
    },
    {
      "epoch": 2.9146666666666667,
      "grad_norm": 0.5637156963348389,
      "learning_rate": 3.178333333333334e-05,
      "loss": 0.0023,
      "step": 43720
    },
    {
      "epoch": 2.9153333333333333,
      "grad_norm": 0.45890578627586365,
      "learning_rate": 3.177916666666666e-05,
      "loss": 0.0027,
      "step": 43730
    },
    {
      "epoch": 2.916,
      "grad_norm": 0.7060915231704712,
      "learning_rate": 3.1775e-05,
      "loss": 0.0016,
      "step": 43740
    },
    {
      "epoch": 2.9166666666666665,
      "grad_norm": 0.30875837802886963,
      "learning_rate": 3.177083333333333e-05,
      "loss": 0.0021,
      "step": 43750
    },
    {
      "epoch": 2.9173333333333336,
      "grad_norm": 0.06169969215989113,
      "learning_rate": 3.176666666666667e-05,
      "loss": 0.0028,
      "step": 43760
    },
    {
      "epoch": 2.918,
      "grad_norm": 0.6619386672973633,
      "learning_rate": 3.17625e-05,
      "loss": 0.0022,
      "step": 43770
    },
    {
      "epoch": 2.9186666666666667,
      "grad_norm": 0.555359423160553,
      "learning_rate": 3.175833333333334e-05,
      "loss": 0.0017,
      "step": 43780
    },
    {
      "epoch": 2.9193333333333333,
      "grad_norm": 0.5791242718696594,
      "learning_rate": 3.175416666666667e-05,
      "loss": 0.0026,
      "step": 43790
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.05776045098900795,
      "learning_rate": 3.175e-05,
      "loss": 0.0019,
      "step": 43800
    },
    {
      "epoch": 2.9206666666666665,
      "grad_norm": 0.44639676809310913,
      "learning_rate": 3.174583333333334e-05,
      "loss": 0.0017,
      "step": 43810
    },
    {
      "epoch": 2.921333333333333,
      "grad_norm": 0.8047650456428528,
      "learning_rate": 3.174166666666667e-05,
      "loss": 0.0025,
      "step": 43820
    },
    {
      "epoch": 2.922,
      "grad_norm": 0.5204234719276428,
      "learning_rate": 3.1737500000000006e-05,
      "loss": 0.0027,
      "step": 43830
    },
    {
      "epoch": 2.9226666666666667,
      "grad_norm": 0.4781040549278259,
      "learning_rate": 3.173333333333334e-05,
      "loss": 0.002,
      "step": 43840
    },
    {
      "epoch": 2.9233333333333333,
      "grad_norm": 0.06695108115673065,
      "learning_rate": 3.172916666666667e-05,
      "loss": 0.0022,
      "step": 43850
    },
    {
      "epoch": 2.924,
      "grad_norm": 0.058733001351356506,
      "learning_rate": 3.1725e-05,
      "loss": 0.0034,
      "step": 43860
    },
    {
      "epoch": 2.9246666666666665,
      "grad_norm": 0.5180479288101196,
      "learning_rate": 3.172083333333333e-05,
      "loss": 0.0022,
      "step": 43870
    },
    {
      "epoch": 2.9253333333333336,
      "grad_norm": 0.2160089761018753,
      "learning_rate": 3.171666666666667e-05,
      "loss": 0.0033,
      "step": 43880
    },
    {
      "epoch": 2.926,
      "grad_norm": 0.8550332188606262,
      "learning_rate": 3.17125e-05,
      "loss": 0.0022,
      "step": 43890
    },
    {
      "epoch": 2.9266666666666667,
      "grad_norm": 0.2727696895599365,
      "learning_rate": 3.1708333333333336e-05,
      "loss": 0.0021,
      "step": 43900
    },
    {
      "epoch": 2.9273333333333333,
      "grad_norm": 0.11864494532346725,
      "learning_rate": 3.170416666666667e-05,
      "loss": 0.0021,
      "step": 43910
    },
    {
      "epoch": 2.928,
      "grad_norm": 0.37668368220329285,
      "learning_rate": 3.1700000000000005e-05,
      "loss": 0.0017,
      "step": 43920
    },
    {
      "epoch": 2.9286666666666665,
      "grad_norm": 0.15292172133922577,
      "learning_rate": 3.1695833333333336e-05,
      "loss": 0.0021,
      "step": 43930
    },
    {
      "epoch": 2.929333333333333,
      "grad_norm": 0.24332401156425476,
      "learning_rate": 3.169166666666667e-05,
      "loss": 0.0021,
      "step": 43940
    },
    {
      "epoch": 2.93,
      "grad_norm": 0.6093634963035583,
      "learning_rate": 3.1687500000000005e-05,
      "loss": 0.0023,
      "step": 43950
    },
    {
      "epoch": 2.9306666666666668,
      "grad_norm": 0.15685267746448517,
      "learning_rate": 3.1683333333333335e-05,
      "loss": 0.0021,
      "step": 43960
    },
    {
      "epoch": 2.9313333333333333,
      "grad_norm": 0.06317798048257828,
      "learning_rate": 3.167916666666667e-05,
      "loss": 0.0025,
      "step": 43970
    },
    {
      "epoch": 2.932,
      "grad_norm": 0.4095531105995178,
      "learning_rate": 3.1675e-05,
      "loss": 0.002,
      "step": 43980
    },
    {
      "epoch": 2.9326666666666665,
      "grad_norm": 0.1769823431968689,
      "learning_rate": 3.1670833333333335e-05,
      "loss": 0.0017,
      "step": 43990
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 0.4220508933067322,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 0.0021,
      "step": 44000
    },
    {
      "epoch": 2.934,
      "grad_norm": 0.04274710640311241,
      "learning_rate": 3.16625e-05,
      "loss": 0.0022,
      "step": 44010
    },
    {
      "epoch": 2.9346666666666668,
      "grad_norm": 0.35129204392433167,
      "learning_rate": 3.1658333333333335e-05,
      "loss": 0.0018,
      "step": 44020
    },
    {
      "epoch": 2.9353333333333333,
      "grad_norm": 0.7250564098358154,
      "learning_rate": 3.1654166666666666e-05,
      "loss": 0.0026,
      "step": 44030
    },
    {
      "epoch": 2.936,
      "grad_norm": 0.29669249057769775,
      "learning_rate": 3.1650000000000004e-05,
      "loss": 0.0021,
      "step": 44040
    },
    {
      "epoch": 2.9366666666666665,
      "grad_norm": 0.044196635484695435,
      "learning_rate": 3.1645833333333335e-05,
      "loss": 0.0024,
      "step": 44050
    },
    {
      "epoch": 2.937333333333333,
      "grad_norm": 0.3536028265953064,
      "learning_rate": 3.164166666666667e-05,
      "loss": 0.0022,
      "step": 44060
    },
    {
      "epoch": 2.9379999999999997,
      "grad_norm": 0.10314130038022995,
      "learning_rate": 3.16375e-05,
      "loss": 0.0037,
      "step": 44070
    },
    {
      "epoch": 2.9386666666666668,
      "grad_norm": 0.14572004973888397,
      "learning_rate": 3.1633333333333334e-05,
      "loss": 0.002,
      "step": 44080
    },
    {
      "epoch": 2.9393333333333334,
      "grad_norm": 0.7114430665969849,
      "learning_rate": 3.162916666666667e-05,
      "loss": 0.0027,
      "step": 44090
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.21416184306144714,
      "learning_rate": 3.1624999999999996e-05,
      "loss": 0.0024,
      "step": 44100
    },
    {
      "epoch": 2.9406666666666665,
      "grad_norm": 0.48622024059295654,
      "learning_rate": 3.1620833333333334e-05,
      "loss": 0.0028,
      "step": 44110
    },
    {
      "epoch": 2.9413333333333336,
      "grad_norm": 0.6367545127868652,
      "learning_rate": 3.1616666666666665e-05,
      "loss": 0.0023,
      "step": 44120
    },
    {
      "epoch": 2.942,
      "grad_norm": 0.1287143975496292,
      "learning_rate": 3.16125e-05,
      "loss": 0.0021,
      "step": 44130
    },
    {
      "epoch": 2.9426666666666668,
      "grad_norm": 0.7207971811294556,
      "learning_rate": 3.1608333333333334e-05,
      "loss": 0.003,
      "step": 44140
    },
    {
      "epoch": 2.9433333333333334,
      "grad_norm": 0.459061861038208,
      "learning_rate": 3.160416666666667e-05,
      "loss": 0.0027,
      "step": 44150
    },
    {
      "epoch": 2.944,
      "grad_norm": 0.47632312774658203,
      "learning_rate": 3.16e-05,
      "loss": 0.0015,
      "step": 44160
    },
    {
      "epoch": 2.9446666666666665,
      "grad_norm": 0.14715667068958282,
      "learning_rate": 3.159583333333333e-05,
      "loss": 0.002,
      "step": 44170
    },
    {
      "epoch": 2.945333333333333,
      "grad_norm": 0.4399241805076599,
      "learning_rate": 3.159166666666667e-05,
      "loss": 0.0023,
      "step": 44180
    },
    {
      "epoch": 2.9459999999999997,
      "grad_norm": 0.7103613018989563,
      "learning_rate": 3.15875e-05,
      "loss": 0.0028,
      "step": 44190
    },
    {
      "epoch": 2.9466666666666668,
      "grad_norm": 0.29304489493370056,
      "learning_rate": 3.158333333333334e-05,
      "loss": 0.0023,
      "step": 44200
    },
    {
      "epoch": 2.9473333333333334,
      "grad_norm": 0.37279167771339417,
      "learning_rate": 3.157916666666667e-05,
      "loss": 0.0022,
      "step": 44210
    },
    {
      "epoch": 2.948,
      "grad_norm": 0.12276595830917358,
      "learning_rate": 3.1575e-05,
      "loss": 0.002,
      "step": 44220
    },
    {
      "epoch": 2.9486666666666665,
      "grad_norm": 0.43505868315696716,
      "learning_rate": 3.157083333333333e-05,
      "loss": 0.0021,
      "step": 44230
    },
    {
      "epoch": 2.9493333333333336,
      "grad_norm": 0.29884785413742065,
      "learning_rate": 3.1566666666666664e-05,
      "loss": 0.0018,
      "step": 44240
    },
    {
      "epoch": 2.95,
      "grad_norm": 0.6075679659843445,
      "learning_rate": 3.15625e-05,
      "loss": 0.0024,
      "step": 44250
    },
    {
      "epoch": 2.9506666666666668,
      "grad_norm": 0.4342055320739746,
      "learning_rate": 3.155833333333333e-05,
      "loss": 0.0037,
      "step": 44260
    },
    {
      "epoch": 2.9513333333333334,
      "grad_norm": 0.8798474669456482,
      "learning_rate": 3.155416666666667e-05,
      "loss": 0.0027,
      "step": 44270
    },
    {
      "epoch": 2.952,
      "grad_norm": 0.8681951761245728,
      "learning_rate": 3.155e-05,
      "loss": 0.0016,
      "step": 44280
    },
    {
      "epoch": 2.9526666666666666,
      "grad_norm": 0.370819628238678,
      "learning_rate": 3.154583333333334e-05,
      "loss": 0.0024,
      "step": 44290
    },
    {
      "epoch": 2.953333333333333,
      "grad_norm": 0.8308403491973877,
      "learning_rate": 3.154166666666667e-05,
      "loss": 0.0036,
      "step": 44300
    },
    {
      "epoch": 2.9539999999999997,
      "grad_norm": 0.021636126562952995,
      "learning_rate": 3.15375e-05,
      "loss": 0.0015,
      "step": 44310
    },
    {
      "epoch": 2.9546666666666668,
      "grad_norm": 0.3698312044143677,
      "learning_rate": 3.153333333333334e-05,
      "loss": 0.0023,
      "step": 44320
    },
    {
      "epoch": 2.9553333333333334,
      "grad_norm": 0.05877668410539627,
      "learning_rate": 3.152916666666667e-05,
      "loss": 0.0022,
      "step": 44330
    },
    {
      "epoch": 2.956,
      "grad_norm": 0.5998774170875549,
      "learning_rate": 3.1525e-05,
      "loss": 0.0019,
      "step": 44340
    },
    {
      "epoch": 2.9566666666666666,
      "grad_norm": 0.7925334572792053,
      "learning_rate": 3.152083333333333e-05,
      "loss": 0.002,
      "step": 44350
    },
    {
      "epoch": 2.9573333333333336,
      "grad_norm": 1.1529161930084229,
      "learning_rate": 3.151666666666667e-05,
      "loss": 0.0024,
      "step": 44360
    },
    {
      "epoch": 2.958,
      "grad_norm": 0.9127397537231445,
      "learning_rate": 3.15125e-05,
      "loss": 0.003,
      "step": 44370
    },
    {
      "epoch": 2.958666666666667,
      "grad_norm": 0.02318514883518219,
      "learning_rate": 3.150833333333333e-05,
      "loss": 0.0015,
      "step": 44380
    },
    {
      "epoch": 2.9593333333333334,
      "grad_norm": 0.6028963327407837,
      "learning_rate": 3.150416666666667e-05,
      "loss": 0.0021,
      "step": 44390
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.26147088408470154,
      "learning_rate": 3.15e-05,
      "loss": 0.0019,
      "step": 44400
    },
    {
      "epoch": 2.9606666666666666,
      "grad_norm": 0.6086875200271606,
      "learning_rate": 3.149583333333334e-05,
      "loss": 0.0024,
      "step": 44410
    },
    {
      "epoch": 2.961333333333333,
      "grad_norm": 0.08802265673875809,
      "learning_rate": 3.149166666666667e-05,
      "loss": 0.0041,
      "step": 44420
    },
    {
      "epoch": 2.9619999999999997,
      "grad_norm": 0.07903539389371872,
      "learning_rate": 3.1487500000000006e-05,
      "loss": 0.0025,
      "step": 44430
    },
    {
      "epoch": 2.962666666666667,
      "grad_norm": 0.7049627900123596,
      "learning_rate": 3.148333333333334e-05,
      "loss": 0.0017,
      "step": 44440
    },
    {
      "epoch": 2.9633333333333334,
      "grad_norm": 0.9265470504760742,
      "learning_rate": 3.147916666666667e-05,
      "loss": 0.0019,
      "step": 44450
    },
    {
      "epoch": 2.964,
      "grad_norm": 0.08994997292757034,
      "learning_rate": 3.1475e-05,
      "loss": 0.002,
      "step": 44460
    },
    {
      "epoch": 2.9646666666666666,
      "grad_norm": 0.23361745476722717,
      "learning_rate": 3.147083333333333e-05,
      "loss": 0.002,
      "step": 44470
    },
    {
      "epoch": 2.9653333333333336,
      "grad_norm": 0.2872925102710724,
      "learning_rate": 3.146666666666667e-05,
      "loss": 0.0019,
      "step": 44480
    },
    {
      "epoch": 2.966,
      "grad_norm": 0.13305553793907166,
      "learning_rate": 3.14625e-05,
      "loss": 0.0026,
      "step": 44490
    },
    {
      "epoch": 2.966666666666667,
      "grad_norm": 0.3263596296310425,
      "learning_rate": 3.145833333333334e-05,
      "loss": 0.002,
      "step": 44500
    },
    {
      "epoch": 2.9673333333333334,
      "grad_norm": 0.09206957370042801,
      "learning_rate": 3.145416666666667e-05,
      "loss": 0.0032,
      "step": 44510
    },
    {
      "epoch": 2.968,
      "grad_norm": 0.19882306456565857,
      "learning_rate": 3.145e-05,
      "loss": 0.0022,
      "step": 44520
    },
    {
      "epoch": 2.9686666666666666,
      "grad_norm": 0.14853237569332123,
      "learning_rate": 3.1445833333333336e-05,
      "loss": 0.0033,
      "step": 44530
    },
    {
      "epoch": 2.969333333333333,
      "grad_norm": 0.6297246217727661,
      "learning_rate": 3.144166666666667e-05,
      "loss": 0.0014,
      "step": 44540
    },
    {
      "epoch": 2.9699999999999998,
      "grad_norm": 0.21308837831020355,
      "learning_rate": 3.1437500000000005e-05,
      "loss": 0.0019,
      "step": 44550
    },
    {
      "epoch": 2.970666666666667,
      "grad_norm": 0.7988212704658508,
      "learning_rate": 3.1433333333333336e-05,
      "loss": 0.0021,
      "step": 44560
    },
    {
      "epoch": 2.9713333333333334,
      "grad_norm": 0.11695531755685806,
      "learning_rate": 3.1429166666666674e-05,
      "loss": 0.0013,
      "step": 44570
    },
    {
      "epoch": 2.972,
      "grad_norm": 0.5309967398643494,
      "learning_rate": 3.1425e-05,
      "loss": 0.0016,
      "step": 44580
    },
    {
      "epoch": 2.9726666666666666,
      "grad_norm": 0.3413386046886444,
      "learning_rate": 3.1420833333333336e-05,
      "loss": 0.0031,
      "step": 44590
    },
    {
      "epoch": 2.9733333333333336,
      "grad_norm": 0.45724672079086304,
      "learning_rate": 3.141666666666667e-05,
      "loss": 0.0027,
      "step": 44600
    },
    {
      "epoch": 2.974,
      "grad_norm": 0.4561518430709839,
      "learning_rate": 3.14125e-05,
      "loss": 0.0022,
      "step": 44610
    },
    {
      "epoch": 2.974666666666667,
      "grad_norm": 0.5524665713310242,
      "learning_rate": 3.1408333333333336e-05,
      "loss": 0.0024,
      "step": 44620
    },
    {
      "epoch": 2.9753333333333334,
      "grad_norm": 0.9345069527626038,
      "learning_rate": 3.1404166666666667e-05,
      "loss": 0.0026,
      "step": 44630
    },
    {
      "epoch": 2.976,
      "grad_norm": 0.5837034583091736,
      "learning_rate": 3.1400000000000004e-05,
      "loss": 0.0018,
      "step": 44640
    },
    {
      "epoch": 2.9766666666666666,
      "grad_norm": 0.5760365724563599,
      "learning_rate": 3.1395833333333335e-05,
      "loss": 0.0023,
      "step": 44650
    },
    {
      "epoch": 2.977333333333333,
      "grad_norm": 0.7816122770309448,
      "learning_rate": 3.1391666666666666e-05,
      "loss": 0.0022,
      "step": 44660
    },
    {
      "epoch": 2.9779999999999998,
      "grad_norm": 0.3261915445327759,
      "learning_rate": 3.1387500000000004e-05,
      "loss": 0.002,
      "step": 44670
    },
    {
      "epoch": 2.978666666666667,
      "grad_norm": 0.7659595608711243,
      "learning_rate": 3.1383333333333335e-05,
      "loss": 0.0025,
      "step": 44680
    },
    {
      "epoch": 2.9793333333333334,
      "grad_norm": 0.11970192939043045,
      "learning_rate": 3.137916666666667e-05,
      "loss": 0.0023,
      "step": 44690
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.7407470941543579,
      "learning_rate": 3.1375e-05,
      "loss": 0.0015,
      "step": 44700
    },
    {
      "epoch": 2.9806666666666666,
      "grad_norm": 0.9131202697753906,
      "learning_rate": 3.1370833333333335e-05,
      "loss": 0.0016,
      "step": 44710
    },
    {
      "epoch": 2.981333333333333,
      "grad_norm": 0.14312918484210968,
      "learning_rate": 3.1366666666666666e-05,
      "loss": 0.0027,
      "step": 44720
    },
    {
      "epoch": 2.982,
      "grad_norm": 0.4682101011276245,
      "learning_rate": 3.13625e-05,
      "loss": 0.0023,
      "step": 44730
    },
    {
      "epoch": 2.982666666666667,
      "grad_norm": 0.30954229831695557,
      "learning_rate": 3.1358333333333334e-05,
      "loss": 0.0014,
      "step": 44740
    },
    {
      "epoch": 2.9833333333333334,
      "grad_norm": 0.3962000906467438,
      "learning_rate": 3.1354166666666665e-05,
      "loss": 0.002,
      "step": 44750
    },
    {
      "epoch": 2.984,
      "grad_norm": 0.41403669118881226,
      "learning_rate": 3.135e-05,
      "loss": 0.0028,
      "step": 44760
    },
    {
      "epoch": 2.9846666666666666,
      "grad_norm": 0.03184720128774643,
      "learning_rate": 3.1345833333333334e-05,
      "loss": 0.0022,
      "step": 44770
    },
    {
      "epoch": 2.985333333333333,
      "grad_norm": 0.4211147725582123,
      "learning_rate": 3.134166666666667e-05,
      "loss": 0.0014,
      "step": 44780
    },
    {
      "epoch": 2.9859999999999998,
      "grad_norm": 0.048444367945194244,
      "learning_rate": 3.13375e-05,
      "loss": 0.0023,
      "step": 44790
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 0.10734085738658905,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 0.0018,
      "step": 44800
    },
    {
      "epoch": 2.9873333333333334,
      "grad_norm": 0.35914337635040283,
      "learning_rate": 3.132916666666667e-05,
      "loss": 0.0029,
      "step": 44810
    },
    {
      "epoch": 2.988,
      "grad_norm": 0.37327492237091064,
      "learning_rate": 3.1324999999999996e-05,
      "loss": 0.0035,
      "step": 44820
    },
    {
      "epoch": 2.9886666666666666,
      "grad_norm": 0.03763146698474884,
      "learning_rate": 3.1320833333333333e-05,
      "loss": 0.0017,
      "step": 44830
    },
    {
      "epoch": 2.989333333333333,
      "grad_norm": 0.023041367530822754,
      "learning_rate": 3.1316666666666664e-05,
      "loss": 0.0017,
      "step": 44840
    },
    {
      "epoch": 2.99,
      "grad_norm": 0.22480928897857666,
      "learning_rate": 3.13125e-05,
      "loss": 0.0015,
      "step": 44850
    },
    {
      "epoch": 2.990666666666667,
      "grad_norm": 0.5559606552124023,
      "learning_rate": 3.130833333333333e-05,
      "loss": 0.0027,
      "step": 44860
    },
    {
      "epoch": 2.9913333333333334,
      "grad_norm": 0.07582706212997437,
      "learning_rate": 3.130416666666667e-05,
      "loss": 0.0025,
      "step": 44870
    },
    {
      "epoch": 2.992,
      "grad_norm": 0.07656228542327881,
      "learning_rate": 3.13e-05,
      "loss": 0.0023,
      "step": 44880
    },
    {
      "epoch": 2.9926666666666666,
      "grad_norm": 0.23836585879325867,
      "learning_rate": 3.129583333333333e-05,
      "loss": 0.0016,
      "step": 44890
    },
    {
      "epoch": 2.993333333333333,
      "grad_norm": 0.03880777209997177,
      "learning_rate": 3.129166666666667e-05,
      "loss": 0.003,
      "step": 44900
    },
    {
      "epoch": 2.9939999999999998,
      "grad_norm": 0.33479708433151245,
      "learning_rate": 3.12875e-05,
      "loss": 0.0026,
      "step": 44910
    },
    {
      "epoch": 2.994666666666667,
      "grad_norm": 0.36215174198150635,
      "learning_rate": 3.128333333333334e-05,
      "loss": 0.0021,
      "step": 44920
    },
    {
      "epoch": 2.9953333333333334,
      "grad_norm": 0.22224853932857513,
      "learning_rate": 3.127916666666667e-05,
      "loss": 0.0015,
      "step": 44930
    },
    {
      "epoch": 2.996,
      "grad_norm": 0.20518970489501953,
      "learning_rate": 3.1275e-05,
      "loss": 0.0025,
      "step": 44940
    },
    {
      "epoch": 2.9966666666666666,
      "grad_norm": 0.2664467692375183,
      "learning_rate": 3.127083333333333e-05,
      "loss": 0.0023,
      "step": 44950
    },
    {
      "epoch": 2.997333333333333,
      "grad_norm": 0.14952300488948822,
      "learning_rate": 3.126666666666666e-05,
      "loss": 0.0025,
      "step": 44960
    },
    {
      "epoch": 2.998,
      "grad_norm": 0.47293075919151306,
      "learning_rate": 3.12625e-05,
      "loss": 0.0026,
      "step": 44970
    },
    {
      "epoch": 2.998666666666667,
      "grad_norm": 0.21526971459388733,
      "learning_rate": 3.125833333333333e-05,
      "loss": 0.0019,
      "step": 44980
    },
    {
      "epoch": 2.9993333333333334,
      "grad_norm": 0.31669944524765015,
      "learning_rate": 3.125416666666667e-05,
      "loss": 0.0024,
      "step": 44990
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.8104233741760254,
      "learning_rate": 3.125e-05,
      "loss": 0.0044,
      "step": 45000
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.0022722494322806597,
      "eval_runtime": 161.5127,
      "eval_samples_per_second": 1238.293,
      "eval_steps_per_second": 30.957,
      "step": 45000
    },
    {
      "epoch": 3.0006666666666666,
      "grad_norm": 0.2267434149980545,
      "learning_rate": 3.124583333333334e-05,
      "loss": 0.0021,
      "step": 45010
    },
    {
      "epoch": 3.001333333333333,
      "grad_norm": 0.4563681483268738,
      "learning_rate": 3.124166666666667e-05,
      "loss": 0.0027,
      "step": 45020
    },
    {
      "epoch": 3.002,
      "grad_norm": 0.355466365814209,
      "learning_rate": 3.12375e-05,
      "loss": 0.0029,
      "step": 45030
    },
    {
      "epoch": 3.002666666666667,
      "grad_norm": 0.7335726022720337,
      "learning_rate": 3.123333333333334e-05,
      "loss": 0.0016,
      "step": 45040
    },
    {
      "epoch": 3.0033333333333334,
      "grad_norm": 0.5969759225845337,
      "learning_rate": 3.122916666666667e-05,
      "loss": 0.0019,
      "step": 45050
    },
    {
      "epoch": 3.004,
      "grad_norm": 0.7785273790359497,
      "learning_rate": 3.122500000000001e-05,
      "loss": 0.0027,
      "step": 45060
    },
    {
      "epoch": 3.0046666666666666,
      "grad_norm": 0.5720705986022949,
      "learning_rate": 3.122083333333333e-05,
      "loss": 0.0039,
      "step": 45070
    },
    {
      "epoch": 3.005333333333333,
      "grad_norm": 0.04181442782282829,
      "learning_rate": 3.121666666666667e-05,
      "loss": 0.0027,
      "step": 45080
    },
    {
      "epoch": 3.006,
      "grad_norm": 0.04859111085534096,
      "learning_rate": 3.12125e-05,
      "loss": 0.0029,
      "step": 45090
    },
    {
      "epoch": 3.006666666666667,
      "grad_norm": 0.027124477550387383,
      "learning_rate": 3.120833333333333e-05,
      "loss": 0.0025,
      "step": 45100
    },
    {
      "epoch": 3.0073333333333334,
      "grad_norm": 0.3810581564903259,
      "learning_rate": 3.120416666666667e-05,
      "loss": 0.0023,
      "step": 45110
    },
    {
      "epoch": 3.008,
      "grad_norm": 0.07804469019174576,
      "learning_rate": 3.12e-05,
      "loss": 0.0021,
      "step": 45120
    },
    {
      "epoch": 3.0086666666666666,
      "grad_norm": 0.29621654748916626,
      "learning_rate": 3.119583333333334e-05,
      "loss": 0.0018,
      "step": 45130
    },
    {
      "epoch": 3.009333333333333,
      "grad_norm": 0.09201622754335403,
      "learning_rate": 3.119166666666667e-05,
      "loss": 0.0026,
      "step": 45140
    },
    {
      "epoch": 3.01,
      "grad_norm": 0.3048633635044098,
      "learning_rate": 3.1187500000000006e-05,
      "loss": 0.0028,
      "step": 45150
    },
    {
      "epoch": 3.010666666666667,
      "grad_norm": 0.7291446328163147,
      "learning_rate": 3.118333333333334e-05,
      "loss": 0.0019,
      "step": 45160
    },
    {
      "epoch": 3.0113333333333334,
      "grad_norm": 0.05035579949617386,
      "learning_rate": 3.117916666666667e-05,
      "loss": 0.0015,
      "step": 45170
    },
    {
      "epoch": 3.012,
      "grad_norm": 0.040961455553770065,
      "learning_rate": 3.1175000000000006e-05,
      "loss": 0.0019,
      "step": 45180
    },
    {
      "epoch": 3.0126666666666666,
      "grad_norm": 0.3020998537540436,
      "learning_rate": 3.117083333333333e-05,
      "loss": 0.0028,
      "step": 45190
    },
    {
      "epoch": 3.013333333333333,
      "grad_norm": 0.2814134955406189,
      "learning_rate": 3.116666666666667e-05,
      "loss": 0.0026,
      "step": 45200
    },
    {
      "epoch": 3.014,
      "grad_norm": 0.7069724798202515,
      "learning_rate": 3.11625e-05,
      "loss": 0.0029,
      "step": 45210
    },
    {
      "epoch": 3.014666666666667,
      "grad_norm": 0.04678474739193916,
      "learning_rate": 3.1158333333333336e-05,
      "loss": 0.0026,
      "step": 45220
    },
    {
      "epoch": 3.0153333333333334,
      "grad_norm": 0.4342266321182251,
      "learning_rate": 3.115416666666667e-05,
      "loss": 0.0017,
      "step": 45230
    },
    {
      "epoch": 3.016,
      "grad_norm": 0.45180371403694153,
      "learning_rate": 3.115e-05,
      "loss": 0.0024,
      "step": 45240
    },
    {
      "epoch": 3.0166666666666666,
      "grad_norm": 0.9749082922935486,
      "learning_rate": 3.1145833333333336e-05,
      "loss": 0.0018,
      "step": 45250
    },
    {
      "epoch": 3.017333333333333,
      "grad_norm": 0.45655956864356995,
      "learning_rate": 3.114166666666667e-05,
      "loss": 0.0024,
      "step": 45260
    },
    {
      "epoch": 3.018,
      "grad_norm": 0.08655086904764175,
      "learning_rate": 3.1137500000000005e-05,
      "loss": 0.0014,
      "step": 45270
    },
    {
      "epoch": 3.018666666666667,
      "grad_norm": 0.8656352162361145,
      "learning_rate": 3.1133333333333336e-05,
      "loss": 0.0021,
      "step": 45280
    },
    {
      "epoch": 3.0193333333333334,
      "grad_norm": 0.5886190533638,
      "learning_rate": 3.1129166666666673e-05,
      "loss": 0.0022,
      "step": 45290
    },
    {
      "epoch": 3.02,
      "grad_norm": 0.49194735288619995,
      "learning_rate": 3.1125000000000004e-05,
      "loss": 0.0014,
      "step": 45300
    },
    {
      "epoch": 3.0206666666666666,
      "grad_norm": 0.9385485649108887,
      "learning_rate": 3.1120833333333335e-05,
      "loss": 0.0049,
      "step": 45310
    },
    {
      "epoch": 3.021333333333333,
      "grad_norm": 0.09772340208292007,
      "learning_rate": 3.1116666666666666e-05,
      "loss": 0.0022,
      "step": 45320
    },
    {
      "epoch": 3.022,
      "grad_norm": 0.04576922580599785,
      "learning_rate": 3.11125e-05,
      "loss": 0.0013,
      "step": 45330
    },
    {
      "epoch": 3.022666666666667,
      "grad_norm": 0.5091443657875061,
      "learning_rate": 3.1108333333333335e-05,
      "loss": 0.0014,
      "step": 45340
    },
    {
      "epoch": 3.0233333333333334,
      "grad_norm": 0.2719838321208954,
      "learning_rate": 3.1104166666666666e-05,
      "loss": 0.0023,
      "step": 45350
    },
    {
      "epoch": 3.024,
      "grad_norm": 0.27437421679496765,
      "learning_rate": 3.1100000000000004e-05,
      "loss": 0.002,
      "step": 45360
    },
    {
      "epoch": 3.0246666666666666,
      "grad_norm": 0.1170608252286911,
      "learning_rate": 3.1095833333333335e-05,
      "loss": 0.0018,
      "step": 45370
    },
    {
      "epoch": 3.025333333333333,
      "grad_norm": 0.06067655235528946,
      "learning_rate": 3.1091666666666666e-05,
      "loss": 0.003,
      "step": 45380
    },
    {
      "epoch": 3.026,
      "grad_norm": 0.4825275242328644,
      "learning_rate": 3.1087500000000003e-05,
      "loss": 0.0023,
      "step": 45390
    },
    {
      "epoch": 3.026666666666667,
      "grad_norm": 0.45826080441474915,
      "learning_rate": 3.1083333333333334e-05,
      "loss": 0.0035,
      "step": 45400
    },
    {
      "epoch": 3.0273333333333334,
      "grad_norm": 0.37037527561187744,
      "learning_rate": 3.107916666666667e-05,
      "loss": 0.0017,
      "step": 45410
    },
    {
      "epoch": 3.028,
      "grad_norm": 0.2217741310596466,
      "learning_rate": 3.1075e-05,
      "loss": 0.0025,
      "step": 45420
    },
    {
      "epoch": 3.0286666666666666,
      "grad_norm": 0.033501263707876205,
      "learning_rate": 3.1070833333333334e-05,
      "loss": 0.0021,
      "step": 45430
    },
    {
      "epoch": 3.029333333333333,
      "grad_norm": 0.4294039309024811,
      "learning_rate": 3.1066666666666665e-05,
      "loss": 0.0019,
      "step": 45440
    },
    {
      "epoch": 3.03,
      "grad_norm": 0.40480712056159973,
      "learning_rate": 3.10625e-05,
      "loss": 0.0017,
      "step": 45450
    },
    {
      "epoch": 3.030666666666667,
      "grad_norm": 0.8279741406440735,
      "learning_rate": 3.1058333333333334e-05,
      "loss": 0.0015,
      "step": 45460
    },
    {
      "epoch": 3.0313333333333334,
      "grad_norm": 0.25558552145957947,
      "learning_rate": 3.1054166666666665e-05,
      "loss": 0.0019,
      "step": 45470
    },
    {
      "epoch": 3.032,
      "grad_norm": 0.2951427102088928,
      "learning_rate": 3.105e-05,
      "loss": 0.0022,
      "step": 45480
    },
    {
      "epoch": 3.0326666666666666,
      "grad_norm": 0.47891753911972046,
      "learning_rate": 3.1045833333333334e-05,
      "loss": 0.0022,
      "step": 45490
    },
    {
      "epoch": 3.033333333333333,
      "grad_norm": 0.32776138186454773,
      "learning_rate": 3.104166666666667e-05,
      "loss": 0.002,
      "step": 45500
    },
    {
      "epoch": 3.034,
      "grad_norm": 0.21956227719783783,
      "learning_rate": 3.10375e-05,
      "loss": 0.0031,
      "step": 45510
    },
    {
      "epoch": 3.034666666666667,
      "grad_norm": 0.10472945123910904,
      "learning_rate": 3.103333333333333e-05,
      "loss": 0.0017,
      "step": 45520
    },
    {
      "epoch": 3.0353333333333334,
      "grad_norm": 0.614678680896759,
      "learning_rate": 3.102916666666667e-05,
      "loss": 0.002,
      "step": 45530
    },
    {
      "epoch": 3.036,
      "grad_norm": 0.13337169587612152,
      "learning_rate": 3.1025e-05,
      "loss": 0.002,
      "step": 45540
    },
    {
      "epoch": 3.0366666666666666,
      "grad_norm": 0.4854210913181305,
      "learning_rate": 3.102083333333333e-05,
      "loss": 0.0024,
      "step": 45550
    },
    {
      "epoch": 3.037333333333333,
      "grad_norm": 0.28253862261772156,
      "learning_rate": 3.1016666666666664e-05,
      "loss": 0.0019,
      "step": 45560
    },
    {
      "epoch": 3.038,
      "grad_norm": 0.4726964831352234,
      "learning_rate": 3.10125e-05,
      "loss": 0.0015,
      "step": 45570
    },
    {
      "epoch": 3.038666666666667,
      "grad_norm": 0.7622305154800415,
      "learning_rate": 3.100833333333333e-05,
      "loss": 0.0021,
      "step": 45580
    },
    {
      "epoch": 3.0393333333333334,
      "grad_norm": 0.3709598183631897,
      "learning_rate": 3.100416666666667e-05,
      "loss": 0.0019,
      "step": 45590
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.581377387046814,
      "learning_rate": 3.1e-05,
      "loss": 0.0023,
      "step": 45600
    },
    {
      "epoch": 3.0406666666666666,
      "grad_norm": 0.09463822096586227,
      "learning_rate": 3.099583333333333e-05,
      "loss": 0.0026,
      "step": 45610
    },
    {
      "epoch": 3.041333333333333,
      "grad_norm": 0.5264917016029358,
      "learning_rate": 3.099166666666667e-05,
      "loss": 0.0017,
      "step": 45620
    },
    {
      "epoch": 3.042,
      "grad_norm": 0.3758850395679474,
      "learning_rate": 3.09875e-05,
      "loss": 0.0034,
      "step": 45630
    },
    {
      "epoch": 3.042666666666667,
      "grad_norm": 0.05890313908457756,
      "learning_rate": 3.098333333333334e-05,
      "loss": 0.0018,
      "step": 45640
    },
    {
      "epoch": 3.0433333333333334,
      "grad_norm": 0.2633151710033417,
      "learning_rate": 3.097916666666667e-05,
      "loss": 0.0034,
      "step": 45650
    },
    {
      "epoch": 3.044,
      "grad_norm": 0.49426987767219543,
      "learning_rate": 3.0975e-05,
      "loss": 0.002,
      "step": 45660
    },
    {
      "epoch": 3.0446666666666666,
      "grad_norm": 0.3245029151439667,
      "learning_rate": 3.097083333333333e-05,
      "loss": 0.0025,
      "step": 45670
    },
    {
      "epoch": 3.0453333333333332,
      "grad_norm": 0.5904513597488403,
      "learning_rate": 3.096666666666666e-05,
      "loss": 0.0024,
      "step": 45680
    },
    {
      "epoch": 3.046,
      "grad_norm": 0.4800395965576172,
      "learning_rate": 3.09625e-05,
      "loss": 0.0026,
      "step": 45690
    },
    {
      "epoch": 3.046666666666667,
      "grad_norm": 0.028109943494200706,
      "learning_rate": 3.095833333333333e-05,
      "loss": 0.0017,
      "step": 45700
    },
    {
      "epoch": 3.0473333333333334,
      "grad_norm": 0.19982659816741943,
      "learning_rate": 3.095416666666667e-05,
      "loss": 0.0017,
      "step": 45710
    },
    {
      "epoch": 3.048,
      "grad_norm": 0.02887379750609398,
      "learning_rate": 3.095e-05,
      "loss": 0.0014,
      "step": 45720
    },
    {
      "epoch": 3.0486666666666666,
      "grad_norm": 0.7712156772613525,
      "learning_rate": 3.094583333333334e-05,
      "loss": 0.0018,
      "step": 45730
    },
    {
      "epoch": 3.0493333333333332,
      "grad_norm": 0.5192179679870605,
      "learning_rate": 3.094166666666667e-05,
      "loss": 0.0036,
      "step": 45740
    },
    {
      "epoch": 3.05,
      "grad_norm": 0.28731924295425415,
      "learning_rate": 3.09375e-05,
      "loss": 0.0021,
      "step": 45750
    },
    {
      "epoch": 3.050666666666667,
      "grad_norm": 0.4382418394088745,
      "learning_rate": 3.093333333333334e-05,
      "loss": 0.0035,
      "step": 45760
    },
    {
      "epoch": 3.0513333333333335,
      "grad_norm": 0.2965611517429352,
      "learning_rate": 3.092916666666667e-05,
      "loss": 0.0021,
      "step": 45770
    },
    {
      "epoch": 3.052,
      "grad_norm": 0.04517007991671562,
      "learning_rate": 3.0925000000000006e-05,
      "loss": 0.0034,
      "step": 45780
    },
    {
      "epoch": 3.0526666666666666,
      "grad_norm": 0.8387246131896973,
      "learning_rate": 3.092083333333333e-05,
      "loss": 0.0016,
      "step": 45790
    },
    {
      "epoch": 3.0533333333333332,
      "grad_norm": 0.6082617044448853,
      "learning_rate": 3.091666666666667e-05,
      "loss": 0.0018,
      "step": 45800
    },
    {
      "epoch": 3.054,
      "grad_norm": 0.061128102242946625,
      "learning_rate": 3.09125e-05,
      "loss": 0.0019,
      "step": 45810
    },
    {
      "epoch": 3.054666666666667,
      "grad_norm": 0.223176047205925,
      "learning_rate": 3.090833333333334e-05,
      "loss": 0.0019,
      "step": 45820
    },
    {
      "epoch": 3.0553333333333335,
      "grad_norm": 0.2250247299671173,
      "learning_rate": 3.090416666666667e-05,
      "loss": 0.0016,
      "step": 45830
    },
    {
      "epoch": 3.056,
      "grad_norm": 0.32071858644485474,
      "learning_rate": 3.09e-05,
      "loss": 0.0013,
      "step": 45840
    },
    {
      "epoch": 3.0566666666666666,
      "grad_norm": 0.6494011282920837,
      "learning_rate": 3.089583333333334e-05,
      "loss": 0.0022,
      "step": 45850
    },
    {
      "epoch": 3.0573333333333332,
      "grad_norm": 0.36150991916656494,
      "learning_rate": 3.089166666666667e-05,
      "loss": 0.0017,
      "step": 45860
    },
    {
      "epoch": 3.058,
      "grad_norm": 0.32585233449935913,
      "learning_rate": 3.0887500000000005e-05,
      "loss": 0.0031,
      "step": 45870
    },
    {
      "epoch": 3.058666666666667,
      "grad_norm": 0.15811023116111755,
      "learning_rate": 3.0883333333333336e-05,
      "loss": 0.0017,
      "step": 45880
    },
    {
      "epoch": 3.0593333333333335,
      "grad_norm": 0.552253782749176,
      "learning_rate": 3.087916666666667e-05,
      "loss": 0.0023,
      "step": 45890
    },
    {
      "epoch": 3.06,
      "grad_norm": 0.45429158210754395,
      "learning_rate": 3.0875000000000005e-05,
      "loss": 0.0031,
      "step": 45900
    },
    {
      "epoch": 3.0606666666666666,
      "grad_norm": 0.9009444117546082,
      "learning_rate": 3.087083333333333e-05,
      "loss": 0.002,
      "step": 45910
    },
    {
      "epoch": 3.0613333333333332,
      "grad_norm": 0.22114412486553192,
      "learning_rate": 3.086666666666667e-05,
      "loss": 0.0022,
      "step": 45920
    },
    {
      "epoch": 3.062,
      "grad_norm": 0.34813618659973145,
      "learning_rate": 3.08625e-05,
      "loss": 0.0024,
      "step": 45930
    },
    {
      "epoch": 3.062666666666667,
      "grad_norm": 0.1874229460954666,
      "learning_rate": 3.0858333333333336e-05,
      "loss": 0.0017,
      "step": 45940
    },
    {
      "epoch": 3.0633333333333335,
      "grad_norm": 0.3215421140193939,
      "learning_rate": 3.085416666666667e-05,
      "loss": 0.0024,
      "step": 45950
    },
    {
      "epoch": 3.064,
      "grad_norm": 0.40357398986816406,
      "learning_rate": 3.0850000000000004e-05,
      "loss": 0.0024,
      "step": 45960
    },
    {
      "epoch": 3.0646666666666667,
      "grad_norm": 0.0869322344660759,
      "learning_rate": 3.0845833333333335e-05,
      "loss": 0.0025,
      "step": 45970
    },
    {
      "epoch": 3.0653333333333332,
      "grad_norm": 0.31463512778282166,
      "learning_rate": 3.0841666666666666e-05,
      "loss": 0.0015,
      "step": 45980
    },
    {
      "epoch": 3.066,
      "grad_norm": 0.04510849714279175,
      "learning_rate": 3.0837500000000004e-05,
      "loss": 0.0022,
      "step": 45990
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 0.38062629103660583,
      "learning_rate": 3.0833333333333335e-05,
      "loss": 0.0029,
      "step": 46000
    },
    {
      "epoch": 3.0673333333333335,
      "grad_norm": 0.10899416357278824,
      "learning_rate": 3.082916666666667e-05,
      "loss": 0.0021,
      "step": 46010
    },
    {
      "epoch": 3.068,
      "grad_norm": 0.05838600546121597,
      "learning_rate": 3.0825000000000004e-05,
      "loss": 0.0022,
      "step": 46020
    },
    {
      "epoch": 3.0686666666666667,
      "grad_norm": 0.6296107172966003,
      "learning_rate": 3.0820833333333335e-05,
      "loss": 0.0027,
      "step": 46030
    },
    {
      "epoch": 3.0693333333333332,
      "grad_norm": 0.41859427094459534,
      "learning_rate": 3.0816666666666666e-05,
      "loss": 0.0014,
      "step": 46040
    },
    {
      "epoch": 3.07,
      "grad_norm": 0.29889172315597534,
      "learning_rate": 3.08125e-05,
      "loss": 0.0032,
      "step": 46050
    },
    {
      "epoch": 3.070666666666667,
      "grad_norm": 0.41051340103149414,
      "learning_rate": 3.0808333333333335e-05,
      "loss": 0.0022,
      "step": 46060
    },
    {
      "epoch": 3.0713333333333335,
      "grad_norm": 0.1445850431919098,
      "learning_rate": 3.0804166666666666e-05,
      "loss": 0.0014,
      "step": 46070
    },
    {
      "epoch": 3.072,
      "grad_norm": 0.5185155868530273,
      "learning_rate": 3.08e-05,
      "loss": 0.0018,
      "step": 46080
    },
    {
      "epoch": 3.0726666666666667,
      "grad_norm": 0.45851072669029236,
      "learning_rate": 3.0795833333333334e-05,
      "loss": 0.002,
      "step": 46090
    },
    {
      "epoch": 3.0733333333333333,
      "grad_norm": 0.19145582616329193,
      "learning_rate": 3.079166666666667e-05,
      "loss": 0.002,
      "step": 46100
    },
    {
      "epoch": 3.074,
      "grad_norm": 0.44787073135375977,
      "learning_rate": 3.07875e-05,
      "loss": 0.0017,
      "step": 46110
    },
    {
      "epoch": 3.074666666666667,
      "grad_norm": 0.4737773537635803,
      "learning_rate": 3.0783333333333334e-05,
      "loss": 0.0022,
      "step": 46120
    },
    {
      "epoch": 3.0753333333333335,
      "grad_norm": 0.8423123359680176,
      "learning_rate": 3.077916666666667e-05,
      "loss": 0.0022,
      "step": 46130
    },
    {
      "epoch": 3.076,
      "grad_norm": 0.2629581391811371,
      "learning_rate": 3.0775e-05,
      "loss": 0.0023,
      "step": 46140
    },
    {
      "epoch": 3.0766666666666667,
      "grad_norm": 0.2618832290172577,
      "learning_rate": 3.0770833333333334e-05,
      "loss": 0.0019,
      "step": 46150
    },
    {
      "epoch": 3.0773333333333333,
      "grad_norm": 0.29456406831741333,
      "learning_rate": 3.0766666666666665e-05,
      "loss": 0.0016,
      "step": 46160
    },
    {
      "epoch": 3.078,
      "grad_norm": 0.15589621663093567,
      "learning_rate": 3.07625e-05,
      "loss": 0.0022,
      "step": 46170
    },
    {
      "epoch": 3.078666666666667,
      "grad_norm": 0.3648732900619507,
      "learning_rate": 3.075833333333333e-05,
      "loss": 0.0019,
      "step": 46180
    },
    {
      "epoch": 3.0793333333333335,
      "grad_norm": 0.1266685128211975,
      "learning_rate": 3.0754166666666664e-05,
      "loss": 0.0023,
      "step": 46190
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.2201148420572281,
      "learning_rate": 3.075e-05,
      "loss": 0.0018,
      "step": 46200
    },
    {
      "epoch": 3.0806666666666667,
      "grad_norm": 0.18961550295352936,
      "learning_rate": 3.074583333333333e-05,
      "loss": 0.0019,
      "step": 46210
    },
    {
      "epoch": 3.0813333333333333,
      "grad_norm": 0.1474628448486328,
      "learning_rate": 3.074166666666667e-05,
      "loss": 0.0016,
      "step": 46220
    },
    {
      "epoch": 3.082,
      "grad_norm": 0.21995088458061218,
      "learning_rate": 3.07375e-05,
      "loss": 0.0024,
      "step": 46230
    },
    {
      "epoch": 3.0826666666666664,
      "grad_norm": 0.1465931236743927,
      "learning_rate": 3.073333333333334e-05,
      "loss": 0.0023,
      "step": 46240
    },
    {
      "epoch": 3.0833333333333335,
      "grad_norm": 0.08968412131071091,
      "learning_rate": 3.072916666666667e-05,
      "loss": 0.0026,
      "step": 46250
    },
    {
      "epoch": 3.084,
      "grad_norm": 0.40776702761650085,
      "learning_rate": 3.0725e-05,
      "loss": 0.0017,
      "step": 46260
    },
    {
      "epoch": 3.0846666666666667,
      "grad_norm": 0.6807087063789368,
      "learning_rate": 3.072083333333334e-05,
      "loss": 0.002,
      "step": 46270
    },
    {
      "epoch": 3.0853333333333333,
      "grad_norm": 0.7063248157501221,
      "learning_rate": 3.0716666666666663e-05,
      "loss": 0.0022,
      "step": 46280
    },
    {
      "epoch": 3.086,
      "grad_norm": 0.4084179997444153,
      "learning_rate": 3.07125e-05,
      "loss": 0.0027,
      "step": 46290
    },
    {
      "epoch": 3.086666666666667,
      "grad_norm": 0.7804431319236755,
      "learning_rate": 3.070833333333333e-05,
      "loss": 0.0018,
      "step": 46300
    },
    {
      "epoch": 3.0873333333333335,
      "grad_norm": 1.1617828607559204,
      "learning_rate": 3.070416666666667e-05,
      "loss": 0.0026,
      "step": 46310
    },
    {
      "epoch": 3.088,
      "grad_norm": 0.37025734782218933,
      "learning_rate": 3.07e-05,
      "loss": 0.002,
      "step": 46320
    },
    {
      "epoch": 3.0886666666666667,
      "grad_norm": 0.05557429790496826,
      "learning_rate": 3.069583333333333e-05,
      "loss": 0.0015,
      "step": 46330
    },
    {
      "epoch": 3.0893333333333333,
      "grad_norm": 0.2777469754219055,
      "learning_rate": 3.069166666666667e-05,
      "loss": 0.0014,
      "step": 46340
    },
    {
      "epoch": 3.09,
      "grad_norm": 0.08518524467945099,
      "learning_rate": 3.06875e-05,
      "loss": 0.0014,
      "step": 46350
    },
    {
      "epoch": 3.0906666666666665,
      "grad_norm": 0.5879659056663513,
      "learning_rate": 3.068333333333334e-05,
      "loss": 0.0024,
      "step": 46360
    },
    {
      "epoch": 3.0913333333333335,
      "grad_norm": 0.4961327016353607,
      "learning_rate": 3.067916666666667e-05,
      "loss": 0.0016,
      "step": 46370
    },
    {
      "epoch": 3.092,
      "grad_norm": 0.2995956242084503,
      "learning_rate": 3.067500000000001e-05,
      "loss": 0.0025,
      "step": 46380
    },
    {
      "epoch": 3.0926666666666667,
      "grad_norm": 0.14472492039203644,
      "learning_rate": 3.067083333333334e-05,
      "loss": 0.0016,
      "step": 46390
    },
    {
      "epoch": 3.0933333333333333,
      "grad_norm": 0.6435710191726685,
      "learning_rate": 3.066666666666667e-05,
      "loss": 0.0021,
      "step": 46400
    },
    {
      "epoch": 3.094,
      "grad_norm": 0.3892329931259155,
      "learning_rate": 3.06625e-05,
      "loss": 0.002,
      "step": 46410
    },
    {
      "epoch": 3.0946666666666665,
      "grad_norm": 0.8940808773040771,
      "learning_rate": 3.065833333333333e-05,
      "loss": 0.0021,
      "step": 46420
    },
    {
      "epoch": 3.0953333333333335,
      "grad_norm": 0.18181906640529633,
      "learning_rate": 3.065416666666667e-05,
      "loss": 0.002,
      "step": 46430
    },
    {
      "epoch": 3.096,
      "grad_norm": 0.6947900652885437,
      "learning_rate": 3.065e-05,
      "loss": 0.002,
      "step": 46440
    },
    {
      "epoch": 3.0966666666666667,
      "grad_norm": 0.34335336089134216,
      "learning_rate": 3.064583333333334e-05,
      "loss": 0.0021,
      "step": 46450
    },
    {
      "epoch": 3.0973333333333333,
      "grad_norm": 0.039170995354652405,
      "learning_rate": 3.064166666666667e-05,
      "loss": 0.0023,
      "step": 46460
    },
    {
      "epoch": 3.098,
      "grad_norm": 0.17314934730529785,
      "learning_rate": 3.06375e-05,
      "loss": 0.0036,
      "step": 46470
    },
    {
      "epoch": 3.0986666666666665,
      "grad_norm": 0.3218628764152527,
      "learning_rate": 3.063333333333334e-05,
      "loss": 0.0022,
      "step": 46480
    },
    {
      "epoch": 3.0993333333333335,
      "grad_norm": 0.766972005367279,
      "learning_rate": 3.062916666666667e-05,
      "loss": 0.0021,
      "step": 46490
    },
    {
      "epoch": 3.1,
      "grad_norm": 0.3763960003852844,
      "learning_rate": 3.0625000000000006e-05,
      "loss": 0.0021,
      "step": 46500
    },
    {
      "epoch": 3.1006666666666667,
      "grad_norm": 0.44404613971710205,
      "learning_rate": 3.062083333333334e-05,
      "loss": 0.0015,
      "step": 46510
    },
    {
      "epoch": 3.1013333333333333,
      "grad_norm": 0.14141619205474854,
      "learning_rate": 3.061666666666667e-05,
      "loss": 0.0023,
      "step": 46520
    },
    {
      "epoch": 3.102,
      "grad_norm": 0.09753213077783585,
      "learning_rate": 3.06125e-05,
      "loss": 0.0021,
      "step": 46530
    },
    {
      "epoch": 3.1026666666666665,
      "grad_norm": 0.23385199904441833,
      "learning_rate": 3.0608333333333336e-05,
      "loss": 0.0035,
      "step": 46540
    },
    {
      "epoch": 3.1033333333333335,
      "grad_norm": 0.06314262002706528,
      "learning_rate": 3.060416666666667e-05,
      "loss": 0.0017,
      "step": 46550
    },
    {
      "epoch": 3.104,
      "grad_norm": 0.07799921184778214,
      "learning_rate": 3.06e-05,
      "loss": 0.0017,
      "step": 46560
    },
    {
      "epoch": 3.1046666666666667,
      "grad_norm": 0.08928412944078445,
      "learning_rate": 3.0595833333333336e-05,
      "loss": 0.002,
      "step": 46570
    },
    {
      "epoch": 3.1053333333333333,
      "grad_norm": 0.22585082054138184,
      "learning_rate": 3.059166666666667e-05,
      "loss": 0.003,
      "step": 46580
    },
    {
      "epoch": 3.106,
      "grad_norm": 0.4555983543395996,
      "learning_rate": 3.0587500000000005e-05,
      "loss": 0.0023,
      "step": 46590
    },
    {
      "epoch": 3.1066666666666665,
      "grad_norm": 0.37298253178596497,
      "learning_rate": 3.0583333333333336e-05,
      "loss": 0.0019,
      "step": 46600
    },
    {
      "epoch": 3.1073333333333335,
      "grad_norm": 0.10631617903709412,
      "learning_rate": 3.057916666666667e-05,
      "loss": 0.0024,
      "step": 46610
    },
    {
      "epoch": 3.108,
      "grad_norm": 0.5193852782249451,
      "learning_rate": 3.0575000000000005e-05,
      "loss": 0.0024,
      "step": 46620
    },
    {
      "epoch": 3.1086666666666667,
      "grad_norm": 0.5523600578308105,
      "learning_rate": 3.0570833333333336e-05,
      "loss": 0.0022,
      "step": 46630
    },
    {
      "epoch": 3.1093333333333333,
      "grad_norm": 0.1236315369606018,
      "learning_rate": 3.0566666666666667e-05,
      "loss": 0.0023,
      "step": 46640
    },
    {
      "epoch": 3.11,
      "grad_norm": 0.2883799970149994,
      "learning_rate": 3.05625e-05,
      "loss": 0.0021,
      "step": 46650
    },
    {
      "epoch": 3.1106666666666665,
      "grad_norm": 0.4797821044921875,
      "learning_rate": 3.0558333333333335e-05,
      "loss": 0.0016,
      "step": 46660
    },
    {
      "epoch": 3.1113333333333335,
      "grad_norm": 0.2899952232837677,
      "learning_rate": 3.0554166666666666e-05,
      "loss": 0.0026,
      "step": 46670
    },
    {
      "epoch": 3.112,
      "grad_norm": 0.1619785577058792,
      "learning_rate": 3.0550000000000004e-05,
      "loss": 0.0015,
      "step": 46680
    },
    {
      "epoch": 3.1126666666666667,
      "grad_norm": 0.2211768627166748,
      "learning_rate": 3.0545833333333335e-05,
      "loss": 0.002,
      "step": 46690
    },
    {
      "epoch": 3.1133333333333333,
      "grad_norm": 0.665544331073761,
      "learning_rate": 3.0541666666666666e-05,
      "loss": 0.0022,
      "step": 46700
    },
    {
      "epoch": 3.114,
      "grad_norm": 0.15394969284534454,
      "learning_rate": 3.0537500000000004e-05,
      "loss": 0.002,
      "step": 46710
    },
    {
      "epoch": 3.1146666666666665,
      "grad_norm": 0.07808385044336319,
      "learning_rate": 3.0533333333333335e-05,
      "loss": 0.0016,
      "step": 46720
    },
    {
      "epoch": 3.1153333333333335,
      "grad_norm": 0.3480113446712494,
      "learning_rate": 3.052916666666667e-05,
      "loss": 0.0023,
      "step": 46730
    },
    {
      "epoch": 3.116,
      "grad_norm": 0.6543209552764893,
      "learning_rate": 3.0525e-05,
      "loss": 0.0031,
      "step": 46740
    },
    {
      "epoch": 3.1166666666666667,
      "grad_norm": 0.5032888054847717,
      "learning_rate": 3.0520833333333334e-05,
      "loss": 0.002,
      "step": 46750
    },
    {
      "epoch": 3.1173333333333333,
      "grad_norm": 0.5260461568832397,
      "learning_rate": 3.0516666666666665e-05,
      "loss": 0.0014,
      "step": 46760
    },
    {
      "epoch": 3.118,
      "grad_norm": 0.4425380229949951,
      "learning_rate": 3.05125e-05,
      "loss": 0.0012,
      "step": 46770
    },
    {
      "epoch": 3.1186666666666665,
      "grad_norm": 0.596168041229248,
      "learning_rate": 3.0508333333333334e-05,
      "loss": 0.0018,
      "step": 46780
    },
    {
      "epoch": 3.1193333333333335,
      "grad_norm": 0.28355875611305237,
      "learning_rate": 3.050416666666667e-05,
      "loss": 0.0031,
      "step": 46790
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.14244285225868225,
      "learning_rate": 3.05e-05,
      "loss": 0.002,
      "step": 46800
    },
    {
      "epoch": 3.1206666666666667,
      "grad_norm": 0.4828973114490509,
      "learning_rate": 3.0495833333333334e-05,
      "loss": 0.0025,
      "step": 46810
    },
    {
      "epoch": 3.1213333333333333,
      "grad_norm": 0.2634616792201996,
      "learning_rate": 3.0491666666666668e-05,
      "loss": 0.002,
      "step": 46820
    },
    {
      "epoch": 3.122,
      "grad_norm": 0.059835318475961685,
      "learning_rate": 3.0487500000000002e-05,
      "loss": 0.0022,
      "step": 46830
    },
    {
      "epoch": 3.1226666666666665,
      "grad_norm": 0.12631110846996307,
      "learning_rate": 3.0483333333333337e-05,
      "loss": 0.0015,
      "step": 46840
    },
    {
      "epoch": 3.1233333333333335,
      "grad_norm": 0.35399481654167175,
      "learning_rate": 3.047916666666667e-05,
      "loss": 0.0018,
      "step": 46850
    },
    {
      "epoch": 3.124,
      "grad_norm": 0.1205049455165863,
      "learning_rate": 3.0475000000000002e-05,
      "loss": 0.0028,
      "step": 46860
    },
    {
      "epoch": 3.1246666666666667,
      "grad_norm": 0.04422205686569214,
      "learning_rate": 3.0470833333333337e-05,
      "loss": 0.0019,
      "step": 46870
    },
    {
      "epoch": 3.1253333333333333,
      "grad_norm": 0.21646805107593536,
      "learning_rate": 3.0466666666666664e-05,
      "loss": 0.0017,
      "step": 46880
    },
    {
      "epoch": 3.126,
      "grad_norm": 0.2547156810760498,
      "learning_rate": 3.04625e-05,
      "loss": 0.0021,
      "step": 46890
    },
    {
      "epoch": 3.1266666666666665,
      "grad_norm": 0.1762670874595642,
      "learning_rate": 3.0458333333333333e-05,
      "loss": 0.002,
      "step": 46900
    },
    {
      "epoch": 3.1273333333333335,
      "grad_norm": 0.18625260889530182,
      "learning_rate": 3.0454166666666667e-05,
      "loss": 0.0031,
      "step": 46910
    },
    {
      "epoch": 3.128,
      "grad_norm": 0.7514709234237671,
      "learning_rate": 3.045e-05,
      "loss": 0.0023,
      "step": 46920
    },
    {
      "epoch": 3.1286666666666667,
      "grad_norm": 0.9082474112510681,
      "learning_rate": 3.0445833333333336e-05,
      "loss": 0.003,
      "step": 46930
    },
    {
      "epoch": 3.1293333333333333,
      "grad_norm": 0.1489812731742859,
      "learning_rate": 3.0441666666666667e-05,
      "loss": 0.0015,
      "step": 46940
    },
    {
      "epoch": 3.13,
      "grad_norm": 0.2859189510345459,
      "learning_rate": 3.04375e-05,
      "loss": 0.0021,
      "step": 46950
    },
    {
      "epoch": 3.1306666666666665,
      "grad_norm": 0.14589127898216248,
      "learning_rate": 3.0433333333333336e-05,
      "loss": 0.0014,
      "step": 46960
    },
    {
      "epoch": 3.1313333333333335,
      "grad_norm": 0.5913461446762085,
      "learning_rate": 3.042916666666667e-05,
      "loss": 0.0024,
      "step": 46970
    },
    {
      "epoch": 3.132,
      "grad_norm": 0.22277288138866425,
      "learning_rate": 3.0425000000000004e-05,
      "loss": 0.0027,
      "step": 46980
    },
    {
      "epoch": 3.1326666666666667,
      "grad_norm": 0.6192108392715454,
      "learning_rate": 3.042083333333334e-05,
      "loss": 0.0027,
      "step": 46990
    },
    {
      "epoch": 3.1333333333333333,
      "grad_norm": 0.16492103040218353,
      "learning_rate": 3.0416666666666666e-05,
      "loss": 0.0029,
      "step": 47000
    },
    {
      "epoch": 3.134,
      "grad_norm": 0.43753743171691895,
      "learning_rate": 3.04125e-05,
      "loss": 0.0017,
      "step": 47010
    },
    {
      "epoch": 3.1346666666666665,
      "grad_norm": 0.47305235266685486,
      "learning_rate": 3.040833333333333e-05,
      "loss": 0.0019,
      "step": 47020
    },
    {
      "epoch": 3.1353333333333335,
      "grad_norm": 0.12381497770547867,
      "learning_rate": 3.0404166666666666e-05,
      "loss": 0.0036,
      "step": 47030
    },
    {
      "epoch": 3.136,
      "grad_norm": 0.5238366723060608,
      "learning_rate": 3.04e-05,
      "loss": 0.0018,
      "step": 47040
    },
    {
      "epoch": 3.1366666666666667,
      "grad_norm": 0.059352535754442215,
      "learning_rate": 3.0395833333333335e-05,
      "loss": 0.0019,
      "step": 47050
    },
    {
      "epoch": 3.1373333333333333,
      "grad_norm": 0.33583369851112366,
      "learning_rate": 3.039166666666667e-05,
      "loss": 0.0028,
      "step": 47060
    },
    {
      "epoch": 3.138,
      "grad_norm": 0.4351106584072113,
      "learning_rate": 3.0387500000000003e-05,
      "loss": 0.0013,
      "step": 47070
    },
    {
      "epoch": 3.1386666666666665,
      "grad_norm": 0.1290888637304306,
      "learning_rate": 3.0383333333333334e-05,
      "loss": 0.0023,
      "step": 47080
    },
    {
      "epoch": 3.1393333333333335,
      "grad_norm": 0.14887011051177979,
      "learning_rate": 3.037916666666667e-05,
      "loss": 0.0022,
      "step": 47090
    },
    {
      "epoch": 3.14,
      "grad_norm": 0.5843759179115295,
      "learning_rate": 3.0375000000000003e-05,
      "loss": 0.0025,
      "step": 47100
    },
    {
      "epoch": 3.1406666666666667,
      "grad_norm": 0.4655704200267792,
      "learning_rate": 3.0370833333333337e-05,
      "loss": 0.0028,
      "step": 47110
    },
    {
      "epoch": 3.1413333333333333,
      "grad_norm": 0.6499366164207458,
      "learning_rate": 3.0366666666666665e-05,
      "loss": 0.0022,
      "step": 47120
    },
    {
      "epoch": 3.142,
      "grad_norm": 0.36106669902801514,
      "learning_rate": 3.03625e-05,
      "loss": 0.0022,
      "step": 47130
    },
    {
      "epoch": 3.1426666666666665,
      "grad_norm": 0.36754265427589417,
      "learning_rate": 3.0358333333333334e-05,
      "loss": 0.0026,
      "step": 47140
    },
    {
      "epoch": 3.1433333333333335,
      "grad_norm": 0.46812698245048523,
      "learning_rate": 3.0354166666666668e-05,
      "loss": 0.0017,
      "step": 47150
    },
    {
      "epoch": 3.144,
      "grad_norm": 0.22600629925727844,
      "learning_rate": 3.035e-05,
      "loss": 0.0028,
      "step": 47160
    },
    {
      "epoch": 3.1446666666666667,
      "grad_norm": 0.5582205057144165,
      "learning_rate": 3.0345833333333333e-05,
      "loss": 0.0019,
      "step": 47170
    },
    {
      "epoch": 3.1453333333333333,
      "grad_norm": 0.43154269456863403,
      "learning_rate": 3.0341666666666668e-05,
      "loss": 0.0029,
      "step": 47180
    },
    {
      "epoch": 3.146,
      "grad_norm": 0.2980770170688629,
      "learning_rate": 3.0337500000000002e-05,
      "loss": 0.003,
      "step": 47190
    },
    {
      "epoch": 3.1466666666666665,
      "grad_norm": 0.2889447510242462,
      "learning_rate": 3.0333333333333337e-05,
      "loss": 0.0018,
      "step": 47200
    },
    {
      "epoch": 3.1473333333333335,
      "grad_norm": 0.3968346416950226,
      "learning_rate": 3.032916666666667e-05,
      "loss": 0.0023,
      "step": 47210
    },
    {
      "epoch": 3.148,
      "grad_norm": 0.1824362874031067,
      "learning_rate": 3.0325000000000002e-05,
      "loss": 0.0026,
      "step": 47220
    },
    {
      "epoch": 3.1486666666666667,
      "grad_norm": 0.5448728799819946,
      "learning_rate": 3.0320833333333336e-05,
      "loss": 0.0019,
      "step": 47230
    },
    {
      "epoch": 3.1493333333333333,
      "grad_norm": 0.6210100650787354,
      "learning_rate": 3.0316666666666664e-05,
      "loss": 0.0034,
      "step": 47240
    },
    {
      "epoch": 3.15,
      "grad_norm": 0.6545543670654297,
      "learning_rate": 3.0312499999999998e-05,
      "loss": 0.0026,
      "step": 47250
    },
    {
      "epoch": 3.1506666666666665,
      "grad_norm": 0.08427824825048447,
      "learning_rate": 3.0308333333333333e-05,
      "loss": 0.0024,
      "step": 47260
    },
    {
      "epoch": 3.1513333333333335,
      "grad_norm": 0.4842314124107361,
      "learning_rate": 3.0304166666666667e-05,
      "loss": 0.0031,
      "step": 47270
    },
    {
      "epoch": 3.152,
      "grad_norm": 0.33742669224739075,
      "learning_rate": 3.03e-05,
      "loss": 0.0028,
      "step": 47280
    },
    {
      "epoch": 3.1526666666666667,
      "grad_norm": 0.12159449607133865,
      "learning_rate": 3.0295833333333336e-05,
      "loss": 0.0031,
      "step": 47290
    },
    {
      "epoch": 3.1533333333333333,
      "grad_norm": 0.6895008087158203,
      "learning_rate": 3.0291666666666667e-05,
      "loss": 0.0023,
      "step": 47300
    },
    {
      "epoch": 3.154,
      "grad_norm": 0.2632382810115814,
      "learning_rate": 3.02875e-05,
      "loss": 0.0025,
      "step": 47310
    },
    {
      "epoch": 3.1546666666666665,
      "grad_norm": 0.046383198350667953,
      "learning_rate": 3.0283333333333335e-05,
      "loss": 0.0021,
      "step": 47320
    },
    {
      "epoch": 3.155333333333333,
      "grad_norm": 0.8550546765327454,
      "learning_rate": 3.027916666666667e-05,
      "loss": 0.0026,
      "step": 47330
    },
    {
      "epoch": 3.156,
      "grad_norm": 0.11362601071596146,
      "learning_rate": 3.0275000000000004e-05,
      "loss": 0.0016,
      "step": 47340
    },
    {
      "epoch": 3.1566666666666667,
      "grad_norm": 0.07383960485458374,
      "learning_rate": 3.027083333333334e-05,
      "loss": 0.0023,
      "step": 47350
    },
    {
      "epoch": 3.1573333333333333,
      "grad_norm": 0.654061496257782,
      "learning_rate": 3.0266666666666666e-05,
      "loss": 0.0017,
      "step": 47360
    },
    {
      "epoch": 3.158,
      "grad_norm": 0.4160504639148712,
      "learning_rate": 3.02625e-05,
      "loss": 0.0028,
      "step": 47370
    },
    {
      "epoch": 3.1586666666666665,
      "grad_norm": 0.440421462059021,
      "learning_rate": 3.025833333333333e-05,
      "loss": 0.0025,
      "step": 47380
    },
    {
      "epoch": 3.1593333333333335,
      "grad_norm": 0.22254176437854767,
      "learning_rate": 3.0254166666666666e-05,
      "loss": 0.003,
      "step": 47390
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.4419172704219818,
      "learning_rate": 3.025e-05,
      "loss": 0.0023,
      "step": 47400
    },
    {
      "epoch": 3.1606666666666667,
      "grad_norm": 0.15285193920135498,
      "learning_rate": 3.0245833333333334e-05,
      "loss": 0.0018,
      "step": 47410
    },
    {
      "epoch": 3.1613333333333333,
      "grad_norm": 0.288576602935791,
      "learning_rate": 3.024166666666667e-05,
      "loss": 0.002,
      "step": 47420
    },
    {
      "epoch": 3.162,
      "grad_norm": 0.6561797261238098,
      "learning_rate": 3.0237500000000003e-05,
      "loss": 0.0025,
      "step": 47430
    },
    {
      "epoch": 3.1626666666666665,
      "grad_norm": 0.3313889801502228,
      "learning_rate": 3.0233333333333334e-05,
      "loss": 0.0024,
      "step": 47440
    },
    {
      "epoch": 3.163333333333333,
      "grad_norm": 0.7316460609436035,
      "learning_rate": 3.022916666666667e-05,
      "loss": 0.0018,
      "step": 47450
    },
    {
      "epoch": 3.164,
      "grad_norm": 0.04333127662539482,
      "learning_rate": 3.0225000000000003e-05,
      "loss": 0.0028,
      "step": 47460
    },
    {
      "epoch": 3.1646666666666667,
      "grad_norm": 0.3690151274204254,
      "learning_rate": 3.0220833333333337e-05,
      "loss": 0.0014,
      "step": 47470
    },
    {
      "epoch": 3.1653333333333333,
      "grad_norm": 0.06632815301418304,
      "learning_rate": 3.0216666666666665e-05,
      "loss": 0.0018,
      "step": 47480
    },
    {
      "epoch": 3.166,
      "grad_norm": 0.04696156829595566,
      "learning_rate": 3.02125e-05,
      "loss": 0.0025,
      "step": 47490
    },
    {
      "epoch": 3.1666666666666665,
      "grad_norm": 0.15290765464305878,
      "learning_rate": 3.0208333333333334e-05,
      "loss": 0.0017,
      "step": 47500
    },
    {
      "epoch": 3.1673333333333336,
      "grad_norm": 0.1143566370010376,
      "learning_rate": 3.0204166666666668e-05,
      "loss": 0.0022,
      "step": 47510
    },
    {
      "epoch": 3.168,
      "grad_norm": 0.09225154668092728,
      "learning_rate": 3.02e-05,
      "loss": 0.002,
      "step": 47520
    },
    {
      "epoch": 3.1686666666666667,
      "grad_norm": 0.05985359475016594,
      "learning_rate": 3.0195833333333333e-05,
      "loss": 0.002,
      "step": 47530
    },
    {
      "epoch": 3.1693333333333333,
      "grad_norm": 0.15576298534870148,
      "learning_rate": 3.0191666666666668e-05,
      "loss": 0.0021,
      "step": 47540
    },
    {
      "epoch": 3.17,
      "grad_norm": 0.0691872164607048,
      "learning_rate": 3.0187500000000002e-05,
      "loss": 0.0017,
      "step": 47550
    },
    {
      "epoch": 3.1706666666666665,
      "grad_norm": 0.5268415808677673,
      "learning_rate": 3.0183333333333336e-05,
      "loss": 0.0029,
      "step": 47560
    },
    {
      "epoch": 3.171333333333333,
      "grad_norm": 0.23669728636741638,
      "learning_rate": 3.017916666666667e-05,
      "loss": 0.0025,
      "step": 47570
    },
    {
      "epoch": 3.172,
      "grad_norm": 0.6895994544029236,
      "learning_rate": 3.0175e-05,
      "loss": 0.0023,
      "step": 47580
    },
    {
      "epoch": 3.1726666666666667,
      "grad_norm": 0.18911270797252655,
      "learning_rate": 3.0170833333333336e-05,
      "loss": 0.0025,
      "step": 47590
    },
    {
      "epoch": 3.1733333333333333,
      "grad_norm": 0.219877228140831,
      "learning_rate": 3.016666666666667e-05,
      "loss": 0.0015,
      "step": 47600
    },
    {
      "epoch": 3.174,
      "grad_norm": 0.06040127947926521,
      "learning_rate": 3.0162499999999998e-05,
      "loss": 0.0029,
      "step": 47610
    },
    {
      "epoch": 3.1746666666666665,
      "grad_norm": 0.2199001908302307,
      "learning_rate": 3.0158333333333332e-05,
      "loss": 0.0023,
      "step": 47620
    },
    {
      "epoch": 3.1753333333333336,
      "grad_norm": 0.07949468493461609,
      "learning_rate": 3.0154166666666667e-05,
      "loss": 0.0013,
      "step": 47630
    },
    {
      "epoch": 3.176,
      "grad_norm": 0.6738625168800354,
      "learning_rate": 3.015e-05,
      "loss": 0.0026,
      "step": 47640
    },
    {
      "epoch": 3.1766666666666667,
      "grad_norm": 0.10056103020906448,
      "learning_rate": 3.0145833333333335e-05,
      "loss": 0.0016,
      "step": 47650
    },
    {
      "epoch": 3.1773333333333333,
      "grad_norm": 0.30522218346595764,
      "learning_rate": 3.014166666666667e-05,
      "loss": 0.0027,
      "step": 47660
    },
    {
      "epoch": 3.178,
      "grad_norm": 0.4524172842502594,
      "learning_rate": 3.01375e-05,
      "loss": 0.0033,
      "step": 47670
    },
    {
      "epoch": 3.1786666666666665,
      "grad_norm": 0.2543281316757202,
      "learning_rate": 3.0133333333333335e-05,
      "loss": 0.0023,
      "step": 47680
    },
    {
      "epoch": 3.179333333333333,
      "grad_norm": 0.5791966319084167,
      "learning_rate": 3.012916666666667e-05,
      "loss": 0.0028,
      "step": 47690
    },
    {
      "epoch": 3.18,
      "grad_norm": 0.042624861001968384,
      "learning_rate": 3.0125000000000004e-05,
      "loss": 0.0021,
      "step": 47700
    },
    {
      "epoch": 3.1806666666666668,
      "grad_norm": 0.5516800880432129,
      "learning_rate": 3.0120833333333338e-05,
      "loss": 0.0024,
      "step": 47710
    },
    {
      "epoch": 3.1813333333333333,
      "grad_norm": 0.12367751449346542,
      "learning_rate": 3.011666666666667e-05,
      "loss": 0.002,
      "step": 47720
    },
    {
      "epoch": 3.182,
      "grad_norm": 0.7543664574623108,
      "learning_rate": 3.01125e-05,
      "loss": 0.0025,
      "step": 47730
    },
    {
      "epoch": 3.1826666666666665,
      "grad_norm": 0.6908496022224426,
      "learning_rate": 3.0108333333333334e-05,
      "loss": 0.0023,
      "step": 47740
    },
    {
      "epoch": 3.183333333333333,
      "grad_norm": 0.08167833834886551,
      "learning_rate": 3.0104166666666665e-05,
      "loss": 0.0021,
      "step": 47750
    },
    {
      "epoch": 3.184,
      "grad_norm": 0.4262062907218933,
      "learning_rate": 3.01e-05,
      "loss": 0.0026,
      "step": 47760
    },
    {
      "epoch": 3.1846666666666668,
      "grad_norm": 0.08773349225521088,
      "learning_rate": 3.0095833333333334e-05,
      "loss": 0.002,
      "step": 47770
    },
    {
      "epoch": 3.1853333333333333,
      "grad_norm": 0.151633158326149,
      "learning_rate": 3.009166666666667e-05,
      "loss": 0.0025,
      "step": 47780
    },
    {
      "epoch": 3.186,
      "grad_norm": 0.26647791266441345,
      "learning_rate": 3.0087500000000003e-05,
      "loss": 0.002,
      "step": 47790
    },
    {
      "epoch": 3.1866666666666665,
      "grad_norm": 0.054430361837148666,
      "learning_rate": 3.0083333333333337e-05,
      "loss": 0.0018,
      "step": 47800
    },
    {
      "epoch": 3.187333333333333,
      "grad_norm": 0.26431897282600403,
      "learning_rate": 3.0079166666666668e-05,
      "loss": 0.0019,
      "step": 47810
    },
    {
      "epoch": 3.188,
      "grad_norm": 0.6642653346061707,
      "learning_rate": 3.0075000000000003e-05,
      "loss": 0.0016,
      "step": 47820
    },
    {
      "epoch": 3.1886666666666668,
      "grad_norm": 0.23624677956104279,
      "learning_rate": 3.0070833333333337e-05,
      "loss": 0.0019,
      "step": 47830
    },
    {
      "epoch": 3.1893333333333334,
      "grad_norm": 0.5497812032699585,
      "learning_rate": 3.006666666666667e-05,
      "loss": 0.0018,
      "step": 47840
    },
    {
      "epoch": 3.19,
      "grad_norm": 0.38347750902175903,
      "learning_rate": 3.00625e-05,
      "loss": 0.0019,
      "step": 47850
    },
    {
      "epoch": 3.1906666666666665,
      "grad_norm": 0.32919827103614807,
      "learning_rate": 3.0058333333333333e-05,
      "loss": 0.0033,
      "step": 47860
    },
    {
      "epoch": 3.191333333333333,
      "grad_norm": 1.119528889656067,
      "learning_rate": 3.0054166666666668e-05,
      "loss": 0.0022,
      "step": 47870
    },
    {
      "epoch": 3.192,
      "grad_norm": 0.2927609384059906,
      "learning_rate": 3.0050000000000002e-05,
      "loss": 0.0024,
      "step": 47880
    },
    {
      "epoch": 3.1926666666666668,
      "grad_norm": 0.8068408370018005,
      "learning_rate": 3.0045833333333333e-05,
      "loss": 0.0027,
      "step": 47890
    },
    {
      "epoch": 3.1933333333333334,
      "grad_norm": 0.5513566732406616,
      "learning_rate": 3.0041666666666667e-05,
      "loss": 0.0023,
      "step": 47900
    },
    {
      "epoch": 3.194,
      "grad_norm": 0.29477256536483765,
      "learning_rate": 3.00375e-05,
      "loss": 0.0015,
      "step": 47910
    },
    {
      "epoch": 3.1946666666666665,
      "grad_norm": 0.16560888290405273,
      "learning_rate": 3.0033333333333336e-05,
      "loss": 0.0021,
      "step": 47920
    },
    {
      "epoch": 3.195333333333333,
      "grad_norm": 0.08326564729213715,
      "learning_rate": 3.002916666666667e-05,
      "loss": 0.0026,
      "step": 47930
    },
    {
      "epoch": 3.196,
      "grad_norm": 0.37875017523765564,
      "learning_rate": 3.0025000000000005e-05,
      "loss": 0.0013,
      "step": 47940
    },
    {
      "epoch": 3.1966666666666668,
      "grad_norm": 0.2240203320980072,
      "learning_rate": 3.0020833333333336e-05,
      "loss": 0.0025,
      "step": 47950
    },
    {
      "epoch": 3.1973333333333334,
      "grad_norm": 0.4744637608528137,
      "learning_rate": 3.001666666666667e-05,
      "loss": 0.0015,
      "step": 47960
    },
    {
      "epoch": 3.198,
      "grad_norm": 0.6860437393188477,
      "learning_rate": 3.0012499999999998e-05,
      "loss": 0.0042,
      "step": 47970
    },
    {
      "epoch": 3.1986666666666665,
      "grad_norm": 0.215722918510437,
      "learning_rate": 3.0008333333333332e-05,
      "loss": 0.0017,
      "step": 47980
    },
    {
      "epoch": 3.199333333333333,
      "grad_norm": 0.047823455184698105,
      "learning_rate": 3.0004166666666666e-05,
      "loss": 0.0021,
      "step": 47990
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.5441840887069702,
      "learning_rate": 3e-05,
      "loss": 0.0025,
      "step": 48000
    },
    {
      "epoch": 3.2006666666666668,
      "grad_norm": 0.3298359513282776,
      "learning_rate": 2.9995833333333335e-05,
      "loss": 0.0019,
      "step": 48010
    },
    {
      "epoch": 3.2013333333333334,
      "grad_norm": 0.39664962887763977,
      "learning_rate": 2.999166666666667e-05,
      "loss": 0.0025,
      "step": 48020
    },
    {
      "epoch": 3.202,
      "grad_norm": 0.4048219323158264,
      "learning_rate": 2.99875e-05,
      "loss": 0.0023,
      "step": 48030
    },
    {
      "epoch": 3.2026666666666666,
      "grad_norm": 0.8068379163742065,
      "learning_rate": 2.9983333333333335e-05,
      "loss": 0.002,
      "step": 48040
    },
    {
      "epoch": 3.203333333333333,
      "grad_norm": 0.38055530190467834,
      "learning_rate": 2.997916666666667e-05,
      "loss": 0.002,
      "step": 48050
    },
    {
      "epoch": 3.204,
      "grad_norm": 0.2151796668767929,
      "learning_rate": 2.9975000000000004e-05,
      "loss": 0.0021,
      "step": 48060
    },
    {
      "epoch": 3.2046666666666668,
      "grad_norm": 0.620826005935669,
      "learning_rate": 2.9970833333333338e-05,
      "loss": 0.0023,
      "step": 48070
    },
    {
      "epoch": 3.2053333333333334,
      "grad_norm": 0.08373896032571793,
      "learning_rate": 2.9966666666666672e-05,
      "loss": 0.0027,
      "step": 48080
    },
    {
      "epoch": 3.206,
      "grad_norm": 0.6264665126800537,
      "learning_rate": 2.99625e-05,
      "loss": 0.0022,
      "step": 48090
    },
    {
      "epoch": 3.2066666666666666,
      "grad_norm": 0.5138260126113892,
      "learning_rate": 2.9958333333333334e-05,
      "loss": 0.0028,
      "step": 48100
    },
    {
      "epoch": 3.207333333333333,
      "grad_norm": 0.5242042541503906,
      "learning_rate": 2.9954166666666665e-05,
      "loss": 0.0019,
      "step": 48110
    },
    {
      "epoch": 3.208,
      "grad_norm": 0.441152423620224,
      "learning_rate": 2.995e-05,
      "loss": 0.0016,
      "step": 48120
    },
    {
      "epoch": 3.208666666666667,
      "grad_norm": 0.12929807603359222,
      "learning_rate": 2.9945833333333334e-05,
      "loss": 0.0017,
      "step": 48130
    },
    {
      "epoch": 3.2093333333333334,
      "grad_norm": 0.24888873100280762,
      "learning_rate": 2.9941666666666668e-05,
      "loss": 0.0017,
      "step": 48140
    },
    {
      "epoch": 3.21,
      "grad_norm": 0.07851668447256088,
      "learning_rate": 2.9937500000000003e-05,
      "loss": 0.0022,
      "step": 48150
    },
    {
      "epoch": 3.2106666666666666,
      "grad_norm": 0.35070598125457764,
      "learning_rate": 2.9933333333333337e-05,
      "loss": 0.0016,
      "step": 48160
    },
    {
      "epoch": 3.211333333333333,
      "grad_norm": 0.10712694376707077,
      "learning_rate": 2.9929166666666668e-05,
      "loss": 0.0018,
      "step": 48170
    },
    {
      "epoch": 3.212,
      "grad_norm": 0.15542295575141907,
      "learning_rate": 2.9925000000000002e-05,
      "loss": 0.0019,
      "step": 48180
    },
    {
      "epoch": 3.212666666666667,
      "grad_norm": 0.046985700726509094,
      "learning_rate": 2.9920833333333337e-05,
      "loss": 0.0024,
      "step": 48190
    },
    {
      "epoch": 3.2133333333333334,
      "grad_norm": 0.06742953509092331,
      "learning_rate": 2.991666666666667e-05,
      "loss": 0.0024,
      "step": 48200
    },
    {
      "epoch": 3.214,
      "grad_norm": 0.2298903614282608,
      "learning_rate": 2.99125e-05,
      "loss": 0.0021,
      "step": 48210
    },
    {
      "epoch": 3.2146666666666666,
      "grad_norm": 0.15245021879673004,
      "learning_rate": 2.9908333333333333e-05,
      "loss": 0.0022,
      "step": 48220
    },
    {
      "epoch": 3.215333333333333,
      "grad_norm": 0.08101875334978104,
      "learning_rate": 2.9904166666666667e-05,
      "loss": 0.0028,
      "step": 48230
    },
    {
      "epoch": 3.216,
      "grad_norm": 0.10917709767818451,
      "learning_rate": 2.9900000000000002e-05,
      "loss": 0.0037,
      "step": 48240
    },
    {
      "epoch": 3.216666666666667,
      "grad_norm": 0.11322548240423203,
      "learning_rate": 2.9895833333333333e-05,
      "loss": 0.002,
      "step": 48250
    },
    {
      "epoch": 3.2173333333333334,
      "grad_norm": 0.24790002405643463,
      "learning_rate": 2.9891666666666667e-05,
      "loss": 0.0017,
      "step": 48260
    },
    {
      "epoch": 3.218,
      "grad_norm": 0.19043537974357605,
      "learning_rate": 2.98875e-05,
      "loss": 0.0023,
      "step": 48270
    },
    {
      "epoch": 3.2186666666666666,
      "grad_norm": 0.4279473125934601,
      "learning_rate": 2.9883333333333336e-05,
      "loss": 0.0022,
      "step": 48280
    },
    {
      "epoch": 3.219333333333333,
      "grad_norm": 0.15732404589653015,
      "learning_rate": 2.987916666666667e-05,
      "loss": 0.0021,
      "step": 48290
    },
    {
      "epoch": 3.22,
      "grad_norm": 0.2517075836658478,
      "learning_rate": 2.9875000000000004e-05,
      "loss": 0.0013,
      "step": 48300
    },
    {
      "epoch": 3.220666666666667,
      "grad_norm": 0.1121833324432373,
      "learning_rate": 2.9870833333333335e-05,
      "loss": 0.0021,
      "step": 48310
    },
    {
      "epoch": 3.2213333333333334,
      "grad_norm": 0.6252819895744324,
      "learning_rate": 2.986666666666667e-05,
      "loss": 0.002,
      "step": 48320
    },
    {
      "epoch": 3.222,
      "grad_norm": 0.557596743106842,
      "learning_rate": 2.9862499999999997e-05,
      "loss": 0.0036,
      "step": 48330
    },
    {
      "epoch": 3.2226666666666666,
      "grad_norm": 0.2146623134613037,
      "learning_rate": 2.9858333333333332e-05,
      "loss": 0.0015,
      "step": 48340
    },
    {
      "epoch": 3.223333333333333,
      "grad_norm": 0.07692822068929672,
      "learning_rate": 2.9854166666666666e-05,
      "loss": 0.0019,
      "step": 48350
    },
    {
      "epoch": 3.224,
      "grad_norm": 0.4640808403491974,
      "learning_rate": 2.985e-05,
      "loss": 0.0024,
      "step": 48360
    },
    {
      "epoch": 3.224666666666667,
      "grad_norm": 0.3670874834060669,
      "learning_rate": 2.9845833333333335e-05,
      "loss": 0.002,
      "step": 48370
    },
    {
      "epoch": 3.2253333333333334,
      "grad_norm": 0.11322247236967087,
      "learning_rate": 2.984166666666667e-05,
      "loss": 0.0024,
      "step": 48380
    },
    {
      "epoch": 3.226,
      "grad_norm": 0.02115180343389511,
      "learning_rate": 2.98375e-05,
      "loss": 0.0027,
      "step": 48390
    },
    {
      "epoch": 3.2266666666666666,
      "grad_norm": 0.4988248348236084,
      "learning_rate": 2.9833333333333335e-05,
      "loss": 0.0025,
      "step": 48400
    },
    {
      "epoch": 3.227333333333333,
      "grad_norm": 0.37316960096359253,
      "learning_rate": 2.982916666666667e-05,
      "loss": 0.0018,
      "step": 48410
    },
    {
      "epoch": 3.228,
      "grad_norm": 0.0468728244304657,
      "learning_rate": 2.9825000000000003e-05,
      "loss": 0.0015,
      "step": 48420
    },
    {
      "epoch": 3.228666666666667,
      "grad_norm": 0.8359156847000122,
      "learning_rate": 2.9820833333333338e-05,
      "loss": 0.0021,
      "step": 48430
    },
    {
      "epoch": 3.2293333333333334,
      "grad_norm": 0.1464344561100006,
      "learning_rate": 2.9816666666666672e-05,
      "loss": 0.0028,
      "step": 48440
    },
    {
      "epoch": 3.23,
      "grad_norm": 0.1475125402212143,
      "learning_rate": 2.98125e-05,
      "loss": 0.0019,
      "step": 48450
    },
    {
      "epoch": 3.2306666666666666,
      "grad_norm": 0.4038518965244293,
      "learning_rate": 2.9808333333333334e-05,
      "loss": 0.0025,
      "step": 48460
    },
    {
      "epoch": 3.231333333333333,
      "grad_norm": 0.049304891377687454,
      "learning_rate": 2.9804166666666665e-05,
      "loss": 0.0014,
      "step": 48470
    },
    {
      "epoch": 3.232,
      "grad_norm": 0.35705116391181946,
      "learning_rate": 2.98e-05,
      "loss": 0.0022,
      "step": 48480
    },
    {
      "epoch": 3.232666666666667,
      "grad_norm": 0.8847212195396423,
      "learning_rate": 2.9795833333333334e-05,
      "loss": 0.0019,
      "step": 48490
    },
    {
      "epoch": 3.2333333333333334,
      "grad_norm": 0.44789883494377136,
      "learning_rate": 2.9791666666666668e-05,
      "loss": 0.0027,
      "step": 48500
    },
    {
      "epoch": 3.234,
      "grad_norm": 0.46308380365371704,
      "learning_rate": 2.9787500000000002e-05,
      "loss": 0.002,
      "step": 48510
    },
    {
      "epoch": 3.2346666666666666,
      "grad_norm": 0.6892299056053162,
      "learning_rate": 2.9783333333333337e-05,
      "loss": 0.0023,
      "step": 48520
    },
    {
      "epoch": 3.235333333333333,
      "grad_norm": 0.08441298454999924,
      "learning_rate": 2.9779166666666668e-05,
      "loss": 0.0023,
      "step": 48530
    },
    {
      "epoch": 3.2359999999999998,
      "grad_norm": 0.0803082063794136,
      "learning_rate": 2.9775000000000002e-05,
      "loss": 0.0014,
      "step": 48540
    },
    {
      "epoch": 3.236666666666667,
      "grad_norm": 0.1503138691186905,
      "learning_rate": 2.9770833333333336e-05,
      "loss": 0.0027,
      "step": 48550
    },
    {
      "epoch": 3.2373333333333334,
      "grad_norm": 0.20093993842601776,
      "learning_rate": 2.976666666666667e-05,
      "loss": 0.0029,
      "step": 48560
    },
    {
      "epoch": 3.238,
      "grad_norm": 0.1862010508775711,
      "learning_rate": 2.97625e-05,
      "loss": 0.0018,
      "step": 48570
    },
    {
      "epoch": 3.2386666666666666,
      "grad_norm": 0.4385358691215515,
      "learning_rate": 2.9758333333333333e-05,
      "loss": 0.0025,
      "step": 48580
    },
    {
      "epoch": 3.239333333333333,
      "grad_norm": 0.06868638098239899,
      "learning_rate": 2.9754166666666667e-05,
      "loss": 0.0018,
      "step": 48590
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.1564735323190689,
      "learning_rate": 2.975e-05,
      "loss": 0.0021,
      "step": 48600
    },
    {
      "epoch": 3.240666666666667,
      "grad_norm": 0.19517844915390015,
      "learning_rate": 2.9745833333333332e-05,
      "loss": 0.0022,
      "step": 48610
    },
    {
      "epoch": 3.2413333333333334,
      "grad_norm": 0.36364245414733887,
      "learning_rate": 2.9741666666666667e-05,
      "loss": 0.0022,
      "step": 48620
    },
    {
      "epoch": 3.242,
      "grad_norm": 0.10296928137540817,
      "learning_rate": 2.97375e-05,
      "loss": 0.0029,
      "step": 48630
    },
    {
      "epoch": 3.2426666666666666,
      "grad_norm": 0.35736000537872314,
      "learning_rate": 2.9733333333333336e-05,
      "loss": 0.0016,
      "step": 48640
    },
    {
      "epoch": 3.243333333333333,
      "grad_norm": 0.3554867208003998,
      "learning_rate": 2.972916666666667e-05,
      "loss": 0.002,
      "step": 48650
    },
    {
      "epoch": 3.2439999999999998,
      "grad_norm": 0.9519206285476685,
      "learning_rate": 2.9725000000000004e-05,
      "loss": 0.002,
      "step": 48660
    },
    {
      "epoch": 3.244666666666667,
      "grad_norm": 0.5986011028289795,
      "learning_rate": 2.9720833333333335e-05,
      "loss": 0.0032,
      "step": 48670
    },
    {
      "epoch": 3.2453333333333334,
      "grad_norm": 0.04059026762843132,
      "learning_rate": 2.971666666666667e-05,
      "loss": 0.0029,
      "step": 48680
    },
    {
      "epoch": 3.246,
      "grad_norm": 0.19423829019069672,
      "learning_rate": 2.9712499999999997e-05,
      "loss": 0.0032,
      "step": 48690
    },
    {
      "epoch": 3.2466666666666666,
      "grad_norm": 0.09304986149072647,
      "learning_rate": 2.970833333333333e-05,
      "loss": 0.0025,
      "step": 48700
    },
    {
      "epoch": 3.247333333333333,
      "grad_norm": 0.29933202266693115,
      "learning_rate": 2.9704166666666666e-05,
      "loss": 0.0024,
      "step": 48710
    },
    {
      "epoch": 3.248,
      "grad_norm": 0.4920261800289154,
      "learning_rate": 2.97e-05,
      "loss": 0.0018,
      "step": 48720
    },
    {
      "epoch": 3.248666666666667,
      "grad_norm": 0.4925259053707123,
      "learning_rate": 2.9695833333333335e-05,
      "loss": 0.0018,
      "step": 48730
    },
    {
      "epoch": 3.2493333333333334,
      "grad_norm": 0.028086330741643906,
      "learning_rate": 2.969166666666667e-05,
      "loss": 0.0018,
      "step": 48740
    },
    {
      "epoch": 3.25,
      "grad_norm": 0.4133140444755554,
      "learning_rate": 2.96875e-05,
      "loss": 0.0018,
      "step": 48750
    },
    {
      "epoch": 3.2506666666666666,
      "grad_norm": 0.307370126247406,
      "learning_rate": 2.9683333333333334e-05,
      "loss": 0.0026,
      "step": 48760
    },
    {
      "epoch": 3.251333333333333,
      "grad_norm": 0.39439207315444946,
      "learning_rate": 2.967916666666667e-05,
      "loss": 0.0024,
      "step": 48770
    },
    {
      "epoch": 3.252,
      "grad_norm": 0.27138784527778625,
      "learning_rate": 2.9675000000000003e-05,
      "loss": 0.0026,
      "step": 48780
    },
    {
      "epoch": 3.252666666666667,
      "grad_norm": 0.6083771586418152,
      "learning_rate": 2.9670833333333337e-05,
      "loss": 0.0018,
      "step": 48790
    },
    {
      "epoch": 3.2533333333333334,
      "grad_norm": 0.6931379437446594,
      "learning_rate": 2.9666666666666672e-05,
      "loss": 0.0024,
      "step": 48800
    },
    {
      "epoch": 3.254,
      "grad_norm": 0.28490084409713745,
      "learning_rate": 2.9662500000000003e-05,
      "loss": 0.0031,
      "step": 48810
    },
    {
      "epoch": 3.2546666666666666,
      "grad_norm": 0.10882192105054855,
      "learning_rate": 2.9658333333333334e-05,
      "loss": 0.0014,
      "step": 48820
    },
    {
      "epoch": 3.255333333333333,
      "grad_norm": 0.0848783478140831,
      "learning_rate": 2.9654166666666665e-05,
      "loss": 0.0036,
      "step": 48830
    },
    {
      "epoch": 3.2560000000000002,
      "grad_norm": 0.1186157613992691,
      "learning_rate": 2.965e-05,
      "loss": 0.0029,
      "step": 48840
    },
    {
      "epoch": 3.256666666666667,
      "grad_norm": 0.1829429268836975,
      "learning_rate": 2.9645833333333333e-05,
      "loss": 0.003,
      "step": 48850
    },
    {
      "epoch": 3.2573333333333334,
      "grad_norm": 0.04383020102977753,
      "learning_rate": 2.9641666666666668e-05,
      "loss": 0.0027,
      "step": 48860
    },
    {
      "epoch": 3.258,
      "grad_norm": 0.577299177646637,
      "learning_rate": 2.9637500000000002e-05,
      "loss": 0.0023,
      "step": 48870
    },
    {
      "epoch": 3.2586666666666666,
      "grad_norm": 0.09633585065603256,
      "learning_rate": 2.9633333333333336e-05,
      "loss": 0.0025,
      "step": 48880
    },
    {
      "epoch": 3.259333333333333,
      "grad_norm": 0.22917701303958893,
      "learning_rate": 2.9629166666666667e-05,
      "loss": 0.0027,
      "step": 48890
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.37496083974838257,
      "learning_rate": 2.9625000000000002e-05,
      "loss": 0.0023,
      "step": 48900
    },
    {
      "epoch": 3.260666666666667,
      "grad_norm": 0.807841420173645,
      "learning_rate": 2.9620833333333336e-05,
      "loss": 0.0024,
      "step": 48910
    },
    {
      "epoch": 3.2613333333333334,
      "grad_norm": 0.8502917289733887,
      "learning_rate": 2.961666666666667e-05,
      "loss": 0.0016,
      "step": 48920
    },
    {
      "epoch": 3.262,
      "grad_norm": 0.5249181389808655,
      "learning_rate": 2.9612500000000005e-05,
      "loss": 0.0026,
      "step": 48930
    },
    {
      "epoch": 3.2626666666666666,
      "grad_norm": 0.33656927943229675,
      "learning_rate": 2.9608333333333332e-05,
      "loss": 0.002,
      "step": 48940
    },
    {
      "epoch": 3.263333333333333,
      "grad_norm": 0.3664119243621826,
      "learning_rate": 2.9604166666666667e-05,
      "loss": 0.0015,
      "step": 48950
    },
    {
      "epoch": 3.2640000000000002,
      "grad_norm": 0.33736592531204224,
      "learning_rate": 2.96e-05,
      "loss": 0.0023,
      "step": 48960
    },
    {
      "epoch": 3.264666666666667,
      "grad_norm": 0.3298523724079132,
      "learning_rate": 2.9595833333333332e-05,
      "loss": 0.0021,
      "step": 48970
    },
    {
      "epoch": 3.2653333333333334,
      "grad_norm": 0.8036820292472839,
      "learning_rate": 2.9591666666666667e-05,
      "loss": 0.0023,
      "step": 48980
    },
    {
      "epoch": 3.266,
      "grad_norm": 0.19824188947677612,
      "learning_rate": 2.95875e-05,
      "loss": 0.0028,
      "step": 48990
    },
    {
      "epoch": 3.2666666666666666,
      "grad_norm": 0.4145917296409607,
      "learning_rate": 2.9583333333333335e-05,
      "loss": 0.0028,
      "step": 49000
    },
    {
      "epoch": 3.267333333333333,
      "grad_norm": 0.3038374185562134,
      "learning_rate": 2.957916666666667e-05,
      "loss": 0.0016,
      "step": 49010
    },
    {
      "epoch": 3.268,
      "grad_norm": 0.48342394828796387,
      "learning_rate": 2.9575000000000004e-05,
      "loss": 0.0027,
      "step": 49020
    },
    {
      "epoch": 3.268666666666667,
      "grad_norm": 0.07993936538696289,
      "learning_rate": 2.9570833333333335e-05,
      "loss": 0.0021,
      "step": 49030
    },
    {
      "epoch": 3.2693333333333334,
      "grad_norm": 0.2539341151714325,
      "learning_rate": 2.956666666666667e-05,
      "loss": 0.0023,
      "step": 49040
    },
    {
      "epoch": 3.27,
      "grad_norm": 0.07190271466970444,
      "learning_rate": 2.9562500000000004e-05,
      "loss": 0.0015,
      "step": 49050
    },
    {
      "epoch": 3.2706666666666666,
      "grad_norm": 0.1217561587691307,
      "learning_rate": 2.955833333333333e-05,
      "loss": 0.0027,
      "step": 49060
    },
    {
      "epoch": 3.271333333333333,
      "grad_norm": 0.11976607143878937,
      "learning_rate": 2.9554166666666666e-05,
      "loss": 0.0018,
      "step": 49070
    },
    {
      "epoch": 3.2720000000000002,
      "grad_norm": 0.36404281854629517,
      "learning_rate": 2.955e-05,
      "loss": 0.0026,
      "step": 49080
    },
    {
      "epoch": 3.272666666666667,
      "grad_norm": 0.631705641746521,
      "learning_rate": 2.9545833333333334e-05,
      "loss": 0.0023,
      "step": 49090
    },
    {
      "epoch": 3.2733333333333334,
      "grad_norm": 0.19982518255710602,
      "learning_rate": 2.954166666666667e-05,
      "loss": 0.0021,
      "step": 49100
    },
    {
      "epoch": 3.274,
      "grad_norm": 0.26105862855911255,
      "learning_rate": 2.95375e-05,
      "loss": 0.0015,
      "step": 49110
    },
    {
      "epoch": 3.2746666666666666,
      "grad_norm": 0.14819787442684174,
      "learning_rate": 2.9533333333333334e-05,
      "loss": 0.0024,
      "step": 49120
    },
    {
      "epoch": 3.275333333333333,
      "grad_norm": 0.40312832593917847,
      "learning_rate": 2.952916666666667e-05,
      "loss": 0.0024,
      "step": 49130
    },
    {
      "epoch": 3.276,
      "grad_norm": 0.07596628367900848,
      "learning_rate": 2.9525000000000003e-05,
      "loss": 0.0025,
      "step": 49140
    },
    {
      "epoch": 3.276666666666667,
      "grad_norm": 0.3650675415992737,
      "learning_rate": 2.9520833333333337e-05,
      "loss": 0.0014,
      "step": 49150
    },
    {
      "epoch": 3.2773333333333334,
      "grad_norm": 0.21763703227043152,
      "learning_rate": 2.951666666666667e-05,
      "loss": 0.0021,
      "step": 49160
    },
    {
      "epoch": 3.278,
      "grad_norm": 0.18210463225841522,
      "learning_rate": 2.9512500000000002e-05,
      "loss": 0.0025,
      "step": 49170
    },
    {
      "epoch": 3.2786666666666666,
      "grad_norm": 0.07953023910522461,
      "learning_rate": 2.9508333333333333e-05,
      "loss": 0.0024,
      "step": 49180
    },
    {
      "epoch": 3.279333333333333,
      "grad_norm": 0.8704161643981934,
      "learning_rate": 2.9504166666666664e-05,
      "loss": 0.0026,
      "step": 49190
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 0.9371575117111206,
      "learning_rate": 2.95e-05,
      "loss": 0.0025,
      "step": 49200
    },
    {
      "epoch": 3.280666666666667,
      "grad_norm": 0.7281743884086609,
      "learning_rate": 2.9495833333333333e-05,
      "loss": 0.0013,
      "step": 49210
    },
    {
      "epoch": 3.2813333333333334,
      "grad_norm": 0.25522613525390625,
      "learning_rate": 2.9491666666666667e-05,
      "loss": 0.002,
      "step": 49220
    },
    {
      "epoch": 3.282,
      "grad_norm": 0.3584357798099518,
      "learning_rate": 2.9487500000000002e-05,
      "loss": 0.0016,
      "step": 49230
    },
    {
      "epoch": 3.2826666666666666,
      "grad_norm": 0.30470073223114014,
      "learning_rate": 2.9483333333333336e-05,
      "loss": 0.0019,
      "step": 49240
    },
    {
      "epoch": 3.283333333333333,
      "grad_norm": 0.3272589147090912,
      "learning_rate": 2.9479166666666667e-05,
      "loss": 0.0019,
      "step": 49250
    },
    {
      "epoch": 3.284,
      "grad_norm": 0.5054344534873962,
      "learning_rate": 2.9475e-05,
      "loss": 0.0027,
      "step": 49260
    },
    {
      "epoch": 3.284666666666667,
      "grad_norm": 0.4276711046695709,
      "learning_rate": 2.9470833333333336e-05,
      "loss": 0.0028,
      "step": 49270
    },
    {
      "epoch": 3.2853333333333334,
      "grad_norm": 0.058672282844781876,
      "learning_rate": 2.946666666666667e-05,
      "loss": 0.0019,
      "step": 49280
    },
    {
      "epoch": 3.286,
      "grad_norm": 0.051555804908275604,
      "learning_rate": 2.9462500000000005e-05,
      "loss": 0.0023,
      "step": 49290
    },
    {
      "epoch": 3.2866666666666666,
      "grad_norm": 0.4646854102611542,
      "learning_rate": 2.9458333333333332e-05,
      "loss": 0.0023,
      "step": 49300
    },
    {
      "epoch": 3.287333333333333,
      "grad_norm": 0.38052818179130554,
      "learning_rate": 2.9454166666666667e-05,
      "loss": 0.0018,
      "step": 49310
    },
    {
      "epoch": 3.288,
      "grad_norm": 0.41271889209747314,
      "learning_rate": 2.945e-05,
      "loss": 0.0025,
      "step": 49320
    },
    {
      "epoch": 3.288666666666667,
      "grad_norm": 0.15537357330322266,
      "learning_rate": 2.9445833333333332e-05,
      "loss": 0.0027,
      "step": 49330
    },
    {
      "epoch": 3.2893333333333334,
      "grad_norm": 0.19853512942790985,
      "learning_rate": 2.9441666666666666e-05,
      "loss": 0.0027,
      "step": 49340
    },
    {
      "epoch": 3.29,
      "grad_norm": 0.09469406306743622,
      "learning_rate": 2.94375e-05,
      "loss": 0.0018,
      "step": 49350
    },
    {
      "epoch": 3.2906666666666666,
      "grad_norm": 0.09164448827505112,
      "learning_rate": 2.9433333333333335e-05,
      "loss": 0.0022,
      "step": 49360
    },
    {
      "epoch": 3.291333333333333,
      "grad_norm": 0.7056668996810913,
      "learning_rate": 2.942916666666667e-05,
      "loss": 0.0022,
      "step": 49370
    },
    {
      "epoch": 3.292,
      "grad_norm": 0.8383349180221558,
      "learning_rate": 2.9425000000000004e-05,
      "loss": 0.003,
      "step": 49380
    },
    {
      "epoch": 3.292666666666667,
      "grad_norm": 0.8101473450660706,
      "learning_rate": 2.9420833333333335e-05,
      "loss": 0.0021,
      "step": 49390
    },
    {
      "epoch": 3.2933333333333334,
      "grad_norm": 0.11109988391399384,
      "learning_rate": 2.941666666666667e-05,
      "loss": 0.0014,
      "step": 49400
    },
    {
      "epoch": 3.294,
      "grad_norm": 0.1879783570766449,
      "learning_rate": 2.9412500000000003e-05,
      "loss": 0.0024,
      "step": 49410
    },
    {
      "epoch": 3.2946666666666666,
      "grad_norm": 0.29513680934906006,
      "learning_rate": 2.940833333333333e-05,
      "loss": 0.0034,
      "step": 49420
    },
    {
      "epoch": 3.2953333333333332,
      "grad_norm": 0.6002061367034912,
      "learning_rate": 2.9404166666666665e-05,
      "loss": 0.0023,
      "step": 49430
    },
    {
      "epoch": 3.296,
      "grad_norm": 0.2826378047466278,
      "learning_rate": 2.94e-05,
      "loss": 0.0019,
      "step": 49440
    },
    {
      "epoch": 3.296666666666667,
      "grad_norm": 0.061888571828603745,
      "learning_rate": 2.9395833333333334e-05,
      "loss": 0.0026,
      "step": 49450
    },
    {
      "epoch": 3.2973333333333334,
      "grad_norm": 0.05778806656599045,
      "learning_rate": 2.939166666666667e-05,
      "loss": 0.0033,
      "step": 49460
    },
    {
      "epoch": 3.298,
      "grad_norm": 0.3053063154220581,
      "learning_rate": 2.9387500000000003e-05,
      "loss": 0.0025,
      "step": 49470
    },
    {
      "epoch": 3.2986666666666666,
      "grad_norm": 0.28724873065948486,
      "learning_rate": 2.9383333333333334e-05,
      "loss": 0.0019,
      "step": 49480
    },
    {
      "epoch": 3.2993333333333332,
      "grad_norm": 0.48675021529197693,
      "learning_rate": 2.9379166666666668e-05,
      "loss": 0.0028,
      "step": 49490
    },
    {
      "epoch": 3.3,
      "grad_norm": 0.40058982372283936,
      "learning_rate": 2.9375000000000003e-05,
      "loss": 0.0022,
      "step": 49500
    },
    {
      "epoch": 3.300666666666667,
      "grad_norm": 0.041627541184425354,
      "learning_rate": 2.9370833333333337e-05,
      "loss": 0.0016,
      "step": 49510
    },
    {
      "epoch": 3.3013333333333335,
      "grad_norm": 0.18214187026023865,
      "learning_rate": 2.936666666666667e-05,
      "loss": 0.0021,
      "step": 49520
    },
    {
      "epoch": 3.302,
      "grad_norm": 0.2991197407245636,
      "learning_rate": 2.9362500000000002e-05,
      "loss": 0.0029,
      "step": 49530
    },
    {
      "epoch": 3.3026666666666666,
      "grad_norm": 0.5192923545837402,
      "learning_rate": 2.9358333333333333e-05,
      "loss": 0.0015,
      "step": 49540
    },
    {
      "epoch": 3.3033333333333332,
      "grad_norm": 0.1916361153125763,
      "learning_rate": 2.9354166666666668e-05,
      "loss": 0.002,
      "step": 49550
    },
    {
      "epoch": 3.304,
      "grad_norm": 0.3417012095451355,
      "learning_rate": 2.935e-05,
      "loss": 0.002,
      "step": 49560
    },
    {
      "epoch": 3.304666666666667,
      "grad_norm": 0.6509342193603516,
      "learning_rate": 2.9345833333333333e-05,
      "loss": 0.0022,
      "step": 49570
    },
    {
      "epoch": 3.3053333333333335,
      "grad_norm": 0.2049570083618164,
      "learning_rate": 2.9341666666666667e-05,
      "loss": 0.0016,
      "step": 49580
    },
    {
      "epoch": 3.306,
      "grad_norm": 0.15214373171329498,
      "learning_rate": 2.93375e-05,
      "loss": 0.0023,
      "step": 49590
    },
    {
      "epoch": 3.3066666666666666,
      "grad_norm": 0.38324835896492004,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 0.003,
      "step": 49600
    },
    {
      "epoch": 3.3073333333333332,
      "grad_norm": 0.2925031781196594,
      "learning_rate": 2.932916666666667e-05,
      "loss": 0.0035,
      "step": 49610
    },
    {
      "epoch": 3.308,
      "grad_norm": 1.030008316040039,
      "learning_rate": 2.9325e-05,
      "loss": 0.0022,
      "step": 49620
    },
    {
      "epoch": 3.3086666666666664,
      "grad_norm": 0.267322838306427,
      "learning_rate": 2.9320833333333336e-05,
      "loss": 0.0017,
      "step": 49630
    },
    {
      "epoch": 3.3093333333333335,
      "grad_norm": 0.2983540892601013,
      "learning_rate": 2.931666666666667e-05,
      "loss": 0.0022,
      "step": 49640
    },
    {
      "epoch": 3.31,
      "grad_norm": 0.21764642000198364,
      "learning_rate": 2.9312500000000004e-05,
      "loss": 0.0031,
      "step": 49650
    },
    {
      "epoch": 3.3106666666666666,
      "grad_norm": 0.7490677833557129,
      "learning_rate": 2.9308333333333332e-05,
      "loss": 0.0025,
      "step": 49660
    },
    {
      "epoch": 3.3113333333333332,
      "grad_norm": 0.29341980814933777,
      "learning_rate": 2.9304166666666666e-05,
      "loss": 0.0033,
      "step": 49670
    },
    {
      "epoch": 3.312,
      "grad_norm": 0.03948989138007164,
      "learning_rate": 2.93e-05,
      "loss": 0.0021,
      "step": 49680
    },
    {
      "epoch": 3.312666666666667,
      "grad_norm": 0.18600021302700043,
      "learning_rate": 2.9295833333333335e-05,
      "loss": 0.0017,
      "step": 49690
    },
    {
      "epoch": 3.3133333333333335,
      "grad_norm": 0.7113556861877441,
      "learning_rate": 2.9291666666666666e-05,
      "loss": 0.0029,
      "step": 49700
    },
    {
      "epoch": 3.314,
      "grad_norm": 0.35955992341041565,
      "learning_rate": 2.92875e-05,
      "loss": 0.0013,
      "step": 49710
    },
    {
      "epoch": 3.3146666666666667,
      "grad_norm": 0.38924553990364075,
      "learning_rate": 2.9283333333333335e-05,
      "loss": 0.0028,
      "step": 49720
    },
    {
      "epoch": 3.3153333333333332,
      "grad_norm": 0.4585036337375641,
      "learning_rate": 2.927916666666667e-05,
      "loss": 0.0025,
      "step": 49730
    },
    {
      "epoch": 3.316,
      "grad_norm": 0.289538711309433,
      "learning_rate": 2.9275000000000003e-05,
      "loss": 0.0022,
      "step": 49740
    },
    {
      "epoch": 3.3166666666666664,
      "grad_norm": 0.2831920087337494,
      "learning_rate": 2.9270833333333338e-05,
      "loss": 0.002,
      "step": 49750
    },
    {
      "epoch": 3.3173333333333335,
      "grad_norm": 0.2943674623966217,
      "learning_rate": 2.926666666666667e-05,
      "loss": 0.0024,
      "step": 49760
    },
    {
      "epoch": 3.318,
      "grad_norm": 0.30802130699157715,
      "learning_rate": 2.9262500000000003e-05,
      "loss": 0.0019,
      "step": 49770
    },
    {
      "epoch": 3.3186666666666667,
      "grad_norm": 0.18962207436561584,
      "learning_rate": 2.925833333333333e-05,
      "loss": 0.0019,
      "step": 49780
    },
    {
      "epoch": 3.3193333333333332,
      "grad_norm": 0.3696040213108063,
      "learning_rate": 2.9254166666666665e-05,
      "loss": 0.0027,
      "step": 49790
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.44479963183403015,
      "learning_rate": 2.925e-05,
      "loss": 0.0026,
      "step": 49800
    },
    {
      "epoch": 3.320666666666667,
      "grad_norm": 0.6551932692527771,
      "learning_rate": 2.9245833333333334e-05,
      "loss": 0.0014,
      "step": 49810
    },
    {
      "epoch": 3.3213333333333335,
      "grad_norm": 0.22761671245098114,
      "learning_rate": 2.9241666666666668e-05,
      "loss": 0.0019,
      "step": 49820
    },
    {
      "epoch": 3.322,
      "grad_norm": 0.2632075250148773,
      "learning_rate": 2.9237500000000003e-05,
      "loss": 0.0029,
      "step": 49830
    },
    {
      "epoch": 3.3226666666666667,
      "grad_norm": 0.2601572275161743,
      "learning_rate": 2.9233333333333334e-05,
      "loss": 0.003,
      "step": 49840
    },
    {
      "epoch": 3.3233333333333333,
      "grad_norm": 0.36624446511268616,
      "learning_rate": 2.9229166666666668e-05,
      "loss": 0.0032,
      "step": 49850
    },
    {
      "epoch": 3.324,
      "grad_norm": 0.17488573491573334,
      "learning_rate": 2.9225000000000002e-05,
      "loss": 0.0017,
      "step": 49860
    },
    {
      "epoch": 3.3246666666666664,
      "grad_norm": 0.4682410657405853,
      "learning_rate": 2.9220833333333337e-05,
      "loss": 0.0026,
      "step": 49870
    },
    {
      "epoch": 3.3253333333333335,
      "grad_norm": 0.37000522017478943,
      "learning_rate": 2.921666666666667e-05,
      "loss": 0.0012,
      "step": 49880
    },
    {
      "epoch": 3.326,
      "grad_norm": 0.6614024043083191,
      "learning_rate": 2.9212500000000005e-05,
      "loss": 0.0027,
      "step": 49890
    },
    {
      "epoch": 3.3266666666666667,
      "grad_norm": 0.6706178188323975,
      "learning_rate": 2.9208333333333333e-05,
      "loss": 0.0022,
      "step": 49900
    },
    {
      "epoch": 3.3273333333333333,
      "grad_norm": 0.666047215461731,
      "learning_rate": 2.9204166666666667e-05,
      "loss": 0.0018,
      "step": 49910
    },
    {
      "epoch": 3.328,
      "grad_norm": 0.0565277561545372,
      "learning_rate": 2.9199999999999998e-05,
      "loss": 0.0029,
      "step": 49920
    },
    {
      "epoch": 3.328666666666667,
      "grad_norm": 0.22331944108009338,
      "learning_rate": 2.9195833333333333e-05,
      "loss": 0.0019,
      "step": 49930
    },
    {
      "epoch": 3.3293333333333335,
      "grad_norm": 0.38198569416999817,
      "learning_rate": 2.9191666666666667e-05,
      "loss": 0.0019,
      "step": 49940
    },
    {
      "epoch": 3.33,
      "grad_norm": 0.2952573895454407,
      "learning_rate": 2.91875e-05,
      "loss": 0.0019,
      "step": 49950
    },
    {
      "epoch": 3.3306666666666667,
      "grad_norm": 0.4392258822917938,
      "learning_rate": 2.9183333333333336e-05,
      "loss": 0.0025,
      "step": 49960
    },
    {
      "epoch": 3.3313333333333333,
      "grad_norm": 0.7706867456436157,
      "learning_rate": 2.917916666666667e-05,
      "loss": 0.0019,
      "step": 49970
    },
    {
      "epoch": 3.332,
      "grad_norm": 0.32302168011665344,
      "learning_rate": 2.9175e-05,
      "loss": 0.0024,
      "step": 49980
    },
    {
      "epoch": 3.3326666666666664,
      "grad_norm": 0.3042205572128296,
      "learning_rate": 2.9170833333333335e-05,
      "loss": 0.0028,
      "step": 49990
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.4289151728153229,
      "learning_rate": 2.916666666666667e-05,
      "loss": 0.0027,
      "step": 50000
    },
    {
      "epoch": 3.334,
      "grad_norm": 0.23794828355312347,
      "learning_rate": 2.9162500000000004e-05,
      "loss": 0.0023,
      "step": 50010
    },
    {
      "epoch": 3.3346666666666667,
      "grad_norm": 0.0811915472149849,
      "learning_rate": 2.915833333333334e-05,
      "loss": 0.0024,
      "step": 50020
    },
    {
      "epoch": 3.3353333333333333,
      "grad_norm": 0.18048956990242004,
      "learning_rate": 2.9154166666666666e-05,
      "loss": 0.0029,
      "step": 50030
    },
    {
      "epoch": 3.336,
      "grad_norm": 0.07750239223241806,
      "learning_rate": 2.915e-05,
      "loss": 0.0013,
      "step": 50040
    },
    {
      "epoch": 3.336666666666667,
      "grad_norm": 0.21642537415027618,
      "learning_rate": 2.9145833333333335e-05,
      "loss": 0.0025,
      "step": 50050
    },
    {
      "epoch": 3.3373333333333335,
      "grad_norm": 0.7550141215324402,
      "learning_rate": 2.9141666666666666e-05,
      "loss": 0.0025,
      "step": 50060
    },
    {
      "epoch": 3.338,
      "grad_norm": 0.262356698513031,
      "learning_rate": 2.91375e-05,
      "loss": 0.0017,
      "step": 50070
    },
    {
      "epoch": 3.3386666666666667,
      "grad_norm": 0.7992563247680664,
      "learning_rate": 2.9133333333333334e-05,
      "loss": 0.0019,
      "step": 50080
    },
    {
      "epoch": 3.3393333333333333,
      "grad_norm": 0.3269796371459961,
      "learning_rate": 2.912916666666667e-05,
      "loss": 0.0023,
      "step": 50090
    },
    {
      "epoch": 3.34,
      "grad_norm": 0.25180625915527344,
      "learning_rate": 2.9125000000000003e-05,
      "loss": 0.0015,
      "step": 50100
    },
    {
      "epoch": 3.3406666666666665,
      "grad_norm": 0.6104432344436646,
      "learning_rate": 2.9120833333333338e-05,
      "loss": 0.0015,
      "step": 50110
    },
    {
      "epoch": 3.3413333333333335,
      "grad_norm": 0.6416584849357605,
      "learning_rate": 2.911666666666667e-05,
      "loss": 0.0024,
      "step": 50120
    },
    {
      "epoch": 3.342,
      "grad_norm": 0.05155789852142334,
      "learning_rate": 2.9112500000000003e-05,
      "loss": 0.0018,
      "step": 50130
    },
    {
      "epoch": 3.3426666666666667,
      "grad_norm": 0.3997216522693634,
      "learning_rate": 2.9108333333333337e-05,
      "loss": 0.0018,
      "step": 50140
    },
    {
      "epoch": 3.3433333333333333,
      "grad_norm": 0.5140715837478638,
      "learning_rate": 2.9104166666666665e-05,
      "loss": 0.0018,
      "step": 50150
    },
    {
      "epoch": 3.344,
      "grad_norm": 0.0823255181312561,
      "learning_rate": 2.91e-05,
      "loss": 0.0022,
      "step": 50160
    },
    {
      "epoch": 3.344666666666667,
      "grad_norm": 0.5472075343132019,
      "learning_rate": 2.9095833333333334e-05,
      "loss": 0.0028,
      "step": 50170
    },
    {
      "epoch": 3.3453333333333335,
      "grad_norm": 1.027858853340149,
      "learning_rate": 2.9091666666666668e-05,
      "loss": 0.0024,
      "step": 50180
    },
    {
      "epoch": 3.346,
      "grad_norm": 0.19372262060642242,
      "learning_rate": 2.9087500000000002e-05,
      "loss": 0.0026,
      "step": 50190
    },
    {
      "epoch": 3.3466666666666667,
      "grad_norm": 0.15507769584655762,
      "learning_rate": 2.9083333333333333e-05,
      "loss": 0.0019,
      "step": 50200
    },
    {
      "epoch": 3.3473333333333333,
      "grad_norm": 0.1426285207271576,
      "learning_rate": 2.9079166666666668e-05,
      "loss": 0.0026,
      "step": 50210
    },
    {
      "epoch": 3.348,
      "grad_norm": 0.14869296550750732,
      "learning_rate": 2.9075000000000002e-05,
      "loss": 0.0021,
      "step": 50220
    },
    {
      "epoch": 3.3486666666666665,
      "grad_norm": 0.5185303092002869,
      "learning_rate": 2.9070833333333336e-05,
      "loss": 0.0022,
      "step": 50230
    },
    {
      "epoch": 3.3493333333333335,
      "grad_norm": 0.7586427330970764,
      "learning_rate": 2.906666666666667e-05,
      "loss": 0.0018,
      "step": 50240
    },
    {
      "epoch": 3.35,
      "grad_norm": 0.1837611347436905,
      "learning_rate": 2.9062500000000005e-05,
      "loss": 0.002,
      "step": 50250
    },
    {
      "epoch": 3.3506666666666667,
      "grad_norm": 0.2244165539741516,
      "learning_rate": 2.9058333333333336e-05,
      "loss": 0.0015,
      "step": 50260
    },
    {
      "epoch": 3.3513333333333333,
      "grad_norm": 0.5455856919288635,
      "learning_rate": 2.9054166666666667e-05,
      "loss": 0.0018,
      "step": 50270
    },
    {
      "epoch": 3.352,
      "grad_norm": 0.2522188425064087,
      "learning_rate": 2.9049999999999998e-05,
      "loss": 0.0034,
      "step": 50280
    },
    {
      "epoch": 3.352666666666667,
      "grad_norm": 0.39966607093811035,
      "learning_rate": 2.9045833333333332e-05,
      "loss": 0.002,
      "step": 50290
    },
    {
      "epoch": 3.3533333333333335,
      "grad_norm": 0.36553633213043213,
      "learning_rate": 2.9041666666666667e-05,
      "loss": 0.0027,
      "step": 50300
    },
    {
      "epoch": 3.354,
      "grad_norm": 0.25223222374916077,
      "learning_rate": 2.90375e-05,
      "loss": 0.0022,
      "step": 50310
    },
    {
      "epoch": 3.3546666666666667,
      "grad_norm": 0.44301190972328186,
      "learning_rate": 2.9033333333333335e-05,
      "loss": 0.0027,
      "step": 50320
    },
    {
      "epoch": 3.3553333333333333,
      "grad_norm": 0.2521524727344513,
      "learning_rate": 2.902916666666667e-05,
      "loss": 0.0018,
      "step": 50330
    },
    {
      "epoch": 3.356,
      "grad_norm": 0.1790114939212799,
      "learning_rate": 2.9025e-05,
      "loss": 0.0029,
      "step": 50340
    },
    {
      "epoch": 3.3566666666666665,
      "grad_norm": 0.25278204679489136,
      "learning_rate": 2.9020833333333335e-05,
      "loss": 0.0016,
      "step": 50350
    },
    {
      "epoch": 3.3573333333333335,
      "grad_norm": 0.43086501955986023,
      "learning_rate": 2.901666666666667e-05,
      "loss": 0.0016,
      "step": 50360
    },
    {
      "epoch": 3.358,
      "grad_norm": 0.10997231304645538,
      "learning_rate": 2.9012500000000004e-05,
      "loss": 0.0028,
      "step": 50370
    },
    {
      "epoch": 3.3586666666666667,
      "grad_norm": 0.7902839779853821,
      "learning_rate": 2.9008333333333338e-05,
      "loss": 0.0027,
      "step": 50380
    },
    {
      "epoch": 3.3593333333333333,
      "grad_norm": 0.07607930153608322,
      "learning_rate": 2.9004166666666666e-05,
      "loss": 0.0015,
      "step": 50390
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.05493130534887314,
      "learning_rate": 2.9e-05,
      "loss": 0.0027,
      "step": 50400
    },
    {
      "epoch": 3.360666666666667,
      "grad_norm": 0.2125236541032791,
      "learning_rate": 2.8995833333333335e-05,
      "loss": 0.0021,
      "step": 50410
    },
    {
      "epoch": 3.3613333333333335,
      "grad_norm": 0.6626009941101074,
      "learning_rate": 2.8991666666666666e-05,
      "loss": 0.002,
      "step": 50420
    },
    {
      "epoch": 3.362,
      "grad_norm": 0.44080454111099243,
      "learning_rate": 2.89875e-05,
      "loss": 0.0023,
      "step": 50430
    },
    {
      "epoch": 3.3626666666666667,
      "grad_norm": 0.12362286448478699,
      "learning_rate": 2.8983333333333334e-05,
      "loss": 0.0021,
      "step": 50440
    },
    {
      "epoch": 3.3633333333333333,
      "grad_norm": 0.1471024453639984,
      "learning_rate": 2.897916666666667e-05,
      "loss": 0.0027,
      "step": 50450
    },
    {
      "epoch": 3.364,
      "grad_norm": 0.2926290035247803,
      "learning_rate": 2.8975000000000003e-05,
      "loss": 0.0025,
      "step": 50460
    },
    {
      "epoch": 3.3646666666666665,
      "grad_norm": 0.6634342670440674,
      "learning_rate": 2.8970833333333337e-05,
      "loss": 0.003,
      "step": 50470
    },
    {
      "epoch": 3.3653333333333335,
      "grad_norm": 0.04808397591114044,
      "learning_rate": 2.8966666666666668e-05,
      "loss": 0.0042,
      "step": 50480
    },
    {
      "epoch": 3.366,
      "grad_norm": 0.851840078830719,
      "learning_rate": 2.8962500000000003e-05,
      "loss": 0.0022,
      "step": 50490
    },
    {
      "epoch": 3.3666666666666667,
      "grad_norm": 0.4033867418766022,
      "learning_rate": 2.8958333333333337e-05,
      "loss": 0.0015,
      "step": 50500
    },
    {
      "epoch": 3.3673333333333333,
      "grad_norm": 0.08926738798618317,
      "learning_rate": 2.8954166666666665e-05,
      "loss": 0.0025,
      "step": 50510
    },
    {
      "epoch": 3.368,
      "grad_norm": 0.7333824038505554,
      "learning_rate": 2.895e-05,
      "loss": 0.0018,
      "step": 50520
    },
    {
      "epoch": 3.3686666666666665,
      "grad_norm": 0.39602068066596985,
      "learning_rate": 2.8945833333333333e-05,
      "loss": 0.002,
      "step": 50530
    },
    {
      "epoch": 3.3693333333333335,
      "grad_norm": 0.18808476626873016,
      "learning_rate": 2.8941666666666668e-05,
      "loss": 0.0022,
      "step": 50540
    },
    {
      "epoch": 3.37,
      "grad_norm": 0.2286939024925232,
      "learning_rate": 2.8937500000000002e-05,
      "loss": 0.0017,
      "step": 50550
    },
    {
      "epoch": 3.3706666666666667,
      "grad_norm": 0.41023167967796326,
      "learning_rate": 2.8933333333333333e-05,
      "loss": 0.0015,
      "step": 50560
    },
    {
      "epoch": 3.3713333333333333,
      "grad_norm": 0.52259361743927,
      "learning_rate": 2.8929166666666667e-05,
      "loss": 0.0013,
      "step": 50570
    },
    {
      "epoch": 3.372,
      "grad_norm": 0.8979170322418213,
      "learning_rate": 2.8925000000000002e-05,
      "loss": 0.0029,
      "step": 50580
    },
    {
      "epoch": 3.3726666666666665,
      "grad_norm": 0.5698859691619873,
      "learning_rate": 2.8920833333333336e-05,
      "loss": 0.0017,
      "step": 50590
    },
    {
      "epoch": 3.3733333333333335,
      "grad_norm": 1.084958553314209,
      "learning_rate": 2.891666666666667e-05,
      "loss": 0.0014,
      "step": 50600
    },
    {
      "epoch": 3.374,
      "grad_norm": 0.5465332865715027,
      "learning_rate": 2.8912500000000005e-05,
      "loss": 0.0017,
      "step": 50610
    },
    {
      "epoch": 3.3746666666666667,
      "grad_norm": 0.1824166178703308,
      "learning_rate": 2.8908333333333336e-05,
      "loss": 0.0021,
      "step": 50620
    },
    {
      "epoch": 3.3753333333333333,
      "grad_norm": 0.2878749966621399,
      "learning_rate": 2.8904166666666667e-05,
      "loss": 0.0017,
      "step": 50630
    },
    {
      "epoch": 3.376,
      "grad_norm": 0.4642462134361267,
      "learning_rate": 2.8899999999999998e-05,
      "loss": 0.0017,
      "step": 50640
    },
    {
      "epoch": 3.3766666666666665,
      "grad_norm": 0.5134312510490417,
      "learning_rate": 2.8895833333333332e-05,
      "loss": 0.0019,
      "step": 50650
    },
    {
      "epoch": 3.3773333333333335,
      "grad_norm": 0.4200688600540161,
      "learning_rate": 2.8891666666666666e-05,
      "loss": 0.0017,
      "step": 50660
    },
    {
      "epoch": 3.378,
      "grad_norm": 0.059507958590984344,
      "learning_rate": 2.88875e-05,
      "loss": 0.0024,
      "step": 50670
    },
    {
      "epoch": 3.3786666666666667,
      "grad_norm": 0.549269437789917,
      "learning_rate": 2.8883333333333335e-05,
      "loss": 0.0023,
      "step": 50680
    },
    {
      "epoch": 3.3793333333333333,
      "grad_norm": 0.0774836540222168,
      "learning_rate": 2.887916666666667e-05,
      "loss": 0.0027,
      "step": 50690
    },
    {
      "epoch": 3.38,
      "grad_norm": 0.1847681701183319,
      "learning_rate": 2.8875e-05,
      "loss": 0.0015,
      "step": 50700
    },
    {
      "epoch": 3.3806666666666665,
      "grad_norm": 0.6378982067108154,
      "learning_rate": 2.8870833333333335e-05,
      "loss": 0.002,
      "step": 50710
    },
    {
      "epoch": 3.3813333333333335,
      "grad_norm": 0.4282980263233185,
      "learning_rate": 2.886666666666667e-05,
      "loss": 0.0019,
      "step": 50720
    },
    {
      "epoch": 3.382,
      "grad_norm": 0.05222059786319733,
      "learning_rate": 2.8862500000000004e-05,
      "loss": 0.0024,
      "step": 50730
    },
    {
      "epoch": 3.3826666666666667,
      "grad_norm": 0.5332667827606201,
      "learning_rate": 2.8858333333333338e-05,
      "loss": 0.0027,
      "step": 50740
    },
    {
      "epoch": 3.3833333333333333,
      "grad_norm": 0.3586585521697998,
      "learning_rate": 2.8854166666666666e-05,
      "loss": 0.0027,
      "step": 50750
    },
    {
      "epoch": 3.384,
      "grad_norm": 0.08031605929136276,
      "learning_rate": 2.885e-05,
      "loss": 0.0013,
      "step": 50760
    },
    {
      "epoch": 3.3846666666666665,
      "grad_norm": 0.18355996906757355,
      "learning_rate": 2.8845833333333334e-05,
      "loss": 0.0019,
      "step": 50770
    },
    {
      "epoch": 3.3853333333333335,
      "grad_norm": 0.3646303415298462,
      "learning_rate": 2.8841666666666665e-05,
      "loss": 0.0025,
      "step": 50780
    },
    {
      "epoch": 3.386,
      "grad_norm": 0.048938702791929245,
      "learning_rate": 2.88375e-05,
      "loss": 0.0019,
      "step": 50790
    },
    {
      "epoch": 3.3866666666666667,
      "grad_norm": 0.2192334085702896,
      "learning_rate": 2.8833333333333334e-05,
      "loss": 0.0019,
      "step": 50800
    },
    {
      "epoch": 3.3873333333333333,
      "grad_norm": 0.21752502024173737,
      "learning_rate": 2.882916666666667e-05,
      "loss": 0.0026,
      "step": 50810
    },
    {
      "epoch": 3.388,
      "grad_norm": 0.9872395992279053,
      "learning_rate": 2.8825000000000003e-05,
      "loss": 0.0022,
      "step": 50820
    },
    {
      "epoch": 3.3886666666666665,
      "grad_norm": 1.0370162725448608,
      "learning_rate": 2.8820833333333337e-05,
      "loss": 0.002,
      "step": 50830
    },
    {
      "epoch": 3.389333333333333,
      "grad_norm": 0.8723801970481873,
      "learning_rate": 2.8816666666666668e-05,
      "loss": 0.0019,
      "step": 50840
    },
    {
      "epoch": 3.39,
      "grad_norm": 0.43826714158058167,
      "learning_rate": 2.8812500000000002e-05,
      "loss": 0.0017,
      "step": 50850
    },
    {
      "epoch": 3.3906666666666667,
      "grad_norm": 0.5138707160949707,
      "learning_rate": 2.8808333333333337e-05,
      "loss": 0.0028,
      "step": 50860
    },
    {
      "epoch": 3.3913333333333333,
      "grad_norm": 0.8116304874420166,
      "learning_rate": 2.8804166666666664e-05,
      "loss": 0.0024,
      "step": 50870
    },
    {
      "epoch": 3.392,
      "grad_norm": 0.5968387722969055,
      "learning_rate": 2.88e-05,
      "loss": 0.0017,
      "step": 50880
    },
    {
      "epoch": 3.3926666666666665,
      "grad_norm": 0.12555065751075745,
      "learning_rate": 2.8795833333333333e-05,
      "loss": 0.0023,
      "step": 50890
    },
    {
      "epoch": 3.3933333333333335,
      "grad_norm": 0.14634153246879578,
      "learning_rate": 2.8791666666666667e-05,
      "loss": 0.0021,
      "step": 50900
    },
    {
      "epoch": 3.394,
      "grad_norm": 0.3240467607975006,
      "learning_rate": 2.8787500000000002e-05,
      "loss": 0.0024,
      "step": 50910
    },
    {
      "epoch": 3.3946666666666667,
      "grad_norm": 0.4828253984451294,
      "learning_rate": 2.8783333333333333e-05,
      "loss": 0.0018,
      "step": 50920
    },
    {
      "epoch": 3.3953333333333333,
      "grad_norm": 0.12958931922912598,
      "learning_rate": 2.8779166666666667e-05,
      "loss": 0.0023,
      "step": 50930
    },
    {
      "epoch": 3.396,
      "grad_norm": 0.29018542170524597,
      "learning_rate": 2.8775e-05,
      "loss": 0.0023,
      "step": 50940
    },
    {
      "epoch": 3.3966666666666665,
      "grad_norm": 0.5741149187088013,
      "learning_rate": 2.8770833333333336e-05,
      "loss": 0.0025,
      "step": 50950
    },
    {
      "epoch": 3.397333333333333,
      "grad_norm": 0.43908241391181946,
      "learning_rate": 2.876666666666667e-05,
      "loss": 0.0026,
      "step": 50960
    },
    {
      "epoch": 3.398,
      "grad_norm": 0.08981388062238693,
      "learning_rate": 2.8762500000000005e-05,
      "loss": 0.002,
      "step": 50970
    },
    {
      "epoch": 3.3986666666666667,
      "grad_norm": 0.2565751373767853,
      "learning_rate": 2.8758333333333336e-05,
      "loss": 0.0027,
      "step": 50980
    },
    {
      "epoch": 3.3993333333333333,
      "grad_norm": 0.6303450465202332,
      "learning_rate": 2.8754166666666667e-05,
      "loss": 0.0015,
      "step": 50990
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.875735342502594,
      "learning_rate": 2.8749999999999997e-05,
      "loss": 0.0023,
      "step": 51000
    },
    {
      "epoch": 3.4006666666666665,
      "grad_norm": 0.04349417984485626,
      "learning_rate": 2.8745833333333332e-05,
      "loss": 0.0022,
      "step": 51010
    },
    {
      "epoch": 3.4013333333333335,
      "grad_norm": 0.40588051080703735,
      "learning_rate": 2.8741666666666666e-05,
      "loss": 0.0022,
      "step": 51020
    },
    {
      "epoch": 3.402,
      "grad_norm": 0.18770331144332886,
      "learning_rate": 2.87375e-05,
      "loss": 0.0014,
      "step": 51030
    },
    {
      "epoch": 3.4026666666666667,
      "grad_norm": 0.4459863603115082,
      "learning_rate": 2.8733333333333335e-05,
      "loss": 0.0029,
      "step": 51040
    },
    {
      "epoch": 3.4033333333333333,
      "grad_norm": 0.672443151473999,
      "learning_rate": 2.872916666666667e-05,
      "loss": 0.0022,
      "step": 51050
    },
    {
      "epoch": 3.404,
      "grad_norm": 0.0879230946302414,
      "learning_rate": 2.8725e-05,
      "loss": 0.0021,
      "step": 51060
    },
    {
      "epoch": 3.4046666666666665,
      "grad_norm": 0.09196756035089493,
      "learning_rate": 2.8720833333333335e-05,
      "loss": 0.002,
      "step": 51070
    },
    {
      "epoch": 3.405333333333333,
      "grad_norm": 0.5134406089782715,
      "learning_rate": 2.871666666666667e-05,
      "loss": 0.0024,
      "step": 51080
    },
    {
      "epoch": 3.406,
      "grad_norm": 0.4770452082157135,
      "learning_rate": 2.8712500000000003e-05,
      "loss": 0.002,
      "step": 51090
    },
    {
      "epoch": 3.4066666666666667,
      "grad_norm": 0.3639351427555084,
      "learning_rate": 2.8708333333333338e-05,
      "loss": 0.0023,
      "step": 51100
    },
    {
      "epoch": 3.4073333333333333,
      "grad_norm": 0.05278441682457924,
      "learning_rate": 2.8704166666666665e-05,
      "loss": 0.0022,
      "step": 51110
    },
    {
      "epoch": 3.408,
      "grad_norm": 0.2651071548461914,
      "learning_rate": 2.87e-05,
      "loss": 0.0029,
      "step": 51120
    },
    {
      "epoch": 3.4086666666666665,
      "grad_norm": 0.36247310042381287,
      "learning_rate": 2.8695833333333334e-05,
      "loss": 0.0029,
      "step": 51130
    },
    {
      "epoch": 3.4093333333333335,
      "grad_norm": 0.22274981439113617,
      "learning_rate": 2.869166666666667e-05,
      "loss": 0.0035,
      "step": 51140
    },
    {
      "epoch": 3.41,
      "grad_norm": 0.4799472391605377,
      "learning_rate": 2.86875e-05,
      "loss": 0.0029,
      "step": 51150
    },
    {
      "epoch": 3.4106666666666667,
      "grad_norm": 0.12402302026748657,
      "learning_rate": 2.8683333333333334e-05,
      "loss": 0.0027,
      "step": 51160
    },
    {
      "epoch": 3.4113333333333333,
      "grad_norm": 0.4042477309703827,
      "learning_rate": 2.8679166666666668e-05,
      "loss": 0.0017,
      "step": 51170
    },
    {
      "epoch": 3.412,
      "grad_norm": 0.6582719683647156,
      "learning_rate": 2.8675000000000002e-05,
      "loss": 0.0019,
      "step": 51180
    },
    {
      "epoch": 3.4126666666666665,
      "grad_norm": 0.21084417402744293,
      "learning_rate": 2.8670833333333337e-05,
      "loss": 0.0024,
      "step": 51190
    },
    {
      "epoch": 3.413333333333333,
      "grad_norm": 0.07722213119268417,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 0.0013,
      "step": 51200
    },
    {
      "epoch": 3.414,
      "grad_norm": 1.0952023267745972,
      "learning_rate": 2.8662500000000002e-05,
      "loss": 0.0025,
      "step": 51210
    },
    {
      "epoch": 3.4146666666666667,
      "grad_norm": 0.17071647942066193,
      "learning_rate": 2.8658333333333336e-05,
      "loss": 0.0023,
      "step": 51220
    },
    {
      "epoch": 3.4153333333333333,
      "grad_norm": 0.8142168521881104,
      "learning_rate": 2.8654166666666664e-05,
      "loss": 0.0023,
      "step": 51230
    },
    {
      "epoch": 3.416,
      "grad_norm": 0.6964890956878662,
      "learning_rate": 2.865e-05,
      "loss": 0.0026,
      "step": 51240
    },
    {
      "epoch": 3.4166666666666665,
      "grad_norm": 0.06218556687235832,
      "learning_rate": 2.8645833333333333e-05,
      "loss": 0.0022,
      "step": 51250
    },
    {
      "epoch": 3.4173333333333336,
      "grad_norm": 0.3058788776397705,
      "learning_rate": 2.8641666666666667e-05,
      "loss": 0.003,
      "step": 51260
    },
    {
      "epoch": 3.418,
      "grad_norm": 0.6633288264274597,
      "learning_rate": 2.86375e-05,
      "loss": 0.002,
      "step": 51270
    },
    {
      "epoch": 3.4186666666666667,
      "grad_norm": 0.2551954686641693,
      "learning_rate": 2.8633333333333336e-05,
      "loss": 0.0032,
      "step": 51280
    },
    {
      "epoch": 3.4193333333333333,
      "grad_norm": 0.5822457671165466,
      "learning_rate": 2.8629166666666667e-05,
      "loss": 0.0026,
      "step": 51290
    },
    {
      "epoch": 3.42,
      "grad_norm": 0.4016265273094177,
      "learning_rate": 2.8625e-05,
      "loss": 0.0025,
      "step": 51300
    },
    {
      "epoch": 3.4206666666666665,
      "grad_norm": 0.5584937930107117,
      "learning_rate": 2.8620833333333336e-05,
      "loss": 0.0018,
      "step": 51310
    },
    {
      "epoch": 3.421333333333333,
      "grad_norm": 0.254658579826355,
      "learning_rate": 2.861666666666667e-05,
      "loss": 0.002,
      "step": 51320
    },
    {
      "epoch": 3.422,
      "grad_norm": 0.4107314348220825,
      "learning_rate": 2.8612500000000004e-05,
      "loss": 0.0027,
      "step": 51330
    },
    {
      "epoch": 3.4226666666666667,
      "grad_norm": 0.06231452152132988,
      "learning_rate": 2.860833333333334e-05,
      "loss": 0.0021,
      "step": 51340
    },
    {
      "epoch": 3.4233333333333333,
      "grad_norm": 0.09208653122186661,
      "learning_rate": 2.860416666666667e-05,
      "loss": 0.0016,
      "step": 51350
    },
    {
      "epoch": 3.424,
      "grad_norm": 0.17768393456935883,
      "learning_rate": 2.86e-05,
      "loss": 0.0027,
      "step": 51360
    },
    {
      "epoch": 3.4246666666666665,
      "grad_norm": 0.4393692910671234,
      "learning_rate": 2.859583333333333e-05,
      "loss": 0.0015,
      "step": 51370
    },
    {
      "epoch": 3.4253333333333336,
      "grad_norm": 0.6049549579620361,
      "learning_rate": 2.8591666666666666e-05,
      "loss": 0.0014,
      "step": 51380
    },
    {
      "epoch": 3.426,
      "grad_norm": 0.03026747703552246,
      "learning_rate": 2.85875e-05,
      "loss": 0.0034,
      "step": 51390
    },
    {
      "epoch": 3.4266666666666667,
      "grad_norm": 0.22853580117225647,
      "learning_rate": 2.8583333333333335e-05,
      "loss": 0.0016,
      "step": 51400
    },
    {
      "epoch": 3.4273333333333333,
      "grad_norm": 0.6671590209007263,
      "learning_rate": 2.857916666666667e-05,
      "loss": 0.0021,
      "step": 51410
    },
    {
      "epoch": 3.428,
      "grad_norm": 0.3918918967247009,
      "learning_rate": 2.8575000000000003e-05,
      "loss": 0.0016,
      "step": 51420
    },
    {
      "epoch": 3.4286666666666665,
      "grad_norm": 0.5796197056770325,
      "learning_rate": 2.8570833333333334e-05,
      "loss": 0.0019,
      "step": 51430
    },
    {
      "epoch": 3.429333333333333,
      "grad_norm": 0.35840335488319397,
      "learning_rate": 2.856666666666667e-05,
      "loss": 0.0017,
      "step": 51440
    },
    {
      "epoch": 3.43,
      "grad_norm": 0.32339659333229065,
      "learning_rate": 2.8562500000000003e-05,
      "loss": 0.0021,
      "step": 51450
    },
    {
      "epoch": 3.4306666666666668,
      "grad_norm": 0.3187377452850342,
      "learning_rate": 2.8558333333333337e-05,
      "loss": 0.0016,
      "step": 51460
    },
    {
      "epoch": 3.4313333333333333,
      "grad_norm": 0.32829079031944275,
      "learning_rate": 2.8554166666666672e-05,
      "loss": 0.0021,
      "step": 51470
    },
    {
      "epoch": 3.432,
      "grad_norm": 0.10648743808269501,
      "learning_rate": 2.855e-05,
      "loss": 0.0019,
      "step": 51480
    },
    {
      "epoch": 3.4326666666666665,
      "grad_norm": 0.4494195282459259,
      "learning_rate": 2.8545833333333334e-05,
      "loss": 0.0013,
      "step": 51490
    },
    {
      "epoch": 3.4333333333333336,
      "grad_norm": 0.25427448749542236,
      "learning_rate": 2.8541666666666668e-05,
      "loss": 0.0011,
      "step": 51500
    },
    {
      "epoch": 3.434,
      "grad_norm": 0.2558612823486328,
      "learning_rate": 2.85375e-05,
      "loss": 0.0013,
      "step": 51510
    },
    {
      "epoch": 3.4346666666666668,
      "grad_norm": 0.4648323357105255,
      "learning_rate": 2.8533333333333333e-05,
      "loss": 0.0018,
      "step": 51520
    },
    {
      "epoch": 3.4353333333333333,
      "grad_norm": 0.29761776328086853,
      "learning_rate": 2.8529166666666668e-05,
      "loss": 0.0023,
      "step": 51530
    },
    {
      "epoch": 3.436,
      "grad_norm": 0.05956514924764633,
      "learning_rate": 2.8525000000000002e-05,
      "loss": 0.0016,
      "step": 51540
    },
    {
      "epoch": 3.4366666666666665,
      "grad_norm": 0.2860718071460724,
      "learning_rate": 2.8520833333333337e-05,
      "loss": 0.0019,
      "step": 51550
    },
    {
      "epoch": 3.437333333333333,
      "grad_norm": 0.9065784811973572,
      "learning_rate": 2.851666666666667e-05,
      "loss": 0.0022,
      "step": 51560
    },
    {
      "epoch": 3.438,
      "grad_norm": 0.22473099827766418,
      "learning_rate": 2.8512500000000002e-05,
      "loss": 0.0014,
      "step": 51570
    },
    {
      "epoch": 3.4386666666666668,
      "grad_norm": 0.1922428011894226,
      "learning_rate": 2.8508333333333336e-05,
      "loss": 0.0021,
      "step": 51580
    },
    {
      "epoch": 3.4393333333333334,
      "grad_norm": 0.23558391630649567,
      "learning_rate": 2.850416666666667e-05,
      "loss": 0.0019,
      "step": 51590
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.6414523720741272,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 0.0025,
      "step": 51600
    },
    {
      "epoch": 3.4406666666666665,
      "grad_norm": 0.2204420566558838,
      "learning_rate": 2.8495833333333333e-05,
      "loss": 0.0022,
      "step": 51610
    },
    {
      "epoch": 3.4413333333333336,
      "grad_norm": 0.3585878312587738,
      "learning_rate": 2.8491666666666667e-05,
      "loss": 0.0015,
      "step": 51620
    },
    {
      "epoch": 3.442,
      "grad_norm": 0.07391690462827682,
      "learning_rate": 2.84875e-05,
      "loss": 0.0027,
      "step": 51630
    },
    {
      "epoch": 3.4426666666666668,
      "grad_norm": 0.22926048934459686,
      "learning_rate": 2.8483333333333336e-05,
      "loss": 0.002,
      "step": 51640
    },
    {
      "epoch": 3.4433333333333334,
      "grad_norm": 0.31672629714012146,
      "learning_rate": 2.8479166666666667e-05,
      "loss": 0.0022,
      "step": 51650
    },
    {
      "epoch": 3.444,
      "grad_norm": 0.11178731173276901,
      "learning_rate": 2.8475e-05,
      "loss": 0.0016,
      "step": 51660
    },
    {
      "epoch": 3.4446666666666665,
      "grad_norm": 0.29508957266807556,
      "learning_rate": 2.8470833333333335e-05,
      "loss": 0.0017,
      "step": 51670
    },
    {
      "epoch": 3.445333333333333,
      "grad_norm": 0.29255059361457825,
      "learning_rate": 2.846666666666667e-05,
      "loss": 0.0014,
      "step": 51680
    },
    {
      "epoch": 3.446,
      "grad_norm": 0.29003819823265076,
      "learning_rate": 2.8462500000000004e-05,
      "loss": 0.0015,
      "step": 51690
    },
    {
      "epoch": 3.4466666666666668,
      "grad_norm": 0.18335214257240295,
      "learning_rate": 2.845833333333334e-05,
      "loss": 0.0019,
      "step": 51700
    },
    {
      "epoch": 3.4473333333333334,
      "grad_norm": 0.58025723695755,
      "learning_rate": 2.845416666666667e-05,
      "loss": 0.0039,
      "step": 51710
    },
    {
      "epoch": 3.448,
      "grad_norm": 0.32696038484573364,
      "learning_rate": 2.845e-05,
      "loss": 0.0021,
      "step": 51720
    },
    {
      "epoch": 3.4486666666666665,
      "grad_norm": 0.3379840850830078,
      "learning_rate": 2.844583333333333e-05,
      "loss": 0.0021,
      "step": 51730
    },
    {
      "epoch": 3.449333333333333,
      "grad_norm": 0.08666939288377762,
      "learning_rate": 2.8441666666666666e-05,
      "loss": 0.0022,
      "step": 51740
    },
    {
      "epoch": 3.45,
      "grad_norm": 0.33656078577041626,
      "learning_rate": 2.84375e-05,
      "loss": 0.0034,
      "step": 51750
    },
    {
      "epoch": 3.4506666666666668,
      "grad_norm": 0.40208160877227783,
      "learning_rate": 2.8433333333333334e-05,
      "loss": 0.002,
      "step": 51760
    },
    {
      "epoch": 3.4513333333333334,
      "grad_norm": 0.4923611879348755,
      "learning_rate": 2.842916666666667e-05,
      "loss": 0.0022,
      "step": 51770
    },
    {
      "epoch": 3.452,
      "grad_norm": 0.2503286302089691,
      "learning_rate": 2.8425000000000003e-05,
      "loss": 0.0023,
      "step": 51780
    },
    {
      "epoch": 3.4526666666666666,
      "grad_norm": 0.1162843182682991,
      "learning_rate": 2.8420833333333334e-05,
      "loss": 0.0026,
      "step": 51790
    },
    {
      "epoch": 3.453333333333333,
      "grad_norm": 0.7136648297309875,
      "learning_rate": 2.841666666666667e-05,
      "loss": 0.0015,
      "step": 51800
    },
    {
      "epoch": 3.454,
      "grad_norm": 0.07221153378486633,
      "learning_rate": 2.8412500000000003e-05,
      "loss": 0.0018,
      "step": 51810
    },
    {
      "epoch": 3.4546666666666668,
      "grad_norm": 0.7563468813896179,
      "learning_rate": 2.8408333333333337e-05,
      "loss": 0.0037,
      "step": 51820
    },
    {
      "epoch": 3.4553333333333334,
      "grad_norm": 0.17001678049564362,
      "learning_rate": 2.840416666666667e-05,
      "loss": 0.0015,
      "step": 51830
    },
    {
      "epoch": 3.456,
      "grad_norm": 0.46793651580810547,
      "learning_rate": 2.84e-05,
      "loss": 0.0019,
      "step": 51840
    },
    {
      "epoch": 3.4566666666666666,
      "grad_norm": 0.6341878175735474,
      "learning_rate": 2.8395833333333333e-05,
      "loss": 0.0025,
      "step": 51850
    },
    {
      "epoch": 3.457333333333333,
      "grad_norm": 0.49334609508514404,
      "learning_rate": 2.8391666666666668e-05,
      "loss": 0.0017,
      "step": 51860
    },
    {
      "epoch": 3.458,
      "grad_norm": 0.5040909051895142,
      "learning_rate": 2.83875e-05,
      "loss": 0.0019,
      "step": 51870
    },
    {
      "epoch": 3.458666666666667,
      "grad_norm": 0.339312881231308,
      "learning_rate": 2.8383333333333333e-05,
      "loss": 0.0019,
      "step": 51880
    },
    {
      "epoch": 3.4593333333333334,
      "grad_norm": 0.3546005189418793,
      "learning_rate": 2.8379166666666668e-05,
      "loss": 0.0018,
      "step": 51890
    },
    {
      "epoch": 3.46,
      "grad_norm": 0.683505117893219,
      "learning_rate": 2.8375000000000002e-05,
      "loss": 0.0021,
      "step": 51900
    },
    {
      "epoch": 3.4606666666666666,
      "grad_norm": 0.4581076502799988,
      "learning_rate": 2.8370833333333336e-05,
      "loss": 0.0017,
      "step": 51910
    },
    {
      "epoch": 3.461333333333333,
      "grad_norm": 0.4336141049861908,
      "learning_rate": 2.836666666666667e-05,
      "loss": 0.0022,
      "step": 51920
    },
    {
      "epoch": 3.462,
      "grad_norm": 0.3497621417045593,
      "learning_rate": 2.83625e-05,
      "loss": 0.0022,
      "step": 51930
    },
    {
      "epoch": 3.462666666666667,
      "grad_norm": 0.39034461975097656,
      "learning_rate": 2.8358333333333336e-05,
      "loss": 0.0027,
      "step": 51940
    },
    {
      "epoch": 3.4633333333333334,
      "grad_norm": 0.03657694533467293,
      "learning_rate": 2.835416666666667e-05,
      "loss": 0.0013,
      "step": 51950
    },
    {
      "epoch": 3.464,
      "grad_norm": 0.6792653203010559,
      "learning_rate": 2.8349999999999998e-05,
      "loss": 0.0016,
      "step": 51960
    },
    {
      "epoch": 3.4646666666666666,
      "grad_norm": 0.1404373198747635,
      "learning_rate": 2.8345833333333332e-05,
      "loss": 0.002,
      "step": 51970
    },
    {
      "epoch": 3.465333333333333,
      "grad_norm": 0.43464645743370056,
      "learning_rate": 2.8341666666666667e-05,
      "loss": 0.0017,
      "step": 51980
    },
    {
      "epoch": 3.466,
      "grad_norm": 0.8960802555084229,
      "learning_rate": 2.83375e-05,
      "loss": 0.0033,
      "step": 51990
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 0.5772970914840698,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 0.0012,
      "step": 52000
    },
    {
      "epoch": 3.4673333333333334,
      "grad_norm": 0.3221750855445862,
      "learning_rate": 2.8329166666666666e-05,
      "loss": 0.0016,
      "step": 52010
    },
    {
      "epoch": 3.468,
      "grad_norm": 0.14640174806118011,
      "learning_rate": 2.8325e-05,
      "loss": 0.0015,
      "step": 52020
    },
    {
      "epoch": 3.4686666666666666,
      "grad_norm": 0.08354722708463669,
      "learning_rate": 2.8320833333333335e-05,
      "loss": 0.0017,
      "step": 52030
    },
    {
      "epoch": 3.469333333333333,
      "grad_norm": 0.3173336386680603,
      "learning_rate": 2.831666666666667e-05,
      "loss": 0.002,
      "step": 52040
    },
    {
      "epoch": 3.4699999999999998,
      "grad_norm": 0.2186928689479828,
      "learning_rate": 2.8312500000000004e-05,
      "loss": 0.0026,
      "step": 52050
    },
    {
      "epoch": 3.470666666666667,
      "grad_norm": 0.359333336353302,
      "learning_rate": 2.8308333333333338e-05,
      "loss": 0.0017,
      "step": 52060
    },
    {
      "epoch": 3.4713333333333334,
      "grad_norm": 0.4353939890861511,
      "learning_rate": 2.830416666666667e-05,
      "loss": 0.0023,
      "step": 52070
    },
    {
      "epoch": 3.472,
      "grad_norm": 0.5145314931869507,
      "learning_rate": 2.83e-05,
      "loss": 0.0016,
      "step": 52080
    },
    {
      "epoch": 3.4726666666666666,
      "grad_norm": 0.117218978703022,
      "learning_rate": 2.829583333333333e-05,
      "loss": 0.0023,
      "step": 52090
    },
    {
      "epoch": 3.473333333333333,
      "grad_norm": 0.2799806296825409,
      "learning_rate": 2.8291666666666665e-05,
      "loss": 0.0029,
      "step": 52100
    },
    {
      "epoch": 3.474,
      "grad_norm": 0.4159330427646637,
      "learning_rate": 2.82875e-05,
      "loss": 0.002,
      "step": 52110
    },
    {
      "epoch": 3.474666666666667,
      "grad_norm": 0.3681800961494446,
      "learning_rate": 2.8283333333333334e-05,
      "loss": 0.0019,
      "step": 52120
    },
    {
      "epoch": 3.4753333333333334,
      "grad_norm": 0.15986236929893494,
      "learning_rate": 2.827916666666667e-05,
      "loss": 0.0032,
      "step": 52130
    },
    {
      "epoch": 3.476,
      "grad_norm": 0.04413813352584839,
      "learning_rate": 2.8275000000000003e-05,
      "loss": 0.0023,
      "step": 52140
    },
    {
      "epoch": 3.4766666666666666,
      "grad_norm": 0.01931185834109783,
      "learning_rate": 2.8270833333333334e-05,
      "loss": 0.0021,
      "step": 52150
    },
    {
      "epoch": 3.477333333333333,
      "grad_norm": 0.18818299472332,
      "learning_rate": 2.8266666666666668e-05,
      "loss": 0.002,
      "step": 52160
    },
    {
      "epoch": 3.4779999999999998,
      "grad_norm": 0.22228895127773285,
      "learning_rate": 2.8262500000000003e-05,
      "loss": 0.0018,
      "step": 52170
    },
    {
      "epoch": 3.478666666666667,
      "grad_norm": 0.04823445528745651,
      "learning_rate": 2.8258333333333337e-05,
      "loss": 0.002,
      "step": 52180
    },
    {
      "epoch": 3.4793333333333334,
      "grad_norm": 1.0172699689865112,
      "learning_rate": 2.825416666666667e-05,
      "loss": 0.0029,
      "step": 52190
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.7939016819000244,
      "learning_rate": 2.825e-05,
      "loss": 0.0015,
      "step": 52200
    },
    {
      "epoch": 3.4806666666666666,
      "grad_norm": 0.6161213517189026,
      "learning_rate": 2.8245833333333333e-05,
      "loss": 0.0023,
      "step": 52210
    },
    {
      "epoch": 3.481333333333333,
      "grad_norm": 0.11627554893493652,
      "learning_rate": 2.8241666666666668e-05,
      "loss": 0.002,
      "step": 52220
    },
    {
      "epoch": 3.482,
      "grad_norm": 0.04463888332247734,
      "learning_rate": 2.82375e-05,
      "loss": 0.0017,
      "step": 52230
    },
    {
      "epoch": 3.482666666666667,
      "grad_norm": 0.47516676783561707,
      "learning_rate": 2.8233333333333333e-05,
      "loss": 0.0018,
      "step": 52240
    },
    {
      "epoch": 3.4833333333333334,
      "grad_norm": 0.07706502825021744,
      "learning_rate": 2.8229166666666667e-05,
      "loss": 0.0016,
      "step": 52250
    },
    {
      "epoch": 3.484,
      "grad_norm": 0.32048025727272034,
      "learning_rate": 2.8225e-05,
      "loss": 0.0021,
      "step": 52260
    },
    {
      "epoch": 3.4846666666666666,
      "grad_norm": 0.22667771577835083,
      "learning_rate": 2.8220833333333336e-05,
      "loss": 0.0018,
      "step": 52270
    },
    {
      "epoch": 3.485333333333333,
      "grad_norm": 0.3327993154525757,
      "learning_rate": 2.821666666666667e-05,
      "loss": 0.0018,
      "step": 52280
    },
    {
      "epoch": 3.4859999999999998,
      "grad_norm": 0.20039592683315277,
      "learning_rate": 2.82125e-05,
      "loss": 0.0037,
      "step": 52290
    },
    {
      "epoch": 3.486666666666667,
      "grad_norm": 0.10865890979766846,
      "learning_rate": 2.8208333333333336e-05,
      "loss": 0.0027,
      "step": 52300
    },
    {
      "epoch": 3.4873333333333334,
      "grad_norm": 0.35404396057128906,
      "learning_rate": 2.820416666666667e-05,
      "loss": 0.0026,
      "step": 52310
    },
    {
      "epoch": 3.488,
      "grad_norm": 0.07740834355354309,
      "learning_rate": 2.8199999999999998e-05,
      "loss": 0.0017,
      "step": 52320
    },
    {
      "epoch": 3.4886666666666666,
      "grad_norm": 0.21738764643669128,
      "learning_rate": 2.8195833333333332e-05,
      "loss": 0.0021,
      "step": 52330
    },
    {
      "epoch": 3.489333333333333,
      "grad_norm": 0.6861200332641602,
      "learning_rate": 2.8191666666666666e-05,
      "loss": 0.003,
      "step": 52340
    },
    {
      "epoch": 3.49,
      "grad_norm": 0.7560189962387085,
      "learning_rate": 2.81875e-05,
      "loss": 0.0032,
      "step": 52350
    },
    {
      "epoch": 3.490666666666667,
      "grad_norm": 0.15025028586387634,
      "learning_rate": 2.8183333333333335e-05,
      "loss": 0.0035,
      "step": 52360
    },
    {
      "epoch": 3.4913333333333334,
      "grad_norm": 0.04856753349304199,
      "learning_rate": 2.8179166666666666e-05,
      "loss": 0.0021,
      "step": 52370
    },
    {
      "epoch": 3.492,
      "grad_norm": 0.16229060292243958,
      "learning_rate": 2.8175e-05,
      "loss": 0.0029,
      "step": 52380
    },
    {
      "epoch": 3.4926666666666666,
      "grad_norm": 0.5108643174171448,
      "learning_rate": 2.8170833333333335e-05,
      "loss": 0.0019,
      "step": 52390
    },
    {
      "epoch": 3.493333333333333,
      "grad_norm": 0.43995457887649536,
      "learning_rate": 2.816666666666667e-05,
      "loss": 0.002,
      "step": 52400
    },
    {
      "epoch": 3.4939999999999998,
      "grad_norm": 0.2869851291179657,
      "learning_rate": 2.8162500000000004e-05,
      "loss": 0.0024,
      "step": 52410
    },
    {
      "epoch": 3.494666666666667,
      "grad_norm": 0.14962907135486603,
      "learning_rate": 2.8158333333333338e-05,
      "loss": 0.0019,
      "step": 52420
    },
    {
      "epoch": 3.4953333333333334,
      "grad_norm": 0.6151658296585083,
      "learning_rate": 2.815416666666667e-05,
      "loss": 0.0022,
      "step": 52430
    },
    {
      "epoch": 3.496,
      "grad_norm": 0.07302587479352951,
      "learning_rate": 2.815e-05,
      "loss": 0.0018,
      "step": 52440
    },
    {
      "epoch": 3.4966666666666666,
      "grad_norm": 0.3406478464603424,
      "learning_rate": 2.814583333333333e-05,
      "loss": 0.0021,
      "step": 52450
    },
    {
      "epoch": 3.497333333333333,
      "grad_norm": 0.4808628559112549,
      "learning_rate": 2.8141666666666665e-05,
      "loss": 0.0029,
      "step": 52460
    },
    {
      "epoch": 3.498,
      "grad_norm": 0.2075369507074356,
      "learning_rate": 2.81375e-05,
      "loss": 0.0018,
      "step": 52470
    },
    {
      "epoch": 3.498666666666667,
      "grad_norm": 0.2977429926395416,
      "learning_rate": 2.8133333333333334e-05,
      "loss": 0.0014,
      "step": 52480
    },
    {
      "epoch": 3.4993333333333334,
      "grad_norm": 0.3986760973930359,
      "learning_rate": 2.8129166666666668e-05,
      "loss": 0.0014,
      "step": 52490
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.5466008186340332,
      "learning_rate": 2.8125000000000003e-05,
      "loss": 0.0014,
      "step": 52500
    },
    {
      "epoch": 3.5006666666666666,
      "grad_norm": 0.19830237329006195,
      "learning_rate": 2.8120833333333334e-05,
      "loss": 0.0025,
      "step": 52510
    },
    {
      "epoch": 3.501333333333333,
      "grad_norm": 0.6141646504402161,
      "learning_rate": 2.8116666666666668e-05,
      "loss": 0.0027,
      "step": 52520
    },
    {
      "epoch": 3.502,
      "grad_norm": 0.12242159992456436,
      "learning_rate": 2.8112500000000002e-05,
      "loss": 0.0022,
      "step": 52530
    },
    {
      "epoch": 3.502666666666667,
      "grad_norm": 0.46671944856643677,
      "learning_rate": 2.8108333333333337e-05,
      "loss": 0.0024,
      "step": 52540
    },
    {
      "epoch": 3.5033333333333334,
      "grad_norm": 0.47278639674186707,
      "learning_rate": 2.810416666666667e-05,
      "loss": 0.0023,
      "step": 52550
    },
    {
      "epoch": 3.504,
      "grad_norm": 0.18081213533878326,
      "learning_rate": 2.8100000000000005e-05,
      "loss": 0.0018,
      "step": 52560
    },
    {
      "epoch": 3.5046666666666666,
      "grad_norm": 0.2505950927734375,
      "learning_rate": 2.8095833333333333e-05,
      "loss": 0.0026,
      "step": 52570
    },
    {
      "epoch": 3.505333333333333,
      "grad_norm": 0.18932007253170013,
      "learning_rate": 2.8091666666666667e-05,
      "loss": 0.0021,
      "step": 52580
    },
    {
      "epoch": 3.5060000000000002,
      "grad_norm": 0.3257637917995453,
      "learning_rate": 2.80875e-05,
      "loss": 0.0015,
      "step": 52590
    },
    {
      "epoch": 3.506666666666667,
      "grad_norm": 0.8877443671226501,
      "learning_rate": 2.8083333333333333e-05,
      "loss": 0.0018,
      "step": 52600
    },
    {
      "epoch": 3.5073333333333334,
      "grad_norm": 0.04354402422904968,
      "learning_rate": 2.8079166666666667e-05,
      "loss": 0.0015,
      "step": 52610
    },
    {
      "epoch": 3.508,
      "grad_norm": 0.47137176990509033,
      "learning_rate": 2.8075e-05,
      "loss": 0.0017,
      "step": 52620
    },
    {
      "epoch": 3.5086666666666666,
      "grad_norm": 0.2178492397069931,
      "learning_rate": 2.8070833333333336e-05,
      "loss": 0.0018,
      "step": 52630
    },
    {
      "epoch": 3.509333333333333,
      "grad_norm": 0.484196275472641,
      "learning_rate": 2.806666666666667e-05,
      "loss": 0.0027,
      "step": 52640
    },
    {
      "epoch": 3.51,
      "grad_norm": 0.07995520532131195,
      "learning_rate": 2.80625e-05,
      "loss": 0.0025,
      "step": 52650
    },
    {
      "epoch": 3.510666666666667,
      "grad_norm": 0.44768792390823364,
      "learning_rate": 2.8058333333333335e-05,
      "loss": 0.0018,
      "step": 52660
    },
    {
      "epoch": 3.5113333333333334,
      "grad_norm": 0.2849065065383911,
      "learning_rate": 2.805416666666667e-05,
      "loss": 0.0015,
      "step": 52670
    },
    {
      "epoch": 3.512,
      "grad_norm": 0.1170295998454094,
      "learning_rate": 2.8050000000000004e-05,
      "loss": 0.0022,
      "step": 52680
    },
    {
      "epoch": 3.5126666666666666,
      "grad_norm": 0.25802186131477356,
      "learning_rate": 2.8045833333333332e-05,
      "loss": 0.0014,
      "step": 52690
    },
    {
      "epoch": 3.513333333333333,
      "grad_norm": 0.3951962888240814,
      "learning_rate": 2.8041666666666666e-05,
      "loss": 0.0016,
      "step": 52700
    },
    {
      "epoch": 3.5140000000000002,
      "grad_norm": 0.5577754974365234,
      "learning_rate": 2.80375e-05,
      "loss": 0.0019,
      "step": 52710
    },
    {
      "epoch": 3.514666666666667,
      "grad_norm": 0.3173912763595581,
      "learning_rate": 2.8033333333333335e-05,
      "loss": 0.0023,
      "step": 52720
    },
    {
      "epoch": 3.5153333333333334,
      "grad_norm": 0.1848752796649933,
      "learning_rate": 2.8029166666666666e-05,
      "loss": 0.0025,
      "step": 52730
    },
    {
      "epoch": 3.516,
      "grad_norm": 0.9286395311355591,
      "learning_rate": 2.8025e-05,
      "loss": 0.0024,
      "step": 52740
    },
    {
      "epoch": 3.5166666666666666,
      "grad_norm": 0.25203996896743774,
      "learning_rate": 2.8020833333333335e-05,
      "loss": 0.0022,
      "step": 52750
    },
    {
      "epoch": 3.517333333333333,
      "grad_norm": 0.4977947175502777,
      "learning_rate": 2.801666666666667e-05,
      "loss": 0.0025,
      "step": 52760
    },
    {
      "epoch": 3.518,
      "grad_norm": 0.676999032497406,
      "learning_rate": 2.8012500000000003e-05,
      "loss": 0.0021,
      "step": 52770
    },
    {
      "epoch": 3.518666666666667,
      "grad_norm": 0.5053889751434326,
      "learning_rate": 2.8008333333333338e-05,
      "loss": 0.0027,
      "step": 52780
    },
    {
      "epoch": 3.5193333333333334,
      "grad_norm": 0.23650746047496796,
      "learning_rate": 2.800416666666667e-05,
      "loss": 0.0022,
      "step": 52790
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.07872333377599716,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.0014,
      "step": 52800
    },
    {
      "epoch": 3.5206666666666666,
      "grad_norm": 0.18630073964595795,
      "learning_rate": 2.799583333333333e-05,
      "loss": 0.0024,
      "step": 52810
    },
    {
      "epoch": 3.521333333333333,
      "grad_norm": 0.11707767844200134,
      "learning_rate": 2.7991666666666665e-05,
      "loss": 0.0015,
      "step": 52820
    },
    {
      "epoch": 3.5220000000000002,
      "grad_norm": 0.14950990676879883,
      "learning_rate": 2.79875e-05,
      "loss": 0.0028,
      "step": 52830
    },
    {
      "epoch": 3.522666666666667,
      "grad_norm": 0.11451949924230576,
      "learning_rate": 2.7983333333333334e-05,
      "loss": 0.0019,
      "step": 52840
    },
    {
      "epoch": 3.5233333333333334,
      "grad_norm": 0.11472340673208237,
      "learning_rate": 2.7979166666666668e-05,
      "loss": 0.0018,
      "step": 52850
    },
    {
      "epoch": 3.524,
      "grad_norm": 0.5107468962669373,
      "learning_rate": 2.7975000000000002e-05,
      "loss": 0.0028,
      "step": 52860
    },
    {
      "epoch": 3.5246666666666666,
      "grad_norm": 0.11112421005964279,
      "learning_rate": 2.7970833333333333e-05,
      "loss": 0.0018,
      "step": 52870
    },
    {
      "epoch": 3.525333333333333,
      "grad_norm": 0.28506460785865784,
      "learning_rate": 2.7966666666666668e-05,
      "loss": 0.0024,
      "step": 52880
    },
    {
      "epoch": 3.526,
      "grad_norm": 0.33393993973731995,
      "learning_rate": 2.7962500000000002e-05,
      "loss": 0.0021,
      "step": 52890
    },
    {
      "epoch": 3.5266666666666664,
      "grad_norm": 0.07268854230642319,
      "learning_rate": 2.7958333333333336e-05,
      "loss": 0.0019,
      "step": 52900
    },
    {
      "epoch": 3.5273333333333334,
      "grad_norm": 0.058478839695453644,
      "learning_rate": 2.795416666666667e-05,
      "loss": 0.0022,
      "step": 52910
    },
    {
      "epoch": 3.528,
      "grad_norm": 0.6209957599639893,
      "learning_rate": 2.7950000000000005e-05,
      "loss": 0.0018,
      "step": 52920
    },
    {
      "epoch": 3.5286666666666666,
      "grad_norm": 0.26333338022232056,
      "learning_rate": 2.7945833333333333e-05,
      "loss": 0.0025,
      "step": 52930
    },
    {
      "epoch": 3.529333333333333,
      "grad_norm": 0.5818318724632263,
      "learning_rate": 2.7941666666666667e-05,
      "loss": 0.0029,
      "step": 52940
    },
    {
      "epoch": 3.5300000000000002,
      "grad_norm": 0.22402024269104004,
      "learning_rate": 2.79375e-05,
      "loss": 0.0024,
      "step": 52950
    },
    {
      "epoch": 3.530666666666667,
      "grad_norm": 0.18627676367759705,
      "learning_rate": 2.7933333333333332e-05,
      "loss": 0.0017,
      "step": 52960
    },
    {
      "epoch": 3.5313333333333334,
      "grad_norm": 0.5447136759757996,
      "learning_rate": 2.7929166666666667e-05,
      "loss": 0.0016,
      "step": 52970
    },
    {
      "epoch": 3.532,
      "grad_norm": 0.47053036093711853,
      "learning_rate": 2.7925e-05,
      "loss": 0.0014,
      "step": 52980
    },
    {
      "epoch": 3.5326666666666666,
      "grad_norm": 0.33568549156188965,
      "learning_rate": 2.7920833333333336e-05,
      "loss": 0.0021,
      "step": 52990
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 0.8010972142219543,
      "learning_rate": 2.791666666666667e-05,
      "loss": 0.0025,
      "step": 53000
    },
    {
      "epoch": 3.534,
      "grad_norm": 0.15158340334892273,
      "learning_rate": 2.7912500000000004e-05,
      "loss": 0.0027,
      "step": 53010
    },
    {
      "epoch": 3.5346666666666664,
      "grad_norm": 0.5422457456588745,
      "learning_rate": 2.7908333333333335e-05,
      "loss": 0.0014,
      "step": 53020
    },
    {
      "epoch": 3.5353333333333334,
      "grad_norm": 0.4332234859466553,
      "learning_rate": 2.790416666666667e-05,
      "loss": 0.0024,
      "step": 53030
    },
    {
      "epoch": 3.536,
      "grad_norm": 0.22148257493972778,
      "learning_rate": 2.7900000000000004e-05,
      "loss": 0.0031,
      "step": 53040
    },
    {
      "epoch": 3.5366666666666666,
      "grad_norm": 0.07938628643751144,
      "learning_rate": 2.789583333333333e-05,
      "loss": 0.0014,
      "step": 53050
    },
    {
      "epoch": 3.537333333333333,
      "grad_norm": 0.24300017952919006,
      "learning_rate": 2.7891666666666666e-05,
      "loss": 0.0017,
      "step": 53060
    },
    {
      "epoch": 3.5380000000000003,
      "grad_norm": 0.05892835184931755,
      "learning_rate": 2.78875e-05,
      "loss": 0.0017,
      "step": 53070
    },
    {
      "epoch": 3.538666666666667,
      "grad_norm": 0.653182864189148,
      "learning_rate": 2.7883333333333335e-05,
      "loss": 0.0015,
      "step": 53080
    },
    {
      "epoch": 3.5393333333333334,
      "grad_norm": 0.3271625339984894,
      "learning_rate": 2.787916666666667e-05,
      "loss": 0.0016,
      "step": 53090
    },
    {
      "epoch": 3.54,
      "grad_norm": 0.2570134997367859,
      "learning_rate": 2.7875e-05,
      "loss": 0.0016,
      "step": 53100
    },
    {
      "epoch": 3.5406666666666666,
      "grad_norm": 0.029447264969348907,
      "learning_rate": 2.7870833333333334e-05,
      "loss": 0.0022,
      "step": 53110
    },
    {
      "epoch": 3.541333333333333,
      "grad_norm": 0.11433742195367813,
      "learning_rate": 2.786666666666667e-05,
      "loss": 0.0027,
      "step": 53120
    },
    {
      "epoch": 3.542,
      "grad_norm": 0.36527055501937866,
      "learning_rate": 2.7862500000000003e-05,
      "loss": 0.0025,
      "step": 53130
    },
    {
      "epoch": 3.5426666666666664,
      "grad_norm": 0.5485861301422119,
      "learning_rate": 2.7858333333333337e-05,
      "loss": 0.0014,
      "step": 53140
    },
    {
      "epoch": 3.5433333333333334,
      "grad_norm": 0.055370721966028214,
      "learning_rate": 2.7854166666666672e-05,
      "loss": 0.0027,
      "step": 53150
    },
    {
      "epoch": 3.544,
      "grad_norm": 0.19204065203666687,
      "learning_rate": 2.7850000000000003e-05,
      "loss": 0.0025,
      "step": 53160
    },
    {
      "epoch": 3.5446666666666666,
      "grad_norm": 0.2279352992773056,
      "learning_rate": 2.7845833333333334e-05,
      "loss": 0.0018,
      "step": 53170
    },
    {
      "epoch": 3.5453333333333332,
      "grad_norm": 0.5915564298629761,
      "learning_rate": 2.7841666666666665e-05,
      "loss": 0.0023,
      "step": 53180
    },
    {
      "epoch": 3.5460000000000003,
      "grad_norm": 0.3146669864654541,
      "learning_rate": 2.78375e-05,
      "loss": 0.0028,
      "step": 53190
    },
    {
      "epoch": 3.546666666666667,
      "grad_norm": 0.13460595905780792,
      "learning_rate": 2.7833333333333333e-05,
      "loss": 0.0025,
      "step": 53200
    },
    {
      "epoch": 3.5473333333333334,
      "grad_norm": 0.1339263617992401,
      "learning_rate": 2.7829166666666668e-05,
      "loss": 0.0016,
      "step": 53210
    },
    {
      "epoch": 3.548,
      "grad_norm": 0.12716902792453766,
      "learning_rate": 2.7825000000000002e-05,
      "loss": 0.0018,
      "step": 53220
    },
    {
      "epoch": 3.5486666666666666,
      "grad_norm": 0.6811315417289734,
      "learning_rate": 2.7820833333333336e-05,
      "loss": 0.0013,
      "step": 53230
    },
    {
      "epoch": 3.5493333333333332,
      "grad_norm": 0.35755613446235657,
      "learning_rate": 2.7816666666666667e-05,
      "loss": 0.0026,
      "step": 53240
    },
    {
      "epoch": 3.55,
      "grad_norm": 0.39981526136398315,
      "learning_rate": 2.7812500000000002e-05,
      "loss": 0.0028,
      "step": 53250
    },
    {
      "epoch": 3.5506666666666664,
      "grad_norm": 0.5767139196395874,
      "learning_rate": 2.7808333333333336e-05,
      "loss": 0.0026,
      "step": 53260
    },
    {
      "epoch": 3.5513333333333335,
      "grad_norm": 0.6001318693161011,
      "learning_rate": 2.780416666666667e-05,
      "loss": 0.0023,
      "step": 53270
    },
    {
      "epoch": 3.552,
      "grad_norm": 0.11140383034944534,
      "learning_rate": 2.7800000000000005e-05,
      "loss": 0.002,
      "step": 53280
    },
    {
      "epoch": 3.5526666666666666,
      "grad_norm": 0.1483970731496811,
      "learning_rate": 2.7795833333333332e-05,
      "loss": 0.0017,
      "step": 53290
    },
    {
      "epoch": 3.5533333333333332,
      "grad_norm": 0.05099309980869293,
      "learning_rate": 2.7791666666666667e-05,
      "loss": 0.0015,
      "step": 53300
    },
    {
      "epoch": 3.5540000000000003,
      "grad_norm": 0.3547162115573883,
      "learning_rate": 2.77875e-05,
      "loss": 0.0023,
      "step": 53310
    },
    {
      "epoch": 3.554666666666667,
      "grad_norm": 0.24941493570804596,
      "learning_rate": 2.7783333333333332e-05,
      "loss": 0.0026,
      "step": 53320
    },
    {
      "epoch": 3.5553333333333335,
      "grad_norm": 0.060931846499443054,
      "learning_rate": 2.7779166666666667e-05,
      "loss": 0.0035,
      "step": 53330
    },
    {
      "epoch": 3.556,
      "grad_norm": 0.15200074017047882,
      "learning_rate": 2.7775e-05,
      "loss": 0.003,
      "step": 53340
    },
    {
      "epoch": 3.5566666666666666,
      "grad_norm": 0.43528154492378235,
      "learning_rate": 2.7770833333333335e-05,
      "loss": 0.0018,
      "step": 53350
    },
    {
      "epoch": 3.5573333333333332,
      "grad_norm": 0.47921741008758545,
      "learning_rate": 2.776666666666667e-05,
      "loss": 0.002,
      "step": 53360
    },
    {
      "epoch": 3.558,
      "grad_norm": 0.6088581085205078,
      "learning_rate": 2.7762500000000004e-05,
      "loss": 0.002,
      "step": 53370
    },
    {
      "epoch": 3.5586666666666664,
      "grad_norm": 0.3339705765247345,
      "learning_rate": 2.7758333333333335e-05,
      "loss": 0.0016,
      "step": 53380
    },
    {
      "epoch": 3.5593333333333335,
      "grad_norm": 0.7838256359100342,
      "learning_rate": 2.775416666666667e-05,
      "loss": 0.002,
      "step": 53390
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.27462390065193176,
      "learning_rate": 2.7750000000000004e-05,
      "loss": 0.0025,
      "step": 53400
    },
    {
      "epoch": 3.5606666666666666,
      "grad_norm": 0.2974066734313965,
      "learning_rate": 2.774583333333333e-05,
      "loss": 0.0012,
      "step": 53410
    },
    {
      "epoch": 3.5613333333333332,
      "grad_norm": 0.21555426716804504,
      "learning_rate": 2.7741666666666666e-05,
      "loss": 0.0024,
      "step": 53420
    },
    {
      "epoch": 3.5620000000000003,
      "grad_norm": 0.29944074153900146,
      "learning_rate": 2.77375e-05,
      "loss": 0.0019,
      "step": 53430
    },
    {
      "epoch": 3.562666666666667,
      "grad_norm": 0.0856095552444458,
      "learning_rate": 2.7733333333333334e-05,
      "loss": 0.0022,
      "step": 53440
    },
    {
      "epoch": 3.5633333333333335,
      "grad_norm": 0.7872602939605713,
      "learning_rate": 2.772916666666667e-05,
      "loss": 0.0025,
      "step": 53450
    },
    {
      "epoch": 3.564,
      "grad_norm": 0.15510453283786774,
      "learning_rate": 2.7725e-05,
      "loss": 0.002,
      "step": 53460
    },
    {
      "epoch": 3.5646666666666667,
      "grad_norm": 0.25548502802848816,
      "learning_rate": 2.7720833333333334e-05,
      "loss": 0.0014,
      "step": 53470
    },
    {
      "epoch": 3.5653333333333332,
      "grad_norm": 0.08774387091398239,
      "learning_rate": 2.771666666666667e-05,
      "loss": 0.0024,
      "step": 53480
    },
    {
      "epoch": 3.566,
      "grad_norm": 0.4702391028404236,
      "learning_rate": 2.7712500000000003e-05,
      "loss": 0.0036,
      "step": 53490
    },
    {
      "epoch": 3.5666666666666664,
      "grad_norm": 0.1503509134054184,
      "learning_rate": 2.7708333333333337e-05,
      "loss": 0.0023,
      "step": 53500
    },
    {
      "epoch": 3.5673333333333335,
      "grad_norm": 0.03877149149775505,
      "learning_rate": 2.770416666666667e-05,
      "loss": 0.0024,
      "step": 53510
    },
    {
      "epoch": 3.568,
      "grad_norm": 0.4298458397388458,
      "learning_rate": 2.7700000000000002e-05,
      "loss": 0.0018,
      "step": 53520
    },
    {
      "epoch": 3.5686666666666667,
      "grad_norm": 0.12041860818862915,
      "learning_rate": 2.7695833333333333e-05,
      "loss": 0.0021,
      "step": 53530
    },
    {
      "epoch": 3.5693333333333332,
      "grad_norm": 0.21230252087116241,
      "learning_rate": 2.7691666666666664e-05,
      "loss": 0.0022,
      "step": 53540
    },
    {
      "epoch": 3.57,
      "grad_norm": 0.3624890446662903,
      "learning_rate": 2.76875e-05,
      "loss": 0.0026,
      "step": 53550
    },
    {
      "epoch": 3.570666666666667,
      "grad_norm": 0.053831763565540314,
      "learning_rate": 2.7683333333333333e-05,
      "loss": 0.0026,
      "step": 53560
    },
    {
      "epoch": 3.5713333333333335,
      "grad_norm": 0.046470895409584045,
      "learning_rate": 2.7679166666666667e-05,
      "loss": 0.0021,
      "step": 53570
    },
    {
      "epoch": 3.572,
      "grad_norm": 0.08805214613676071,
      "learning_rate": 2.7675000000000002e-05,
      "loss": 0.0022,
      "step": 53580
    },
    {
      "epoch": 3.5726666666666667,
      "grad_norm": 0.9016239643096924,
      "learning_rate": 2.7670833333333336e-05,
      "loss": 0.0021,
      "step": 53590
    },
    {
      "epoch": 3.5733333333333333,
      "grad_norm": 0.7192584276199341,
      "learning_rate": 2.7666666666666667e-05,
      "loss": 0.0024,
      "step": 53600
    },
    {
      "epoch": 3.574,
      "grad_norm": 0.1388036608695984,
      "learning_rate": 2.76625e-05,
      "loss": 0.0022,
      "step": 53610
    },
    {
      "epoch": 3.5746666666666664,
      "grad_norm": 0.5366812944412231,
      "learning_rate": 2.7658333333333336e-05,
      "loss": 0.0018,
      "step": 53620
    },
    {
      "epoch": 3.5753333333333335,
      "grad_norm": 0.18306270241737366,
      "learning_rate": 2.765416666666667e-05,
      "loss": 0.0023,
      "step": 53630
    },
    {
      "epoch": 3.576,
      "grad_norm": 0.5969101190567017,
      "learning_rate": 2.7650000000000005e-05,
      "loss": 0.0013,
      "step": 53640
    },
    {
      "epoch": 3.5766666666666667,
      "grad_norm": 0.4979064166545868,
      "learning_rate": 2.7645833333333332e-05,
      "loss": 0.0022,
      "step": 53650
    },
    {
      "epoch": 3.5773333333333333,
      "grad_norm": 0.354291707277298,
      "learning_rate": 2.7641666666666667e-05,
      "loss": 0.0028,
      "step": 53660
    },
    {
      "epoch": 3.578,
      "grad_norm": 0.43957510590553284,
      "learning_rate": 2.76375e-05,
      "loss": 0.0015,
      "step": 53670
    },
    {
      "epoch": 3.578666666666667,
      "grad_norm": 0.5889555215835571,
      "learning_rate": 2.7633333333333332e-05,
      "loss": 0.0018,
      "step": 53680
    },
    {
      "epoch": 3.5793333333333335,
      "grad_norm": 0.7522940635681152,
      "learning_rate": 2.7629166666666666e-05,
      "loss": 0.0024,
      "step": 53690
    },
    {
      "epoch": 3.58,
      "grad_norm": 0.21791234612464905,
      "learning_rate": 2.7625e-05,
      "loss": 0.0019,
      "step": 53700
    },
    {
      "epoch": 3.5806666666666667,
      "grad_norm": 0.5837142467498779,
      "learning_rate": 2.7620833333333335e-05,
      "loss": 0.0024,
      "step": 53710
    },
    {
      "epoch": 3.5813333333333333,
      "grad_norm": 1.4413868188858032,
      "learning_rate": 2.761666666666667e-05,
      "loss": 0.0035,
      "step": 53720
    },
    {
      "epoch": 3.582,
      "grad_norm": 0.5125672817230225,
      "learning_rate": 2.7612500000000004e-05,
      "loss": 0.0023,
      "step": 53730
    },
    {
      "epoch": 3.5826666666666664,
      "grad_norm": 0.08335231989622116,
      "learning_rate": 2.7608333333333335e-05,
      "loss": 0.0018,
      "step": 53740
    },
    {
      "epoch": 3.5833333333333335,
      "grad_norm": 0.6107212901115417,
      "learning_rate": 2.760416666666667e-05,
      "loss": 0.002,
      "step": 53750
    },
    {
      "epoch": 3.584,
      "grad_norm": 0.28681156039237976,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 0.0016,
      "step": 53760
    },
    {
      "epoch": 3.5846666666666667,
      "grad_norm": 0.11185991764068604,
      "learning_rate": 2.7595833333333338e-05,
      "loss": 0.0023,
      "step": 53770
    },
    {
      "epoch": 3.5853333333333333,
      "grad_norm": 0.29596275091171265,
      "learning_rate": 2.7591666666666665e-05,
      "loss": 0.0027,
      "step": 53780
    },
    {
      "epoch": 3.586,
      "grad_norm": 0.4821067750453949,
      "learning_rate": 2.75875e-05,
      "loss": 0.002,
      "step": 53790
    },
    {
      "epoch": 3.586666666666667,
      "grad_norm": 0.6229442954063416,
      "learning_rate": 2.7583333333333334e-05,
      "loss": 0.0027,
      "step": 53800
    },
    {
      "epoch": 3.5873333333333335,
      "grad_norm": 0.397132009267807,
      "learning_rate": 2.757916666666667e-05,
      "loss": 0.0026,
      "step": 53810
    },
    {
      "epoch": 3.588,
      "grad_norm": 0.19272631406784058,
      "learning_rate": 2.7575e-05,
      "loss": 0.0021,
      "step": 53820
    },
    {
      "epoch": 3.5886666666666667,
      "grad_norm": 0.07402455806732178,
      "learning_rate": 2.7570833333333334e-05,
      "loss": 0.0023,
      "step": 53830
    },
    {
      "epoch": 3.5893333333333333,
      "grad_norm": 0.5992611050605774,
      "learning_rate": 2.7566666666666668e-05,
      "loss": 0.0022,
      "step": 53840
    },
    {
      "epoch": 3.59,
      "grad_norm": 0.11722326278686523,
      "learning_rate": 2.7562500000000002e-05,
      "loss": 0.0026,
      "step": 53850
    },
    {
      "epoch": 3.5906666666666665,
      "grad_norm": 0.44484537839889526,
      "learning_rate": 2.7558333333333337e-05,
      "loss": 0.0017,
      "step": 53860
    },
    {
      "epoch": 3.5913333333333335,
      "grad_norm": 0.5969895124435425,
      "learning_rate": 2.755416666666667e-05,
      "loss": 0.0028,
      "step": 53870
    },
    {
      "epoch": 3.592,
      "grad_norm": 0.2607001066207886,
      "learning_rate": 2.7550000000000002e-05,
      "loss": 0.0017,
      "step": 53880
    },
    {
      "epoch": 3.5926666666666667,
      "grad_norm": 0.25314605236053467,
      "learning_rate": 2.7545833333333337e-05,
      "loss": 0.0014,
      "step": 53890
    },
    {
      "epoch": 3.5933333333333333,
      "grad_norm": 0.08403104543685913,
      "learning_rate": 2.7541666666666664e-05,
      "loss": 0.0021,
      "step": 53900
    },
    {
      "epoch": 3.594,
      "grad_norm": 0.5790687799453735,
      "learning_rate": 2.75375e-05,
      "loss": 0.0018,
      "step": 53910
    },
    {
      "epoch": 3.594666666666667,
      "grad_norm": 0.08912063390016556,
      "learning_rate": 2.7533333333333333e-05,
      "loss": 0.002,
      "step": 53920
    },
    {
      "epoch": 3.5953333333333335,
      "grad_norm": 0.5119194388389587,
      "learning_rate": 2.7529166666666667e-05,
      "loss": 0.0033,
      "step": 53930
    },
    {
      "epoch": 3.596,
      "grad_norm": 0.19138400256633759,
      "learning_rate": 2.7525e-05,
      "loss": 0.0018,
      "step": 53940
    },
    {
      "epoch": 3.5966666666666667,
      "grad_norm": 0.36264723539352417,
      "learning_rate": 2.7520833333333336e-05,
      "loss": 0.0016,
      "step": 53950
    },
    {
      "epoch": 3.5973333333333333,
      "grad_norm": 0.828441858291626,
      "learning_rate": 2.7516666666666667e-05,
      "loss": 0.0017,
      "step": 53960
    },
    {
      "epoch": 3.598,
      "grad_norm": 0.8959299921989441,
      "learning_rate": 2.75125e-05,
      "loss": 0.0019,
      "step": 53970
    },
    {
      "epoch": 3.5986666666666665,
      "grad_norm": 0.42464935779571533,
      "learning_rate": 2.7508333333333336e-05,
      "loss": 0.0023,
      "step": 53980
    },
    {
      "epoch": 3.5993333333333335,
      "grad_norm": 0.27593693137168884,
      "learning_rate": 2.750416666666667e-05,
      "loss": 0.0019,
      "step": 53990
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.25373589992523193,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 0.0016,
      "step": 54000
    },
    {
      "epoch": 3.6006666666666667,
      "grad_norm": 1.0028115510940552,
      "learning_rate": 2.749583333333334e-05,
      "loss": 0.0021,
      "step": 54010
    },
    {
      "epoch": 3.6013333333333333,
      "grad_norm": 0.5761139392852783,
      "learning_rate": 2.7491666666666666e-05,
      "loss": 0.0036,
      "step": 54020
    },
    {
      "epoch": 3.602,
      "grad_norm": 0.1497410237789154,
      "learning_rate": 2.74875e-05,
      "loss": 0.002,
      "step": 54030
    },
    {
      "epoch": 3.602666666666667,
      "grad_norm": 0.06274481862783432,
      "learning_rate": 2.748333333333333e-05,
      "loss": 0.0028,
      "step": 54040
    },
    {
      "epoch": 3.6033333333333335,
      "grad_norm": 0.02816302888095379,
      "learning_rate": 2.7479166666666666e-05,
      "loss": 0.0036,
      "step": 54050
    },
    {
      "epoch": 3.604,
      "grad_norm": 0.1819700002670288,
      "learning_rate": 2.7475e-05,
      "loss": 0.0022,
      "step": 54060
    },
    {
      "epoch": 3.6046666666666667,
      "grad_norm": 0.6909846663475037,
      "learning_rate": 2.7470833333333335e-05,
      "loss": 0.0019,
      "step": 54070
    },
    {
      "epoch": 3.6053333333333333,
      "grad_norm": 0.3549788296222687,
      "learning_rate": 2.746666666666667e-05,
      "loss": 0.0016,
      "step": 54080
    },
    {
      "epoch": 3.606,
      "grad_norm": 0.5714548230171204,
      "learning_rate": 2.7462500000000003e-05,
      "loss": 0.002,
      "step": 54090
    },
    {
      "epoch": 3.6066666666666665,
      "grad_norm": 0.3251594305038452,
      "learning_rate": 2.7458333333333334e-05,
      "loss": 0.0016,
      "step": 54100
    },
    {
      "epoch": 3.607333333333333,
      "grad_norm": 0.512366771697998,
      "learning_rate": 2.745416666666667e-05,
      "loss": 0.002,
      "step": 54110
    },
    {
      "epoch": 3.608,
      "grad_norm": 0.6547939777374268,
      "learning_rate": 2.7450000000000003e-05,
      "loss": 0.0019,
      "step": 54120
    },
    {
      "epoch": 3.6086666666666667,
      "grad_norm": 0.1924176812171936,
      "learning_rate": 2.7445833333333338e-05,
      "loss": 0.0012,
      "step": 54130
    },
    {
      "epoch": 3.6093333333333333,
      "grad_norm": 0.14686401188373566,
      "learning_rate": 2.7441666666666665e-05,
      "loss": 0.0022,
      "step": 54140
    },
    {
      "epoch": 3.61,
      "grad_norm": 0.1320011466741562,
      "learning_rate": 2.74375e-05,
      "loss": 0.0016,
      "step": 54150
    },
    {
      "epoch": 3.610666666666667,
      "grad_norm": 0.2984752953052521,
      "learning_rate": 2.7433333333333334e-05,
      "loss": 0.0019,
      "step": 54160
    },
    {
      "epoch": 3.6113333333333335,
      "grad_norm": 0.39624708890914917,
      "learning_rate": 2.7429166666666668e-05,
      "loss": 0.0014,
      "step": 54170
    },
    {
      "epoch": 3.612,
      "grad_norm": 0.07548099011182785,
      "learning_rate": 2.7425e-05,
      "loss": 0.0018,
      "step": 54180
    },
    {
      "epoch": 3.6126666666666667,
      "grad_norm": 0.18381905555725098,
      "learning_rate": 2.7420833333333334e-05,
      "loss": 0.0016,
      "step": 54190
    },
    {
      "epoch": 3.6133333333333333,
      "grad_norm": 0.030693406239151955,
      "learning_rate": 2.7416666666666668e-05,
      "loss": 0.0021,
      "step": 54200
    },
    {
      "epoch": 3.614,
      "grad_norm": 0.04554992914199829,
      "learning_rate": 2.7412500000000002e-05,
      "loss": 0.0018,
      "step": 54210
    },
    {
      "epoch": 3.6146666666666665,
      "grad_norm": 0.2645796239376068,
      "learning_rate": 2.7408333333333337e-05,
      "loss": 0.0025,
      "step": 54220
    },
    {
      "epoch": 3.615333333333333,
      "grad_norm": 0.060281749814748764,
      "learning_rate": 2.740416666666667e-05,
      "loss": 0.0032,
      "step": 54230
    },
    {
      "epoch": 3.616,
      "grad_norm": 0.32541772723197937,
      "learning_rate": 2.7400000000000002e-05,
      "loss": 0.0022,
      "step": 54240
    },
    {
      "epoch": 3.6166666666666667,
      "grad_norm": 0.04918382689356804,
      "learning_rate": 2.7395833333333336e-05,
      "loss": 0.0018,
      "step": 54250
    },
    {
      "epoch": 3.6173333333333333,
      "grad_norm": 0.359348863363266,
      "learning_rate": 2.7391666666666664e-05,
      "loss": 0.0022,
      "step": 54260
    },
    {
      "epoch": 3.618,
      "grad_norm": 0.2729191184043884,
      "learning_rate": 2.7387499999999998e-05,
      "loss": 0.0026,
      "step": 54270
    },
    {
      "epoch": 3.618666666666667,
      "grad_norm": 0.3282848000526428,
      "learning_rate": 2.7383333333333333e-05,
      "loss": 0.002,
      "step": 54280
    },
    {
      "epoch": 3.6193333333333335,
      "grad_norm": 0.31917446851730347,
      "learning_rate": 2.7379166666666667e-05,
      "loss": 0.0015,
      "step": 54290
    },
    {
      "epoch": 3.62,
      "grad_norm": 0.21711015701293945,
      "learning_rate": 2.7375e-05,
      "loss": 0.0012,
      "step": 54300
    },
    {
      "epoch": 3.6206666666666667,
      "grad_norm": 0.17757655680179596,
      "learning_rate": 2.7370833333333336e-05,
      "loss": 0.0015,
      "step": 54310
    },
    {
      "epoch": 3.6213333333333333,
      "grad_norm": 0.3556286096572876,
      "learning_rate": 2.7366666666666667e-05,
      "loss": 0.0018,
      "step": 54320
    },
    {
      "epoch": 3.622,
      "grad_norm": 0.22200389206409454,
      "learning_rate": 2.73625e-05,
      "loss": 0.0014,
      "step": 54330
    },
    {
      "epoch": 3.6226666666666665,
      "grad_norm": 0.5723977088928223,
      "learning_rate": 2.7358333333333335e-05,
      "loss": 0.0018,
      "step": 54340
    },
    {
      "epoch": 3.623333333333333,
      "grad_norm": 0.5401991009712219,
      "learning_rate": 2.735416666666667e-05,
      "loss": 0.0018,
      "step": 54350
    },
    {
      "epoch": 3.624,
      "grad_norm": 0.5278910398483276,
      "learning_rate": 2.7350000000000004e-05,
      "loss": 0.0021,
      "step": 54360
    },
    {
      "epoch": 3.6246666666666667,
      "grad_norm": 0.528038740158081,
      "learning_rate": 2.734583333333334e-05,
      "loss": 0.0016,
      "step": 54370
    },
    {
      "epoch": 3.6253333333333333,
      "grad_norm": 0.6292407512664795,
      "learning_rate": 2.7341666666666666e-05,
      "loss": 0.0017,
      "step": 54380
    },
    {
      "epoch": 3.626,
      "grad_norm": 0.2833229601383209,
      "learning_rate": 2.73375e-05,
      "loss": 0.0019,
      "step": 54390
    },
    {
      "epoch": 3.626666666666667,
      "grad_norm": 0.06280822306871414,
      "learning_rate": 2.733333333333333e-05,
      "loss": 0.002,
      "step": 54400
    },
    {
      "epoch": 3.6273333333333335,
      "grad_norm": 0.06241180747747421,
      "learning_rate": 2.7329166666666666e-05,
      "loss": 0.0014,
      "step": 54410
    },
    {
      "epoch": 3.628,
      "grad_norm": 0.16352017223834991,
      "learning_rate": 2.7325e-05,
      "loss": 0.0014,
      "step": 54420
    },
    {
      "epoch": 3.6286666666666667,
      "grad_norm": 0.09645525366067886,
      "learning_rate": 2.7320833333333334e-05,
      "loss": 0.0024,
      "step": 54430
    },
    {
      "epoch": 3.6293333333333333,
      "grad_norm": 0.2843154966831207,
      "learning_rate": 2.731666666666667e-05,
      "loss": 0.0022,
      "step": 54440
    },
    {
      "epoch": 3.63,
      "grad_norm": 0.17659896612167358,
      "learning_rate": 2.7312500000000003e-05,
      "loss": 0.0017,
      "step": 54450
    },
    {
      "epoch": 3.6306666666666665,
      "grad_norm": 0.25331810116767883,
      "learning_rate": 2.7308333333333334e-05,
      "loss": 0.0016,
      "step": 54460
    },
    {
      "epoch": 3.631333333333333,
      "grad_norm": 0.06313560158014297,
      "learning_rate": 2.730416666666667e-05,
      "loss": 0.0024,
      "step": 54470
    },
    {
      "epoch": 3.632,
      "grad_norm": 0.16294637322425842,
      "learning_rate": 2.7300000000000003e-05,
      "loss": 0.0018,
      "step": 54480
    },
    {
      "epoch": 3.6326666666666667,
      "grad_norm": 0.12074636667966843,
      "learning_rate": 2.7295833333333337e-05,
      "loss": 0.0021,
      "step": 54490
    },
    {
      "epoch": 3.6333333333333333,
      "grad_norm": 0.18086525797843933,
      "learning_rate": 2.7291666666666665e-05,
      "loss": 0.0014,
      "step": 54500
    },
    {
      "epoch": 3.634,
      "grad_norm": 0.05927829071879387,
      "learning_rate": 2.72875e-05,
      "loss": 0.0021,
      "step": 54510
    },
    {
      "epoch": 3.634666666666667,
      "grad_norm": 0.014367351308465004,
      "learning_rate": 2.7283333333333334e-05,
      "loss": 0.002,
      "step": 54520
    },
    {
      "epoch": 3.6353333333333335,
      "grad_norm": 0.033955641090869904,
      "learning_rate": 2.7279166666666668e-05,
      "loss": 0.0024,
      "step": 54530
    },
    {
      "epoch": 3.636,
      "grad_norm": 0.28756898641586304,
      "learning_rate": 2.7275e-05,
      "loss": 0.0013,
      "step": 54540
    },
    {
      "epoch": 3.6366666666666667,
      "grad_norm": 0.8142186403274536,
      "learning_rate": 2.7270833333333333e-05,
      "loss": 0.0019,
      "step": 54550
    },
    {
      "epoch": 3.6373333333333333,
      "grad_norm": 0.021262696012854576,
      "learning_rate": 2.7266666666666668e-05,
      "loss": 0.0024,
      "step": 54560
    },
    {
      "epoch": 3.638,
      "grad_norm": 0.7627814412117004,
      "learning_rate": 2.7262500000000002e-05,
      "loss": 0.0023,
      "step": 54570
    },
    {
      "epoch": 3.6386666666666665,
      "grad_norm": 0.05766068771481514,
      "learning_rate": 2.7258333333333336e-05,
      "loss": 0.0019,
      "step": 54580
    },
    {
      "epoch": 3.639333333333333,
      "grad_norm": 0.5691734552383423,
      "learning_rate": 2.725416666666667e-05,
      "loss": 0.0029,
      "step": 54590
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.4997749328613281,
      "learning_rate": 2.725e-05,
      "loss": 0.003,
      "step": 54600
    },
    {
      "epoch": 3.6406666666666667,
      "grad_norm": 0.5221484303474426,
      "learning_rate": 2.7245833333333336e-05,
      "loss": 0.0026,
      "step": 54610
    },
    {
      "epoch": 3.6413333333333333,
      "grad_norm": 0.034962963312864304,
      "learning_rate": 2.7241666666666667e-05,
      "loss": 0.0021,
      "step": 54620
    },
    {
      "epoch": 3.642,
      "grad_norm": 0.059578560292720795,
      "learning_rate": 2.7237499999999998e-05,
      "loss": 0.0016,
      "step": 54630
    },
    {
      "epoch": 3.642666666666667,
      "grad_norm": 0.3625144362449646,
      "learning_rate": 2.7233333333333332e-05,
      "loss": 0.0016,
      "step": 54640
    },
    {
      "epoch": 3.6433333333333335,
      "grad_norm": 0.404466450214386,
      "learning_rate": 2.7229166666666667e-05,
      "loss": 0.0022,
      "step": 54650
    },
    {
      "epoch": 3.644,
      "grad_norm": 0.17813877761363983,
      "learning_rate": 2.7225e-05,
      "loss": 0.0012,
      "step": 54660
    },
    {
      "epoch": 3.6446666666666667,
      "grad_norm": 0.1461179107427597,
      "learning_rate": 2.7220833333333335e-05,
      "loss": 0.0018,
      "step": 54670
    },
    {
      "epoch": 3.6453333333333333,
      "grad_norm": 0.5858315825462341,
      "learning_rate": 2.7216666666666666e-05,
      "loss": 0.0038,
      "step": 54680
    },
    {
      "epoch": 3.646,
      "grad_norm": 0.46804285049438477,
      "learning_rate": 2.72125e-05,
      "loss": 0.0024,
      "step": 54690
    },
    {
      "epoch": 3.6466666666666665,
      "grad_norm": 0.5587131381034851,
      "learning_rate": 2.7208333333333335e-05,
      "loss": 0.0024,
      "step": 54700
    },
    {
      "epoch": 3.647333333333333,
      "grad_norm": 0.14463210105895996,
      "learning_rate": 2.720416666666667e-05,
      "loss": 0.0021,
      "step": 54710
    },
    {
      "epoch": 3.648,
      "grad_norm": 0.036497484892606735,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 0.0031,
      "step": 54720
    },
    {
      "epoch": 3.6486666666666667,
      "grad_norm": 0.9740584492683411,
      "learning_rate": 2.7195833333333338e-05,
      "loss": 0.0019,
      "step": 54730
    },
    {
      "epoch": 3.6493333333333333,
      "grad_norm": 0.10845110565423965,
      "learning_rate": 2.7191666666666666e-05,
      "loss": 0.0018,
      "step": 54740
    },
    {
      "epoch": 3.65,
      "grad_norm": 0.7027281522750854,
      "learning_rate": 2.71875e-05,
      "loss": 0.002,
      "step": 54750
    },
    {
      "epoch": 3.6506666666666665,
      "grad_norm": 0.7265079617500305,
      "learning_rate": 2.7183333333333335e-05,
      "loss": 0.0018,
      "step": 54760
    },
    {
      "epoch": 3.6513333333333335,
      "grad_norm": 0.67360520362854,
      "learning_rate": 2.7179166666666665e-05,
      "loss": 0.0028,
      "step": 54770
    },
    {
      "epoch": 3.652,
      "grad_norm": 0.7274544835090637,
      "learning_rate": 2.7175e-05,
      "loss": 0.0016,
      "step": 54780
    },
    {
      "epoch": 3.6526666666666667,
      "grad_norm": 0.7247242331504822,
      "learning_rate": 2.7170833333333334e-05,
      "loss": 0.0018,
      "step": 54790
    },
    {
      "epoch": 3.6533333333333333,
      "grad_norm": 0.33651912212371826,
      "learning_rate": 2.716666666666667e-05,
      "loss": 0.002,
      "step": 54800
    },
    {
      "epoch": 3.654,
      "grad_norm": 0.4429260790348053,
      "learning_rate": 2.7162500000000003e-05,
      "loss": 0.0025,
      "step": 54810
    },
    {
      "epoch": 3.6546666666666665,
      "grad_norm": 0.08773306757211685,
      "learning_rate": 2.7158333333333337e-05,
      "loss": 0.0016,
      "step": 54820
    },
    {
      "epoch": 3.655333333333333,
      "grad_norm": 0.05748145654797554,
      "learning_rate": 2.7154166666666668e-05,
      "loss": 0.0019,
      "step": 54830
    },
    {
      "epoch": 3.656,
      "grad_norm": 0.4802500605583191,
      "learning_rate": 2.7150000000000003e-05,
      "loss": 0.0023,
      "step": 54840
    },
    {
      "epoch": 3.6566666666666667,
      "grad_norm": 0.6382219195365906,
      "learning_rate": 2.7145833333333337e-05,
      "loss": 0.0023,
      "step": 54850
    },
    {
      "epoch": 3.6573333333333333,
      "grad_norm": 0.07564692199230194,
      "learning_rate": 2.7141666666666665e-05,
      "loss": 0.0018,
      "step": 54860
    },
    {
      "epoch": 3.658,
      "grad_norm": 0.3186444640159607,
      "learning_rate": 2.71375e-05,
      "loss": 0.002,
      "step": 54870
    },
    {
      "epoch": 3.6586666666666665,
      "grad_norm": 0.10393300652503967,
      "learning_rate": 2.7133333333333333e-05,
      "loss": 0.0017,
      "step": 54880
    },
    {
      "epoch": 3.6593333333333335,
      "grad_norm": 0.049183495342731476,
      "learning_rate": 2.7129166666666668e-05,
      "loss": 0.002,
      "step": 54890
    },
    {
      "epoch": 3.66,
      "grad_norm": 0.23572100698947906,
      "learning_rate": 2.7125000000000002e-05,
      "loss": 0.0026,
      "step": 54900
    },
    {
      "epoch": 3.6606666666666667,
      "grad_norm": 0.21689696609973907,
      "learning_rate": 2.7120833333333333e-05,
      "loss": 0.0022,
      "step": 54910
    },
    {
      "epoch": 3.6613333333333333,
      "grad_norm": 0.6806251406669617,
      "learning_rate": 2.7116666666666667e-05,
      "loss": 0.0021,
      "step": 54920
    },
    {
      "epoch": 3.662,
      "grad_norm": 0.16096392273902893,
      "learning_rate": 2.7112500000000002e-05,
      "loss": 0.0023,
      "step": 54930
    },
    {
      "epoch": 3.6626666666666665,
      "grad_norm": 0.21416258811950684,
      "learning_rate": 2.7108333333333336e-05,
      "loss": 0.0018,
      "step": 54940
    },
    {
      "epoch": 3.663333333333333,
      "grad_norm": 0.3961278200149536,
      "learning_rate": 2.710416666666667e-05,
      "loss": 0.0027,
      "step": 54950
    },
    {
      "epoch": 3.664,
      "grad_norm": 0.5764394402503967,
      "learning_rate": 2.7100000000000005e-05,
      "loss": 0.0017,
      "step": 54960
    },
    {
      "epoch": 3.6646666666666667,
      "grad_norm": 0.2916010618209839,
      "learning_rate": 2.7095833333333336e-05,
      "loss": 0.0017,
      "step": 54970
    },
    {
      "epoch": 3.6653333333333333,
      "grad_norm": 0.2760305106639862,
      "learning_rate": 2.7091666666666667e-05,
      "loss": 0.0017,
      "step": 54980
    },
    {
      "epoch": 3.666,
      "grad_norm": 0.19900229573249817,
      "learning_rate": 2.7087499999999998e-05,
      "loss": 0.0023,
      "step": 54990
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.2928716838359833,
      "learning_rate": 2.7083333333333332e-05,
      "loss": 0.0024,
      "step": 55000
    },
    {
      "epoch": 3.6673333333333336,
      "grad_norm": 0.1091332733631134,
      "learning_rate": 2.7079166666666666e-05,
      "loss": 0.0017,
      "step": 55010
    },
    {
      "epoch": 3.668,
      "grad_norm": 0.2166268676519394,
      "learning_rate": 2.7075e-05,
      "loss": 0.0027,
      "step": 55020
    },
    {
      "epoch": 3.6686666666666667,
      "grad_norm": 0.4423801600933075,
      "learning_rate": 2.7070833333333335e-05,
      "loss": 0.0017,
      "step": 55030
    },
    {
      "epoch": 3.6693333333333333,
      "grad_norm": 0.32967609167099,
      "learning_rate": 2.706666666666667e-05,
      "loss": 0.0027,
      "step": 55040
    },
    {
      "epoch": 3.67,
      "grad_norm": 0.5332717299461365,
      "learning_rate": 2.70625e-05,
      "loss": 0.0026,
      "step": 55050
    },
    {
      "epoch": 3.6706666666666665,
      "grad_norm": 0.283474862575531,
      "learning_rate": 2.7058333333333335e-05,
      "loss": 0.0032,
      "step": 55060
    },
    {
      "epoch": 3.671333333333333,
      "grad_norm": 0.24925532937049866,
      "learning_rate": 2.705416666666667e-05,
      "loss": 0.0018,
      "step": 55070
    },
    {
      "epoch": 3.672,
      "grad_norm": 0.25967663526535034,
      "learning_rate": 2.7050000000000004e-05,
      "loss": 0.0018,
      "step": 55080
    },
    {
      "epoch": 3.6726666666666667,
      "grad_norm": 0.05826115608215332,
      "learning_rate": 2.7045833333333338e-05,
      "loss": 0.002,
      "step": 55090
    },
    {
      "epoch": 3.6733333333333333,
      "grad_norm": 0.6441780924797058,
      "learning_rate": 2.7041666666666672e-05,
      "loss": 0.0019,
      "step": 55100
    },
    {
      "epoch": 3.674,
      "grad_norm": 0.8781682848930359,
      "learning_rate": 2.70375e-05,
      "loss": 0.0017,
      "step": 55110
    },
    {
      "epoch": 3.6746666666666665,
      "grad_norm": 0.7167600989341736,
      "learning_rate": 2.7033333333333334e-05,
      "loss": 0.002,
      "step": 55120
    },
    {
      "epoch": 3.6753333333333336,
      "grad_norm": 0.09235372394323349,
      "learning_rate": 2.7029166666666665e-05,
      "loss": 0.0017,
      "step": 55130
    },
    {
      "epoch": 3.676,
      "grad_norm": 0.045254744589328766,
      "learning_rate": 2.7025e-05,
      "loss": 0.0021,
      "step": 55140
    },
    {
      "epoch": 3.6766666666666667,
      "grad_norm": 0.1110442504286766,
      "learning_rate": 2.7020833333333334e-05,
      "loss": 0.0026,
      "step": 55150
    },
    {
      "epoch": 3.6773333333333333,
      "grad_norm": 0.08163256198167801,
      "learning_rate": 2.701666666666667e-05,
      "loss": 0.003,
      "step": 55160
    },
    {
      "epoch": 3.678,
      "grad_norm": 0.5504955053329468,
      "learning_rate": 2.7012500000000003e-05,
      "loss": 0.0023,
      "step": 55170
    },
    {
      "epoch": 3.6786666666666665,
      "grad_norm": 0.21738718450069427,
      "learning_rate": 2.7008333333333337e-05,
      "loss": 0.0029,
      "step": 55180
    },
    {
      "epoch": 3.679333333333333,
      "grad_norm": 0.35843217372894287,
      "learning_rate": 2.7004166666666668e-05,
      "loss": 0.0023,
      "step": 55190
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.26795586943626404,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.0023,
      "step": 55200
    },
    {
      "epoch": 3.6806666666666668,
      "grad_norm": 0.05503401532769203,
      "learning_rate": 2.6995833333333337e-05,
      "loss": 0.0031,
      "step": 55210
    },
    {
      "epoch": 3.6813333333333333,
      "grad_norm": 0.05688930302858353,
      "learning_rate": 2.699166666666667e-05,
      "loss": 0.0021,
      "step": 55220
    },
    {
      "epoch": 3.682,
      "grad_norm": 0.4220013916492462,
      "learning_rate": 2.69875e-05,
      "loss": 0.0021,
      "step": 55230
    },
    {
      "epoch": 3.6826666666666665,
      "grad_norm": 0.5421189665794373,
      "learning_rate": 2.6983333333333333e-05,
      "loss": 0.0013,
      "step": 55240
    },
    {
      "epoch": 3.6833333333333336,
      "grad_norm": 0.3228629529476166,
      "learning_rate": 2.6979166666666667e-05,
      "loss": 0.0022,
      "step": 55250
    },
    {
      "epoch": 3.684,
      "grad_norm": 0.22008715569972992,
      "learning_rate": 2.6975000000000002e-05,
      "loss": 0.0019,
      "step": 55260
    },
    {
      "epoch": 3.6846666666666668,
      "grad_norm": 0.2109556496143341,
      "learning_rate": 2.6970833333333333e-05,
      "loss": 0.0022,
      "step": 55270
    },
    {
      "epoch": 3.6853333333333333,
      "grad_norm": 0.06321332603693008,
      "learning_rate": 2.6966666666666667e-05,
      "loss": 0.002,
      "step": 55280
    },
    {
      "epoch": 3.686,
      "grad_norm": 0.3857016861438751,
      "learning_rate": 2.69625e-05,
      "loss": 0.0022,
      "step": 55290
    },
    {
      "epoch": 3.6866666666666665,
      "grad_norm": 0.25903570652008057,
      "learning_rate": 2.6958333333333336e-05,
      "loss": 0.0027,
      "step": 55300
    },
    {
      "epoch": 3.687333333333333,
      "grad_norm": 0.4266781806945801,
      "learning_rate": 2.695416666666667e-05,
      "loss": 0.002,
      "step": 55310
    },
    {
      "epoch": 3.6879999999999997,
      "grad_norm": 0.9246386885643005,
      "learning_rate": 2.6950000000000005e-05,
      "loss": 0.0019,
      "step": 55320
    },
    {
      "epoch": 3.6886666666666668,
      "grad_norm": 0.7473073601722717,
      "learning_rate": 2.6945833333333336e-05,
      "loss": 0.002,
      "step": 55330
    },
    {
      "epoch": 3.6893333333333334,
      "grad_norm": 0.057977236807346344,
      "learning_rate": 2.694166666666667e-05,
      "loss": 0.0021,
      "step": 55340
    },
    {
      "epoch": 3.69,
      "grad_norm": 0.3980746567249298,
      "learning_rate": 2.6937499999999997e-05,
      "loss": 0.0026,
      "step": 55350
    },
    {
      "epoch": 3.6906666666666665,
      "grad_norm": 0.10371750593185425,
      "learning_rate": 2.6933333333333332e-05,
      "loss": 0.0026,
      "step": 55360
    },
    {
      "epoch": 3.6913333333333336,
      "grad_norm": 0.21911285817623138,
      "learning_rate": 2.6929166666666666e-05,
      "loss": 0.0019,
      "step": 55370
    },
    {
      "epoch": 3.692,
      "grad_norm": 0.10997650027275085,
      "learning_rate": 2.6925e-05,
      "loss": 0.0016,
      "step": 55380
    },
    {
      "epoch": 3.6926666666666668,
      "grad_norm": 0.7174238562583923,
      "learning_rate": 2.6920833333333335e-05,
      "loss": 0.0015,
      "step": 55390
    },
    {
      "epoch": 3.6933333333333334,
      "grad_norm": 0.08083312958478928,
      "learning_rate": 2.691666666666667e-05,
      "loss": 0.0016,
      "step": 55400
    },
    {
      "epoch": 3.694,
      "grad_norm": 0.7196187973022461,
      "learning_rate": 2.69125e-05,
      "loss": 0.0019,
      "step": 55410
    },
    {
      "epoch": 3.6946666666666665,
      "grad_norm": 0.10380496084690094,
      "learning_rate": 2.6908333333333335e-05,
      "loss": 0.0021,
      "step": 55420
    },
    {
      "epoch": 3.695333333333333,
      "grad_norm": 0.398474782705307,
      "learning_rate": 2.690416666666667e-05,
      "loss": 0.0023,
      "step": 55430
    },
    {
      "epoch": 3.6959999999999997,
      "grad_norm": 0.7870453000068665,
      "learning_rate": 2.6900000000000003e-05,
      "loss": 0.0021,
      "step": 55440
    },
    {
      "epoch": 3.6966666666666668,
      "grad_norm": 0.3538650572299957,
      "learning_rate": 2.6895833333333338e-05,
      "loss": 0.0022,
      "step": 55450
    },
    {
      "epoch": 3.6973333333333334,
      "grad_norm": 0.4872000515460968,
      "learning_rate": 2.6891666666666672e-05,
      "loss": 0.0019,
      "step": 55460
    },
    {
      "epoch": 3.698,
      "grad_norm": 0.11264267563819885,
      "learning_rate": 2.68875e-05,
      "loss": 0.0029,
      "step": 55470
    },
    {
      "epoch": 3.6986666666666665,
      "grad_norm": 0.5411856770515442,
      "learning_rate": 2.6883333333333334e-05,
      "loss": 0.0028,
      "step": 55480
    },
    {
      "epoch": 3.6993333333333336,
      "grad_norm": 0.41637179255485535,
      "learning_rate": 2.6879166666666665e-05,
      "loss": 0.0023,
      "step": 55490
    },
    {
      "epoch": 3.7,
      "grad_norm": 0.18524301052093506,
      "learning_rate": 2.6875e-05,
      "loss": 0.0023,
      "step": 55500
    },
    {
      "epoch": 3.7006666666666668,
      "grad_norm": 0.5614860653877258,
      "learning_rate": 2.6870833333333334e-05,
      "loss": 0.0016,
      "step": 55510
    },
    {
      "epoch": 3.7013333333333334,
      "grad_norm": 0.5393558144569397,
      "learning_rate": 2.6866666666666668e-05,
      "loss": 0.0016,
      "step": 55520
    },
    {
      "epoch": 3.702,
      "grad_norm": 0.06168721988797188,
      "learning_rate": 2.6862500000000002e-05,
      "loss": 0.003,
      "step": 55530
    },
    {
      "epoch": 3.7026666666666666,
      "grad_norm": 1.0927557945251465,
      "learning_rate": 2.6858333333333337e-05,
      "loss": 0.0026,
      "step": 55540
    },
    {
      "epoch": 3.703333333333333,
      "grad_norm": 0.14067766070365906,
      "learning_rate": 2.6854166666666668e-05,
      "loss": 0.0022,
      "step": 55550
    },
    {
      "epoch": 3.7039999999999997,
      "grad_norm": 0.3053544759750366,
      "learning_rate": 2.6850000000000002e-05,
      "loss": 0.0022,
      "step": 55560
    },
    {
      "epoch": 3.7046666666666668,
      "grad_norm": 0.08415315300226212,
      "learning_rate": 2.6845833333333336e-05,
      "loss": 0.0016,
      "step": 55570
    },
    {
      "epoch": 3.7053333333333334,
      "grad_norm": 0.36631402373313904,
      "learning_rate": 2.684166666666667e-05,
      "loss": 0.002,
      "step": 55580
    },
    {
      "epoch": 3.706,
      "grad_norm": 0.18158689141273499,
      "learning_rate": 2.68375e-05,
      "loss": 0.0029,
      "step": 55590
    },
    {
      "epoch": 3.7066666666666666,
      "grad_norm": 0.219193235039711,
      "learning_rate": 2.6833333333333333e-05,
      "loss": 0.0013,
      "step": 55600
    },
    {
      "epoch": 3.7073333333333336,
      "grad_norm": 0.4674653708934784,
      "learning_rate": 2.6829166666666667e-05,
      "loss": 0.0019,
      "step": 55610
    },
    {
      "epoch": 3.708,
      "grad_norm": 0.25101491808891296,
      "learning_rate": 2.6825e-05,
      "loss": 0.0019,
      "step": 55620
    },
    {
      "epoch": 3.708666666666667,
      "grad_norm": 0.18456795811653137,
      "learning_rate": 2.6820833333333332e-05,
      "loss": 0.0019,
      "step": 55630
    },
    {
      "epoch": 3.7093333333333334,
      "grad_norm": 0.22600941359996796,
      "learning_rate": 2.6816666666666667e-05,
      "loss": 0.0017,
      "step": 55640
    },
    {
      "epoch": 3.71,
      "grad_norm": 0.5733206272125244,
      "learning_rate": 2.68125e-05,
      "loss": 0.0017,
      "step": 55650
    },
    {
      "epoch": 3.7106666666666666,
      "grad_norm": 0.2513492703437805,
      "learning_rate": 2.6808333333333336e-05,
      "loss": 0.0026,
      "step": 55660
    },
    {
      "epoch": 3.711333333333333,
      "grad_norm": 0.45856645703315735,
      "learning_rate": 2.680416666666667e-05,
      "loss": 0.0014,
      "step": 55670
    },
    {
      "epoch": 3.7119999999999997,
      "grad_norm": 0.8194279074668884,
      "learning_rate": 2.6800000000000004e-05,
      "loss": 0.0019,
      "step": 55680
    },
    {
      "epoch": 3.712666666666667,
      "grad_norm": 0.09450292587280273,
      "learning_rate": 2.6795833333333335e-05,
      "loss": 0.0027,
      "step": 55690
    },
    {
      "epoch": 3.7133333333333334,
      "grad_norm": 0.04810415208339691,
      "learning_rate": 2.679166666666667e-05,
      "loss": 0.002,
      "step": 55700
    },
    {
      "epoch": 3.714,
      "grad_norm": 0.4993976354598999,
      "learning_rate": 2.6787499999999997e-05,
      "loss": 0.0019,
      "step": 55710
    },
    {
      "epoch": 3.7146666666666666,
      "grad_norm": 0.1317158043384552,
      "learning_rate": 2.678333333333333e-05,
      "loss": 0.0019,
      "step": 55720
    },
    {
      "epoch": 3.7153333333333336,
      "grad_norm": 0.2891610562801361,
      "learning_rate": 2.6779166666666666e-05,
      "loss": 0.0021,
      "step": 55730
    },
    {
      "epoch": 3.716,
      "grad_norm": 0.53627610206604,
      "learning_rate": 2.6775e-05,
      "loss": 0.002,
      "step": 55740
    },
    {
      "epoch": 3.716666666666667,
      "grad_norm": 0.6470583081245422,
      "learning_rate": 2.6770833333333335e-05,
      "loss": 0.0022,
      "step": 55750
    },
    {
      "epoch": 3.7173333333333334,
      "grad_norm": 0.1795177310705185,
      "learning_rate": 2.676666666666667e-05,
      "loss": 0.0016,
      "step": 55760
    },
    {
      "epoch": 3.718,
      "grad_norm": 0.42931875586509705,
      "learning_rate": 2.67625e-05,
      "loss": 0.002,
      "step": 55770
    },
    {
      "epoch": 3.7186666666666666,
      "grad_norm": 0.04695611447095871,
      "learning_rate": 2.6758333333333334e-05,
      "loss": 0.0022,
      "step": 55780
    },
    {
      "epoch": 3.719333333333333,
      "grad_norm": 0.5521923899650574,
      "learning_rate": 2.675416666666667e-05,
      "loss": 0.0016,
      "step": 55790
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 0.08022843301296234,
      "learning_rate": 2.6750000000000003e-05,
      "loss": 0.0023,
      "step": 55800
    },
    {
      "epoch": 3.720666666666667,
      "grad_norm": 0.22238688170909882,
      "learning_rate": 2.6745833333333337e-05,
      "loss": 0.002,
      "step": 55810
    },
    {
      "epoch": 3.7213333333333334,
      "grad_norm": 0.05035149306058884,
      "learning_rate": 2.6741666666666672e-05,
      "loss": 0.0023,
      "step": 55820
    },
    {
      "epoch": 3.722,
      "grad_norm": 0.15219689905643463,
      "learning_rate": 2.67375e-05,
      "loss": 0.0017,
      "step": 55830
    },
    {
      "epoch": 3.7226666666666666,
      "grad_norm": 0.18166644871234894,
      "learning_rate": 2.6733333333333334e-05,
      "loss": 0.0019,
      "step": 55840
    },
    {
      "epoch": 3.7233333333333336,
      "grad_norm": 0.2630724608898163,
      "learning_rate": 2.6729166666666665e-05,
      "loss": 0.0015,
      "step": 55850
    },
    {
      "epoch": 3.724,
      "grad_norm": 0.11439213901758194,
      "learning_rate": 2.6725e-05,
      "loss": 0.0022,
      "step": 55860
    },
    {
      "epoch": 3.724666666666667,
      "grad_norm": 0.19028255343437195,
      "learning_rate": 2.6720833333333333e-05,
      "loss": 0.0018,
      "step": 55870
    },
    {
      "epoch": 3.7253333333333334,
      "grad_norm": 0.7180737853050232,
      "learning_rate": 2.6716666666666668e-05,
      "loss": 0.0025,
      "step": 55880
    },
    {
      "epoch": 3.726,
      "grad_norm": 0.0750855877995491,
      "learning_rate": 2.6712500000000002e-05,
      "loss": 0.0018,
      "step": 55890
    },
    {
      "epoch": 3.7266666666666666,
      "grad_norm": 0.3168388307094574,
      "learning_rate": 2.6708333333333337e-05,
      "loss": 0.0018,
      "step": 55900
    },
    {
      "epoch": 3.727333333333333,
      "grad_norm": 0.2987093925476074,
      "learning_rate": 2.6704166666666668e-05,
      "loss": 0.0026,
      "step": 55910
    },
    {
      "epoch": 3.7279999999999998,
      "grad_norm": 0.4702964723110199,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 0.0018,
      "step": 55920
    },
    {
      "epoch": 3.728666666666667,
      "grad_norm": 0.18128128349781036,
      "learning_rate": 2.6695833333333336e-05,
      "loss": 0.0018,
      "step": 55930
    },
    {
      "epoch": 3.7293333333333334,
      "grad_norm": 0.3597632050514221,
      "learning_rate": 2.669166666666667e-05,
      "loss": 0.0019,
      "step": 55940
    },
    {
      "epoch": 3.73,
      "grad_norm": 0.28703662753105164,
      "learning_rate": 2.6687499999999998e-05,
      "loss": 0.0024,
      "step": 55950
    },
    {
      "epoch": 3.7306666666666666,
      "grad_norm": 0.43238839507102966,
      "learning_rate": 2.6683333333333333e-05,
      "loss": 0.0017,
      "step": 55960
    },
    {
      "epoch": 3.731333333333333,
      "grad_norm": 0.9977552890777588,
      "learning_rate": 2.6679166666666667e-05,
      "loss": 0.002,
      "step": 55970
    },
    {
      "epoch": 3.732,
      "grad_norm": 1.0828253030776978,
      "learning_rate": 2.6675e-05,
      "loss": 0.0023,
      "step": 55980
    },
    {
      "epoch": 3.732666666666667,
      "grad_norm": 0.237582266330719,
      "learning_rate": 2.6670833333333332e-05,
      "loss": 0.0027,
      "step": 55990
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 0.06280922144651413,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.0025,
      "step": 56000
    },
    {
      "epoch": 3.734,
      "grad_norm": 0.3958619236946106,
      "learning_rate": 2.66625e-05,
      "loss": 0.0027,
      "step": 56010
    },
    {
      "epoch": 3.7346666666666666,
      "grad_norm": 0.39774784445762634,
      "learning_rate": 2.6658333333333335e-05,
      "loss": 0.0022,
      "step": 56020
    },
    {
      "epoch": 3.735333333333333,
      "grad_norm": 0.5655427575111389,
      "learning_rate": 2.665416666666667e-05,
      "loss": 0.0019,
      "step": 56030
    },
    {
      "epoch": 3.7359999999999998,
      "grad_norm": 0.5058708786964417,
      "learning_rate": 2.6650000000000004e-05,
      "loss": 0.0016,
      "step": 56040
    },
    {
      "epoch": 3.736666666666667,
      "grad_norm": 0.1439753621816635,
      "learning_rate": 2.6645833333333335e-05,
      "loss": 0.0021,
      "step": 56050
    },
    {
      "epoch": 3.7373333333333334,
      "grad_norm": 0.045684996992349625,
      "learning_rate": 2.664166666666667e-05,
      "loss": 0.0025,
      "step": 56060
    },
    {
      "epoch": 3.738,
      "grad_norm": 0.42858704924583435,
      "learning_rate": 2.6637499999999997e-05,
      "loss": 0.0027,
      "step": 56070
    },
    {
      "epoch": 3.7386666666666666,
      "grad_norm": 0.8163274526596069,
      "learning_rate": 2.663333333333333e-05,
      "loss": 0.0023,
      "step": 56080
    },
    {
      "epoch": 3.739333333333333,
      "grad_norm": 0.690692663192749,
      "learning_rate": 2.6629166666666666e-05,
      "loss": 0.0024,
      "step": 56090
    },
    {
      "epoch": 3.74,
      "grad_norm": 0.4295239746570587,
      "learning_rate": 2.6625e-05,
      "loss": 0.003,
      "step": 56100
    },
    {
      "epoch": 3.740666666666667,
      "grad_norm": 0.09408380091190338,
      "learning_rate": 2.6620833333333334e-05,
      "loss": 0.0021,
      "step": 56110
    },
    {
      "epoch": 3.7413333333333334,
      "grad_norm": 0.0483756884932518,
      "learning_rate": 2.661666666666667e-05,
      "loss": 0.0024,
      "step": 56120
    },
    {
      "epoch": 3.742,
      "grad_norm": 0.6339743137359619,
      "learning_rate": 2.66125e-05,
      "loss": 0.0024,
      "step": 56130
    },
    {
      "epoch": 3.7426666666666666,
      "grad_norm": 0.2885661721229553,
      "learning_rate": 2.6608333333333334e-05,
      "loss": 0.0017,
      "step": 56140
    },
    {
      "epoch": 3.743333333333333,
      "grad_norm": 0.4709206223487854,
      "learning_rate": 2.660416666666667e-05,
      "loss": 0.0017,
      "step": 56150
    },
    {
      "epoch": 3.7439999999999998,
      "grad_norm": 0.07399918884038925,
      "learning_rate": 2.6600000000000003e-05,
      "loss": 0.0027,
      "step": 56160
    },
    {
      "epoch": 3.744666666666667,
      "grad_norm": 0.4652474820613861,
      "learning_rate": 2.6595833333333337e-05,
      "loss": 0.0024,
      "step": 56170
    },
    {
      "epoch": 3.7453333333333334,
      "grad_norm": 0.501264750957489,
      "learning_rate": 2.659166666666667e-05,
      "loss": 0.0019,
      "step": 56180
    },
    {
      "epoch": 3.746,
      "grad_norm": 0.41528427600860596,
      "learning_rate": 2.65875e-05,
      "loss": 0.0022,
      "step": 56190
    },
    {
      "epoch": 3.7466666666666666,
      "grad_norm": 0.39219143986701965,
      "learning_rate": 2.6583333333333333e-05,
      "loss": 0.0014,
      "step": 56200
    },
    {
      "epoch": 3.747333333333333,
      "grad_norm": 0.43285906314849854,
      "learning_rate": 2.6579166666666664e-05,
      "loss": 0.002,
      "step": 56210
    },
    {
      "epoch": 3.748,
      "grad_norm": 0.1810421347618103,
      "learning_rate": 2.6575e-05,
      "loss": 0.0019,
      "step": 56220
    },
    {
      "epoch": 3.748666666666667,
      "grad_norm": 0.31625843048095703,
      "learning_rate": 2.6570833333333333e-05,
      "loss": 0.0021,
      "step": 56230
    },
    {
      "epoch": 3.7493333333333334,
      "grad_norm": 0.4502151310443878,
      "learning_rate": 2.6566666666666668e-05,
      "loss": 0.0022,
      "step": 56240
    },
    {
      "epoch": 3.75,
      "grad_norm": 0.4311170279979706,
      "learning_rate": 2.6562500000000002e-05,
      "loss": 0.0019,
      "step": 56250
    },
    {
      "epoch": 3.7506666666666666,
      "grad_norm": 0.532727062702179,
      "learning_rate": 2.6558333333333336e-05,
      "loss": 0.0031,
      "step": 56260
    },
    {
      "epoch": 3.751333333333333,
      "grad_norm": 0.6369550824165344,
      "learning_rate": 2.6554166666666667e-05,
      "loss": 0.0025,
      "step": 56270
    },
    {
      "epoch": 3.752,
      "grad_norm": 0.32063910365104675,
      "learning_rate": 2.655e-05,
      "loss": 0.0013,
      "step": 56280
    },
    {
      "epoch": 3.752666666666667,
      "grad_norm": 0.4235810339450836,
      "learning_rate": 2.6545833333333336e-05,
      "loss": 0.0026,
      "step": 56290
    },
    {
      "epoch": 3.7533333333333334,
      "grad_norm": 0.5261256694793701,
      "learning_rate": 2.654166666666667e-05,
      "loss": 0.0018,
      "step": 56300
    },
    {
      "epoch": 3.754,
      "grad_norm": 0.05539307743310928,
      "learning_rate": 2.6537500000000005e-05,
      "loss": 0.0025,
      "step": 56310
    },
    {
      "epoch": 3.7546666666666666,
      "grad_norm": 0.0458688959479332,
      "learning_rate": 2.6533333333333332e-05,
      "loss": 0.0015,
      "step": 56320
    },
    {
      "epoch": 3.755333333333333,
      "grad_norm": 0.6669294834136963,
      "learning_rate": 2.6529166666666667e-05,
      "loss": 0.0014,
      "step": 56330
    },
    {
      "epoch": 3.7560000000000002,
      "grad_norm": 0.2576500177383423,
      "learning_rate": 2.6525e-05,
      "loss": 0.0017,
      "step": 56340
    },
    {
      "epoch": 3.756666666666667,
      "grad_norm": 0.18064039945602417,
      "learning_rate": 2.6520833333333332e-05,
      "loss": 0.0014,
      "step": 56350
    },
    {
      "epoch": 3.7573333333333334,
      "grad_norm": 0.1202339455485344,
      "learning_rate": 2.6516666666666666e-05,
      "loss": 0.0019,
      "step": 56360
    },
    {
      "epoch": 3.758,
      "grad_norm": 0.08975962549448013,
      "learning_rate": 2.65125e-05,
      "loss": 0.0018,
      "step": 56370
    },
    {
      "epoch": 3.7586666666666666,
      "grad_norm": 0.34603285789489746,
      "learning_rate": 2.6508333333333335e-05,
      "loss": 0.0025,
      "step": 56380
    },
    {
      "epoch": 3.759333333333333,
      "grad_norm": 0.5667335987091064,
      "learning_rate": 2.650416666666667e-05,
      "loss": 0.0023,
      "step": 56390
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.0506913885474205,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 0.0016,
      "step": 56400
    },
    {
      "epoch": 3.760666666666667,
      "grad_norm": 0.33438727259635925,
      "learning_rate": 2.6495833333333335e-05,
      "loss": 0.0017,
      "step": 56410
    },
    {
      "epoch": 3.7613333333333334,
      "grad_norm": 0.045019399374723434,
      "learning_rate": 2.649166666666667e-05,
      "loss": 0.0019,
      "step": 56420
    },
    {
      "epoch": 3.762,
      "grad_norm": 0.6086969375610352,
      "learning_rate": 2.6487500000000003e-05,
      "loss": 0.0014,
      "step": 56430
    },
    {
      "epoch": 3.7626666666666666,
      "grad_norm": 0.39216598868370056,
      "learning_rate": 2.648333333333333e-05,
      "loss": 0.0022,
      "step": 56440
    },
    {
      "epoch": 3.763333333333333,
      "grad_norm": 0.14807195961475372,
      "learning_rate": 2.6479166666666665e-05,
      "loss": 0.0015,
      "step": 56450
    },
    {
      "epoch": 3.7640000000000002,
      "grad_norm": 0.31204092502593994,
      "learning_rate": 2.6475e-05,
      "loss": 0.0019,
      "step": 56460
    },
    {
      "epoch": 3.764666666666667,
      "grad_norm": 0.052207548171281815,
      "learning_rate": 2.6470833333333334e-05,
      "loss": 0.0021,
      "step": 56470
    },
    {
      "epoch": 3.7653333333333334,
      "grad_norm": 0.18458139896392822,
      "learning_rate": 2.646666666666667e-05,
      "loss": 0.0019,
      "step": 56480
    },
    {
      "epoch": 3.766,
      "grad_norm": 0.5016621351242065,
      "learning_rate": 2.6462500000000003e-05,
      "loss": 0.002,
      "step": 56490
    },
    {
      "epoch": 3.7666666666666666,
      "grad_norm": 0.3434908986091614,
      "learning_rate": 2.6458333333333334e-05,
      "loss": 0.0018,
      "step": 56500
    },
    {
      "epoch": 3.767333333333333,
      "grad_norm": 0.19067758321762085,
      "learning_rate": 2.6454166666666668e-05,
      "loss": 0.0024,
      "step": 56510
    },
    {
      "epoch": 3.768,
      "grad_norm": 0.5667270421981812,
      "learning_rate": 2.6450000000000003e-05,
      "loss": 0.0025,
      "step": 56520
    },
    {
      "epoch": 3.768666666666667,
      "grad_norm": 0.39220699667930603,
      "learning_rate": 2.6445833333333337e-05,
      "loss": 0.0023,
      "step": 56530
    },
    {
      "epoch": 3.7693333333333334,
      "grad_norm": 0.14790932834148407,
      "learning_rate": 2.644166666666667e-05,
      "loss": 0.0019,
      "step": 56540
    },
    {
      "epoch": 3.77,
      "grad_norm": 0.049372997134923935,
      "learning_rate": 2.6437500000000002e-05,
      "loss": 0.0021,
      "step": 56550
    },
    {
      "epoch": 3.7706666666666666,
      "grad_norm": 0.11318391561508179,
      "learning_rate": 2.6433333333333333e-05,
      "loss": 0.0022,
      "step": 56560
    },
    {
      "epoch": 3.771333333333333,
      "grad_norm": 0.5476011037826538,
      "learning_rate": 2.6429166666666668e-05,
      "loss": 0.0023,
      "step": 56570
    },
    {
      "epoch": 3.7720000000000002,
      "grad_norm": 0.9763866066932678,
      "learning_rate": 2.6425e-05,
      "loss": 0.0019,
      "step": 56580
    },
    {
      "epoch": 3.772666666666667,
      "grad_norm": 0.45750540494918823,
      "learning_rate": 2.6420833333333333e-05,
      "loss": 0.0024,
      "step": 56590
    },
    {
      "epoch": 3.7733333333333334,
      "grad_norm": 0.8597297072410583,
      "learning_rate": 2.6416666666666667e-05,
      "loss": 0.0028,
      "step": 56600
    },
    {
      "epoch": 3.774,
      "grad_norm": 1.0359351634979248,
      "learning_rate": 2.64125e-05,
      "loss": 0.0016,
      "step": 56610
    },
    {
      "epoch": 3.7746666666666666,
      "grad_norm": 0.366659015417099,
      "learning_rate": 2.6408333333333336e-05,
      "loss": 0.0025,
      "step": 56620
    },
    {
      "epoch": 3.775333333333333,
      "grad_norm": 0.8497991561889648,
      "learning_rate": 2.640416666666667e-05,
      "loss": 0.0017,
      "step": 56630
    },
    {
      "epoch": 3.776,
      "grad_norm": 0.252262145280838,
      "learning_rate": 2.64e-05,
      "loss": 0.0016,
      "step": 56640
    },
    {
      "epoch": 3.7766666666666664,
      "grad_norm": 0.4363762438297272,
      "learning_rate": 2.6395833333333336e-05,
      "loss": 0.0017,
      "step": 56650
    },
    {
      "epoch": 3.7773333333333334,
      "grad_norm": 0.43113261461257935,
      "learning_rate": 2.639166666666667e-05,
      "loss": 0.0018,
      "step": 56660
    },
    {
      "epoch": 3.778,
      "grad_norm": 0.21597829461097717,
      "learning_rate": 2.6387500000000004e-05,
      "loss": 0.0016,
      "step": 56670
    },
    {
      "epoch": 3.7786666666666666,
      "grad_norm": 0.6174818277359009,
      "learning_rate": 2.6383333333333332e-05,
      "loss": 0.0019,
      "step": 56680
    },
    {
      "epoch": 3.779333333333333,
      "grad_norm": 0.33113938570022583,
      "learning_rate": 2.6379166666666666e-05,
      "loss": 0.0018,
      "step": 56690
    },
    {
      "epoch": 3.7800000000000002,
      "grad_norm": 0.17788814008235931,
      "learning_rate": 2.6375e-05,
      "loss": 0.0013,
      "step": 56700
    },
    {
      "epoch": 3.780666666666667,
      "grad_norm": 0.1922338902950287,
      "learning_rate": 2.6370833333333335e-05,
      "loss": 0.0023,
      "step": 56710
    },
    {
      "epoch": 3.7813333333333334,
      "grad_norm": 0.18787412345409393,
      "learning_rate": 2.6366666666666666e-05,
      "loss": 0.0029,
      "step": 56720
    },
    {
      "epoch": 3.782,
      "grad_norm": 0.4375324547290802,
      "learning_rate": 2.63625e-05,
      "loss": 0.0021,
      "step": 56730
    },
    {
      "epoch": 3.7826666666666666,
      "grad_norm": 0.7562397718429565,
      "learning_rate": 2.6358333333333335e-05,
      "loss": 0.0022,
      "step": 56740
    },
    {
      "epoch": 3.783333333333333,
      "grad_norm": 0.7228103280067444,
      "learning_rate": 2.635416666666667e-05,
      "loss": 0.0019,
      "step": 56750
    },
    {
      "epoch": 3.784,
      "grad_norm": 0.5527734160423279,
      "learning_rate": 2.6350000000000004e-05,
      "loss": 0.0024,
      "step": 56760
    },
    {
      "epoch": 3.7846666666666664,
      "grad_norm": 0.4279246926307678,
      "learning_rate": 2.6345833333333338e-05,
      "loss": 0.0031,
      "step": 56770
    },
    {
      "epoch": 3.7853333333333334,
      "grad_norm": 0.47172072529792786,
      "learning_rate": 2.634166666666667e-05,
      "loss": 0.002,
      "step": 56780
    },
    {
      "epoch": 3.786,
      "grad_norm": 0.11079251766204834,
      "learning_rate": 2.6337500000000003e-05,
      "loss": 0.0023,
      "step": 56790
    },
    {
      "epoch": 3.7866666666666666,
      "grad_norm": 0.12212560325860977,
      "learning_rate": 2.633333333333333e-05,
      "loss": 0.0027,
      "step": 56800
    },
    {
      "epoch": 3.787333333333333,
      "grad_norm": 0.05564945563673973,
      "learning_rate": 2.6329166666666665e-05,
      "loss": 0.0017,
      "step": 56810
    },
    {
      "epoch": 3.7880000000000003,
      "grad_norm": 0.7708616256713867,
      "learning_rate": 2.6325e-05,
      "loss": 0.0017,
      "step": 56820
    },
    {
      "epoch": 3.788666666666667,
      "grad_norm": 0.32844477891921997,
      "learning_rate": 2.6320833333333334e-05,
      "loss": 0.0022,
      "step": 56830
    },
    {
      "epoch": 3.7893333333333334,
      "grad_norm": 0.9370293617248535,
      "learning_rate": 2.6316666666666668e-05,
      "loss": 0.0016,
      "step": 56840
    },
    {
      "epoch": 3.79,
      "grad_norm": 0.24705730378627777,
      "learning_rate": 2.6312500000000003e-05,
      "loss": 0.0019,
      "step": 56850
    },
    {
      "epoch": 3.7906666666666666,
      "grad_norm": 0.09297718107700348,
      "learning_rate": 2.6308333333333334e-05,
      "loss": 0.0017,
      "step": 56860
    },
    {
      "epoch": 3.791333333333333,
      "grad_norm": 0.6893731355667114,
      "learning_rate": 2.6304166666666668e-05,
      "loss": 0.0013,
      "step": 56870
    },
    {
      "epoch": 3.792,
      "grad_norm": 0.2544634938240051,
      "learning_rate": 2.6300000000000002e-05,
      "loss": 0.0017,
      "step": 56880
    },
    {
      "epoch": 3.7926666666666664,
      "grad_norm": 0.7818661332130432,
      "learning_rate": 2.6295833333333337e-05,
      "loss": 0.0021,
      "step": 56890
    },
    {
      "epoch": 3.7933333333333334,
      "grad_norm": 0.5122090578079224,
      "learning_rate": 2.629166666666667e-05,
      "loss": 0.002,
      "step": 56900
    },
    {
      "epoch": 3.794,
      "grad_norm": 0.4173946678638458,
      "learning_rate": 2.6287500000000005e-05,
      "loss": 0.0021,
      "step": 56910
    },
    {
      "epoch": 3.7946666666666666,
      "grad_norm": 0.2914727032184601,
      "learning_rate": 2.6283333333333333e-05,
      "loss": 0.0022,
      "step": 56920
    },
    {
      "epoch": 3.7953333333333332,
      "grad_norm": 0.7654797434806824,
      "learning_rate": 2.6279166666666667e-05,
      "loss": 0.0023,
      "step": 56930
    },
    {
      "epoch": 3.7960000000000003,
      "grad_norm": 0.6017678380012512,
      "learning_rate": 2.6275e-05,
      "loss": 0.0026,
      "step": 56940
    },
    {
      "epoch": 3.796666666666667,
      "grad_norm": 0.25030744075775146,
      "learning_rate": 2.6270833333333333e-05,
      "loss": 0.0021,
      "step": 56950
    },
    {
      "epoch": 3.7973333333333334,
      "grad_norm": 0.10938186943531036,
      "learning_rate": 2.6266666666666667e-05,
      "loss": 0.0016,
      "step": 56960
    },
    {
      "epoch": 3.798,
      "grad_norm": 0.1597355604171753,
      "learning_rate": 2.62625e-05,
      "loss": 0.0019,
      "step": 56970
    },
    {
      "epoch": 3.7986666666666666,
      "grad_norm": 0.11158202588558197,
      "learning_rate": 2.6258333333333336e-05,
      "loss": 0.0012,
      "step": 56980
    },
    {
      "epoch": 3.7993333333333332,
      "grad_norm": 0.6669997572898865,
      "learning_rate": 2.625416666666667e-05,
      "loss": 0.0017,
      "step": 56990
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.39582592248916626,
      "learning_rate": 2.625e-05,
      "loss": 0.0019,
      "step": 57000
    },
    {
      "epoch": 3.8006666666666664,
      "grad_norm": 0.3983306884765625,
      "learning_rate": 2.6245833333333335e-05,
      "loss": 0.0023,
      "step": 57010
    },
    {
      "epoch": 3.8013333333333335,
      "grad_norm": 0.8433958888053894,
      "learning_rate": 2.624166666666667e-05,
      "loss": 0.0024,
      "step": 57020
    },
    {
      "epoch": 3.802,
      "grad_norm": 0.5749797821044922,
      "learning_rate": 2.6237500000000004e-05,
      "loss": 0.0035,
      "step": 57030
    },
    {
      "epoch": 3.8026666666666666,
      "grad_norm": 0.5758789777755737,
      "learning_rate": 2.6233333333333332e-05,
      "loss": 0.002,
      "step": 57040
    },
    {
      "epoch": 3.8033333333333332,
      "grad_norm": 0.19331803917884827,
      "learning_rate": 2.6229166666666666e-05,
      "loss": 0.0016,
      "step": 57050
    },
    {
      "epoch": 3.8040000000000003,
      "grad_norm": 0.5739579796791077,
      "learning_rate": 2.6225e-05,
      "loss": 0.0038,
      "step": 57060
    },
    {
      "epoch": 3.804666666666667,
      "grad_norm": 0.21706415712833405,
      "learning_rate": 2.6220833333333335e-05,
      "loss": 0.0021,
      "step": 57070
    },
    {
      "epoch": 3.8053333333333335,
      "grad_norm": 0.12563979625701904,
      "learning_rate": 2.6216666666666666e-05,
      "loss": 0.0022,
      "step": 57080
    },
    {
      "epoch": 3.806,
      "grad_norm": 0.10954184085130692,
      "learning_rate": 2.62125e-05,
      "loss": 0.0023,
      "step": 57090
    },
    {
      "epoch": 3.8066666666666666,
      "grad_norm": 0.038974396884441376,
      "learning_rate": 2.6208333333333335e-05,
      "loss": 0.0022,
      "step": 57100
    },
    {
      "epoch": 3.8073333333333332,
      "grad_norm": 0.5909824371337891,
      "learning_rate": 2.620416666666667e-05,
      "loss": 0.0015,
      "step": 57110
    },
    {
      "epoch": 3.808,
      "grad_norm": 0.682389497756958,
      "learning_rate": 2.6200000000000003e-05,
      "loss": 0.0019,
      "step": 57120
    },
    {
      "epoch": 3.8086666666666664,
      "grad_norm": 0.21611832082271576,
      "learning_rate": 2.6195833333333338e-05,
      "loss": 0.0017,
      "step": 57130
    },
    {
      "epoch": 3.8093333333333335,
      "grad_norm": 0.4580658972263336,
      "learning_rate": 2.619166666666667e-05,
      "loss": 0.0015,
      "step": 57140
    },
    {
      "epoch": 3.81,
      "grad_norm": 0.3366905450820923,
      "learning_rate": 2.6187500000000003e-05,
      "loss": 0.0025,
      "step": 57150
    },
    {
      "epoch": 3.8106666666666666,
      "grad_norm": 0.01868179440498352,
      "learning_rate": 2.618333333333333e-05,
      "loss": 0.0019,
      "step": 57160
    },
    {
      "epoch": 3.8113333333333332,
      "grad_norm": 0.263869971036911,
      "learning_rate": 2.6179166666666665e-05,
      "loss": 0.0022,
      "step": 57170
    },
    {
      "epoch": 3.8120000000000003,
      "grad_norm": 0.44258445501327515,
      "learning_rate": 2.6175e-05,
      "loss": 0.0016,
      "step": 57180
    },
    {
      "epoch": 3.812666666666667,
      "grad_norm": 0.3695172369480133,
      "learning_rate": 2.6170833333333334e-05,
      "loss": 0.0028,
      "step": 57190
    },
    {
      "epoch": 3.8133333333333335,
      "grad_norm": 0.05163300782442093,
      "learning_rate": 2.6166666666666668e-05,
      "loss": 0.0022,
      "step": 57200
    },
    {
      "epoch": 3.814,
      "grad_norm": 0.5089191794395447,
      "learning_rate": 2.6162500000000002e-05,
      "loss": 0.0023,
      "step": 57210
    },
    {
      "epoch": 3.8146666666666667,
      "grad_norm": 0.43211811780929565,
      "learning_rate": 2.6158333333333333e-05,
      "loss": 0.0014,
      "step": 57220
    },
    {
      "epoch": 3.8153333333333332,
      "grad_norm": 0.6585438251495361,
      "learning_rate": 2.6154166666666668e-05,
      "loss": 0.0021,
      "step": 57230
    },
    {
      "epoch": 3.816,
      "grad_norm": 0.2584596872329712,
      "learning_rate": 2.6150000000000002e-05,
      "loss": 0.0019,
      "step": 57240
    },
    {
      "epoch": 3.8166666666666664,
      "grad_norm": 0.12516844272613525,
      "learning_rate": 2.6145833333333336e-05,
      "loss": 0.0021,
      "step": 57250
    },
    {
      "epoch": 3.8173333333333335,
      "grad_norm": 0.17756888270378113,
      "learning_rate": 2.614166666666667e-05,
      "loss": 0.0028,
      "step": 57260
    },
    {
      "epoch": 3.818,
      "grad_norm": 0.053322162479162216,
      "learning_rate": 2.6137500000000005e-05,
      "loss": 0.002,
      "step": 57270
    },
    {
      "epoch": 3.8186666666666667,
      "grad_norm": 0.566379964351654,
      "learning_rate": 2.6133333333333333e-05,
      "loss": 0.0026,
      "step": 57280
    },
    {
      "epoch": 3.8193333333333332,
      "grad_norm": 0.6695346832275391,
      "learning_rate": 2.6129166666666667e-05,
      "loss": 0.0014,
      "step": 57290
    },
    {
      "epoch": 3.82,
      "grad_norm": 0.45067688822746277,
      "learning_rate": 2.6124999999999998e-05,
      "loss": 0.0021,
      "step": 57300
    },
    {
      "epoch": 3.820666666666667,
      "grad_norm": 0.5183604955673218,
      "learning_rate": 2.6120833333333332e-05,
      "loss": 0.0019,
      "step": 57310
    },
    {
      "epoch": 3.8213333333333335,
      "grad_norm": 0.6614333391189575,
      "learning_rate": 2.6116666666666667e-05,
      "loss": 0.0025,
      "step": 57320
    },
    {
      "epoch": 3.822,
      "grad_norm": 0.22842280566692352,
      "learning_rate": 2.61125e-05,
      "loss": 0.0029,
      "step": 57330
    },
    {
      "epoch": 3.8226666666666667,
      "grad_norm": 0.19408033788204193,
      "learning_rate": 2.6108333333333335e-05,
      "loss": 0.0036,
      "step": 57340
    },
    {
      "epoch": 3.8233333333333333,
      "grad_norm": 1.0600059032440186,
      "learning_rate": 2.610416666666667e-05,
      "loss": 0.0031,
      "step": 57350
    },
    {
      "epoch": 3.824,
      "grad_norm": 0.5416162014007568,
      "learning_rate": 2.61e-05,
      "loss": 0.0019,
      "step": 57360
    },
    {
      "epoch": 3.8246666666666664,
      "grad_norm": 0.37652090191841125,
      "learning_rate": 2.6095833333333335e-05,
      "loss": 0.0021,
      "step": 57370
    },
    {
      "epoch": 3.8253333333333335,
      "grad_norm": 0.4045642614364624,
      "learning_rate": 2.609166666666667e-05,
      "loss": 0.0019,
      "step": 57380
    },
    {
      "epoch": 3.826,
      "grad_norm": 1.2634214162826538,
      "learning_rate": 2.6087500000000004e-05,
      "loss": 0.0029,
      "step": 57390
    },
    {
      "epoch": 3.8266666666666667,
      "grad_norm": 0.6635180711746216,
      "learning_rate": 2.608333333333333e-05,
      "loss": 0.003,
      "step": 57400
    },
    {
      "epoch": 3.8273333333333333,
      "grad_norm": 0.30025678873062134,
      "learning_rate": 2.6079166666666666e-05,
      "loss": 0.0022,
      "step": 57410
    },
    {
      "epoch": 3.828,
      "grad_norm": 0.08647231012582779,
      "learning_rate": 2.6075e-05,
      "loss": 0.0022,
      "step": 57420
    },
    {
      "epoch": 3.828666666666667,
      "grad_norm": 0.11062213033437729,
      "learning_rate": 2.6070833333333335e-05,
      "loss": 0.0022,
      "step": 57430
    },
    {
      "epoch": 3.8293333333333335,
      "grad_norm": 0.4068644046783447,
      "learning_rate": 2.6066666666666666e-05,
      "loss": 0.0019,
      "step": 57440
    },
    {
      "epoch": 3.83,
      "grad_norm": 0.21193920075893402,
      "learning_rate": 2.60625e-05,
      "loss": 0.0019,
      "step": 57450
    },
    {
      "epoch": 3.8306666666666667,
      "grad_norm": 0.4131668508052826,
      "learning_rate": 2.6058333333333334e-05,
      "loss": 0.0019,
      "step": 57460
    },
    {
      "epoch": 3.8313333333333333,
      "grad_norm": 0.6080142855644226,
      "learning_rate": 2.605416666666667e-05,
      "loss": 0.0015,
      "step": 57470
    },
    {
      "epoch": 3.832,
      "grad_norm": 0.03879908099770546,
      "learning_rate": 2.6050000000000003e-05,
      "loss": 0.0031,
      "step": 57480
    },
    {
      "epoch": 3.8326666666666664,
      "grad_norm": 0.5552667379379272,
      "learning_rate": 2.6045833333333337e-05,
      "loss": 0.0023,
      "step": 57490
    },
    {
      "epoch": 3.8333333333333335,
      "grad_norm": 0.14605757594108582,
      "learning_rate": 2.604166666666667e-05,
      "loss": 0.0028,
      "step": 57500
    },
    {
      "epoch": 3.834,
      "grad_norm": 0.42892763018608093,
      "learning_rate": 2.6037500000000003e-05,
      "loss": 0.0025,
      "step": 57510
    },
    {
      "epoch": 3.8346666666666667,
      "grad_norm": 0.6490034461021423,
      "learning_rate": 2.6033333333333337e-05,
      "loss": 0.004,
      "step": 57520
    },
    {
      "epoch": 3.8353333333333333,
      "grad_norm": 0.3260628879070282,
      "learning_rate": 2.6029166666666665e-05,
      "loss": 0.0021,
      "step": 57530
    },
    {
      "epoch": 3.836,
      "grad_norm": 0.4851210415363312,
      "learning_rate": 2.6025e-05,
      "loss": 0.0016,
      "step": 57540
    },
    {
      "epoch": 3.836666666666667,
      "grad_norm": 0.47716742753982544,
      "learning_rate": 2.6020833333333333e-05,
      "loss": 0.0021,
      "step": 57550
    },
    {
      "epoch": 3.8373333333333335,
      "grad_norm": 1.1472302675247192,
      "learning_rate": 2.6016666666666668e-05,
      "loss": 0.0033,
      "step": 57560
    },
    {
      "epoch": 3.838,
      "grad_norm": 0.6543793082237244,
      "learning_rate": 2.6012500000000002e-05,
      "loss": 0.0025,
      "step": 57570
    },
    {
      "epoch": 3.8386666666666667,
      "grad_norm": 0.6886125206947327,
      "learning_rate": 2.6008333333333333e-05,
      "loss": 0.0016,
      "step": 57580
    },
    {
      "epoch": 3.8393333333333333,
      "grad_norm": 0.5084711313247681,
      "learning_rate": 2.6004166666666667e-05,
      "loss": 0.0026,
      "step": 57590
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.11471264809370041,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.003,
      "step": 57600
    },
    {
      "epoch": 3.8406666666666665,
      "grad_norm": 0.037198763340711594,
      "learning_rate": 2.5995833333333336e-05,
      "loss": 0.0025,
      "step": 57610
    },
    {
      "epoch": 3.8413333333333335,
      "grad_norm": 0.047482121735811234,
      "learning_rate": 2.599166666666667e-05,
      "loss": 0.003,
      "step": 57620
    },
    {
      "epoch": 3.842,
      "grad_norm": 0.5721690058708191,
      "learning_rate": 2.5987500000000005e-05,
      "loss": 0.0023,
      "step": 57630
    },
    {
      "epoch": 3.8426666666666667,
      "grad_norm": 0.37541455030441284,
      "learning_rate": 2.5983333333333336e-05,
      "loss": 0.002,
      "step": 57640
    },
    {
      "epoch": 3.8433333333333333,
      "grad_norm": 0.22678424417972565,
      "learning_rate": 2.5979166666666667e-05,
      "loss": 0.0018,
      "step": 57650
    },
    {
      "epoch": 3.844,
      "grad_norm": 0.29424726963043213,
      "learning_rate": 2.5974999999999998e-05,
      "loss": 0.0019,
      "step": 57660
    },
    {
      "epoch": 3.844666666666667,
      "grad_norm": 0.33351513743400574,
      "learning_rate": 2.5970833333333332e-05,
      "loss": 0.0026,
      "step": 57670
    },
    {
      "epoch": 3.8453333333333335,
      "grad_norm": 0.32389140129089355,
      "learning_rate": 2.5966666666666667e-05,
      "loss": 0.002,
      "step": 57680
    },
    {
      "epoch": 3.846,
      "grad_norm": 0.17937734723091125,
      "learning_rate": 2.59625e-05,
      "loss": 0.0017,
      "step": 57690
    },
    {
      "epoch": 3.8466666666666667,
      "grad_norm": 0.16043604910373688,
      "learning_rate": 2.5958333333333335e-05,
      "loss": 0.0021,
      "step": 57700
    },
    {
      "epoch": 3.8473333333333333,
      "grad_norm": 0.02864071913063526,
      "learning_rate": 2.595416666666667e-05,
      "loss": 0.0014,
      "step": 57710
    },
    {
      "epoch": 3.848,
      "grad_norm": 0.10436186194419861,
      "learning_rate": 2.595e-05,
      "loss": 0.0028,
      "step": 57720
    },
    {
      "epoch": 3.8486666666666665,
      "grad_norm": 0.11137863993644714,
      "learning_rate": 2.5945833333333335e-05,
      "loss": 0.0019,
      "step": 57730
    },
    {
      "epoch": 3.8493333333333335,
      "grad_norm": 0.49977537989616394,
      "learning_rate": 2.594166666666667e-05,
      "loss": 0.0017,
      "step": 57740
    },
    {
      "epoch": 3.85,
      "grad_norm": 0.15507234632968903,
      "learning_rate": 2.5937500000000004e-05,
      "loss": 0.003,
      "step": 57750
    },
    {
      "epoch": 3.8506666666666667,
      "grad_norm": 0.7688624262809753,
      "learning_rate": 2.5933333333333338e-05,
      "loss": 0.0034,
      "step": 57760
    },
    {
      "epoch": 3.8513333333333333,
      "grad_norm": 0.25047290325164795,
      "learning_rate": 2.5929166666666666e-05,
      "loss": 0.002,
      "step": 57770
    },
    {
      "epoch": 3.852,
      "grad_norm": 0.12726324796676636,
      "learning_rate": 2.5925e-05,
      "loss": 0.0021,
      "step": 57780
    },
    {
      "epoch": 3.852666666666667,
      "grad_norm": 0.14799106121063232,
      "learning_rate": 2.5920833333333334e-05,
      "loss": 0.0015,
      "step": 57790
    },
    {
      "epoch": 3.8533333333333335,
      "grad_norm": 0.5409387946128845,
      "learning_rate": 2.5916666666666665e-05,
      "loss": 0.0019,
      "step": 57800
    },
    {
      "epoch": 3.854,
      "grad_norm": 0.2149849683046341,
      "learning_rate": 2.59125e-05,
      "loss": 0.0017,
      "step": 57810
    },
    {
      "epoch": 3.8546666666666667,
      "grad_norm": 0.38777366280555725,
      "learning_rate": 2.5908333333333334e-05,
      "loss": 0.0018,
      "step": 57820
    },
    {
      "epoch": 3.8553333333333333,
      "grad_norm": 0.4720149636268616,
      "learning_rate": 2.590416666666667e-05,
      "loss": 0.0025,
      "step": 57830
    },
    {
      "epoch": 3.856,
      "grad_norm": 0.2561964690685272,
      "learning_rate": 2.5900000000000003e-05,
      "loss": 0.002,
      "step": 57840
    },
    {
      "epoch": 3.8566666666666665,
      "grad_norm": 0.2953074276447296,
      "learning_rate": 2.5895833333333337e-05,
      "loss": 0.0023,
      "step": 57850
    },
    {
      "epoch": 3.857333333333333,
      "grad_norm": 0.048520706593990326,
      "learning_rate": 2.5891666666666668e-05,
      "loss": 0.0019,
      "step": 57860
    },
    {
      "epoch": 3.858,
      "grad_norm": 0.6412261724472046,
      "learning_rate": 2.5887500000000002e-05,
      "loss": 0.0021,
      "step": 57870
    },
    {
      "epoch": 3.8586666666666667,
      "grad_norm": 0.18878160417079926,
      "learning_rate": 2.5883333333333337e-05,
      "loss": 0.002,
      "step": 57880
    },
    {
      "epoch": 3.8593333333333333,
      "grad_norm": 0.07062338292598724,
      "learning_rate": 2.5879166666666664e-05,
      "loss": 0.0016,
      "step": 57890
    },
    {
      "epoch": 3.86,
      "grad_norm": 0.3146114945411682,
      "learning_rate": 2.5875e-05,
      "loss": 0.0014,
      "step": 57900
    },
    {
      "epoch": 3.860666666666667,
      "grad_norm": 0.3947204053401947,
      "learning_rate": 2.5870833333333333e-05,
      "loss": 0.0028,
      "step": 57910
    },
    {
      "epoch": 3.8613333333333335,
      "grad_norm": 0.2735767960548401,
      "learning_rate": 2.5866666666666667e-05,
      "loss": 0.0027,
      "step": 57920
    },
    {
      "epoch": 3.862,
      "grad_norm": 0.07736418396234512,
      "learning_rate": 2.5862500000000002e-05,
      "loss": 0.002,
      "step": 57930
    },
    {
      "epoch": 3.8626666666666667,
      "grad_norm": 0.5353299975395203,
      "learning_rate": 2.5858333333333333e-05,
      "loss": 0.0016,
      "step": 57940
    },
    {
      "epoch": 3.8633333333333333,
      "grad_norm": 0.05078743025660515,
      "learning_rate": 2.5854166666666667e-05,
      "loss": 0.0022,
      "step": 57950
    },
    {
      "epoch": 3.864,
      "grad_norm": 0.5644355416297913,
      "learning_rate": 2.585e-05,
      "loss": 0.0024,
      "step": 57960
    },
    {
      "epoch": 3.8646666666666665,
      "grad_norm": 0.3220710754394531,
      "learning_rate": 2.5845833333333336e-05,
      "loss": 0.0017,
      "step": 57970
    },
    {
      "epoch": 3.865333333333333,
      "grad_norm": 0.2537682354450226,
      "learning_rate": 2.584166666666667e-05,
      "loss": 0.0022,
      "step": 57980
    },
    {
      "epoch": 3.866,
      "grad_norm": 0.40035513043403625,
      "learning_rate": 2.5837500000000005e-05,
      "loss": 0.002,
      "step": 57990
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 0.996111273765564,
      "learning_rate": 2.5833333333333336e-05,
      "loss": 0.0024,
      "step": 58000
    },
    {
      "epoch": 3.8673333333333333,
      "grad_norm": 0.04936189204454422,
      "learning_rate": 2.5829166666666667e-05,
      "loss": 0.0022,
      "step": 58010
    },
    {
      "epoch": 3.868,
      "grad_norm": 0.2182464748620987,
      "learning_rate": 2.5824999999999998e-05,
      "loss": 0.0025,
      "step": 58020
    },
    {
      "epoch": 3.868666666666667,
      "grad_norm": 0.5305998921394348,
      "learning_rate": 2.5820833333333332e-05,
      "loss": 0.0023,
      "step": 58030
    },
    {
      "epoch": 3.8693333333333335,
      "grad_norm": 0.4594101309776306,
      "learning_rate": 2.5816666666666666e-05,
      "loss": 0.0017,
      "step": 58040
    },
    {
      "epoch": 3.87,
      "grad_norm": 0.5682608485221863,
      "learning_rate": 2.58125e-05,
      "loss": 0.0017,
      "step": 58050
    },
    {
      "epoch": 3.8706666666666667,
      "grad_norm": 0.243200421333313,
      "learning_rate": 2.5808333333333335e-05,
      "loss": 0.0021,
      "step": 58060
    },
    {
      "epoch": 3.8713333333333333,
      "grad_norm": 0.31937775015830994,
      "learning_rate": 2.580416666666667e-05,
      "loss": 0.0016,
      "step": 58070
    },
    {
      "epoch": 3.872,
      "grad_norm": 0.6881903409957886,
      "learning_rate": 2.58e-05,
      "loss": 0.0018,
      "step": 58080
    },
    {
      "epoch": 3.8726666666666665,
      "grad_norm": 0.038752757012844086,
      "learning_rate": 2.5795833333333335e-05,
      "loss": 0.0016,
      "step": 58090
    },
    {
      "epoch": 3.873333333333333,
      "grad_norm": 0.3206616938114166,
      "learning_rate": 2.579166666666667e-05,
      "loss": 0.0022,
      "step": 58100
    },
    {
      "epoch": 3.874,
      "grad_norm": 0.3929958939552307,
      "learning_rate": 2.5787500000000003e-05,
      "loss": 0.0016,
      "step": 58110
    },
    {
      "epoch": 3.8746666666666667,
      "grad_norm": 0.5299562811851501,
      "learning_rate": 2.5783333333333338e-05,
      "loss": 0.0021,
      "step": 58120
    },
    {
      "epoch": 3.8753333333333333,
      "grad_norm": 0.44861680269241333,
      "learning_rate": 2.5779166666666665e-05,
      "loss": 0.0015,
      "step": 58130
    },
    {
      "epoch": 3.876,
      "grad_norm": 0.8172319531440735,
      "learning_rate": 2.5775e-05,
      "loss": 0.0019,
      "step": 58140
    },
    {
      "epoch": 3.876666666666667,
      "grad_norm": 0.502769947052002,
      "learning_rate": 2.5770833333333334e-05,
      "loss": 0.002,
      "step": 58150
    },
    {
      "epoch": 3.8773333333333335,
      "grad_norm": 0.18098120391368866,
      "learning_rate": 2.5766666666666665e-05,
      "loss": 0.003,
      "step": 58160
    },
    {
      "epoch": 3.878,
      "grad_norm": 0.14166145026683807,
      "learning_rate": 2.57625e-05,
      "loss": 0.0018,
      "step": 58170
    },
    {
      "epoch": 3.8786666666666667,
      "grad_norm": 0.07831886410713196,
      "learning_rate": 2.5758333333333334e-05,
      "loss": 0.0015,
      "step": 58180
    },
    {
      "epoch": 3.8793333333333333,
      "grad_norm": 0.7504481077194214,
      "learning_rate": 2.5754166666666668e-05,
      "loss": 0.0029,
      "step": 58190
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.8285647034645081,
      "learning_rate": 2.5750000000000002e-05,
      "loss": 0.0037,
      "step": 58200
    },
    {
      "epoch": 3.8806666666666665,
      "grad_norm": 0.21499556303024292,
      "learning_rate": 2.5745833333333337e-05,
      "loss": 0.0014,
      "step": 58210
    },
    {
      "epoch": 3.881333333333333,
      "grad_norm": 0.15458253026008606,
      "learning_rate": 2.5741666666666668e-05,
      "loss": 0.0017,
      "step": 58220
    },
    {
      "epoch": 3.882,
      "grad_norm": 0.1477355808019638,
      "learning_rate": 2.5737500000000002e-05,
      "loss": 0.0025,
      "step": 58230
    },
    {
      "epoch": 3.8826666666666667,
      "grad_norm": 0.7247419357299805,
      "learning_rate": 2.5733333333333337e-05,
      "loss": 0.002,
      "step": 58240
    },
    {
      "epoch": 3.8833333333333333,
      "grad_norm": 0.4254912734031677,
      "learning_rate": 2.5729166666666664e-05,
      "loss": 0.002,
      "step": 58250
    },
    {
      "epoch": 3.884,
      "grad_norm": 0.6015534400939941,
      "learning_rate": 2.5725e-05,
      "loss": 0.0019,
      "step": 58260
    },
    {
      "epoch": 3.884666666666667,
      "grad_norm": 0.6386032104492188,
      "learning_rate": 2.5720833333333333e-05,
      "loss": 0.0017,
      "step": 58270
    },
    {
      "epoch": 3.8853333333333335,
      "grad_norm": 0.5670913457870483,
      "learning_rate": 2.5716666666666667e-05,
      "loss": 0.0018,
      "step": 58280
    },
    {
      "epoch": 3.886,
      "grad_norm": 0.17952057719230652,
      "learning_rate": 2.57125e-05,
      "loss": 0.0016,
      "step": 58290
    },
    {
      "epoch": 3.8866666666666667,
      "grad_norm": 0.8378562331199646,
      "learning_rate": 2.5708333333333336e-05,
      "loss": 0.0023,
      "step": 58300
    },
    {
      "epoch": 3.8873333333333333,
      "grad_norm": 0.5037403106689453,
      "learning_rate": 2.5704166666666667e-05,
      "loss": 0.0019,
      "step": 58310
    },
    {
      "epoch": 3.888,
      "grad_norm": 0.08689198642969131,
      "learning_rate": 2.57e-05,
      "loss": 0.0025,
      "step": 58320
    },
    {
      "epoch": 3.8886666666666665,
      "grad_norm": 0.11220042407512665,
      "learning_rate": 2.5695833333333336e-05,
      "loss": 0.0018,
      "step": 58330
    },
    {
      "epoch": 3.889333333333333,
      "grad_norm": 0.049893688410520554,
      "learning_rate": 2.569166666666667e-05,
      "loss": 0.0024,
      "step": 58340
    },
    {
      "epoch": 3.89,
      "grad_norm": 0.28837379813194275,
      "learning_rate": 2.5687500000000004e-05,
      "loss": 0.0018,
      "step": 58350
    },
    {
      "epoch": 3.8906666666666667,
      "grad_norm": 0.17828905582427979,
      "learning_rate": 2.5683333333333335e-05,
      "loss": 0.0021,
      "step": 58360
    },
    {
      "epoch": 3.8913333333333333,
      "grad_norm": 0.18146416544914246,
      "learning_rate": 2.5679166666666666e-05,
      "loss": 0.0017,
      "step": 58370
    },
    {
      "epoch": 3.892,
      "grad_norm": 0.46873387694358826,
      "learning_rate": 2.5675e-05,
      "loss": 0.0027,
      "step": 58380
    },
    {
      "epoch": 3.892666666666667,
      "grad_norm": 0.49648043513298035,
      "learning_rate": 2.567083333333333e-05,
      "loss": 0.0017,
      "step": 58390
    },
    {
      "epoch": 3.8933333333333335,
      "grad_norm": 0.39401936531066895,
      "learning_rate": 2.5666666666666666e-05,
      "loss": 0.0021,
      "step": 58400
    },
    {
      "epoch": 3.894,
      "grad_norm": 0.11229117214679718,
      "learning_rate": 2.56625e-05,
      "loss": 0.0019,
      "step": 58410
    },
    {
      "epoch": 3.8946666666666667,
      "grad_norm": 0.5886712670326233,
      "learning_rate": 2.5658333333333335e-05,
      "loss": 0.0021,
      "step": 58420
    },
    {
      "epoch": 3.8953333333333333,
      "grad_norm": 0.6088545322418213,
      "learning_rate": 2.565416666666667e-05,
      "loss": 0.0018,
      "step": 58430
    },
    {
      "epoch": 3.896,
      "grad_norm": 0.21570178866386414,
      "learning_rate": 2.5650000000000003e-05,
      "loss": 0.0023,
      "step": 58440
    },
    {
      "epoch": 3.8966666666666665,
      "grad_norm": 0.14713245630264282,
      "learning_rate": 2.5645833333333334e-05,
      "loss": 0.0018,
      "step": 58450
    },
    {
      "epoch": 3.897333333333333,
      "grad_norm": 0.08475731313228607,
      "learning_rate": 2.564166666666667e-05,
      "loss": 0.0016,
      "step": 58460
    },
    {
      "epoch": 3.898,
      "grad_norm": 1.0336183309555054,
      "learning_rate": 2.5637500000000003e-05,
      "loss": 0.0016,
      "step": 58470
    },
    {
      "epoch": 3.8986666666666667,
      "grad_norm": 0.5762143135070801,
      "learning_rate": 2.5633333333333338e-05,
      "loss": 0.0022,
      "step": 58480
    },
    {
      "epoch": 3.8993333333333333,
      "grad_norm": 0.5410377979278564,
      "learning_rate": 2.5629166666666665e-05,
      "loss": 0.0021,
      "step": 58490
    },
    {
      "epoch": 3.9,
      "grad_norm": 0.056672096252441406,
      "learning_rate": 2.5625e-05,
      "loss": 0.0017,
      "step": 58500
    },
    {
      "epoch": 3.9006666666666665,
      "grad_norm": 0.3525627851486206,
      "learning_rate": 2.5620833333333334e-05,
      "loss": 0.0016,
      "step": 58510
    },
    {
      "epoch": 3.9013333333333335,
      "grad_norm": 0.14331692457199097,
      "learning_rate": 2.5616666666666668e-05,
      "loss": 0.0019,
      "step": 58520
    },
    {
      "epoch": 3.902,
      "grad_norm": 0.6027198433876038,
      "learning_rate": 2.56125e-05,
      "loss": 0.0019,
      "step": 58530
    },
    {
      "epoch": 3.9026666666666667,
      "grad_norm": 0.3908570408821106,
      "learning_rate": 2.5608333333333334e-05,
      "loss": 0.0019,
      "step": 58540
    },
    {
      "epoch": 3.9033333333333333,
      "grad_norm": 0.17386460304260254,
      "learning_rate": 2.5604166666666668e-05,
      "loss": 0.0014,
      "step": 58550
    },
    {
      "epoch": 3.904,
      "grad_norm": 0.41971009969711304,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 0.0021,
      "step": 58560
    },
    {
      "epoch": 3.9046666666666665,
      "grad_norm": 0.5366080403327942,
      "learning_rate": 2.5595833333333337e-05,
      "loss": 0.002,
      "step": 58570
    },
    {
      "epoch": 3.905333333333333,
      "grad_norm": 0.41035813093185425,
      "learning_rate": 2.559166666666667e-05,
      "loss": 0.0017,
      "step": 58580
    },
    {
      "epoch": 3.906,
      "grad_norm": 0.4320554733276367,
      "learning_rate": 2.5587500000000002e-05,
      "loss": 0.0015,
      "step": 58590
    },
    {
      "epoch": 3.9066666666666667,
      "grad_norm": 0.40292829275131226,
      "learning_rate": 2.5583333333333336e-05,
      "loss": 0.0023,
      "step": 58600
    },
    {
      "epoch": 3.9073333333333333,
      "grad_norm": 0.1419958770275116,
      "learning_rate": 2.5579166666666664e-05,
      "loss": 0.002,
      "step": 58610
    },
    {
      "epoch": 3.908,
      "grad_norm": 0.31947556138038635,
      "learning_rate": 2.5574999999999998e-05,
      "loss": 0.0021,
      "step": 58620
    },
    {
      "epoch": 3.9086666666666665,
      "grad_norm": 0.7052695751190186,
      "learning_rate": 2.5570833333333333e-05,
      "loss": 0.0015,
      "step": 58630
    },
    {
      "epoch": 3.9093333333333335,
      "grad_norm": 0.7567199468612671,
      "learning_rate": 2.5566666666666667e-05,
      "loss": 0.0017,
      "step": 58640
    },
    {
      "epoch": 3.91,
      "grad_norm": 0.47931280732154846,
      "learning_rate": 2.55625e-05,
      "loss": 0.0024,
      "step": 58650
    },
    {
      "epoch": 3.9106666666666667,
      "grad_norm": 0.18237639963626862,
      "learning_rate": 2.5558333333333336e-05,
      "loss": 0.0016,
      "step": 58660
    },
    {
      "epoch": 3.9113333333333333,
      "grad_norm": 0.578208327293396,
      "learning_rate": 2.5554166666666667e-05,
      "loss": 0.0014,
      "step": 58670
    },
    {
      "epoch": 3.912,
      "grad_norm": 0.1430509239435196,
      "learning_rate": 2.555e-05,
      "loss": 0.0019,
      "step": 58680
    },
    {
      "epoch": 3.9126666666666665,
      "grad_norm": 0.502023458480835,
      "learning_rate": 2.5545833333333335e-05,
      "loss": 0.0017,
      "step": 58690
    },
    {
      "epoch": 3.913333333333333,
      "grad_norm": 0.282124787569046,
      "learning_rate": 2.554166666666667e-05,
      "loss": 0.0023,
      "step": 58700
    },
    {
      "epoch": 3.914,
      "grad_norm": 0.07846783846616745,
      "learning_rate": 2.5537500000000004e-05,
      "loss": 0.0022,
      "step": 58710
    },
    {
      "epoch": 3.9146666666666667,
      "grad_norm": 0.057476069778203964,
      "learning_rate": 2.553333333333334e-05,
      "loss": 0.0015,
      "step": 58720
    },
    {
      "epoch": 3.9153333333333333,
      "grad_norm": 0.4170983135700226,
      "learning_rate": 2.5529166666666666e-05,
      "loss": 0.0016,
      "step": 58730
    },
    {
      "epoch": 3.916,
      "grad_norm": 0.5673136711120605,
      "learning_rate": 2.5525e-05,
      "loss": 0.0019,
      "step": 58740
    },
    {
      "epoch": 3.9166666666666665,
      "grad_norm": 0.52425217628479,
      "learning_rate": 2.552083333333333e-05,
      "loss": 0.0019,
      "step": 58750
    },
    {
      "epoch": 3.9173333333333336,
      "grad_norm": 0.6438820362091064,
      "learning_rate": 2.5516666666666666e-05,
      "loss": 0.0031,
      "step": 58760
    },
    {
      "epoch": 3.918,
      "grad_norm": 0.7735350728034973,
      "learning_rate": 2.55125e-05,
      "loss": 0.0019,
      "step": 58770
    },
    {
      "epoch": 3.9186666666666667,
      "grad_norm": 0.08330460637807846,
      "learning_rate": 2.5508333333333334e-05,
      "loss": 0.0015,
      "step": 58780
    },
    {
      "epoch": 3.9193333333333333,
      "grad_norm": 0.14721038937568665,
      "learning_rate": 2.550416666666667e-05,
      "loss": 0.0012,
      "step": 58790
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.4093586206436157,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 0.0024,
      "step": 58800
    },
    {
      "epoch": 3.9206666666666665,
      "grad_norm": 0.05595316365361214,
      "learning_rate": 2.5495833333333334e-05,
      "loss": 0.0026,
      "step": 58810
    },
    {
      "epoch": 3.921333333333333,
      "grad_norm": 0.3567923903465271,
      "learning_rate": 2.549166666666667e-05,
      "loss": 0.0023,
      "step": 58820
    },
    {
      "epoch": 3.922,
      "grad_norm": 0.15296857059001923,
      "learning_rate": 2.5487500000000003e-05,
      "loss": 0.0032,
      "step": 58830
    },
    {
      "epoch": 3.9226666666666667,
      "grad_norm": 0.07535811513662338,
      "learning_rate": 2.5483333333333337e-05,
      "loss": 0.002,
      "step": 58840
    },
    {
      "epoch": 3.9233333333333333,
      "grad_norm": 0.823785662651062,
      "learning_rate": 2.547916666666667e-05,
      "loss": 0.0018,
      "step": 58850
    },
    {
      "epoch": 3.924,
      "grad_norm": 0.29532352089881897,
      "learning_rate": 2.5475e-05,
      "loss": 0.0016,
      "step": 58860
    },
    {
      "epoch": 3.9246666666666665,
      "grad_norm": 0.4392107427120209,
      "learning_rate": 2.5470833333333334e-05,
      "loss": 0.0021,
      "step": 58870
    },
    {
      "epoch": 3.9253333333333336,
      "grad_norm": 0.609216570854187,
      "learning_rate": 2.5466666666666668e-05,
      "loss": 0.0024,
      "step": 58880
    },
    {
      "epoch": 3.926,
      "grad_norm": 0.2580333650112152,
      "learning_rate": 2.54625e-05,
      "loss": 0.0017,
      "step": 58890
    },
    {
      "epoch": 3.9266666666666667,
      "grad_norm": 0.15245118737220764,
      "learning_rate": 2.5458333333333333e-05,
      "loss": 0.002,
      "step": 58900
    },
    {
      "epoch": 3.9273333333333333,
      "grad_norm": 0.18094241619110107,
      "learning_rate": 2.5454166666666668e-05,
      "loss": 0.002,
      "step": 58910
    },
    {
      "epoch": 3.928,
      "grad_norm": 0.4337678551673889,
      "learning_rate": 2.5450000000000002e-05,
      "loss": 0.0032,
      "step": 58920
    },
    {
      "epoch": 3.9286666666666665,
      "grad_norm": 0.5372762680053711,
      "learning_rate": 2.5445833333333336e-05,
      "loss": 0.0022,
      "step": 58930
    },
    {
      "epoch": 3.929333333333333,
      "grad_norm": 0.2990664541721344,
      "learning_rate": 2.544166666666667e-05,
      "loss": 0.002,
      "step": 58940
    },
    {
      "epoch": 3.93,
      "grad_norm": 0.5428357124328613,
      "learning_rate": 2.54375e-05,
      "loss": 0.002,
      "step": 58950
    },
    {
      "epoch": 3.9306666666666668,
      "grad_norm": 0.8019895553588867,
      "learning_rate": 2.5433333333333336e-05,
      "loss": 0.0028,
      "step": 58960
    },
    {
      "epoch": 3.9313333333333333,
      "grad_norm": 0.46540001034736633,
      "learning_rate": 2.542916666666667e-05,
      "loss": 0.0022,
      "step": 58970
    },
    {
      "epoch": 3.932,
      "grad_norm": 0.15435154736042023,
      "learning_rate": 2.5424999999999998e-05,
      "loss": 0.0021,
      "step": 58980
    },
    {
      "epoch": 3.9326666666666665,
      "grad_norm": 0.11507494002580643,
      "learning_rate": 2.5420833333333332e-05,
      "loss": 0.0021,
      "step": 58990
    },
    {
      "epoch": 3.9333333333333336,
      "grad_norm": 0.49786311388015747,
      "learning_rate": 2.5416666666666667e-05,
      "loss": 0.0021,
      "step": 59000
    },
    {
      "epoch": 3.934,
      "grad_norm": 0.1866578608751297,
      "learning_rate": 2.54125e-05,
      "loss": 0.0014,
      "step": 59010
    },
    {
      "epoch": 3.9346666666666668,
      "grad_norm": 0.6707150936126709,
      "learning_rate": 2.5408333333333335e-05,
      "loss": 0.0018,
      "step": 59020
    },
    {
      "epoch": 3.9353333333333333,
      "grad_norm": 0.14779996871948242,
      "learning_rate": 2.5404166666666666e-05,
      "loss": 0.0023,
      "step": 59030
    },
    {
      "epoch": 3.936,
      "grad_norm": 0.46638256311416626,
      "learning_rate": 2.54e-05,
      "loss": 0.0031,
      "step": 59040
    },
    {
      "epoch": 3.9366666666666665,
      "grad_norm": 0.8970933556556702,
      "learning_rate": 2.5395833333333335e-05,
      "loss": 0.0022,
      "step": 59050
    },
    {
      "epoch": 3.937333333333333,
      "grad_norm": 0.4968704581260681,
      "learning_rate": 2.539166666666667e-05,
      "loss": 0.0017,
      "step": 59060
    },
    {
      "epoch": 3.9379999999999997,
      "grad_norm": 0.5144521594047546,
      "learning_rate": 2.5387500000000004e-05,
      "loss": 0.0023,
      "step": 59070
    },
    {
      "epoch": 3.9386666666666668,
      "grad_norm": 0.4287208318710327,
      "learning_rate": 2.5383333333333338e-05,
      "loss": 0.0024,
      "step": 59080
    },
    {
      "epoch": 3.9393333333333334,
      "grad_norm": 0.5265182852745056,
      "learning_rate": 2.537916666666667e-05,
      "loss": 0.0013,
      "step": 59090
    },
    {
      "epoch": 3.94,
      "grad_norm": 0.3500617444515228,
      "learning_rate": 2.5375e-05,
      "loss": 0.0017,
      "step": 59100
    },
    {
      "epoch": 3.9406666666666665,
      "grad_norm": 0.18326276540756226,
      "learning_rate": 2.537083333333333e-05,
      "loss": 0.0017,
      "step": 59110
    },
    {
      "epoch": 3.9413333333333336,
      "grad_norm": 0.5268543362617493,
      "learning_rate": 2.5366666666666665e-05,
      "loss": 0.0017,
      "step": 59120
    },
    {
      "epoch": 3.942,
      "grad_norm": 0.7259705662727356,
      "learning_rate": 2.53625e-05,
      "loss": 0.0023,
      "step": 59130
    },
    {
      "epoch": 3.9426666666666668,
      "grad_norm": 0.17331556975841522,
      "learning_rate": 2.5358333333333334e-05,
      "loss": 0.0018,
      "step": 59140
    },
    {
      "epoch": 3.9433333333333334,
      "grad_norm": 0.15201695263385773,
      "learning_rate": 2.535416666666667e-05,
      "loss": 0.0023,
      "step": 59150
    },
    {
      "epoch": 3.944,
      "grad_norm": 0.47514811158180237,
      "learning_rate": 2.5350000000000003e-05,
      "loss": 0.0029,
      "step": 59160
    },
    {
      "epoch": 3.9446666666666665,
      "grad_norm": 0.21389655768871307,
      "learning_rate": 2.5345833333333334e-05,
      "loss": 0.0018,
      "step": 59170
    },
    {
      "epoch": 3.945333333333333,
      "grad_norm": 0.20867851376533508,
      "learning_rate": 2.5341666666666668e-05,
      "loss": 0.0021,
      "step": 59180
    },
    {
      "epoch": 3.9459999999999997,
      "grad_norm": 0.08600826561450958,
      "learning_rate": 2.5337500000000003e-05,
      "loss": 0.004,
      "step": 59190
    },
    {
      "epoch": 3.9466666666666668,
      "grad_norm": 0.5035498142242432,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 0.0023,
      "step": 59200
    },
    {
      "epoch": 3.9473333333333334,
      "grad_norm": 0.17442023754119873,
      "learning_rate": 2.532916666666667e-05,
      "loss": 0.0028,
      "step": 59210
    },
    {
      "epoch": 3.948,
      "grad_norm": 0.18947206437587738,
      "learning_rate": 2.5325e-05,
      "loss": 0.0022,
      "step": 59220
    },
    {
      "epoch": 3.9486666666666665,
      "grad_norm": 0.11124405264854431,
      "learning_rate": 2.5320833333333333e-05,
      "loss": 0.0026,
      "step": 59230
    },
    {
      "epoch": 3.9493333333333336,
      "grad_norm": 0.5603575110435486,
      "learning_rate": 2.5316666666666668e-05,
      "loss": 0.002,
      "step": 59240
    },
    {
      "epoch": 3.95,
      "grad_norm": 0.4086820185184479,
      "learning_rate": 2.53125e-05,
      "loss": 0.0017,
      "step": 59250
    },
    {
      "epoch": 3.9506666666666668,
      "grad_norm": 0.3562762439250946,
      "learning_rate": 2.5308333333333333e-05,
      "loss": 0.0021,
      "step": 59260
    },
    {
      "epoch": 3.9513333333333334,
      "grad_norm": 0.21396900713443756,
      "learning_rate": 2.5304166666666667e-05,
      "loss": 0.0015,
      "step": 59270
    },
    {
      "epoch": 3.952,
      "grad_norm": 0.13776057958602905,
      "learning_rate": 2.5300000000000002e-05,
      "loss": 0.0017,
      "step": 59280
    },
    {
      "epoch": 3.9526666666666666,
      "grad_norm": 0.23414140939712524,
      "learning_rate": 2.5295833333333336e-05,
      "loss": 0.0024,
      "step": 59290
    },
    {
      "epoch": 3.953333333333333,
      "grad_norm": 0.21598844230175018,
      "learning_rate": 2.529166666666667e-05,
      "loss": 0.0028,
      "step": 59300
    },
    {
      "epoch": 3.9539999999999997,
      "grad_norm": 0.2831931710243225,
      "learning_rate": 2.52875e-05,
      "loss": 0.002,
      "step": 59310
    },
    {
      "epoch": 3.9546666666666668,
      "grad_norm": 0.3614494204521179,
      "learning_rate": 2.5283333333333336e-05,
      "loss": 0.0015,
      "step": 59320
    },
    {
      "epoch": 3.9553333333333334,
      "grad_norm": 0.35121583938598633,
      "learning_rate": 2.527916666666667e-05,
      "loss": 0.0013,
      "step": 59330
    },
    {
      "epoch": 3.956,
      "grad_norm": 0.5603974461555481,
      "learning_rate": 2.5274999999999998e-05,
      "loss": 0.0023,
      "step": 59340
    },
    {
      "epoch": 3.9566666666666666,
      "grad_norm": 0.18281695246696472,
      "learning_rate": 2.5270833333333332e-05,
      "loss": 0.0015,
      "step": 59350
    },
    {
      "epoch": 3.9573333333333336,
      "grad_norm": 0.06763160973787308,
      "learning_rate": 2.5266666666666666e-05,
      "loss": 0.0016,
      "step": 59360
    },
    {
      "epoch": 3.958,
      "grad_norm": 0.14211374521255493,
      "learning_rate": 2.52625e-05,
      "loss": 0.0021,
      "step": 59370
    },
    {
      "epoch": 3.958666666666667,
      "grad_norm": 0.11384088546037674,
      "learning_rate": 2.5258333333333335e-05,
      "loss": 0.0013,
      "step": 59380
    },
    {
      "epoch": 3.9593333333333334,
      "grad_norm": 0.11708369851112366,
      "learning_rate": 2.5254166666666666e-05,
      "loss": 0.0025,
      "step": 59390
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.46182048320770264,
      "learning_rate": 2.525e-05,
      "loss": 0.0016,
      "step": 59400
    },
    {
      "epoch": 3.9606666666666666,
      "grad_norm": 0.08918291330337524,
      "learning_rate": 2.5245833333333335e-05,
      "loss": 0.0016,
      "step": 59410
    },
    {
      "epoch": 3.961333333333333,
      "grad_norm": 0.42797526717185974,
      "learning_rate": 2.524166666666667e-05,
      "loss": 0.002,
      "step": 59420
    },
    {
      "epoch": 3.9619999999999997,
      "grad_norm": 0.6995404958724976,
      "learning_rate": 2.5237500000000004e-05,
      "loss": 0.0037,
      "step": 59430
    },
    {
      "epoch": 3.962666666666667,
      "grad_norm": 0.7755732536315918,
      "learning_rate": 2.5233333333333338e-05,
      "loss": 0.0016,
      "step": 59440
    },
    {
      "epoch": 3.9633333333333334,
      "grad_norm": 0.25612419843673706,
      "learning_rate": 2.522916666666667e-05,
      "loss": 0.0016,
      "step": 59450
    },
    {
      "epoch": 3.964,
      "grad_norm": 0.028293972834944725,
      "learning_rate": 2.5225e-05,
      "loss": 0.0019,
      "step": 59460
    },
    {
      "epoch": 3.9646666666666666,
      "grad_norm": 0.1159157007932663,
      "learning_rate": 2.522083333333333e-05,
      "loss": 0.0028,
      "step": 59470
    },
    {
      "epoch": 3.9653333333333336,
      "grad_norm": 0.02966582588851452,
      "learning_rate": 2.5216666666666665e-05,
      "loss": 0.0023,
      "step": 59480
    },
    {
      "epoch": 3.966,
      "grad_norm": 0.5158022046089172,
      "learning_rate": 2.52125e-05,
      "loss": 0.002,
      "step": 59490
    },
    {
      "epoch": 3.966666666666667,
      "grad_norm": 0.663463294506073,
      "learning_rate": 2.5208333333333334e-05,
      "loss": 0.0032,
      "step": 59500
    },
    {
      "epoch": 3.9673333333333334,
      "grad_norm": 0.3855116665363312,
      "learning_rate": 2.520416666666667e-05,
      "loss": 0.0021,
      "step": 59510
    },
    {
      "epoch": 3.968,
      "grad_norm": 0.4610014855861664,
      "learning_rate": 2.5200000000000003e-05,
      "loss": 0.0027,
      "step": 59520
    },
    {
      "epoch": 3.9686666666666666,
      "grad_norm": 0.1823648363351822,
      "learning_rate": 2.5195833333333334e-05,
      "loss": 0.0022,
      "step": 59530
    },
    {
      "epoch": 3.969333333333333,
      "grad_norm": 0.21472786366939545,
      "learning_rate": 2.5191666666666668e-05,
      "loss": 0.0019,
      "step": 59540
    },
    {
      "epoch": 3.9699999999999998,
      "grad_norm": 0.18366511166095734,
      "learning_rate": 2.5187500000000002e-05,
      "loss": 0.0018,
      "step": 59550
    },
    {
      "epoch": 3.970666666666667,
      "grad_norm": 0.04696749150753021,
      "learning_rate": 2.5183333333333337e-05,
      "loss": 0.0028,
      "step": 59560
    },
    {
      "epoch": 3.9713333333333334,
      "grad_norm": 0.17570336163043976,
      "learning_rate": 2.517916666666667e-05,
      "loss": 0.002,
      "step": 59570
    },
    {
      "epoch": 3.972,
      "grad_norm": 0.7117776870727539,
      "learning_rate": 2.5175e-05,
      "loss": 0.0017,
      "step": 59580
    },
    {
      "epoch": 3.9726666666666666,
      "grad_norm": 0.247534841299057,
      "learning_rate": 2.5170833333333333e-05,
      "loss": 0.002,
      "step": 59590
    },
    {
      "epoch": 3.9733333333333336,
      "grad_norm": 0.17821647226810455,
      "learning_rate": 2.5166666666666667e-05,
      "loss": 0.0021,
      "step": 59600
    },
    {
      "epoch": 3.974,
      "grad_norm": 0.08803019672632217,
      "learning_rate": 2.51625e-05,
      "loss": 0.0029,
      "step": 59610
    },
    {
      "epoch": 3.974666666666667,
      "grad_norm": 0.5045189261436462,
      "learning_rate": 2.5158333333333333e-05,
      "loss": 0.0017,
      "step": 59620
    },
    {
      "epoch": 3.9753333333333334,
      "grad_norm": 0.7810415625572205,
      "learning_rate": 2.5154166666666667e-05,
      "loss": 0.0021,
      "step": 59630
    },
    {
      "epoch": 3.976,
      "grad_norm": 0.11229778826236725,
      "learning_rate": 2.515e-05,
      "loss": 0.0014,
      "step": 59640
    },
    {
      "epoch": 3.9766666666666666,
      "grad_norm": 0.3620864748954773,
      "learning_rate": 2.5145833333333336e-05,
      "loss": 0.0021,
      "step": 59650
    },
    {
      "epoch": 3.977333333333333,
      "grad_norm": 0.11821414530277252,
      "learning_rate": 2.514166666666667e-05,
      "loss": 0.003,
      "step": 59660
    },
    {
      "epoch": 3.9779999999999998,
      "grad_norm": 0.29170188307762146,
      "learning_rate": 2.51375e-05,
      "loss": 0.0019,
      "step": 59670
    },
    {
      "epoch": 3.978666666666667,
      "grad_norm": 0.08277470618486404,
      "learning_rate": 2.5133333333333336e-05,
      "loss": 0.003,
      "step": 59680
    },
    {
      "epoch": 3.9793333333333334,
      "grad_norm": 0.12636461853981018,
      "learning_rate": 2.512916666666667e-05,
      "loss": 0.0015,
      "step": 59690
    },
    {
      "epoch": 3.98,
      "grad_norm": 0.1903209090232849,
      "learning_rate": 2.5124999999999997e-05,
      "loss": 0.0021,
      "step": 59700
    },
    {
      "epoch": 3.9806666666666666,
      "grad_norm": 0.16780687868595123,
      "learning_rate": 2.5120833333333332e-05,
      "loss": 0.0019,
      "step": 59710
    },
    {
      "epoch": 3.981333333333333,
      "grad_norm": 0.22023998200893402,
      "learning_rate": 2.5116666666666666e-05,
      "loss": 0.0021,
      "step": 59720
    },
    {
      "epoch": 3.982,
      "grad_norm": 0.5815687775611877,
      "learning_rate": 2.51125e-05,
      "loss": 0.0025,
      "step": 59730
    },
    {
      "epoch": 3.982666666666667,
      "grad_norm": 0.3216039538383484,
      "learning_rate": 2.5108333333333335e-05,
      "loss": 0.0013,
      "step": 59740
    },
    {
      "epoch": 3.9833333333333334,
      "grad_norm": 0.0979832336306572,
      "learning_rate": 2.5104166666666666e-05,
      "loss": 0.003,
      "step": 59750
    },
    {
      "epoch": 3.984,
      "grad_norm": 0.26488855481147766,
      "learning_rate": 2.51e-05,
      "loss": 0.0022,
      "step": 59760
    },
    {
      "epoch": 3.9846666666666666,
      "grad_norm": 0.04974783584475517,
      "learning_rate": 2.5095833333333335e-05,
      "loss": 0.0017,
      "step": 59770
    },
    {
      "epoch": 3.985333333333333,
      "grad_norm": 0.49440109729766846,
      "learning_rate": 2.509166666666667e-05,
      "loss": 0.0021,
      "step": 59780
    },
    {
      "epoch": 3.9859999999999998,
      "grad_norm": 0.0867559164762497,
      "learning_rate": 2.5087500000000003e-05,
      "loss": 0.002,
      "step": 59790
    },
    {
      "epoch": 3.986666666666667,
      "grad_norm": 0.474530965089798,
      "learning_rate": 2.5083333333333338e-05,
      "loss": 0.0024,
      "step": 59800
    },
    {
      "epoch": 3.9873333333333334,
      "grad_norm": 0.43683603405952454,
      "learning_rate": 2.507916666666667e-05,
      "loss": 0.0021,
      "step": 59810
    },
    {
      "epoch": 3.988,
      "grad_norm": 0.7990239262580872,
      "learning_rate": 2.5075e-05,
      "loss": 0.0021,
      "step": 59820
    },
    {
      "epoch": 3.9886666666666666,
      "grad_norm": 0.9031740427017212,
      "learning_rate": 2.507083333333333e-05,
      "loss": 0.0021,
      "step": 59830
    },
    {
      "epoch": 3.989333333333333,
      "grad_norm": 0.3927781581878662,
      "learning_rate": 2.5066666666666665e-05,
      "loss": 0.0017,
      "step": 59840
    },
    {
      "epoch": 3.99,
      "grad_norm": 0.7664427161216736,
      "learning_rate": 2.50625e-05,
      "loss": 0.0018,
      "step": 59850
    },
    {
      "epoch": 3.990666666666667,
      "grad_norm": 0.14350877702236176,
      "learning_rate": 2.5058333333333334e-05,
      "loss": 0.0018,
      "step": 59860
    },
    {
      "epoch": 3.9913333333333334,
      "grad_norm": 0.4328016936779022,
      "learning_rate": 2.5054166666666668e-05,
      "loss": 0.0018,
      "step": 59870
    },
    {
      "epoch": 3.992,
      "grad_norm": 0.11021793633699417,
      "learning_rate": 2.5050000000000002e-05,
      "loss": 0.002,
      "step": 59880
    },
    {
      "epoch": 3.9926666666666666,
      "grad_norm": 0.11950313299894333,
      "learning_rate": 2.5045833333333333e-05,
      "loss": 0.0021,
      "step": 59890
    },
    {
      "epoch": 3.993333333333333,
      "grad_norm": 0.2205202728509903,
      "learning_rate": 2.5041666666666668e-05,
      "loss": 0.0014,
      "step": 59900
    },
    {
      "epoch": 3.9939999999999998,
      "grad_norm": 0.9695719480514526,
      "learning_rate": 2.5037500000000002e-05,
      "loss": 0.0022,
      "step": 59910
    },
    {
      "epoch": 3.994666666666667,
      "grad_norm": 0.7852575778961182,
      "learning_rate": 2.5033333333333336e-05,
      "loss": 0.0015,
      "step": 59920
    },
    {
      "epoch": 3.9953333333333334,
      "grad_norm": 0.25428083539009094,
      "learning_rate": 2.502916666666667e-05,
      "loss": 0.0016,
      "step": 59930
    },
    {
      "epoch": 3.996,
      "grad_norm": 0.15094605088233948,
      "learning_rate": 2.5025e-05,
      "loss": 0.0022,
      "step": 59940
    },
    {
      "epoch": 3.9966666666666666,
      "grad_norm": 0.24425983428955078,
      "learning_rate": 2.5020833333333333e-05,
      "loss": 0.0015,
      "step": 59950
    },
    {
      "epoch": 3.997333333333333,
      "grad_norm": 0.07896028459072113,
      "learning_rate": 2.5016666666666667e-05,
      "loss": 0.0024,
      "step": 59960
    },
    {
      "epoch": 3.998,
      "grad_norm": 0.43304622173309326,
      "learning_rate": 2.50125e-05,
      "loss": 0.0022,
      "step": 59970
    },
    {
      "epoch": 3.998666666666667,
      "grad_norm": 0.11125128716230392,
      "learning_rate": 2.5008333333333332e-05,
      "loss": 0.0019,
      "step": 59980
    },
    {
      "epoch": 3.9993333333333334,
      "grad_norm": 0.12881849706172943,
      "learning_rate": 2.5004166666666667e-05,
      "loss": 0.0017,
      "step": 59990
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.5452695488929749,
      "learning_rate": 2.5e-05,
      "loss": 0.0016,
      "step": 60000
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.0020615675020962954,
      "eval_runtime": 160.4059,
      "eval_samples_per_second": 1246.837,
      "eval_steps_per_second": 31.171,
      "step": 60000
    },
    {
      "epoch": 4.000666666666667,
      "grad_norm": 0.2161651849746704,
      "learning_rate": 2.4995833333333336e-05,
      "loss": 0.0022,
      "step": 60010
    },
    {
      "epoch": 4.001333333333333,
      "grad_norm": 0.4643881618976593,
      "learning_rate": 2.499166666666667e-05,
      "loss": 0.0023,
      "step": 60020
    },
    {
      "epoch": 4.002,
      "grad_norm": 0.08657025545835495,
      "learning_rate": 2.49875e-05,
      "loss": 0.0023,
      "step": 60030
    },
    {
      "epoch": 4.002666666666666,
      "grad_norm": 0.1818944364786148,
      "learning_rate": 2.4983333333333335e-05,
      "loss": 0.0025,
      "step": 60040
    },
    {
      "epoch": 4.003333333333333,
      "grad_norm": 0.0373317189514637,
      "learning_rate": 2.4979166666666666e-05,
      "loss": 0.0015,
      "step": 60050
    },
    {
      "epoch": 4.004,
      "grad_norm": 0.09802822768688202,
      "learning_rate": 2.4975e-05,
      "loss": 0.0025,
      "step": 60060
    },
    {
      "epoch": 4.004666666666667,
      "grad_norm": 0.5246808528900146,
      "learning_rate": 2.4970833333333335e-05,
      "loss": 0.002,
      "step": 60070
    },
    {
      "epoch": 4.005333333333334,
      "grad_norm": 0.3636651337146759,
      "learning_rate": 2.496666666666667e-05,
      "loss": 0.0019,
      "step": 60080
    },
    {
      "epoch": 4.006,
      "grad_norm": 0.44314783811569214,
      "learning_rate": 2.4962500000000004e-05,
      "loss": 0.0027,
      "step": 60090
    },
    {
      "epoch": 4.006666666666667,
      "grad_norm": 0.359485000371933,
      "learning_rate": 2.4958333333333335e-05,
      "loss": 0.0018,
      "step": 60100
    },
    {
      "epoch": 4.007333333333333,
      "grad_norm": 0.7797538638114929,
      "learning_rate": 2.495416666666667e-05,
      "loss": 0.0021,
      "step": 60110
    },
    {
      "epoch": 4.008,
      "grad_norm": 0.5430665612220764,
      "learning_rate": 2.495e-05,
      "loss": 0.0034,
      "step": 60120
    },
    {
      "epoch": 4.008666666666667,
      "grad_norm": 0.6714335083961487,
      "learning_rate": 2.4945833333333334e-05,
      "loss": 0.0021,
      "step": 60130
    },
    {
      "epoch": 4.009333333333333,
      "grad_norm": 0.4501200020313263,
      "learning_rate": 2.494166666666667e-05,
      "loss": 0.0021,
      "step": 60140
    },
    {
      "epoch": 4.01,
      "grad_norm": 0.12065379321575165,
      "learning_rate": 2.4937500000000003e-05,
      "loss": 0.002,
      "step": 60150
    },
    {
      "epoch": 4.010666666666666,
      "grad_norm": 0.8273531794548035,
      "learning_rate": 2.4933333333333334e-05,
      "loss": 0.0015,
      "step": 60160
    },
    {
      "epoch": 4.011333333333333,
      "grad_norm": 0.4504268765449524,
      "learning_rate": 2.492916666666667e-05,
      "loss": 0.0017,
      "step": 60170
    },
    {
      "epoch": 4.012,
      "grad_norm": 0.5163247585296631,
      "learning_rate": 2.4925000000000003e-05,
      "loss": 0.0024,
      "step": 60180
    },
    {
      "epoch": 4.012666666666667,
      "grad_norm": 0.10942850261926651,
      "learning_rate": 2.4920833333333334e-05,
      "loss": 0.0015,
      "step": 60190
    },
    {
      "epoch": 4.013333333333334,
      "grad_norm": 0.5606813430786133,
      "learning_rate": 2.4916666666666668e-05,
      "loss": 0.0016,
      "step": 60200
    },
    {
      "epoch": 4.014,
      "grad_norm": 0.27563920617103577,
      "learning_rate": 2.4912500000000002e-05,
      "loss": 0.0015,
      "step": 60210
    },
    {
      "epoch": 4.014666666666667,
      "grad_norm": 0.21641746163368225,
      "learning_rate": 2.4908333333333333e-05,
      "loss": 0.0024,
      "step": 60220
    },
    {
      "epoch": 4.015333333333333,
      "grad_norm": 0.4751884639263153,
      "learning_rate": 2.4904166666666668e-05,
      "loss": 0.0018,
      "step": 60230
    },
    {
      "epoch": 4.016,
      "grad_norm": 0.18078307807445526,
      "learning_rate": 2.4900000000000002e-05,
      "loss": 0.0022,
      "step": 60240
    },
    {
      "epoch": 4.016666666666667,
      "grad_norm": 0.5340686440467834,
      "learning_rate": 2.4895833333333337e-05,
      "loss": 0.0025,
      "step": 60250
    },
    {
      "epoch": 4.017333333333333,
      "grad_norm": 0.052352532744407654,
      "learning_rate": 2.4891666666666667e-05,
      "loss": 0.0017,
      "step": 60260
    },
    {
      "epoch": 4.018,
      "grad_norm": 0.2876121401786804,
      "learning_rate": 2.4887500000000002e-05,
      "loss": 0.0019,
      "step": 60270
    },
    {
      "epoch": 4.018666666666666,
      "grad_norm": 0.5527178645133972,
      "learning_rate": 2.4883333333333333e-05,
      "loss": 0.0023,
      "step": 60280
    },
    {
      "epoch": 4.019333333333333,
      "grad_norm": 0.31099259853363037,
      "learning_rate": 2.4879166666666667e-05,
      "loss": 0.0014,
      "step": 60290
    },
    {
      "epoch": 4.02,
      "grad_norm": 0.10858891904354095,
      "learning_rate": 2.4875e-05,
      "loss": 0.0021,
      "step": 60300
    },
    {
      "epoch": 4.020666666666667,
      "grad_norm": 0.11680120974779129,
      "learning_rate": 2.4870833333333336e-05,
      "loss": 0.0016,
      "step": 60310
    },
    {
      "epoch": 4.021333333333334,
      "grad_norm": 0.11175679415464401,
      "learning_rate": 2.486666666666667e-05,
      "loss": 0.0017,
      "step": 60320
    },
    {
      "epoch": 4.022,
      "grad_norm": 0.6430480480194092,
      "learning_rate": 2.48625e-05,
      "loss": 0.0034,
      "step": 60330
    },
    {
      "epoch": 4.022666666666667,
      "grad_norm": 0.1082543358206749,
      "learning_rate": 2.4858333333333332e-05,
      "loss": 0.0018,
      "step": 60340
    },
    {
      "epoch": 4.023333333333333,
      "grad_norm": 0.1387888491153717,
      "learning_rate": 2.4854166666666667e-05,
      "loss": 0.0013,
      "step": 60350
    },
    {
      "epoch": 4.024,
      "grad_norm": 0.1805216670036316,
      "learning_rate": 2.485e-05,
      "loss": 0.0021,
      "step": 60360
    },
    {
      "epoch": 4.024666666666667,
      "grad_norm": 0.1480078250169754,
      "learning_rate": 2.4845833333333335e-05,
      "loss": 0.0027,
      "step": 60370
    },
    {
      "epoch": 4.025333333333333,
      "grad_norm": 0.6466715931892395,
      "learning_rate": 2.484166666666667e-05,
      "loss": 0.0028,
      "step": 60380
    },
    {
      "epoch": 4.026,
      "grad_norm": 0.06025409698486328,
      "learning_rate": 2.4837500000000004e-05,
      "loss": 0.0032,
      "step": 60390
    },
    {
      "epoch": 4.026666666666666,
      "grad_norm": 0.11358822137117386,
      "learning_rate": 2.4833333333333335e-05,
      "loss": 0.0021,
      "step": 60400
    },
    {
      "epoch": 4.027333333333333,
      "grad_norm": 0.05147581920027733,
      "learning_rate": 2.4829166666666666e-05,
      "loss": 0.0018,
      "step": 60410
    },
    {
      "epoch": 4.028,
      "grad_norm": 0.3244965672492981,
      "learning_rate": 2.4825e-05,
      "loss": 0.0016,
      "step": 60420
    },
    {
      "epoch": 4.028666666666667,
      "grad_norm": 0.5631319880485535,
      "learning_rate": 2.4820833333333335e-05,
      "loss": 0.0014,
      "step": 60430
    },
    {
      "epoch": 4.029333333333334,
      "grad_norm": 0.27358201146125793,
      "learning_rate": 2.481666666666667e-05,
      "loss": 0.0019,
      "step": 60440
    },
    {
      "epoch": 4.03,
      "grad_norm": 0.4746495485305786,
      "learning_rate": 2.4812500000000003e-05,
      "loss": 0.0032,
      "step": 60450
    },
    {
      "epoch": 4.030666666666667,
      "grad_norm": 0.687380850315094,
      "learning_rate": 2.4808333333333334e-05,
      "loss": 0.0019,
      "step": 60460
    },
    {
      "epoch": 4.031333333333333,
      "grad_norm": 0.32069891691207886,
      "learning_rate": 2.480416666666667e-05,
      "loss": 0.0028,
      "step": 60470
    },
    {
      "epoch": 4.032,
      "grad_norm": 0.29868677258491516,
      "learning_rate": 2.48e-05,
      "loss": 0.0022,
      "step": 60480
    },
    {
      "epoch": 4.032666666666667,
      "grad_norm": 0.2902330458164215,
      "learning_rate": 2.4795833333333334e-05,
      "loss": 0.003,
      "step": 60490
    },
    {
      "epoch": 4.033333333333333,
      "grad_norm": 0.07948659360408783,
      "learning_rate": 2.479166666666667e-05,
      "loss": 0.0021,
      "step": 60500
    },
    {
      "epoch": 4.034,
      "grad_norm": 0.11015264689922333,
      "learning_rate": 2.4787500000000003e-05,
      "loss": 0.0022,
      "step": 60510
    },
    {
      "epoch": 4.034666666666666,
      "grad_norm": 0.052104827016592026,
      "learning_rate": 2.4783333333333334e-05,
      "loss": 0.0034,
      "step": 60520
    },
    {
      "epoch": 4.035333333333333,
      "grad_norm": 0.44108155369758606,
      "learning_rate": 2.4779166666666668e-05,
      "loss": 0.002,
      "step": 60530
    },
    {
      "epoch": 4.036,
      "grad_norm": 0.14741158485412598,
      "learning_rate": 2.4775000000000003e-05,
      "loss": 0.002,
      "step": 60540
    },
    {
      "epoch": 4.036666666666667,
      "grad_norm": 0.3291729688644409,
      "learning_rate": 2.4770833333333333e-05,
      "loss": 0.0027,
      "step": 60550
    },
    {
      "epoch": 4.037333333333334,
      "grad_norm": 0.33297351002693176,
      "learning_rate": 2.4766666666666668e-05,
      "loss": 0.0021,
      "step": 60560
    },
    {
      "epoch": 4.038,
      "grad_norm": 0.2625699043273926,
      "learning_rate": 2.4762500000000002e-05,
      "loss": 0.0023,
      "step": 60570
    },
    {
      "epoch": 4.038666666666667,
      "grad_norm": 0.10676594078540802,
      "learning_rate": 2.4758333333333333e-05,
      "loss": 0.0023,
      "step": 60580
    },
    {
      "epoch": 4.039333333333333,
      "grad_norm": 0.4307158589363098,
      "learning_rate": 2.4754166666666668e-05,
      "loss": 0.0022,
      "step": 60590
    },
    {
      "epoch": 4.04,
      "grad_norm": 0.32108837366104126,
      "learning_rate": 2.4750000000000002e-05,
      "loss": 0.0023,
      "step": 60600
    },
    {
      "epoch": 4.040666666666667,
      "grad_norm": 0.05031125620007515,
      "learning_rate": 2.4745833333333336e-05,
      "loss": 0.0021,
      "step": 60610
    },
    {
      "epoch": 4.041333333333333,
      "grad_norm": 0.8251221776008606,
      "learning_rate": 2.4741666666666667e-05,
      "loss": 0.0018,
      "step": 60620
    },
    {
      "epoch": 4.042,
      "grad_norm": 0.044675517827272415,
      "learning_rate": 2.47375e-05,
      "loss": 0.0015,
      "step": 60630
    },
    {
      "epoch": 4.042666666666666,
      "grad_norm": 0.14875270426273346,
      "learning_rate": 2.4733333333333333e-05,
      "loss": 0.0024,
      "step": 60640
    },
    {
      "epoch": 4.043333333333333,
      "grad_norm": 0.5742875337600708,
      "learning_rate": 2.4729166666666667e-05,
      "loss": 0.0025,
      "step": 60650
    },
    {
      "epoch": 4.044,
      "grad_norm": 0.14510345458984375,
      "learning_rate": 2.4725e-05,
      "loss": 0.0033,
      "step": 60660
    },
    {
      "epoch": 4.044666666666667,
      "grad_norm": 0.12246755510568619,
      "learning_rate": 2.4720833333333336e-05,
      "loss": 0.0024,
      "step": 60670
    },
    {
      "epoch": 4.045333333333334,
      "grad_norm": 0.11041340231895447,
      "learning_rate": 2.471666666666667e-05,
      "loss": 0.0018,
      "step": 60680
    },
    {
      "epoch": 4.046,
      "grad_norm": 0.6779446005821228,
      "learning_rate": 2.47125e-05,
      "loss": 0.0019,
      "step": 60690
    },
    {
      "epoch": 4.046666666666667,
      "grad_norm": 0.2923871874809265,
      "learning_rate": 2.4708333333333332e-05,
      "loss": 0.0014,
      "step": 60700
    },
    {
      "epoch": 4.0473333333333334,
      "grad_norm": 0.5363345742225647,
      "learning_rate": 2.4704166666666666e-05,
      "loss": 0.0016,
      "step": 60710
    },
    {
      "epoch": 4.048,
      "grad_norm": 0.1490338146686554,
      "learning_rate": 2.47e-05,
      "loss": 0.0022,
      "step": 60720
    },
    {
      "epoch": 4.048666666666667,
      "grad_norm": 0.6462945938110352,
      "learning_rate": 2.4695833333333335e-05,
      "loss": 0.0019,
      "step": 60730
    },
    {
      "epoch": 4.049333333333333,
      "grad_norm": 0.0450776144862175,
      "learning_rate": 2.469166666666667e-05,
      "loss": 0.0012,
      "step": 60740
    },
    {
      "epoch": 4.05,
      "grad_norm": 0.5278254151344299,
      "learning_rate": 2.4687500000000004e-05,
      "loss": 0.0014,
      "step": 60750
    },
    {
      "epoch": 4.050666666666666,
      "grad_norm": 0.4453977346420288,
      "learning_rate": 2.4683333333333335e-05,
      "loss": 0.002,
      "step": 60760
    },
    {
      "epoch": 4.051333333333333,
      "grad_norm": 0.8061950206756592,
      "learning_rate": 2.4679166666666666e-05,
      "loss": 0.0014,
      "step": 60770
    },
    {
      "epoch": 4.052,
      "grad_norm": 0.04648111015558243,
      "learning_rate": 2.4675e-05,
      "loss": 0.0019,
      "step": 60780
    },
    {
      "epoch": 4.052666666666667,
      "grad_norm": 0.057432305067777634,
      "learning_rate": 2.4670833333333334e-05,
      "loss": 0.0026,
      "step": 60790
    },
    {
      "epoch": 4.053333333333334,
      "grad_norm": 0.15924908220767975,
      "learning_rate": 2.466666666666667e-05,
      "loss": 0.0018,
      "step": 60800
    },
    {
      "epoch": 4.054,
      "grad_norm": 0.10885486006736755,
      "learning_rate": 2.4662500000000003e-05,
      "loss": 0.0014,
      "step": 60810
    },
    {
      "epoch": 4.054666666666667,
      "grad_norm": 0.2917289733886719,
      "learning_rate": 2.4658333333333334e-05,
      "loss": 0.0018,
      "step": 60820
    },
    {
      "epoch": 4.0553333333333335,
      "grad_norm": 0.14634166657924652,
      "learning_rate": 2.465416666666667e-05,
      "loss": 0.0016,
      "step": 60830
    },
    {
      "epoch": 4.056,
      "grad_norm": 0.2914331555366516,
      "learning_rate": 2.465e-05,
      "loss": 0.0016,
      "step": 60840
    },
    {
      "epoch": 4.056666666666667,
      "grad_norm": 0.048476554453372955,
      "learning_rate": 2.4645833333333334e-05,
      "loss": 0.0014,
      "step": 60850
    },
    {
      "epoch": 4.057333333333333,
      "grad_norm": 0.4967648684978485,
      "learning_rate": 2.4641666666666668e-05,
      "loss": 0.0014,
      "step": 60860
    },
    {
      "epoch": 4.058,
      "grad_norm": 0.42614638805389404,
      "learning_rate": 2.4637500000000003e-05,
      "loss": 0.0033,
      "step": 60870
    },
    {
      "epoch": 4.058666666666666,
      "grad_norm": 0.07857997715473175,
      "learning_rate": 2.4633333333333334e-05,
      "loss": 0.0017,
      "step": 60880
    },
    {
      "epoch": 4.059333333333333,
      "grad_norm": 0.2124376744031906,
      "learning_rate": 2.4629166666666668e-05,
      "loss": 0.0023,
      "step": 60890
    },
    {
      "epoch": 4.06,
      "grad_norm": 0.17532727122306824,
      "learning_rate": 2.4625000000000002e-05,
      "loss": 0.0018,
      "step": 60900
    },
    {
      "epoch": 4.060666666666667,
      "grad_norm": 0.5931583046913147,
      "learning_rate": 2.4620833333333333e-05,
      "loss": 0.0025,
      "step": 60910
    },
    {
      "epoch": 4.061333333333334,
      "grad_norm": 0.18213942646980286,
      "learning_rate": 2.4616666666666668e-05,
      "loss": 0.0021,
      "step": 60920
    },
    {
      "epoch": 4.062,
      "grad_norm": 0.12479837238788605,
      "learning_rate": 2.4612500000000002e-05,
      "loss": 0.0021,
      "step": 60930
    },
    {
      "epoch": 4.062666666666667,
      "grad_norm": 0.43370938301086426,
      "learning_rate": 2.4608333333333333e-05,
      "loss": 0.0018,
      "step": 60940
    },
    {
      "epoch": 4.0633333333333335,
      "grad_norm": 0.31519725918769836,
      "learning_rate": 2.4604166666666667e-05,
      "loss": 0.0021,
      "step": 60950
    },
    {
      "epoch": 4.064,
      "grad_norm": 0.3909507989883423,
      "learning_rate": 2.46e-05,
      "loss": 0.0023,
      "step": 60960
    },
    {
      "epoch": 4.064666666666667,
      "grad_norm": 0.27966514229774475,
      "learning_rate": 2.4595833333333336e-05,
      "loss": 0.0019,
      "step": 60970
    },
    {
      "epoch": 4.065333333333333,
      "grad_norm": 0.2857860028743744,
      "learning_rate": 2.4591666666666667e-05,
      "loss": 0.0017,
      "step": 60980
    },
    {
      "epoch": 4.066,
      "grad_norm": 0.5306975841522217,
      "learning_rate": 2.45875e-05,
      "loss": 0.0018,
      "step": 60990
    },
    {
      "epoch": 4.066666666666666,
      "grad_norm": 0.10430369526147842,
      "learning_rate": 2.4583333333333332e-05,
      "loss": 0.0017,
      "step": 61000
    },
    {
      "epoch": 4.067333333333333,
      "grad_norm": 0.254332035779953,
      "learning_rate": 2.4579166666666667e-05,
      "loss": 0.0013,
      "step": 61010
    },
    {
      "epoch": 4.068,
      "grad_norm": 0.9162569642066956,
      "learning_rate": 2.4575e-05,
      "loss": 0.0026,
      "step": 61020
    },
    {
      "epoch": 4.068666666666667,
      "grad_norm": 0.43123531341552734,
      "learning_rate": 2.4570833333333335e-05,
      "loss": 0.002,
      "step": 61030
    },
    {
      "epoch": 4.069333333333334,
      "grad_norm": 0.4274221658706665,
      "learning_rate": 2.456666666666667e-05,
      "loss": 0.0024,
      "step": 61040
    },
    {
      "epoch": 4.07,
      "grad_norm": 0.14074411988258362,
      "learning_rate": 2.45625e-05,
      "loss": 0.0013,
      "step": 61050
    },
    {
      "epoch": 4.070666666666667,
      "grad_norm": 0.03976339101791382,
      "learning_rate": 2.4558333333333332e-05,
      "loss": 0.0021,
      "step": 61060
    },
    {
      "epoch": 4.0713333333333335,
      "grad_norm": 0.26685553789138794,
      "learning_rate": 2.4554166666666666e-05,
      "loss": 0.0019,
      "step": 61070
    },
    {
      "epoch": 4.072,
      "grad_norm": 0.05975700914859772,
      "learning_rate": 2.455e-05,
      "loss": 0.0022,
      "step": 61080
    },
    {
      "epoch": 4.072666666666667,
      "grad_norm": 0.25252336263656616,
      "learning_rate": 2.4545833333333335e-05,
      "loss": 0.0016,
      "step": 61090
    },
    {
      "epoch": 4.073333333333333,
      "grad_norm": 0.2581064701080322,
      "learning_rate": 2.454166666666667e-05,
      "loss": 0.002,
      "step": 61100
    },
    {
      "epoch": 4.074,
      "grad_norm": 0.7121772170066833,
      "learning_rate": 2.4537500000000004e-05,
      "loss": 0.0022,
      "step": 61110
    },
    {
      "epoch": 4.074666666666666,
      "grad_norm": 0.18690228462219238,
      "learning_rate": 2.4533333333333334e-05,
      "loss": 0.002,
      "step": 61120
    },
    {
      "epoch": 4.075333333333333,
      "grad_norm": 0.36376655101776123,
      "learning_rate": 2.4529166666666665e-05,
      "loss": 0.0023,
      "step": 61130
    },
    {
      "epoch": 4.076,
      "grad_norm": 0.36606937646865845,
      "learning_rate": 2.4525e-05,
      "loss": 0.0029,
      "step": 61140
    },
    {
      "epoch": 4.076666666666666,
      "grad_norm": 0.5308884382247925,
      "learning_rate": 2.4520833333333334e-05,
      "loss": 0.0018,
      "step": 61150
    },
    {
      "epoch": 4.077333333333334,
      "grad_norm": 0.29387176036834717,
      "learning_rate": 2.451666666666667e-05,
      "loss": 0.0028,
      "step": 61160
    },
    {
      "epoch": 4.078,
      "grad_norm": 0.8685532212257385,
      "learning_rate": 2.4512500000000003e-05,
      "loss": 0.0035,
      "step": 61170
    },
    {
      "epoch": 4.078666666666667,
      "grad_norm": 0.7123919725418091,
      "learning_rate": 2.4508333333333334e-05,
      "loss": 0.0027,
      "step": 61180
    },
    {
      "epoch": 4.0793333333333335,
      "grad_norm": 0.5587747693061829,
      "learning_rate": 2.4504166666666668e-05,
      "loss": 0.0018,
      "step": 61190
    },
    {
      "epoch": 4.08,
      "grad_norm": 0.07351752370595932,
      "learning_rate": 2.45e-05,
      "loss": 0.002,
      "step": 61200
    },
    {
      "epoch": 4.080666666666667,
      "grad_norm": 0.29452767968177795,
      "learning_rate": 2.4495833333333334e-05,
      "loss": 0.0023,
      "step": 61210
    },
    {
      "epoch": 4.081333333333333,
      "grad_norm": 0.2473163902759552,
      "learning_rate": 2.4491666666666668e-05,
      "loss": 0.0022,
      "step": 61220
    },
    {
      "epoch": 4.082,
      "grad_norm": 0.5890081524848938,
      "learning_rate": 2.4487500000000002e-05,
      "loss": 0.0013,
      "step": 61230
    },
    {
      "epoch": 4.082666666666666,
      "grad_norm": 0.5936588644981384,
      "learning_rate": 2.4483333333333333e-05,
      "loss": 0.0017,
      "step": 61240
    },
    {
      "epoch": 4.083333333333333,
      "grad_norm": 0.5233613848686218,
      "learning_rate": 2.4479166666666668e-05,
      "loss": 0.002,
      "step": 61250
    },
    {
      "epoch": 4.084,
      "grad_norm": 0.17579160630702972,
      "learning_rate": 2.4475000000000002e-05,
      "loss": 0.0017,
      "step": 61260
    },
    {
      "epoch": 4.084666666666667,
      "grad_norm": 0.5251074433326721,
      "learning_rate": 2.4470833333333333e-05,
      "loss": 0.0028,
      "step": 61270
    },
    {
      "epoch": 4.085333333333334,
      "grad_norm": 0.14001785218715668,
      "learning_rate": 2.4466666666666667e-05,
      "loss": 0.0015,
      "step": 61280
    },
    {
      "epoch": 4.086,
      "grad_norm": 0.5403525829315186,
      "learning_rate": 2.44625e-05,
      "loss": 0.0024,
      "step": 61290
    },
    {
      "epoch": 4.086666666666667,
      "grad_norm": 0.6721635460853577,
      "learning_rate": 2.4458333333333336e-05,
      "loss": 0.0024,
      "step": 61300
    },
    {
      "epoch": 4.0873333333333335,
      "grad_norm": 0.16426552832126617,
      "learning_rate": 2.4454166666666667e-05,
      "loss": 0.0018,
      "step": 61310
    },
    {
      "epoch": 4.088,
      "grad_norm": 0.11039726436138153,
      "learning_rate": 2.445e-05,
      "loss": 0.0026,
      "step": 61320
    },
    {
      "epoch": 4.088666666666667,
      "grad_norm": 0.5682874917984009,
      "learning_rate": 2.4445833333333336e-05,
      "loss": 0.0022,
      "step": 61330
    },
    {
      "epoch": 4.089333333333333,
      "grad_norm": 0.54313063621521,
      "learning_rate": 2.4441666666666667e-05,
      "loss": 0.002,
      "step": 61340
    },
    {
      "epoch": 4.09,
      "grad_norm": 0.2118028998374939,
      "learning_rate": 2.44375e-05,
      "loss": 0.0028,
      "step": 61350
    },
    {
      "epoch": 4.0906666666666665,
      "grad_norm": 0.6072598099708557,
      "learning_rate": 2.4433333333333335e-05,
      "loss": 0.0023,
      "step": 61360
    },
    {
      "epoch": 4.091333333333333,
      "grad_norm": 0.21556104719638824,
      "learning_rate": 2.4429166666666666e-05,
      "loss": 0.0019,
      "step": 61370
    },
    {
      "epoch": 4.092,
      "grad_norm": 0.07436395436525345,
      "learning_rate": 2.4425e-05,
      "loss": 0.0015,
      "step": 61380
    },
    {
      "epoch": 4.092666666666666,
      "grad_norm": 0.2085326910018921,
      "learning_rate": 2.4420833333333335e-05,
      "loss": 0.0023,
      "step": 61390
    },
    {
      "epoch": 4.093333333333334,
      "grad_norm": 0.08572458475828171,
      "learning_rate": 2.441666666666667e-05,
      "loss": 0.0015,
      "step": 61400
    },
    {
      "epoch": 4.094,
      "grad_norm": 0.08794041723012924,
      "learning_rate": 2.44125e-05,
      "loss": 0.0016,
      "step": 61410
    },
    {
      "epoch": 4.094666666666667,
      "grad_norm": 0.28412023186683655,
      "learning_rate": 2.4408333333333335e-05,
      "loss": 0.0037,
      "step": 61420
    },
    {
      "epoch": 4.0953333333333335,
      "grad_norm": 0.43880364298820496,
      "learning_rate": 2.4404166666666666e-05,
      "loss": 0.002,
      "step": 61430
    },
    {
      "epoch": 4.096,
      "grad_norm": 0.14112095534801483,
      "learning_rate": 2.44e-05,
      "loss": 0.0026,
      "step": 61440
    },
    {
      "epoch": 4.096666666666667,
      "grad_norm": 0.5512892603874207,
      "learning_rate": 2.4395833333333335e-05,
      "loss": 0.002,
      "step": 61450
    },
    {
      "epoch": 4.097333333333333,
      "grad_norm": 0.10827167332172394,
      "learning_rate": 2.439166666666667e-05,
      "loss": 0.0026,
      "step": 61460
    },
    {
      "epoch": 4.098,
      "grad_norm": 0.21041731536388397,
      "learning_rate": 2.4387500000000003e-05,
      "loss": 0.0024,
      "step": 61470
    },
    {
      "epoch": 4.0986666666666665,
      "grad_norm": 0.3194362223148346,
      "learning_rate": 2.4383333333333334e-05,
      "loss": 0.0022,
      "step": 61480
    },
    {
      "epoch": 4.099333333333333,
      "grad_norm": 0.5698378086090088,
      "learning_rate": 2.4379166666666665e-05,
      "loss": 0.003,
      "step": 61490
    },
    {
      "epoch": 4.1,
      "grad_norm": 0.3966214954853058,
      "learning_rate": 2.4375e-05,
      "loss": 0.0021,
      "step": 61500
    },
    {
      "epoch": 4.100666666666666,
      "grad_norm": 0.21742033958435059,
      "learning_rate": 2.4370833333333334e-05,
      "loss": 0.0025,
      "step": 61510
    },
    {
      "epoch": 4.101333333333334,
      "grad_norm": 0.5772640705108643,
      "learning_rate": 2.4366666666666668e-05,
      "loss": 0.0023,
      "step": 61520
    },
    {
      "epoch": 4.102,
      "grad_norm": 0.2883435785770416,
      "learning_rate": 2.4362500000000003e-05,
      "loss": 0.0017,
      "step": 61530
    },
    {
      "epoch": 4.102666666666667,
      "grad_norm": 0.5045080184936523,
      "learning_rate": 2.4358333333333337e-05,
      "loss": 0.0015,
      "step": 61540
    },
    {
      "epoch": 4.1033333333333335,
      "grad_norm": 0.3326382637023926,
      "learning_rate": 2.4354166666666668e-05,
      "loss": 0.0015,
      "step": 61550
    },
    {
      "epoch": 4.104,
      "grad_norm": 0.7183552980422974,
      "learning_rate": 2.435e-05,
      "loss": 0.003,
      "step": 61560
    },
    {
      "epoch": 4.104666666666667,
      "grad_norm": 0.320315420627594,
      "learning_rate": 2.4345833333333333e-05,
      "loss": 0.0013,
      "step": 61570
    },
    {
      "epoch": 4.105333333333333,
      "grad_norm": 0.22272776067256927,
      "learning_rate": 2.4341666666666668e-05,
      "loss": 0.0027,
      "step": 61580
    },
    {
      "epoch": 4.106,
      "grad_norm": 0.29595351219177246,
      "learning_rate": 2.4337500000000002e-05,
      "loss": 0.0017,
      "step": 61590
    },
    {
      "epoch": 4.1066666666666665,
      "grad_norm": 0.782514214515686,
      "learning_rate": 2.4333333333333336e-05,
      "loss": 0.0014,
      "step": 61600
    },
    {
      "epoch": 4.107333333333333,
      "grad_norm": 0.746661365032196,
      "learning_rate": 2.4329166666666667e-05,
      "loss": 0.0017,
      "step": 61610
    },
    {
      "epoch": 4.108,
      "grad_norm": 0.1228635311126709,
      "learning_rate": 2.4325000000000002e-05,
      "loss": 0.0021,
      "step": 61620
    },
    {
      "epoch": 4.108666666666666,
      "grad_norm": 0.05878100544214249,
      "learning_rate": 2.4320833333333333e-05,
      "loss": 0.002,
      "step": 61630
    },
    {
      "epoch": 4.109333333333334,
      "grad_norm": 0.253585547208786,
      "learning_rate": 2.4316666666666667e-05,
      "loss": 0.0029,
      "step": 61640
    },
    {
      "epoch": 4.11,
      "grad_norm": 0.5840259790420532,
      "learning_rate": 2.43125e-05,
      "loss": 0.0018,
      "step": 61650
    },
    {
      "epoch": 4.110666666666667,
      "grad_norm": 0.11806093156337738,
      "learning_rate": 2.4308333333333336e-05,
      "loss": 0.0021,
      "step": 61660
    },
    {
      "epoch": 4.1113333333333335,
      "grad_norm": 0.0400378480553627,
      "learning_rate": 2.4304166666666667e-05,
      "loss": 0.0019,
      "step": 61670
    },
    {
      "epoch": 4.112,
      "grad_norm": 0.027604222297668457,
      "learning_rate": 2.43e-05,
      "loss": 0.0024,
      "step": 61680
    },
    {
      "epoch": 4.112666666666667,
      "grad_norm": 0.24883775413036346,
      "learning_rate": 2.4295833333333335e-05,
      "loss": 0.0019,
      "step": 61690
    },
    {
      "epoch": 4.113333333333333,
      "grad_norm": 0.5623801946640015,
      "learning_rate": 2.4291666666666666e-05,
      "loss": 0.0027,
      "step": 61700
    },
    {
      "epoch": 4.114,
      "grad_norm": 0.38453227281570435,
      "learning_rate": 2.42875e-05,
      "loss": 0.0027,
      "step": 61710
    },
    {
      "epoch": 4.1146666666666665,
      "grad_norm": 0.21540381014347076,
      "learning_rate": 2.4283333333333335e-05,
      "loss": 0.0018,
      "step": 61720
    },
    {
      "epoch": 4.115333333333333,
      "grad_norm": 0.1507904827594757,
      "learning_rate": 2.4279166666666666e-05,
      "loss": 0.0012,
      "step": 61730
    },
    {
      "epoch": 4.116,
      "grad_norm": 0.053279343992471695,
      "learning_rate": 2.4275e-05,
      "loss": 0.002,
      "step": 61740
    },
    {
      "epoch": 4.116666666666666,
      "grad_norm": 0.488757848739624,
      "learning_rate": 2.4270833333333335e-05,
      "loss": 0.002,
      "step": 61750
    },
    {
      "epoch": 4.117333333333334,
      "grad_norm": 0.38660043478012085,
      "learning_rate": 2.426666666666667e-05,
      "loss": 0.0025,
      "step": 61760
    },
    {
      "epoch": 4.118,
      "grad_norm": 0.037201836705207825,
      "learning_rate": 2.42625e-05,
      "loss": 0.0017,
      "step": 61770
    },
    {
      "epoch": 4.118666666666667,
      "grad_norm": 0.31725063920021057,
      "learning_rate": 2.4258333333333335e-05,
      "loss": 0.0021,
      "step": 61780
    },
    {
      "epoch": 4.1193333333333335,
      "grad_norm": 0.07886725664138794,
      "learning_rate": 2.4254166666666666e-05,
      "loss": 0.0027,
      "step": 61790
    },
    {
      "epoch": 4.12,
      "grad_norm": 0.21234986186027527,
      "learning_rate": 2.425e-05,
      "loss": 0.0016,
      "step": 61800
    },
    {
      "epoch": 4.120666666666667,
      "grad_norm": 0.054882969707250595,
      "learning_rate": 2.4245833333333334e-05,
      "loss": 0.002,
      "step": 61810
    },
    {
      "epoch": 4.121333333333333,
      "grad_norm": 0.5690782070159912,
      "learning_rate": 2.424166666666667e-05,
      "loss": 0.0027,
      "step": 61820
    },
    {
      "epoch": 4.122,
      "grad_norm": 0.25390470027923584,
      "learning_rate": 2.4237500000000003e-05,
      "loss": 0.002,
      "step": 61830
    },
    {
      "epoch": 4.1226666666666665,
      "grad_norm": 0.11575420200824738,
      "learning_rate": 2.4233333333333337e-05,
      "loss": 0.0018,
      "step": 61840
    },
    {
      "epoch": 4.123333333333333,
      "grad_norm": 0.18422752618789673,
      "learning_rate": 2.422916666666667e-05,
      "loss": 0.0016,
      "step": 61850
    },
    {
      "epoch": 4.124,
      "grad_norm": 0.12720690667629242,
      "learning_rate": 2.4225e-05,
      "loss": 0.0019,
      "step": 61860
    },
    {
      "epoch": 4.124666666666666,
      "grad_norm": 0.39472466707229614,
      "learning_rate": 2.4220833333333334e-05,
      "loss": 0.0014,
      "step": 61870
    },
    {
      "epoch": 4.125333333333334,
      "grad_norm": 0.08073531836271286,
      "learning_rate": 2.4216666666666668e-05,
      "loss": 0.0025,
      "step": 61880
    },
    {
      "epoch": 4.126,
      "grad_norm": 0.5459544062614441,
      "learning_rate": 2.4212500000000002e-05,
      "loss": 0.002,
      "step": 61890
    },
    {
      "epoch": 4.126666666666667,
      "grad_norm": 0.0526166670024395,
      "learning_rate": 2.4208333333333337e-05,
      "loss": 0.0016,
      "step": 61900
    },
    {
      "epoch": 4.1273333333333335,
      "grad_norm": 0.25485238432884216,
      "learning_rate": 2.4204166666666668e-05,
      "loss": 0.0022,
      "step": 61910
    },
    {
      "epoch": 4.128,
      "grad_norm": 0.4297758638858795,
      "learning_rate": 2.4200000000000002e-05,
      "loss": 0.0017,
      "step": 61920
    },
    {
      "epoch": 4.128666666666667,
      "grad_norm": 0.32749971747398376,
      "learning_rate": 2.4195833333333333e-05,
      "loss": 0.0017,
      "step": 61930
    },
    {
      "epoch": 4.129333333333333,
      "grad_norm": 0.0495532788336277,
      "learning_rate": 2.4191666666666667e-05,
      "loss": 0.0017,
      "step": 61940
    },
    {
      "epoch": 4.13,
      "grad_norm": 0.8494533896446228,
      "learning_rate": 2.4187500000000002e-05,
      "loss": 0.0022,
      "step": 61950
    },
    {
      "epoch": 4.1306666666666665,
      "grad_norm": 0.9901448488235474,
      "learning_rate": 2.4183333333333336e-05,
      "loss": 0.0022,
      "step": 61960
    },
    {
      "epoch": 4.131333333333333,
      "grad_norm": 0.5147742629051208,
      "learning_rate": 2.4179166666666667e-05,
      "loss": 0.0021,
      "step": 61970
    },
    {
      "epoch": 4.132,
      "grad_norm": 0.4341508448123932,
      "learning_rate": 2.4175e-05,
      "loss": 0.0029,
      "step": 61980
    },
    {
      "epoch": 4.132666666666666,
      "grad_norm": 0.0690203532576561,
      "learning_rate": 2.4170833333333336e-05,
      "loss": 0.0016,
      "step": 61990
    },
    {
      "epoch": 4.133333333333334,
      "grad_norm": 0.5949137806892395,
      "learning_rate": 2.4166666666666667e-05,
      "loss": 0.0015,
      "step": 62000
    },
    {
      "epoch": 4.134,
      "grad_norm": 0.10704978555440903,
      "learning_rate": 2.41625e-05,
      "loss": 0.0023,
      "step": 62010
    },
    {
      "epoch": 4.134666666666667,
      "grad_norm": 0.2548089921474457,
      "learning_rate": 2.4158333333333336e-05,
      "loss": 0.0019,
      "step": 62020
    },
    {
      "epoch": 4.1353333333333335,
      "grad_norm": 0.5013651847839355,
      "learning_rate": 2.4154166666666667e-05,
      "loss": 0.002,
      "step": 62030
    },
    {
      "epoch": 4.136,
      "grad_norm": 0.3230251967906952,
      "learning_rate": 2.415e-05,
      "loss": 0.0022,
      "step": 62040
    },
    {
      "epoch": 4.136666666666667,
      "grad_norm": 0.3777841627597809,
      "learning_rate": 2.4145833333333335e-05,
      "loss": 0.0024,
      "step": 62050
    },
    {
      "epoch": 4.137333333333333,
      "grad_norm": 0.19017042219638824,
      "learning_rate": 2.414166666666667e-05,
      "loss": 0.0025,
      "step": 62060
    },
    {
      "epoch": 4.138,
      "grad_norm": 0.38116422295570374,
      "learning_rate": 2.41375e-05,
      "loss": 0.0016,
      "step": 62070
    },
    {
      "epoch": 4.1386666666666665,
      "grad_norm": 0.04935907572507858,
      "learning_rate": 2.4133333333333335e-05,
      "loss": 0.0022,
      "step": 62080
    },
    {
      "epoch": 4.139333333333333,
      "grad_norm": 0.041845254600048065,
      "learning_rate": 2.4129166666666666e-05,
      "loss": 0.0025,
      "step": 62090
    },
    {
      "epoch": 4.14,
      "grad_norm": 0.2513026297092438,
      "learning_rate": 2.4125e-05,
      "loss": 0.0021,
      "step": 62100
    },
    {
      "epoch": 4.140666666666666,
      "grad_norm": 0.3215494453907013,
      "learning_rate": 2.4120833333333335e-05,
      "loss": 0.0033,
      "step": 62110
    },
    {
      "epoch": 4.141333333333334,
      "grad_norm": 0.07424432039260864,
      "learning_rate": 2.411666666666667e-05,
      "loss": 0.0021,
      "step": 62120
    },
    {
      "epoch": 4.142,
      "grad_norm": 0.32948535680770874,
      "learning_rate": 2.4112500000000003e-05,
      "loss": 0.0028,
      "step": 62130
    },
    {
      "epoch": 4.142666666666667,
      "grad_norm": 0.10693942755460739,
      "learning_rate": 2.4108333333333334e-05,
      "loss": 0.0015,
      "step": 62140
    },
    {
      "epoch": 4.1433333333333335,
      "grad_norm": 0.07555517554283142,
      "learning_rate": 2.4104166666666665e-05,
      "loss": 0.0022,
      "step": 62150
    },
    {
      "epoch": 4.144,
      "grad_norm": 0.28728801012039185,
      "learning_rate": 2.41e-05,
      "loss": 0.0028,
      "step": 62160
    },
    {
      "epoch": 4.144666666666667,
      "grad_norm": 0.5390833616256714,
      "learning_rate": 2.4095833333333334e-05,
      "loss": 0.0021,
      "step": 62170
    },
    {
      "epoch": 4.145333333333333,
      "grad_norm": 0.1636696755886078,
      "learning_rate": 2.409166666666667e-05,
      "loss": 0.0015,
      "step": 62180
    },
    {
      "epoch": 4.146,
      "grad_norm": 0.18758246302604675,
      "learning_rate": 2.4087500000000003e-05,
      "loss": 0.0019,
      "step": 62190
    },
    {
      "epoch": 4.1466666666666665,
      "grad_norm": 0.39388608932495117,
      "learning_rate": 2.4083333333333337e-05,
      "loss": 0.0025,
      "step": 62200
    },
    {
      "epoch": 4.147333333333333,
      "grad_norm": 0.39683955907821655,
      "learning_rate": 2.4079166666666668e-05,
      "loss": 0.0017,
      "step": 62210
    },
    {
      "epoch": 4.148,
      "grad_norm": 0.03625400364398956,
      "learning_rate": 2.4075e-05,
      "loss": 0.0018,
      "step": 62220
    },
    {
      "epoch": 4.148666666666666,
      "grad_norm": 0.18655437231063843,
      "learning_rate": 2.4070833333333333e-05,
      "loss": 0.0015,
      "step": 62230
    },
    {
      "epoch": 4.149333333333334,
      "grad_norm": 0.059579722583293915,
      "learning_rate": 2.4066666666666668e-05,
      "loss": 0.0023,
      "step": 62240
    },
    {
      "epoch": 4.15,
      "grad_norm": 0.12498804181814194,
      "learning_rate": 2.4062500000000002e-05,
      "loss": 0.002,
      "step": 62250
    },
    {
      "epoch": 4.150666666666667,
      "grad_norm": 0.0815783441066742,
      "learning_rate": 2.4058333333333336e-05,
      "loss": 0.0012,
      "step": 62260
    },
    {
      "epoch": 4.1513333333333335,
      "grad_norm": 0.11324369162321091,
      "learning_rate": 2.4054166666666667e-05,
      "loss": 0.0023,
      "step": 62270
    },
    {
      "epoch": 4.152,
      "grad_norm": 0.08678746223449707,
      "learning_rate": 2.4050000000000002e-05,
      "loss": 0.0023,
      "step": 62280
    },
    {
      "epoch": 4.152666666666667,
      "grad_norm": 0.4172145128250122,
      "learning_rate": 2.4045833333333333e-05,
      "loss": 0.0016,
      "step": 62290
    },
    {
      "epoch": 4.153333333333333,
      "grad_norm": 0.3971099555492401,
      "learning_rate": 2.4041666666666667e-05,
      "loss": 0.0025,
      "step": 62300
    },
    {
      "epoch": 4.154,
      "grad_norm": 0.1956547647714615,
      "learning_rate": 2.40375e-05,
      "loss": 0.0018,
      "step": 62310
    },
    {
      "epoch": 4.1546666666666665,
      "grad_norm": 0.5051336884498596,
      "learning_rate": 2.4033333333333336e-05,
      "loss": 0.0018,
      "step": 62320
    },
    {
      "epoch": 4.155333333333333,
      "grad_norm": 0.39946773648262024,
      "learning_rate": 2.4029166666666667e-05,
      "loss": 0.0012,
      "step": 62330
    },
    {
      "epoch": 4.156,
      "grad_norm": 0.03667927533388138,
      "learning_rate": 2.4025e-05,
      "loss": 0.0023,
      "step": 62340
    },
    {
      "epoch": 4.156666666666666,
      "grad_norm": 0.31109243631362915,
      "learning_rate": 2.4020833333333336e-05,
      "loss": 0.0013,
      "step": 62350
    },
    {
      "epoch": 4.157333333333334,
      "grad_norm": 0.547400951385498,
      "learning_rate": 2.4016666666666667e-05,
      "loss": 0.0017,
      "step": 62360
    },
    {
      "epoch": 4.158,
      "grad_norm": 0.2167147397994995,
      "learning_rate": 2.40125e-05,
      "loss": 0.0019,
      "step": 62370
    },
    {
      "epoch": 4.158666666666667,
      "grad_norm": 0.5960429310798645,
      "learning_rate": 2.4008333333333335e-05,
      "loss": 0.0017,
      "step": 62380
    },
    {
      "epoch": 4.1593333333333335,
      "grad_norm": 0.42479652166366577,
      "learning_rate": 2.4004166666666666e-05,
      "loss": 0.0021,
      "step": 62390
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.8138456344604492,
      "learning_rate": 2.4e-05,
      "loss": 0.0031,
      "step": 62400
    },
    {
      "epoch": 4.160666666666667,
      "grad_norm": 0.6952593326568604,
      "learning_rate": 2.3995833333333335e-05,
      "loss": 0.0017,
      "step": 62410
    },
    {
      "epoch": 4.161333333333333,
      "grad_norm": 0.4541795253753662,
      "learning_rate": 2.399166666666667e-05,
      "loss": 0.0014,
      "step": 62420
    },
    {
      "epoch": 4.162,
      "grad_norm": 0.08505959808826447,
      "learning_rate": 2.39875e-05,
      "loss": 0.0022,
      "step": 62430
    },
    {
      "epoch": 4.1626666666666665,
      "grad_norm": 0.19164611399173737,
      "learning_rate": 2.3983333333333335e-05,
      "loss": 0.002,
      "step": 62440
    },
    {
      "epoch": 4.163333333333333,
      "grad_norm": 0.3797893524169922,
      "learning_rate": 2.3979166666666666e-05,
      "loss": 0.0023,
      "step": 62450
    },
    {
      "epoch": 4.164,
      "grad_norm": 0.46038246154785156,
      "learning_rate": 2.3975e-05,
      "loss": 0.0032,
      "step": 62460
    },
    {
      "epoch": 4.164666666666666,
      "grad_norm": 0.4209326207637787,
      "learning_rate": 2.3970833333333334e-05,
      "loss": 0.0023,
      "step": 62470
    },
    {
      "epoch": 4.165333333333333,
      "grad_norm": 0.24648578464984894,
      "learning_rate": 2.396666666666667e-05,
      "loss": 0.0021,
      "step": 62480
    },
    {
      "epoch": 4.166,
      "grad_norm": 0.2752329111099243,
      "learning_rate": 2.3962500000000003e-05,
      "loss": 0.0018,
      "step": 62490
    },
    {
      "epoch": 4.166666666666667,
      "grad_norm": 0.4891626536846161,
      "learning_rate": 2.3958333333333334e-05,
      "loss": 0.0013,
      "step": 62500
    },
    {
      "epoch": 4.167333333333334,
      "grad_norm": 0.5278639197349548,
      "learning_rate": 2.395416666666667e-05,
      "loss": 0.0023,
      "step": 62510
    },
    {
      "epoch": 4.168,
      "grad_norm": 0.04981527477502823,
      "learning_rate": 2.395e-05,
      "loss": 0.0016,
      "step": 62520
    },
    {
      "epoch": 4.168666666666667,
      "grad_norm": 0.19019681215286255,
      "learning_rate": 2.3945833333333334e-05,
      "loss": 0.0013,
      "step": 62530
    },
    {
      "epoch": 4.169333333333333,
      "grad_norm": 0.1415015161037445,
      "learning_rate": 2.3941666666666668e-05,
      "loss": 0.0025,
      "step": 62540
    },
    {
      "epoch": 4.17,
      "grad_norm": 0.24338281154632568,
      "learning_rate": 2.3937500000000002e-05,
      "loss": 0.0024,
      "step": 62550
    },
    {
      "epoch": 4.1706666666666665,
      "grad_norm": 0.40251651406288147,
      "learning_rate": 2.3933333333333337e-05,
      "loss": 0.0018,
      "step": 62560
    },
    {
      "epoch": 4.171333333333333,
      "grad_norm": 0.18011459708213806,
      "learning_rate": 2.3929166666666668e-05,
      "loss": 0.0024,
      "step": 62570
    },
    {
      "epoch": 4.172,
      "grad_norm": 0.4801132380962372,
      "learning_rate": 2.3925e-05,
      "loss": 0.0015,
      "step": 62580
    },
    {
      "epoch": 4.172666666666666,
      "grad_norm": 0.39506736397743225,
      "learning_rate": 2.3920833333333333e-05,
      "loss": 0.0022,
      "step": 62590
    },
    {
      "epoch": 4.173333333333334,
      "grad_norm": 0.6712619066238403,
      "learning_rate": 2.3916666666666668e-05,
      "loss": 0.0018,
      "step": 62600
    },
    {
      "epoch": 4.174,
      "grad_norm": 0.044215377420186996,
      "learning_rate": 2.3912500000000002e-05,
      "loss": 0.0015,
      "step": 62610
    },
    {
      "epoch": 4.174666666666667,
      "grad_norm": 0.21495036780834198,
      "learning_rate": 2.3908333333333336e-05,
      "loss": 0.0021,
      "step": 62620
    },
    {
      "epoch": 4.175333333333334,
      "grad_norm": 0.6376559734344482,
      "learning_rate": 2.390416666666667e-05,
      "loss": 0.0025,
      "step": 62630
    },
    {
      "epoch": 4.176,
      "grad_norm": 0.15655842423439026,
      "learning_rate": 2.39e-05,
      "loss": 0.002,
      "step": 62640
    },
    {
      "epoch": 4.176666666666667,
      "grad_norm": 0.45410507917404175,
      "learning_rate": 2.3895833333333333e-05,
      "loss": 0.0022,
      "step": 62650
    },
    {
      "epoch": 4.177333333333333,
      "grad_norm": 0.4990903437137604,
      "learning_rate": 2.3891666666666667e-05,
      "loss": 0.0012,
      "step": 62660
    },
    {
      "epoch": 4.178,
      "grad_norm": 0.39383718371391296,
      "learning_rate": 2.38875e-05,
      "loss": 0.0021,
      "step": 62670
    },
    {
      "epoch": 4.1786666666666665,
      "grad_norm": 0.24311085045337677,
      "learning_rate": 2.3883333333333336e-05,
      "loss": 0.0017,
      "step": 62680
    },
    {
      "epoch": 4.179333333333333,
      "grad_norm": 0.4870680868625641,
      "learning_rate": 2.387916666666667e-05,
      "loss": 0.0021,
      "step": 62690
    },
    {
      "epoch": 4.18,
      "grad_norm": 0.22119230031967163,
      "learning_rate": 2.3875e-05,
      "loss": 0.0023,
      "step": 62700
    },
    {
      "epoch": 4.180666666666666,
      "grad_norm": 0.0783151239156723,
      "learning_rate": 2.3870833333333335e-05,
      "loss": 0.0019,
      "step": 62710
    },
    {
      "epoch": 4.181333333333333,
      "grad_norm": 0.9560650587081909,
      "learning_rate": 2.3866666666666666e-05,
      "loss": 0.0032,
      "step": 62720
    },
    {
      "epoch": 4.182,
      "grad_norm": 0.1090361624956131,
      "learning_rate": 2.38625e-05,
      "loss": 0.0018,
      "step": 62730
    },
    {
      "epoch": 4.182666666666667,
      "grad_norm": 0.46364733576774597,
      "learning_rate": 2.3858333333333335e-05,
      "loss": 0.0022,
      "step": 62740
    },
    {
      "epoch": 4.183333333333334,
      "grad_norm": 0.14519251883029938,
      "learning_rate": 2.385416666666667e-05,
      "loss": 0.0018,
      "step": 62750
    },
    {
      "epoch": 4.184,
      "grad_norm": 0.677915096282959,
      "learning_rate": 2.385e-05,
      "loss": 0.0016,
      "step": 62760
    },
    {
      "epoch": 4.184666666666667,
      "grad_norm": 0.053032878786325455,
      "learning_rate": 2.3845833333333335e-05,
      "loss": 0.0026,
      "step": 62770
    },
    {
      "epoch": 4.185333333333333,
      "grad_norm": 0.724182665348053,
      "learning_rate": 2.384166666666667e-05,
      "loss": 0.0019,
      "step": 62780
    },
    {
      "epoch": 4.186,
      "grad_norm": 0.32258135080337524,
      "learning_rate": 2.38375e-05,
      "loss": 0.0016,
      "step": 62790
    },
    {
      "epoch": 4.1866666666666665,
      "grad_norm": 0.6471301317214966,
      "learning_rate": 2.3833333333333334e-05,
      "loss": 0.0021,
      "step": 62800
    },
    {
      "epoch": 4.187333333333333,
      "grad_norm": 0.07963988184928894,
      "learning_rate": 2.382916666666667e-05,
      "loss": 0.0026,
      "step": 62810
    },
    {
      "epoch": 4.188,
      "grad_norm": 0.1233571395277977,
      "learning_rate": 2.3825e-05,
      "loss": 0.0015,
      "step": 62820
    },
    {
      "epoch": 4.188666666666666,
      "grad_norm": 0.10778658837080002,
      "learning_rate": 2.3820833333333334e-05,
      "loss": 0.0019,
      "step": 62830
    },
    {
      "epoch": 4.189333333333333,
      "grad_norm": 0.1886977106332779,
      "learning_rate": 2.381666666666667e-05,
      "loss": 0.002,
      "step": 62840
    },
    {
      "epoch": 4.19,
      "grad_norm": 0.22892409563064575,
      "learning_rate": 2.3812500000000003e-05,
      "loss": 0.0029,
      "step": 62850
    },
    {
      "epoch": 4.190666666666667,
      "grad_norm": 0.5245108008384705,
      "learning_rate": 2.3808333333333334e-05,
      "loss": 0.0021,
      "step": 62860
    },
    {
      "epoch": 4.191333333333334,
      "grad_norm": 0.6235611438751221,
      "learning_rate": 2.3804166666666668e-05,
      "loss": 0.002,
      "step": 62870
    },
    {
      "epoch": 4.192,
      "grad_norm": 0.030601520091295242,
      "learning_rate": 2.38e-05,
      "loss": 0.0029,
      "step": 62880
    },
    {
      "epoch": 4.192666666666667,
      "grad_norm": 0.32816648483276367,
      "learning_rate": 2.3795833333333333e-05,
      "loss": 0.0019,
      "step": 62890
    },
    {
      "epoch": 4.193333333333333,
      "grad_norm": 0.31478580832481384,
      "learning_rate": 2.3791666666666668e-05,
      "loss": 0.0014,
      "step": 62900
    },
    {
      "epoch": 4.194,
      "grad_norm": 0.3921665847301483,
      "learning_rate": 2.3787500000000002e-05,
      "loss": 0.0016,
      "step": 62910
    },
    {
      "epoch": 4.1946666666666665,
      "grad_norm": 0.5613322257995605,
      "learning_rate": 2.3783333333333337e-05,
      "loss": 0.0017,
      "step": 62920
    },
    {
      "epoch": 4.195333333333333,
      "grad_norm": 0.31659767031669617,
      "learning_rate": 2.3779166666666668e-05,
      "loss": 0.0013,
      "step": 62930
    },
    {
      "epoch": 4.196,
      "grad_norm": 0.06524486094713211,
      "learning_rate": 2.3775e-05,
      "loss": 0.0017,
      "step": 62940
    },
    {
      "epoch": 4.196666666666666,
      "grad_norm": 0.04228617250919342,
      "learning_rate": 2.3770833333333333e-05,
      "loss": 0.0018,
      "step": 62950
    },
    {
      "epoch": 4.197333333333333,
      "grad_norm": 0.6356777548789978,
      "learning_rate": 2.3766666666666667e-05,
      "loss": 0.0021,
      "step": 62960
    },
    {
      "epoch": 4.198,
      "grad_norm": 0.15566766262054443,
      "learning_rate": 2.37625e-05,
      "loss": 0.0015,
      "step": 62970
    },
    {
      "epoch": 4.198666666666667,
      "grad_norm": 0.3794551193714142,
      "learning_rate": 2.3758333333333336e-05,
      "loss": 0.003,
      "step": 62980
    },
    {
      "epoch": 4.199333333333334,
      "grad_norm": 0.04968726262450218,
      "learning_rate": 2.375416666666667e-05,
      "loss": 0.0025,
      "step": 62990
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.4673925042152405,
      "learning_rate": 2.375e-05,
      "loss": 0.0023,
      "step": 63000
    },
    {
      "epoch": 4.200666666666667,
      "grad_norm": 0.12735192477703094,
      "learning_rate": 2.3745833333333332e-05,
      "loss": 0.0023,
      "step": 63010
    },
    {
      "epoch": 4.201333333333333,
      "grad_norm": 0.6082342863082886,
      "learning_rate": 2.3741666666666667e-05,
      "loss": 0.0022,
      "step": 63020
    },
    {
      "epoch": 4.202,
      "grad_norm": 0.5184726715087891,
      "learning_rate": 2.37375e-05,
      "loss": 0.0019,
      "step": 63030
    },
    {
      "epoch": 4.2026666666666666,
      "grad_norm": 0.17868049442768097,
      "learning_rate": 2.3733333333333335e-05,
      "loss": 0.0028,
      "step": 63040
    },
    {
      "epoch": 4.203333333333333,
      "grad_norm": 0.1498895287513733,
      "learning_rate": 2.372916666666667e-05,
      "loss": 0.0017,
      "step": 63050
    },
    {
      "epoch": 4.204,
      "grad_norm": 0.11509253084659576,
      "learning_rate": 2.3725e-05,
      "loss": 0.0024,
      "step": 63060
    },
    {
      "epoch": 4.204666666666666,
      "grad_norm": 0.18221649527549744,
      "learning_rate": 2.3720833333333335e-05,
      "loss": 0.0023,
      "step": 63070
    },
    {
      "epoch": 4.205333333333333,
      "grad_norm": 0.18995767831802368,
      "learning_rate": 2.3716666666666666e-05,
      "loss": 0.0023,
      "step": 63080
    },
    {
      "epoch": 4.206,
      "grad_norm": 0.39459121227264404,
      "learning_rate": 2.37125e-05,
      "loss": 0.0022,
      "step": 63090
    },
    {
      "epoch": 4.206666666666667,
      "grad_norm": 0.3241877555847168,
      "learning_rate": 2.3708333333333335e-05,
      "loss": 0.0016,
      "step": 63100
    },
    {
      "epoch": 4.207333333333334,
      "grad_norm": 0.4952421188354492,
      "learning_rate": 2.370416666666667e-05,
      "loss": 0.0023,
      "step": 63110
    },
    {
      "epoch": 4.208,
      "grad_norm": 0.3765449821949005,
      "learning_rate": 2.37e-05,
      "loss": 0.0017,
      "step": 63120
    },
    {
      "epoch": 4.208666666666667,
      "grad_norm": 0.04278528690338135,
      "learning_rate": 2.3695833333333334e-05,
      "loss": 0.002,
      "step": 63130
    },
    {
      "epoch": 4.209333333333333,
      "grad_norm": 0.8347019553184509,
      "learning_rate": 2.369166666666667e-05,
      "loss": 0.0035,
      "step": 63140
    },
    {
      "epoch": 4.21,
      "grad_norm": 0.18313075602054596,
      "learning_rate": 2.36875e-05,
      "loss": 0.0016,
      "step": 63150
    },
    {
      "epoch": 4.210666666666667,
      "grad_norm": 0.18456679582595825,
      "learning_rate": 2.3683333333333334e-05,
      "loss": 0.0023,
      "step": 63160
    },
    {
      "epoch": 4.211333333333333,
      "grad_norm": 0.10743249952793121,
      "learning_rate": 2.367916666666667e-05,
      "loss": 0.0027,
      "step": 63170
    },
    {
      "epoch": 4.212,
      "grad_norm": 0.04095875844359398,
      "learning_rate": 2.3675e-05,
      "loss": 0.002,
      "step": 63180
    },
    {
      "epoch": 4.212666666666666,
      "grad_norm": 0.5590196251869202,
      "learning_rate": 2.3670833333333334e-05,
      "loss": 0.0016,
      "step": 63190
    },
    {
      "epoch": 4.213333333333333,
      "grad_norm": 0.6459265947341919,
      "learning_rate": 2.3666666666666668e-05,
      "loss": 0.0024,
      "step": 63200
    },
    {
      "epoch": 4.214,
      "grad_norm": 0.0731416568160057,
      "learning_rate": 2.3662500000000003e-05,
      "loss": 0.002,
      "step": 63210
    },
    {
      "epoch": 4.214666666666667,
      "grad_norm": 0.7352827191352844,
      "learning_rate": 2.3658333333333334e-05,
      "loss": 0.0024,
      "step": 63220
    },
    {
      "epoch": 4.215333333333334,
      "grad_norm": 0.13939879834651947,
      "learning_rate": 2.3654166666666668e-05,
      "loss": 0.0019,
      "step": 63230
    },
    {
      "epoch": 4.216,
      "grad_norm": 0.4312366247177124,
      "learning_rate": 2.365e-05,
      "loss": 0.0013,
      "step": 63240
    },
    {
      "epoch": 4.216666666666667,
      "grad_norm": 0.3110780715942383,
      "learning_rate": 2.3645833333333333e-05,
      "loss": 0.0038,
      "step": 63250
    },
    {
      "epoch": 4.217333333333333,
      "grad_norm": 0.5478646159172058,
      "learning_rate": 2.3641666666666668e-05,
      "loss": 0.0017,
      "step": 63260
    },
    {
      "epoch": 4.218,
      "grad_norm": 0.43324920535087585,
      "learning_rate": 2.3637500000000002e-05,
      "loss": 0.0016,
      "step": 63270
    },
    {
      "epoch": 4.218666666666667,
      "grad_norm": 0.649121880531311,
      "learning_rate": 2.3633333333333336e-05,
      "loss": 0.003,
      "step": 63280
    },
    {
      "epoch": 4.219333333333333,
      "grad_norm": 0.6058356761932373,
      "learning_rate": 2.3629166666666667e-05,
      "loss": 0.0015,
      "step": 63290
    },
    {
      "epoch": 4.22,
      "grad_norm": 0.09934636205434799,
      "learning_rate": 2.3624999999999998e-05,
      "loss": 0.0023,
      "step": 63300
    },
    {
      "epoch": 4.220666666666666,
      "grad_norm": 0.0554174967110157,
      "learning_rate": 2.3620833333333333e-05,
      "loss": 0.0024,
      "step": 63310
    },
    {
      "epoch": 4.221333333333333,
      "grad_norm": 0.149677112698555,
      "learning_rate": 2.3616666666666667e-05,
      "loss": 0.0024,
      "step": 63320
    },
    {
      "epoch": 4.222,
      "grad_norm": 0.1403905302286148,
      "learning_rate": 2.36125e-05,
      "loss": 0.0016,
      "step": 63330
    },
    {
      "epoch": 4.222666666666667,
      "grad_norm": 0.6392234563827515,
      "learning_rate": 2.3608333333333336e-05,
      "loss": 0.0012,
      "step": 63340
    },
    {
      "epoch": 4.223333333333334,
      "grad_norm": 0.07527723908424377,
      "learning_rate": 2.360416666666667e-05,
      "loss": 0.0026,
      "step": 63350
    },
    {
      "epoch": 4.224,
      "grad_norm": 0.12349751591682434,
      "learning_rate": 2.36e-05,
      "loss": 0.002,
      "step": 63360
    },
    {
      "epoch": 4.224666666666667,
      "grad_norm": 0.6996496319770813,
      "learning_rate": 2.3595833333333332e-05,
      "loss": 0.0016,
      "step": 63370
    },
    {
      "epoch": 4.225333333333333,
      "grad_norm": 0.12396164238452911,
      "learning_rate": 2.3591666666666666e-05,
      "loss": 0.0015,
      "step": 63380
    },
    {
      "epoch": 4.226,
      "grad_norm": 0.29125311970710754,
      "learning_rate": 2.35875e-05,
      "loss": 0.0019,
      "step": 63390
    },
    {
      "epoch": 4.226666666666667,
      "grad_norm": 0.5645480751991272,
      "learning_rate": 2.3583333333333335e-05,
      "loss": 0.0024,
      "step": 63400
    },
    {
      "epoch": 4.227333333333333,
      "grad_norm": 0.559187650680542,
      "learning_rate": 2.357916666666667e-05,
      "loss": 0.0018,
      "step": 63410
    },
    {
      "epoch": 4.228,
      "grad_norm": 0.07834061235189438,
      "learning_rate": 2.3575e-05,
      "loss": 0.0018,
      "step": 63420
    },
    {
      "epoch": 4.228666666666666,
      "grad_norm": 0.11000454425811768,
      "learning_rate": 2.3570833333333335e-05,
      "loss": 0.0021,
      "step": 63430
    },
    {
      "epoch": 4.229333333333333,
      "grad_norm": 0.07560132443904877,
      "learning_rate": 2.3566666666666666e-05,
      "loss": 0.0029,
      "step": 63440
    },
    {
      "epoch": 4.23,
      "grad_norm": 0.5326305627822876,
      "learning_rate": 2.35625e-05,
      "loss": 0.0026,
      "step": 63450
    },
    {
      "epoch": 4.230666666666667,
      "grad_norm": 0.42044228315353394,
      "learning_rate": 2.3558333333333334e-05,
      "loss": 0.002,
      "step": 63460
    },
    {
      "epoch": 4.231333333333334,
      "grad_norm": 0.4979664981365204,
      "learning_rate": 2.355416666666667e-05,
      "loss": 0.0024,
      "step": 63470
    },
    {
      "epoch": 4.232,
      "grad_norm": 0.1595001071691513,
      "learning_rate": 2.355e-05,
      "loss": 0.0015,
      "step": 63480
    },
    {
      "epoch": 4.232666666666667,
      "grad_norm": 0.7333756685256958,
      "learning_rate": 2.3545833333333334e-05,
      "loss": 0.0018,
      "step": 63490
    },
    {
      "epoch": 4.233333333333333,
      "grad_norm": 0.29615089297294617,
      "learning_rate": 2.354166666666667e-05,
      "loss": 0.0029,
      "step": 63500
    },
    {
      "epoch": 4.234,
      "grad_norm": 0.5161214470863342,
      "learning_rate": 2.35375e-05,
      "loss": 0.0023,
      "step": 63510
    },
    {
      "epoch": 4.234666666666667,
      "grad_norm": 0.6306768655776978,
      "learning_rate": 2.3533333333333334e-05,
      "loss": 0.0024,
      "step": 63520
    },
    {
      "epoch": 4.235333333333333,
      "grad_norm": 0.4742263853549957,
      "learning_rate": 2.3529166666666668e-05,
      "loss": 0.0023,
      "step": 63530
    },
    {
      "epoch": 4.236,
      "grad_norm": 0.2184840440750122,
      "learning_rate": 2.3525e-05,
      "loss": 0.002,
      "step": 63540
    },
    {
      "epoch": 4.236666666666666,
      "grad_norm": 0.29444071650505066,
      "learning_rate": 2.3520833333333334e-05,
      "loss": 0.0021,
      "step": 63550
    },
    {
      "epoch": 4.237333333333333,
      "grad_norm": 0.2180953323841095,
      "learning_rate": 2.3516666666666668e-05,
      "loss": 0.0023,
      "step": 63560
    },
    {
      "epoch": 4.2379999999999995,
      "grad_norm": 0.9466216564178467,
      "learning_rate": 2.3512500000000002e-05,
      "loss": 0.0019,
      "step": 63570
    },
    {
      "epoch": 4.238666666666667,
      "grad_norm": 0.7067781686782837,
      "learning_rate": 2.3508333333333337e-05,
      "loss": 0.0023,
      "step": 63580
    },
    {
      "epoch": 4.239333333333334,
      "grad_norm": 0.06475509703159332,
      "learning_rate": 2.3504166666666668e-05,
      "loss": 0.0021,
      "step": 63590
    },
    {
      "epoch": 4.24,
      "grad_norm": 0.35414332151412964,
      "learning_rate": 2.35e-05,
      "loss": 0.0016,
      "step": 63600
    },
    {
      "epoch": 4.240666666666667,
      "grad_norm": 0.18269884586334229,
      "learning_rate": 2.3495833333333333e-05,
      "loss": 0.0022,
      "step": 63610
    },
    {
      "epoch": 4.241333333333333,
      "grad_norm": 0.3599656820297241,
      "learning_rate": 2.3491666666666667e-05,
      "loss": 0.0022,
      "step": 63620
    },
    {
      "epoch": 4.242,
      "grad_norm": 0.24854913353919983,
      "learning_rate": 2.3487500000000002e-05,
      "loss": 0.0022,
      "step": 63630
    },
    {
      "epoch": 4.242666666666667,
      "grad_norm": 0.9138720631599426,
      "learning_rate": 2.3483333333333336e-05,
      "loss": 0.0022,
      "step": 63640
    },
    {
      "epoch": 4.243333333333333,
      "grad_norm": 0.24218688905239105,
      "learning_rate": 2.347916666666667e-05,
      "loss": 0.0019,
      "step": 63650
    },
    {
      "epoch": 4.244,
      "grad_norm": 0.11736132949590683,
      "learning_rate": 2.3475e-05,
      "loss": 0.0019,
      "step": 63660
    },
    {
      "epoch": 4.244666666666666,
      "grad_norm": 0.1472221165895462,
      "learning_rate": 2.3470833333333332e-05,
      "loss": 0.0018,
      "step": 63670
    },
    {
      "epoch": 4.245333333333333,
      "grad_norm": 0.18841257691383362,
      "learning_rate": 2.3466666666666667e-05,
      "loss": 0.0018,
      "step": 63680
    },
    {
      "epoch": 4.246,
      "grad_norm": 0.1876107007265091,
      "learning_rate": 2.34625e-05,
      "loss": 0.0016,
      "step": 63690
    },
    {
      "epoch": 4.246666666666667,
      "grad_norm": 0.043459005653858185,
      "learning_rate": 2.3458333333333335e-05,
      "loss": 0.0014,
      "step": 63700
    },
    {
      "epoch": 4.247333333333334,
      "grad_norm": 0.4515322744846344,
      "learning_rate": 2.345416666666667e-05,
      "loss": 0.0022,
      "step": 63710
    },
    {
      "epoch": 4.248,
      "grad_norm": 0.42284467816352844,
      "learning_rate": 2.345e-05,
      "loss": 0.002,
      "step": 63720
    },
    {
      "epoch": 4.248666666666667,
      "grad_norm": 0.18495050072669983,
      "learning_rate": 2.3445833333333335e-05,
      "loss": 0.0012,
      "step": 63730
    },
    {
      "epoch": 4.249333333333333,
      "grad_norm": 0.576237678527832,
      "learning_rate": 2.3441666666666666e-05,
      "loss": 0.0026,
      "step": 63740
    },
    {
      "epoch": 4.25,
      "grad_norm": 0.07542901486158371,
      "learning_rate": 2.34375e-05,
      "loss": 0.0022,
      "step": 63750
    },
    {
      "epoch": 4.250666666666667,
      "grad_norm": 0.3495497703552246,
      "learning_rate": 2.3433333333333335e-05,
      "loss": 0.0014,
      "step": 63760
    },
    {
      "epoch": 4.251333333333333,
      "grad_norm": 0.3617975413799286,
      "learning_rate": 2.342916666666667e-05,
      "loss": 0.0019,
      "step": 63770
    },
    {
      "epoch": 4.252,
      "grad_norm": 0.08993115276098251,
      "learning_rate": 2.3425000000000004e-05,
      "loss": 0.0014,
      "step": 63780
    },
    {
      "epoch": 4.252666666666666,
      "grad_norm": 0.49250590801239014,
      "learning_rate": 2.3420833333333335e-05,
      "loss": 0.0016,
      "step": 63790
    },
    {
      "epoch": 4.253333333333333,
      "grad_norm": 0.629222571849823,
      "learning_rate": 2.341666666666667e-05,
      "loss": 0.0017,
      "step": 63800
    },
    {
      "epoch": 4.254,
      "grad_norm": 0.5544891357421875,
      "learning_rate": 2.34125e-05,
      "loss": 0.0016,
      "step": 63810
    },
    {
      "epoch": 4.254666666666667,
      "grad_norm": 0.5010462403297424,
      "learning_rate": 2.3408333333333334e-05,
      "loss": 0.0022,
      "step": 63820
    },
    {
      "epoch": 4.255333333333334,
      "grad_norm": 0.2803807854652405,
      "learning_rate": 2.340416666666667e-05,
      "loss": 0.0018,
      "step": 63830
    },
    {
      "epoch": 4.256,
      "grad_norm": 0.13964524865150452,
      "learning_rate": 2.3400000000000003e-05,
      "loss": 0.0019,
      "step": 63840
    },
    {
      "epoch": 4.256666666666667,
      "grad_norm": 0.11509872227907181,
      "learning_rate": 2.3395833333333334e-05,
      "loss": 0.0027,
      "step": 63850
    },
    {
      "epoch": 4.257333333333333,
      "grad_norm": 0.3489493429660797,
      "learning_rate": 2.3391666666666668e-05,
      "loss": 0.0015,
      "step": 63860
    },
    {
      "epoch": 4.258,
      "grad_norm": 0.4148663580417633,
      "learning_rate": 2.3387500000000003e-05,
      "loss": 0.0022,
      "step": 63870
    },
    {
      "epoch": 4.258666666666667,
      "grad_norm": 0.2433883249759674,
      "learning_rate": 2.3383333333333334e-05,
      "loss": 0.0022,
      "step": 63880
    },
    {
      "epoch": 4.259333333333333,
      "grad_norm": 0.055516574531793594,
      "learning_rate": 2.3379166666666668e-05,
      "loss": 0.0019,
      "step": 63890
    },
    {
      "epoch": 4.26,
      "grad_norm": 0.34817996621131897,
      "learning_rate": 2.3375000000000002e-05,
      "loss": 0.0015,
      "step": 63900
    },
    {
      "epoch": 4.260666666666666,
      "grad_norm": 0.24694912135601044,
      "learning_rate": 2.3370833333333333e-05,
      "loss": 0.002,
      "step": 63910
    },
    {
      "epoch": 4.261333333333333,
      "grad_norm": 0.2808269262313843,
      "learning_rate": 2.3366666666666668e-05,
      "loss": 0.0014,
      "step": 63920
    },
    {
      "epoch": 4.2620000000000005,
      "grad_norm": 0.13971523940563202,
      "learning_rate": 2.3362500000000002e-05,
      "loss": 0.0021,
      "step": 63930
    },
    {
      "epoch": 4.262666666666667,
      "grad_norm": 0.024639928713440895,
      "learning_rate": 2.3358333333333336e-05,
      "loss": 0.0014,
      "step": 63940
    },
    {
      "epoch": 4.263333333333334,
      "grad_norm": 0.07997340708971024,
      "learning_rate": 2.3354166666666667e-05,
      "loss": 0.0015,
      "step": 63950
    },
    {
      "epoch": 4.264,
      "grad_norm": 0.0442841537296772,
      "learning_rate": 2.3350000000000002e-05,
      "loss": 0.0015,
      "step": 63960
    },
    {
      "epoch": 4.264666666666667,
      "grad_norm": 0.24405142664909363,
      "learning_rate": 2.3345833333333333e-05,
      "loss": 0.002,
      "step": 63970
    },
    {
      "epoch": 4.265333333333333,
      "grad_norm": 0.9014459252357483,
      "learning_rate": 2.3341666666666667e-05,
      "loss": 0.0016,
      "step": 63980
    },
    {
      "epoch": 4.266,
      "grad_norm": 0.6737415790557861,
      "learning_rate": 2.33375e-05,
      "loss": 0.0022,
      "step": 63990
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 0.05096378177404404,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.0022,
      "step": 64000
    },
    {
      "epoch": 4.267333333333333,
      "grad_norm": 0.21631745994091034,
      "learning_rate": 2.332916666666667e-05,
      "loss": 0.0017,
      "step": 64010
    },
    {
      "epoch": 4.268,
      "grad_norm": 0.3155267834663391,
      "learning_rate": 2.3325e-05,
      "loss": 0.0015,
      "step": 64020
    },
    {
      "epoch": 4.268666666666666,
      "grad_norm": 0.3605193793773651,
      "learning_rate": 2.3320833333333332e-05,
      "loss": 0.0032,
      "step": 64030
    },
    {
      "epoch": 4.269333333333333,
      "grad_norm": 0.34966182708740234,
      "learning_rate": 2.3316666666666666e-05,
      "loss": 0.0026,
      "step": 64040
    },
    {
      "epoch": 4.27,
      "grad_norm": 0.04674752429127693,
      "learning_rate": 2.33125e-05,
      "loss": 0.0024,
      "step": 64050
    },
    {
      "epoch": 4.270666666666667,
      "grad_norm": 0.21188804507255554,
      "learning_rate": 2.3308333333333335e-05,
      "loss": 0.0013,
      "step": 64060
    },
    {
      "epoch": 4.271333333333334,
      "grad_norm": 0.11135530471801758,
      "learning_rate": 2.330416666666667e-05,
      "loss": 0.0019,
      "step": 64070
    },
    {
      "epoch": 4.272,
      "grad_norm": 0.04275718703866005,
      "learning_rate": 2.3300000000000004e-05,
      "loss": 0.0019,
      "step": 64080
    },
    {
      "epoch": 4.272666666666667,
      "grad_norm": 0.1442815214395523,
      "learning_rate": 2.3295833333333335e-05,
      "loss": 0.0017,
      "step": 64090
    },
    {
      "epoch": 4.273333333333333,
      "grad_norm": 0.01772046461701393,
      "learning_rate": 2.3291666666666666e-05,
      "loss": 0.0026,
      "step": 64100
    },
    {
      "epoch": 4.274,
      "grad_norm": 0.3500422537326813,
      "learning_rate": 2.32875e-05,
      "loss": 0.0014,
      "step": 64110
    },
    {
      "epoch": 4.274666666666667,
      "grad_norm": 0.145102858543396,
      "learning_rate": 2.3283333333333335e-05,
      "loss": 0.0023,
      "step": 64120
    },
    {
      "epoch": 4.275333333333333,
      "grad_norm": 0.08297044038772583,
      "learning_rate": 2.327916666666667e-05,
      "loss": 0.0022,
      "step": 64130
    },
    {
      "epoch": 4.276,
      "grad_norm": 0.07522749155759811,
      "learning_rate": 2.3275000000000003e-05,
      "loss": 0.0021,
      "step": 64140
    },
    {
      "epoch": 4.276666666666666,
      "grad_norm": 0.18570971488952637,
      "learning_rate": 2.3270833333333334e-05,
      "loss": 0.0024,
      "step": 64150
    },
    {
      "epoch": 4.277333333333333,
      "grad_norm": 0.2817564904689789,
      "learning_rate": 2.326666666666667e-05,
      "loss": 0.0015,
      "step": 64160
    },
    {
      "epoch": 4.2780000000000005,
      "grad_norm": 0.1110524982213974,
      "learning_rate": 2.32625e-05,
      "loss": 0.0021,
      "step": 64170
    },
    {
      "epoch": 4.278666666666667,
      "grad_norm": 0.41939571499824524,
      "learning_rate": 2.3258333333333334e-05,
      "loss": 0.0021,
      "step": 64180
    },
    {
      "epoch": 4.279333333333334,
      "grad_norm": 0.2497575879096985,
      "learning_rate": 2.325416666666667e-05,
      "loss": 0.0014,
      "step": 64190
    },
    {
      "epoch": 4.28,
      "grad_norm": 0.08421823382377625,
      "learning_rate": 2.3250000000000003e-05,
      "loss": 0.0024,
      "step": 64200
    },
    {
      "epoch": 4.280666666666667,
      "grad_norm": 0.3820604085922241,
      "learning_rate": 2.3245833333333334e-05,
      "loss": 0.0015,
      "step": 64210
    },
    {
      "epoch": 4.281333333333333,
      "grad_norm": 0.17842286825180054,
      "learning_rate": 2.3241666666666668e-05,
      "loss": 0.0028,
      "step": 64220
    },
    {
      "epoch": 4.282,
      "grad_norm": 0.18473920226097107,
      "learning_rate": 2.3237500000000002e-05,
      "loss": 0.0027,
      "step": 64230
    },
    {
      "epoch": 4.282666666666667,
      "grad_norm": 0.05964747816324234,
      "learning_rate": 2.3233333333333333e-05,
      "loss": 0.0015,
      "step": 64240
    },
    {
      "epoch": 4.283333333333333,
      "grad_norm": 0.5271927118301392,
      "learning_rate": 2.3229166666666668e-05,
      "loss": 0.0022,
      "step": 64250
    },
    {
      "epoch": 4.284,
      "grad_norm": 0.35672515630722046,
      "learning_rate": 2.3225000000000002e-05,
      "loss": 0.0023,
      "step": 64260
    },
    {
      "epoch": 4.284666666666666,
      "grad_norm": 0.3193780481815338,
      "learning_rate": 2.3220833333333333e-05,
      "loss": 0.0023,
      "step": 64270
    },
    {
      "epoch": 4.285333333333333,
      "grad_norm": 0.17849154770374298,
      "learning_rate": 2.3216666666666667e-05,
      "loss": 0.0022,
      "step": 64280
    },
    {
      "epoch": 4.286,
      "grad_norm": 0.5674538612365723,
      "learning_rate": 2.3212500000000002e-05,
      "loss": 0.0023,
      "step": 64290
    },
    {
      "epoch": 4.286666666666667,
      "grad_norm": 0.04558764025568962,
      "learning_rate": 2.3208333333333336e-05,
      "loss": 0.0026,
      "step": 64300
    },
    {
      "epoch": 4.287333333333334,
      "grad_norm": 0.28369849920272827,
      "learning_rate": 2.3204166666666667e-05,
      "loss": 0.0022,
      "step": 64310
    },
    {
      "epoch": 4.288,
      "grad_norm": 0.17436985671520233,
      "learning_rate": 2.32e-05,
      "loss": 0.0018,
      "step": 64320
    },
    {
      "epoch": 4.288666666666667,
      "grad_norm": 0.054532747715711594,
      "learning_rate": 2.3195833333333332e-05,
      "loss": 0.0025,
      "step": 64330
    },
    {
      "epoch": 4.289333333333333,
      "grad_norm": 0.14668814837932587,
      "learning_rate": 2.3191666666666667e-05,
      "loss": 0.0015,
      "step": 64340
    },
    {
      "epoch": 4.29,
      "grad_norm": 0.04517263546586037,
      "learning_rate": 2.31875e-05,
      "loss": 0.0029,
      "step": 64350
    },
    {
      "epoch": 4.290666666666667,
      "grad_norm": 0.10825688391923904,
      "learning_rate": 2.3183333333333336e-05,
      "loss": 0.002,
      "step": 64360
    },
    {
      "epoch": 4.291333333333333,
      "grad_norm": 0.3493150770664215,
      "learning_rate": 2.317916666666667e-05,
      "loss": 0.0016,
      "step": 64370
    },
    {
      "epoch": 4.292,
      "grad_norm": 0.6632195711135864,
      "learning_rate": 2.3175e-05,
      "loss": 0.0016,
      "step": 64380
    },
    {
      "epoch": 4.292666666666666,
      "grad_norm": 0.22730077803134918,
      "learning_rate": 2.3170833333333332e-05,
      "loss": 0.0025,
      "step": 64390
    },
    {
      "epoch": 4.293333333333333,
      "grad_norm": 0.25035667419433594,
      "learning_rate": 2.3166666666666666e-05,
      "loss": 0.0025,
      "step": 64400
    },
    {
      "epoch": 4.294,
      "grad_norm": 0.08538782596588135,
      "learning_rate": 2.31625e-05,
      "loss": 0.0013,
      "step": 64410
    },
    {
      "epoch": 4.294666666666667,
      "grad_norm": 0.3615530729293823,
      "learning_rate": 2.3158333333333335e-05,
      "loss": 0.0033,
      "step": 64420
    },
    {
      "epoch": 4.295333333333334,
      "grad_norm": 0.17370043694972992,
      "learning_rate": 2.315416666666667e-05,
      "loss": 0.002,
      "step": 64430
    },
    {
      "epoch": 4.296,
      "grad_norm": 0.17957279086112976,
      "learning_rate": 2.3150000000000004e-05,
      "loss": 0.0028,
      "step": 64440
    },
    {
      "epoch": 4.296666666666667,
      "grad_norm": 0.28856387734413147,
      "learning_rate": 2.3145833333333335e-05,
      "loss": 0.0018,
      "step": 64450
    },
    {
      "epoch": 4.2973333333333334,
      "grad_norm": 0.1806555688381195,
      "learning_rate": 2.3141666666666666e-05,
      "loss": 0.0017,
      "step": 64460
    },
    {
      "epoch": 4.298,
      "grad_norm": 0.5647185444831848,
      "learning_rate": 2.31375e-05,
      "loss": 0.0018,
      "step": 64470
    },
    {
      "epoch": 4.298666666666667,
      "grad_norm": 0.22411973774433136,
      "learning_rate": 2.3133333333333334e-05,
      "loss": 0.0028,
      "step": 64480
    },
    {
      "epoch": 4.299333333333333,
      "grad_norm": 0.21372102200984955,
      "learning_rate": 2.312916666666667e-05,
      "loss": 0.0014,
      "step": 64490
    },
    {
      "epoch": 4.3,
      "grad_norm": 0.3953312337398529,
      "learning_rate": 2.3125000000000003e-05,
      "loss": 0.0018,
      "step": 64500
    },
    {
      "epoch": 4.300666666666666,
      "grad_norm": 0.0506867989897728,
      "learning_rate": 2.3120833333333334e-05,
      "loss": 0.0022,
      "step": 64510
    },
    {
      "epoch": 4.301333333333333,
      "grad_norm": 0.6588159799575806,
      "learning_rate": 2.311666666666667e-05,
      "loss": 0.0022,
      "step": 64520
    },
    {
      "epoch": 4.302,
      "grad_norm": 0.18328902125358582,
      "learning_rate": 2.31125e-05,
      "loss": 0.0016,
      "step": 64530
    },
    {
      "epoch": 4.302666666666667,
      "grad_norm": 0.7668989896774292,
      "learning_rate": 2.3108333333333334e-05,
      "loss": 0.0017,
      "step": 64540
    },
    {
      "epoch": 4.303333333333334,
      "grad_norm": 0.10438769310712814,
      "learning_rate": 2.3104166666666668e-05,
      "loss": 0.0015,
      "step": 64550
    },
    {
      "epoch": 4.304,
      "grad_norm": 0.4213789105415344,
      "learning_rate": 2.3100000000000002e-05,
      "loss": 0.0014,
      "step": 64560
    },
    {
      "epoch": 4.304666666666667,
      "grad_norm": 0.37927743792533875,
      "learning_rate": 2.3095833333333333e-05,
      "loss": 0.002,
      "step": 64570
    },
    {
      "epoch": 4.3053333333333335,
      "grad_norm": 0.1665894091129303,
      "learning_rate": 2.3091666666666668e-05,
      "loss": 0.0015,
      "step": 64580
    },
    {
      "epoch": 4.306,
      "grad_norm": 0.13769973814487457,
      "learning_rate": 2.3087500000000002e-05,
      "loss": 0.0014,
      "step": 64590
    },
    {
      "epoch": 4.306666666666667,
      "grad_norm": 0.3687349259853363,
      "learning_rate": 2.3083333333333333e-05,
      "loss": 0.0014,
      "step": 64600
    },
    {
      "epoch": 4.307333333333333,
      "grad_norm": 0.17817054688930511,
      "learning_rate": 2.3079166666666667e-05,
      "loss": 0.0022,
      "step": 64610
    },
    {
      "epoch": 4.308,
      "grad_norm": 0.448853462934494,
      "learning_rate": 2.3075000000000002e-05,
      "loss": 0.0016,
      "step": 64620
    },
    {
      "epoch": 4.308666666666666,
      "grad_norm": 0.09216561168432236,
      "learning_rate": 2.3070833333333333e-05,
      "loss": 0.002,
      "step": 64630
    },
    {
      "epoch": 4.309333333333333,
      "grad_norm": 0.53875333070755,
      "learning_rate": 2.3066666666666667e-05,
      "loss": 0.0017,
      "step": 64640
    },
    {
      "epoch": 4.31,
      "grad_norm": 0.6267741322517395,
      "learning_rate": 2.30625e-05,
      "loss": 0.0021,
      "step": 64650
    },
    {
      "epoch": 4.310666666666666,
      "grad_norm": 0.5631896257400513,
      "learning_rate": 2.3058333333333336e-05,
      "loss": 0.0029,
      "step": 64660
    },
    {
      "epoch": 4.311333333333334,
      "grad_norm": 0.17963097989559174,
      "learning_rate": 2.3054166666666667e-05,
      "loss": 0.0017,
      "step": 64670
    },
    {
      "epoch": 4.312,
      "grad_norm": 0.08437749743461609,
      "learning_rate": 2.305e-05,
      "loss": 0.0021,
      "step": 64680
    },
    {
      "epoch": 4.312666666666667,
      "grad_norm": 0.5767828226089478,
      "learning_rate": 2.3045833333333332e-05,
      "loss": 0.0015,
      "step": 64690
    },
    {
      "epoch": 4.3133333333333335,
      "grad_norm": 0.6489717960357666,
      "learning_rate": 2.3041666666666667e-05,
      "loss": 0.0014,
      "step": 64700
    },
    {
      "epoch": 4.314,
      "grad_norm": 0.7251412868499756,
      "learning_rate": 2.30375e-05,
      "loss": 0.0014,
      "step": 64710
    },
    {
      "epoch": 4.314666666666667,
      "grad_norm": 0.31505435705184937,
      "learning_rate": 2.3033333333333335e-05,
      "loss": 0.0014,
      "step": 64720
    },
    {
      "epoch": 4.315333333333333,
      "grad_norm": 0.04994022101163864,
      "learning_rate": 2.302916666666667e-05,
      "loss": 0.0023,
      "step": 64730
    },
    {
      "epoch": 4.316,
      "grad_norm": 0.3793811500072479,
      "learning_rate": 2.3025e-05,
      "loss": 0.0012,
      "step": 64740
    },
    {
      "epoch": 4.316666666666666,
      "grad_norm": 0.8182546496391296,
      "learning_rate": 2.302083333333333e-05,
      "loss": 0.0027,
      "step": 64750
    },
    {
      "epoch": 4.317333333333333,
      "grad_norm": 0.6000403761863708,
      "learning_rate": 2.3016666666666666e-05,
      "loss": 0.0016,
      "step": 64760
    },
    {
      "epoch": 4.318,
      "grad_norm": 0.38173481822013855,
      "learning_rate": 2.30125e-05,
      "loss": 0.0019,
      "step": 64770
    },
    {
      "epoch": 4.318666666666667,
      "grad_norm": 0.5223357677459717,
      "learning_rate": 2.3008333333333335e-05,
      "loss": 0.002,
      "step": 64780
    },
    {
      "epoch": 4.319333333333334,
      "grad_norm": 0.687458336353302,
      "learning_rate": 2.300416666666667e-05,
      "loss": 0.0023,
      "step": 64790
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.3643032908439636,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.0027,
      "step": 64800
    },
    {
      "epoch": 4.320666666666667,
      "grad_norm": 0.18356457352638245,
      "learning_rate": 2.2995833333333334e-05,
      "loss": 0.0018,
      "step": 64810
    },
    {
      "epoch": 4.3213333333333335,
      "grad_norm": 0.8773339986801147,
      "learning_rate": 2.2991666666666665e-05,
      "loss": 0.0026,
      "step": 64820
    },
    {
      "epoch": 4.322,
      "grad_norm": 1.1141612529754639,
      "learning_rate": 2.29875e-05,
      "loss": 0.0024,
      "step": 64830
    },
    {
      "epoch": 4.322666666666667,
      "grad_norm": 0.40658700466156006,
      "learning_rate": 2.2983333333333334e-05,
      "loss": 0.0017,
      "step": 64840
    },
    {
      "epoch": 4.323333333333333,
      "grad_norm": 0.17976342141628265,
      "learning_rate": 2.297916666666667e-05,
      "loss": 0.0027,
      "step": 64850
    },
    {
      "epoch": 4.324,
      "grad_norm": 0.08075364679098129,
      "learning_rate": 2.2975000000000003e-05,
      "loss": 0.0019,
      "step": 64860
    },
    {
      "epoch": 4.324666666666666,
      "grad_norm": 0.8004171252250671,
      "learning_rate": 2.2970833333333334e-05,
      "loss": 0.0016,
      "step": 64870
    },
    {
      "epoch": 4.325333333333333,
      "grad_norm": 0.528556227684021,
      "learning_rate": 2.2966666666666668e-05,
      "loss": 0.0014,
      "step": 64880
    },
    {
      "epoch": 4.326,
      "grad_norm": 0.018630659207701683,
      "learning_rate": 2.29625e-05,
      "loss": 0.0019,
      "step": 64890
    },
    {
      "epoch": 4.326666666666666,
      "grad_norm": 0.21078822016716003,
      "learning_rate": 2.2958333333333333e-05,
      "loss": 0.0023,
      "step": 64900
    },
    {
      "epoch": 4.327333333333334,
      "grad_norm": 0.24672368168830872,
      "learning_rate": 2.2954166666666668e-05,
      "loss": 0.0024,
      "step": 64910
    },
    {
      "epoch": 4.328,
      "grad_norm": 0.2871887981891632,
      "learning_rate": 2.2950000000000002e-05,
      "loss": 0.0024,
      "step": 64920
    },
    {
      "epoch": 4.328666666666667,
      "grad_norm": 0.6549027562141418,
      "learning_rate": 2.2945833333333333e-05,
      "loss": 0.0019,
      "step": 64930
    },
    {
      "epoch": 4.3293333333333335,
      "grad_norm": 0.06515783816576004,
      "learning_rate": 2.2941666666666668e-05,
      "loss": 0.0029,
      "step": 64940
    },
    {
      "epoch": 4.33,
      "grad_norm": 0.2625212073326111,
      "learning_rate": 2.2937500000000002e-05,
      "loss": 0.0012,
      "step": 64950
    },
    {
      "epoch": 4.330666666666667,
      "grad_norm": 0.39568302035331726,
      "learning_rate": 2.2933333333333333e-05,
      "loss": 0.0018,
      "step": 64960
    },
    {
      "epoch": 4.331333333333333,
      "grad_norm": 0.4628552496433258,
      "learning_rate": 2.2929166666666667e-05,
      "loss": 0.0015,
      "step": 64970
    },
    {
      "epoch": 4.332,
      "grad_norm": 0.45699408650398254,
      "learning_rate": 2.2925e-05,
      "loss": 0.002,
      "step": 64980
    },
    {
      "epoch": 4.332666666666666,
      "grad_norm": 0.21737806499004364,
      "learning_rate": 2.2920833333333333e-05,
      "loss": 0.0026,
      "step": 64990
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 0.14299620687961578,
      "learning_rate": 2.2916666666666667e-05,
      "loss": 0.0015,
      "step": 65000
    },
    {
      "epoch": 4.334,
      "grad_norm": 0.056407686322927475,
      "learning_rate": 2.29125e-05,
      "loss": 0.002,
      "step": 65010
    },
    {
      "epoch": 4.334666666666667,
      "grad_norm": 0.08253121376037598,
      "learning_rate": 2.2908333333333336e-05,
      "loss": 0.0015,
      "step": 65020
    },
    {
      "epoch": 4.335333333333334,
      "grad_norm": 0.42078226804733276,
      "learning_rate": 2.2904166666666667e-05,
      "loss": 0.002,
      "step": 65030
    },
    {
      "epoch": 4.336,
      "grad_norm": 0.20305129885673523,
      "learning_rate": 2.29e-05,
      "loss": 0.0022,
      "step": 65040
    },
    {
      "epoch": 4.336666666666667,
      "grad_norm": 0.276321142911911,
      "learning_rate": 2.2895833333333335e-05,
      "loss": 0.002,
      "step": 65050
    },
    {
      "epoch": 4.3373333333333335,
      "grad_norm": 0.3857085704803467,
      "learning_rate": 2.2891666666666666e-05,
      "loss": 0.0027,
      "step": 65060
    },
    {
      "epoch": 4.338,
      "grad_norm": 0.2565912902355194,
      "learning_rate": 2.28875e-05,
      "loss": 0.0019,
      "step": 65070
    },
    {
      "epoch": 4.338666666666667,
      "grad_norm": 0.042090460658073425,
      "learning_rate": 2.2883333333333335e-05,
      "loss": 0.0018,
      "step": 65080
    },
    {
      "epoch": 4.339333333333333,
      "grad_norm": 0.32043546438217163,
      "learning_rate": 2.287916666666667e-05,
      "loss": 0.002,
      "step": 65090
    },
    {
      "epoch": 4.34,
      "grad_norm": 0.39007431268692017,
      "learning_rate": 2.2875e-05,
      "loss": 0.0018,
      "step": 65100
    },
    {
      "epoch": 4.3406666666666665,
      "grad_norm": 0.7731552124023438,
      "learning_rate": 2.2870833333333335e-05,
      "loss": 0.0023,
      "step": 65110
    },
    {
      "epoch": 4.341333333333333,
      "grad_norm": 0.45627492666244507,
      "learning_rate": 2.2866666666666666e-05,
      "loss": 0.0014,
      "step": 65120
    },
    {
      "epoch": 4.342,
      "grad_norm": 0.6610576510429382,
      "learning_rate": 2.28625e-05,
      "loss": 0.0019,
      "step": 65130
    },
    {
      "epoch": 4.342666666666666,
      "grad_norm": 0.42551189661026,
      "learning_rate": 2.2858333333333334e-05,
      "loss": 0.0018,
      "step": 65140
    },
    {
      "epoch": 4.343333333333334,
      "grad_norm": 0.46695783734321594,
      "learning_rate": 2.285416666666667e-05,
      "loss": 0.0019,
      "step": 65150
    },
    {
      "epoch": 4.344,
      "grad_norm": 0.2120172381401062,
      "learning_rate": 2.2850000000000003e-05,
      "loss": 0.0021,
      "step": 65160
    },
    {
      "epoch": 4.344666666666667,
      "grad_norm": 0.5301613807678223,
      "learning_rate": 2.2845833333333334e-05,
      "loss": 0.002,
      "step": 65170
    },
    {
      "epoch": 4.3453333333333335,
      "grad_norm": 0.14121100306510925,
      "learning_rate": 2.2841666666666665e-05,
      "loss": 0.0023,
      "step": 65180
    },
    {
      "epoch": 4.346,
      "grad_norm": 0.1712968796491623,
      "learning_rate": 2.28375e-05,
      "loss": 0.002,
      "step": 65190
    },
    {
      "epoch": 4.346666666666667,
      "grad_norm": 0.21277478337287903,
      "learning_rate": 2.2833333333333334e-05,
      "loss": 0.0025,
      "step": 65200
    },
    {
      "epoch": 4.347333333333333,
      "grad_norm": 0.05254632979631424,
      "learning_rate": 2.2829166666666668e-05,
      "loss": 0.0016,
      "step": 65210
    },
    {
      "epoch": 4.348,
      "grad_norm": 0.05291195213794708,
      "learning_rate": 2.2825000000000003e-05,
      "loss": 0.0016,
      "step": 65220
    },
    {
      "epoch": 4.3486666666666665,
      "grad_norm": 0.4235692024230957,
      "learning_rate": 2.2820833333333337e-05,
      "loss": 0.0016,
      "step": 65230
    },
    {
      "epoch": 4.349333333333333,
      "grad_norm": 0.28276458382606506,
      "learning_rate": 2.2816666666666668e-05,
      "loss": 0.0026,
      "step": 65240
    },
    {
      "epoch": 4.35,
      "grad_norm": 0.42042112350463867,
      "learning_rate": 2.28125e-05,
      "loss": 0.0013,
      "step": 65250
    },
    {
      "epoch": 4.350666666666667,
      "grad_norm": 0.4923411011695862,
      "learning_rate": 2.2808333333333333e-05,
      "loss": 0.0026,
      "step": 65260
    },
    {
      "epoch": 4.351333333333334,
      "grad_norm": 0.35247859358787537,
      "learning_rate": 2.2804166666666668e-05,
      "loss": 0.0024,
      "step": 65270
    },
    {
      "epoch": 4.352,
      "grad_norm": 0.07550300657749176,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 0.002,
      "step": 65280
    },
    {
      "epoch": 4.352666666666667,
      "grad_norm": 0.5087663531303406,
      "learning_rate": 2.2795833333333336e-05,
      "loss": 0.0021,
      "step": 65290
    },
    {
      "epoch": 4.3533333333333335,
      "grad_norm": 0.31471073627471924,
      "learning_rate": 2.2791666666666667e-05,
      "loss": 0.0022,
      "step": 65300
    },
    {
      "epoch": 4.354,
      "grad_norm": 0.703657329082489,
      "learning_rate": 2.27875e-05,
      "loss": 0.0023,
      "step": 65310
    },
    {
      "epoch": 4.354666666666667,
      "grad_norm": 0.38365626335144043,
      "learning_rate": 2.2783333333333336e-05,
      "loss": 0.0024,
      "step": 65320
    },
    {
      "epoch": 4.355333333333333,
      "grad_norm": 0.474016398191452,
      "learning_rate": 2.2779166666666667e-05,
      "loss": 0.0018,
      "step": 65330
    },
    {
      "epoch": 4.356,
      "grad_norm": 0.42098915576934814,
      "learning_rate": 2.2775e-05,
      "loss": 0.0019,
      "step": 65340
    },
    {
      "epoch": 4.3566666666666665,
      "grad_norm": 0.35402432084083557,
      "learning_rate": 2.2770833333333336e-05,
      "loss": 0.0016,
      "step": 65350
    },
    {
      "epoch": 4.357333333333333,
      "grad_norm": 0.6300839185714722,
      "learning_rate": 2.2766666666666667e-05,
      "loss": 0.0026,
      "step": 65360
    },
    {
      "epoch": 4.358,
      "grad_norm": 0.390658974647522,
      "learning_rate": 2.27625e-05,
      "loss": 0.0018,
      "step": 65370
    },
    {
      "epoch": 4.358666666666666,
      "grad_norm": 0.694393515586853,
      "learning_rate": 2.2758333333333335e-05,
      "loss": 0.0017,
      "step": 65380
    },
    {
      "epoch": 4.359333333333334,
      "grad_norm": 0.06852594017982483,
      "learning_rate": 2.275416666666667e-05,
      "loss": 0.0024,
      "step": 65390
    },
    {
      "epoch": 4.36,
      "grad_norm": 0.5514258146286011,
      "learning_rate": 2.275e-05,
      "loss": 0.0022,
      "step": 65400
    },
    {
      "epoch": 4.360666666666667,
      "grad_norm": 0.664943277835846,
      "learning_rate": 2.2745833333333335e-05,
      "loss": 0.0015,
      "step": 65410
    },
    {
      "epoch": 4.3613333333333335,
      "grad_norm": 0.17274099588394165,
      "learning_rate": 2.2741666666666666e-05,
      "loss": 0.0016,
      "step": 65420
    },
    {
      "epoch": 4.362,
      "grad_norm": 0.2135722041130066,
      "learning_rate": 2.27375e-05,
      "loss": 0.0019,
      "step": 65430
    },
    {
      "epoch": 4.362666666666667,
      "grad_norm": 0.21739967167377472,
      "learning_rate": 2.2733333333333335e-05,
      "loss": 0.0024,
      "step": 65440
    },
    {
      "epoch": 4.363333333333333,
      "grad_norm": 0.4729848802089691,
      "learning_rate": 2.272916666666667e-05,
      "loss": 0.0025,
      "step": 65450
    },
    {
      "epoch": 4.364,
      "grad_norm": 0.31262749433517456,
      "learning_rate": 2.2725000000000003e-05,
      "loss": 0.0014,
      "step": 65460
    },
    {
      "epoch": 4.3646666666666665,
      "grad_norm": 0.5268930792808533,
      "learning_rate": 2.2720833333333334e-05,
      "loss": 0.0036,
      "step": 65470
    },
    {
      "epoch": 4.365333333333333,
      "grad_norm": 0.46202149987220764,
      "learning_rate": 2.2716666666666665e-05,
      "loss": 0.003,
      "step": 65480
    },
    {
      "epoch": 4.366,
      "grad_norm": 0.33762457966804504,
      "learning_rate": 2.27125e-05,
      "loss": 0.0027,
      "step": 65490
    },
    {
      "epoch": 4.366666666666666,
      "grad_norm": 0.14633189141750336,
      "learning_rate": 2.2708333333333334e-05,
      "loss": 0.0019,
      "step": 65500
    },
    {
      "epoch": 4.367333333333334,
      "grad_norm": 0.10564562678337097,
      "learning_rate": 2.270416666666667e-05,
      "loss": 0.0017,
      "step": 65510
    },
    {
      "epoch": 4.368,
      "grad_norm": 0.047600433230400085,
      "learning_rate": 2.2700000000000003e-05,
      "loss": 0.0015,
      "step": 65520
    },
    {
      "epoch": 4.368666666666667,
      "grad_norm": 0.14630179107189178,
      "learning_rate": 2.2695833333333337e-05,
      "loss": 0.0023,
      "step": 65530
    },
    {
      "epoch": 4.3693333333333335,
      "grad_norm": 0.08227602392435074,
      "learning_rate": 2.2691666666666668e-05,
      "loss": 0.002,
      "step": 65540
    },
    {
      "epoch": 4.37,
      "grad_norm": 0.2784527838230133,
      "learning_rate": 2.26875e-05,
      "loss": 0.002,
      "step": 65550
    },
    {
      "epoch": 4.370666666666667,
      "grad_norm": 0.41913607716560364,
      "learning_rate": 2.2683333333333334e-05,
      "loss": 0.002,
      "step": 65560
    },
    {
      "epoch": 4.371333333333333,
      "grad_norm": 0.6917383670806885,
      "learning_rate": 2.2679166666666668e-05,
      "loss": 0.0028,
      "step": 65570
    },
    {
      "epoch": 4.372,
      "grad_norm": 0.35937702655792236,
      "learning_rate": 2.2675000000000002e-05,
      "loss": 0.0016,
      "step": 65580
    },
    {
      "epoch": 4.3726666666666665,
      "grad_norm": 0.31991660594940186,
      "learning_rate": 2.2670833333333337e-05,
      "loss": 0.0025,
      "step": 65590
    },
    {
      "epoch": 4.373333333333333,
      "grad_norm": 0.7777035236358643,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 0.002,
      "step": 65600
    },
    {
      "epoch": 4.374,
      "grad_norm": 0.10231447219848633,
      "learning_rate": 2.2662500000000002e-05,
      "loss": 0.0026,
      "step": 65610
    },
    {
      "epoch": 4.374666666666666,
      "grad_norm": 0.43555212020874023,
      "learning_rate": 2.2658333333333333e-05,
      "loss": 0.0021,
      "step": 65620
    },
    {
      "epoch": 4.375333333333334,
      "grad_norm": 0.0515495166182518,
      "learning_rate": 2.2654166666666667e-05,
      "loss": 0.0014,
      "step": 65630
    },
    {
      "epoch": 4.376,
      "grad_norm": 0.3168780505657196,
      "learning_rate": 2.265e-05,
      "loss": 0.0026,
      "step": 65640
    },
    {
      "epoch": 4.376666666666667,
      "grad_norm": 0.5878760814666748,
      "learning_rate": 2.2645833333333336e-05,
      "loss": 0.0014,
      "step": 65650
    },
    {
      "epoch": 4.3773333333333335,
      "grad_norm": 0.67225581407547,
      "learning_rate": 2.2641666666666667e-05,
      "loss": 0.0015,
      "step": 65660
    },
    {
      "epoch": 4.378,
      "grad_norm": 0.40786826610565186,
      "learning_rate": 2.26375e-05,
      "loss": 0.0023,
      "step": 65670
    },
    {
      "epoch": 4.378666666666667,
      "grad_norm": 0.31984296441078186,
      "learning_rate": 2.2633333333333336e-05,
      "loss": 0.002,
      "step": 65680
    },
    {
      "epoch": 4.379333333333333,
      "grad_norm": 0.1852838099002838,
      "learning_rate": 2.2629166666666667e-05,
      "loss": 0.0021,
      "step": 65690
    },
    {
      "epoch": 4.38,
      "grad_norm": 0.18269748985767365,
      "learning_rate": 2.2625e-05,
      "loss": 0.0016,
      "step": 65700
    },
    {
      "epoch": 4.3806666666666665,
      "grad_norm": 0.04835311695933342,
      "learning_rate": 2.2620833333333335e-05,
      "loss": 0.002,
      "step": 65710
    },
    {
      "epoch": 4.381333333333333,
      "grad_norm": 0.18036340177059174,
      "learning_rate": 2.2616666666666666e-05,
      "loss": 0.0016,
      "step": 65720
    },
    {
      "epoch": 4.382,
      "grad_norm": 0.10725166648626328,
      "learning_rate": 2.26125e-05,
      "loss": 0.0018,
      "step": 65730
    },
    {
      "epoch": 4.382666666666666,
      "grad_norm": 0.3145405948162079,
      "learning_rate": 2.2608333333333335e-05,
      "loss": 0.0013,
      "step": 65740
    },
    {
      "epoch": 4.383333333333334,
      "grad_norm": 0.22802089154720306,
      "learning_rate": 2.260416666666667e-05,
      "loss": 0.0037,
      "step": 65750
    },
    {
      "epoch": 4.384,
      "grad_norm": 0.05176793783903122,
      "learning_rate": 2.26e-05,
      "loss": 0.003,
      "step": 65760
    },
    {
      "epoch": 4.384666666666667,
      "grad_norm": 0.24407358467578888,
      "learning_rate": 2.2595833333333335e-05,
      "loss": 0.0025,
      "step": 65770
    },
    {
      "epoch": 4.3853333333333335,
      "grad_norm": 0.5332216024398804,
      "learning_rate": 2.2591666666666666e-05,
      "loss": 0.0013,
      "step": 65780
    },
    {
      "epoch": 4.386,
      "grad_norm": 0.14322170615196228,
      "learning_rate": 2.25875e-05,
      "loss": 0.0018,
      "step": 65790
    },
    {
      "epoch": 4.386666666666667,
      "grad_norm": 0.6333987712860107,
      "learning_rate": 2.2583333333333335e-05,
      "loss": 0.0017,
      "step": 65800
    },
    {
      "epoch": 4.387333333333333,
      "grad_norm": 0.11706911027431488,
      "learning_rate": 2.257916666666667e-05,
      "loss": 0.0024,
      "step": 65810
    },
    {
      "epoch": 4.388,
      "grad_norm": 0.0433896966278553,
      "learning_rate": 2.2575000000000003e-05,
      "loss": 0.0018,
      "step": 65820
    },
    {
      "epoch": 4.3886666666666665,
      "grad_norm": 0.7044200301170349,
      "learning_rate": 2.2570833333333334e-05,
      "loss": 0.0024,
      "step": 65830
    },
    {
      "epoch": 4.389333333333333,
      "grad_norm": 0.2158597707748413,
      "learning_rate": 2.2566666666666665e-05,
      "loss": 0.0025,
      "step": 65840
    },
    {
      "epoch": 4.39,
      "grad_norm": 0.18342751264572144,
      "learning_rate": 2.25625e-05,
      "loss": 0.0018,
      "step": 65850
    },
    {
      "epoch": 4.390666666666666,
      "grad_norm": 0.1583196371793747,
      "learning_rate": 2.2558333333333334e-05,
      "loss": 0.0018,
      "step": 65860
    },
    {
      "epoch": 4.391333333333334,
      "grad_norm": 0.5647451281547546,
      "learning_rate": 2.2554166666666668e-05,
      "loss": 0.0017,
      "step": 65870
    },
    {
      "epoch": 4.392,
      "grad_norm": 0.465447336435318,
      "learning_rate": 2.2550000000000003e-05,
      "loss": 0.0018,
      "step": 65880
    },
    {
      "epoch": 4.392666666666667,
      "grad_norm": 0.34007567167282104,
      "learning_rate": 2.2545833333333337e-05,
      "loss": 0.0013,
      "step": 65890
    },
    {
      "epoch": 4.3933333333333335,
      "grad_norm": 0.08210212737321854,
      "learning_rate": 2.2541666666666668e-05,
      "loss": 0.001,
      "step": 65900
    },
    {
      "epoch": 4.394,
      "grad_norm": 0.18706053495407104,
      "learning_rate": 2.25375e-05,
      "loss": 0.0013,
      "step": 65910
    },
    {
      "epoch": 4.394666666666667,
      "grad_norm": 0.18133461475372314,
      "learning_rate": 2.2533333333333333e-05,
      "loss": 0.0023,
      "step": 65920
    },
    {
      "epoch": 4.395333333333333,
      "grad_norm": 0.5619297027587891,
      "learning_rate": 2.2529166666666668e-05,
      "loss": 0.0027,
      "step": 65930
    },
    {
      "epoch": 4.396,
      "grad_norm": 0.1429317444562912,
      "learning_rate": 2.2525000000000002e-05,
      "loss": 0.0022,
      "step": 65940
    },
    {
      "epoch": 4.3966666666666665,
      "grad_norm": 0.4307805895805359,
      "learning_rate": 2.2520833333333336e-05,
      "loss": 0.0014,
      "step": 65950
    },
    {
      "epoch": 4.397333333333333,
      "grad_norm": 0.25072798132896423,
      "learning_rate": 2.2516666666666667e-05,
      "loss": 0.0023,
      "step": 65960
    },
    {
      "epoch": 4.398,
      "grad_norm": 0.24312429130077362,
      "learning_rate": 2.2512500000000002e-05,
      "loss": 0.0026,
      "step": 65970
    },
    {
      "epoch": 4.398666666666666,
      "grad_norm": 0.6088659167289734,
      "learning_rate": 2.2508333333333333e-05,
      "loss": 0.0017,
      "step": 65980
    },
    {
      "epoch": 4.399333333333333,
      "grad_norm": 0.2586532533168793,
      "learning_rate": 2.2504166666666667e-05,
      "loss": 0.0021,
      "step": 65990
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.07913108915090561,
      "learning_rate": 2.25e-05,
      "loss": 0.003,
      "step": 66000
    },
    {
      "epoch": 4.400666666666667,
      "grad_norm": 0.6834117770195007,
      "learning_rate": 2.2495833333333336e-05,
      "loss": 0.0016,
      "step": 66010
    },
    {
      "epoch": 4.4013333333333335,
      "grad_norm": 0.8871774077415466,
      "learning_rate": 2.2491666666666667e-05,
      "loss": 0.0024,
      "step": 66020
    },
    {
      "epoch": 4.402,
      "grad_norm": 0.11501442641019821,
      "learning_rate": 2.24875e-05,
      "loss": 0.0014,
      "step": 66030
    },
    {
      "epoch": 4.402666666666667,
      "grad_norm": 0.42620787024497986,
      "learning_rate": 2.2483333333333335e-05,
      "loss": 0.002,
      "step": 66040
    },
    {
      "epoch": 4.403333333333333,
      "grad_norm": 0.14486780762672424,
      "learning_rate": 2.2479166666666666e-05,
      "loss": 0.0021,
      "step": 66050
    },
    {
      "epoch": 4.404,
      "grad_norm": 0.07742939889431,
      "learning_rate": 2.2475e-05,
      "loss": 0.0019,
      "step": 66060
    },
    {
      "epoch": 4.4046666666666665,
      "grad_norm": 0.05893740430474281,
      "learning_rate": 2.2470833333333335e-05,
      "loss": 0.0034,
      "step": 66070
    },
    {
      "epoch": 4.405333333333333,
      "grad_norm": 0.18146763741970062,
      "learning_rate": 2.2466666666666666e-05,
      "loss": 0.0026,
      "step": 66080
    },
    {
      "epoch": 4.406,
      "grad_norm": 0.2888432443141937,
      "learning_rate": 2.24625e-05,
      "loss": 0.003,
      "step": 66090
    },
    {
      "epoch": 4.406666666666666,
      "grad_norm": 0.3656308948993683,
      "learning_rate": 2.2458333333333335e-05,
      "loss": 0.0017,
      "step": 66100
    },
    {
      "epoch": 4.407333333333334,
      "grad_norm": 0.5773069262504578,
      "learning_rate": 2.245416666666667e-05,
      "loss": 0.0017,
      "step": 66110
    },
    {
      "epoch": 4.408,
      "grad_norm": 0.1784917563199997,
      "learning_rate": 2.245e-05,
      "loss": 0.0026,
      "step": 66120
    },
    {
      "epoch": 4.408666666666667,
      "grad_norm": 0.12205927819013596,
      "learning_rate": 2.2445833333333335e-05,
      "loss": 0.0017,
      "step": 66130
    },
    {
      "epoch": 4.4093333333333335,
      "grad_norm": 0.6371548771858215,
      "learning_rate": 2.2441666666666666e-05,
      "loss": 0.0017,
      "step": 66140
    },
    {
      "epoch": 4.41,
      "grad_norm": 0.11112219095230103,
      "learning_rate": 2.24375e-05,
      "loss": 0.002,
      "step": 66150
    },
    {
      "epoch": 4.410666666666667,
      "grad_norm": 0.5264872908592224,
      "learning_rate": 2.2433333333333334e-05,
      "loss": 0.002,
      "step": 66160
    },
    {
      "epoch": 4.411333333333333,
      "grad_norm": 0.3910413980484009,
      "learning_rate": 2.242916666666667e-05,
      "loss": 0.0026,
      "step": 66170
    },
    {
      "epoch": 4.412,
      "grad_norm": 0.29122620820999146,
      "learning_rate": 2.2425000000000003e-05,
      "loss": 0.0023,
      "step": 66180
    },
    {
      "epoch": 4.4126666666666665,
      "grad_norm": 0.08073723316192627,
      "learning_rate": 2.2420833333333334e-05,
      "loss": 0.0015,
      "step": 66190
    },
    {
      "epoch": 4.413333333333333,
      "grad_norm": 0.7286444306373596,
      "learning_rate": 2.2416666666666665e-05,
      "loss": 0.0023,
      "step": 66200
    },
    {
      "epoch": 4.414,
      "grad_norm": 0.14355117082595825,
      "learning_rate": 2.24125e-05,
      "loss": 0.0028,
      "step": 66210
    },
    {
      "epoch": 4.414666666666666,
      "grad_norm": 0.07490258663892746,
      "learning_rate": 2.2408333333333334e-05,
      "loss": 0.0017,
      "step": 66220
    },
    {
      "epoch": 4.415333333333333,
      "grad_norm": 0.5329198241233826,
      "learning_rate": 2.2404166666666668e-05,
      "loss": 0.0023,
      "step": 66230
    },
    {
      "epoch": 4.416,
      "grad_norm": 0.7397656440734863,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 0.0015,
      "step": 66240
    },
    {
      "epoch": 4.416666666666667,
      "grad_norm": 0.6030086278915405,
      "learning_rate": 2.2395833333333337e-05,
      "loss": 0.0019,
      "step": 66250
    },
    {
      "epoch": 4.417333333333334,
      "grad_norm": 0.07106626778841019,
      "learning_rate": 2.2391666666666668e-05,
      "loss": 0.0022,
      "step": 66260
    },
    {
      "epoch": 4.418,
      "grad_norm": 0.4182429313659668,
      "learning_rate": 2.23875e-05,
      "loss": 0.0025,
      "step": 66270
    },
    {
      "epoch": 4.418666666666667,
      "grad_norm": 0.1480238437652588,
      "learning_rate": 2.2383333333333333e-05,
      "loss": 0.002,
      "step": 66280
    },
    {
      "epoch": 4.419333333333333,
      "grad_norm": 0.22125619649887085,
      "learning_rate": 2.2379166666666667e-05,
      "loss": 0.0018,
      "step": 66290
    },
    {
      "epoch": 4.42,
      "grad_norm": 0.5066942572593689,
      "learning_rate": 2.2375000000000002e-05,
      "loss": 0.0015,
      "step": 66300
    },
    {
      "epoch": 4.4206666666666665,
      "grad_norm": 0.24874016642570496,
      "learning_rate": 2.2370833333333336e-05,
      "loss": 0.0017,
      "step": 66310
    },
    {
      "epoch": 4.421333333333333,
      "grad_norm": 0.9502806663513184,
      "learning_rate": 2.236666666666667e-05,
      "loss": 0.0024,
      "step": 66320
    },
    {
      "epoch": 4.422,
      "grad_norm": 0.44891202449798584,
      "learning_rate": 2.23625e-05,
      "loss": 0.0021,
      "step": 66330
    },
    {
      "epoch": 4.422666666666666,
      "grad_norm": 0.07184814661741257,
      "learning_rate": 2.2358333333333332e-05,
      "loss": 0.0021,
      "step": 66340
    },
    {
      "epoch": 4.423333333333334,
      "grad_norm": 0.8437286019325256,
      "learning_rate": 2.2354166666666667e-05,
      "loss": 0.0016,
      "step": 66350
    },
    {
      "epoch": 4.424,
      "grad_norm": 0.5379262566566467,
      "learning_rate": 2.235e-05,
      "loss": 0.0041,
      "step": 66360
    },
    {
      "epoch": 4.424666666666667,
      "grad_norm": 0.21246671676635742,
      "learning_rate": 2.2345833333333336e-05,
      "loss": 0.0038,
      "step": 66370
    },
    {
      "epoch": 4.425333333333334,
      "grad_norm": 0.3922785222530365,
      "learning_rate": 2.234166666666667e-05,
      "loss": 0.0022,
      "step": 66380
    },
    {
      "epoch": 4.426,
      "grad_norm": 0.21397735178470612,
      "learning_rate": 2.23375e-05,
      "loss": 0.0024,
      "step": 66390
    },
    {
      "epoch": 4.426666666666667,
      "grad_norm": 0.357582151889801,
      "learning_rate": 2.2333333333333335e-05,
      "loss": 0.0021,
      "step": 66400
    },
    {
      "epoch": 4.427333333333333,
      "grad_norm": 0.314145028591156,
      "learning_rate": 2.2329166666666666e-05,
      "loss": 0.0019,
      "step": 66410
    },
    {
      "epoch": 4.428,
      "grad_norm": 0.4146984815597534,
      "learning_rate": 2.2325e-05,
      "loss": 0.0022,
      "step": 66420
    },
    {
      "epoch": 4.4286666666666665,
      "grad_norm": 0.7401895523071289,
      "learning_rate": 2.2320833333333335e-05,
      "loss": 0.0014,
      "step": 66430
    },
    {
      "epoch": 4.429333333333333,
      "grad_norm": 0.7929807305335999,
      "learning_rate": 2.231666666666667e-05,
      "loss": 0.0019,
      "step": 66440
    },
    {
      "epoch": 4.43,
      "grad_norm": 0.11398657411336899,
      "learning_rate": 2.23125e-05,
      "loss": 0.0016,
      "step": 66450
    },
    {
      "epoch": 4.430666666666666,
      "grad_norm": 0.7135383486747742,
      "learning_rate": 2.2308333333333335e-05,
      "loss": 0.0018,
      "step": 66460
    },
    {
      "epoch": 4.431333333333333,
      "grad_norm": 0.5206646919250488,
      "learning_rate": 2.230416666666667e-05,
      "loss": 0.0026,
      "step": 66470
    },
    {
      "epoch": 4.432,
      "grad_norm": 0.28459253907203674,
      "learning_rate": 2.23e-05,
      "loss": 0.0017,
      "step": 66480
    },
    {
      "epoch": 4.432666666666667,
      "grad_norm": 0.4606665372848511,
      "learning_rate": 2.2295833333333334e-05,
      "loss": 0.0014,
      "step": 66490
    },
    {
      "epoch": 4.433333333333334,
      "grad_norm": 0.24545273184776306,
      "learning_rate": 2.229166666666667e-05,
      "loss": 0.0018,
      "step": 66500
    },
    {
      "epoch": 4.434,
      "grad_norm": 0.42275291681289673,
      "learning_rate": 2.22875e-05,
      "loss": 0.0018,
      "step": 66510
    },
    {
      "epoch": 4.434666666666667,
      "grad_norm": 0.11713327467441559,
      "learning_rate": 2.2283333333333334e-05,
      "loss": 0.0016,
      "step": 66520
    },
    {
      "epoch": 4.435333333333333,
      "grad_norm": 0.2542775571346283,
      "learning_rate": 2.227916666666667e-05,
      "loss": 0.0021,
      "step": 66530
    },
    {
      "epoch": 4.436,
      "grad_norm": 0.04576566070318222,
      "learning_rate": 2.2275000000000003e-05,
      "loss": 0.0017,
      "step": 66540
    },
    {
      "epoch": 4.4366666666666665,
      "grad_norm": 0.45758724212646484,
      "learning_rate": 2.2270833333333334e-05,
      "loss": 0.0019,
      "step": 66550
    },
    {
      "epoch": 4.437333333333333,
      "grad_norm": 0.4155551791191101,
      "learning_rate": 2.2266666666666668e-05,
      "loss": 0.0029,
      "step": 66560
    },
    {
      "epoch": 4.438,
      "grad_norm": 0.3528190851211548,
      "learning_rate": 2.22625e-05,
      "loss": 0.0024,
      "step": 66570
    },
    {
      "epoch": 4.438666666666666,
      "grad_norm": 0.04265004023909569,
      "learning_rate": 2.2258333333333333e-05,
      "loss": 0.0017,
      "step": 66580
    },
    {
      "epoch": 4.439333333333334,
      "grad_norm": 0.18265631794929504,
      "learning_rate": 2.2254166666666668e-05,
      "loss": 0.002,
      "step": 66590
    },
    {
      "epoch": 4.44,
      "grad_norm": 0.2154078483581543,
      "learning_rate": 2.2250000000000002e-05,
      "loss": 0.0022,
      "step": 66600
    },
    {
      "epoch": 4.440666666666667,
      "grad_norm": 0.3210359513759613,
      "learning_rate": 2.2245833333333336e-05,
      "loss": 0.0014,
      "step": 66610
    },
    {
      "epoch": 4.441333333333334,
      "grad_norm": 0.34666624665260315,
      "learning_rate": 2.2241666666666667e-05,
      "loss": 0.0015,
      "step": 66620
    },
    {
      "epoch": 4.442,
      "grad_norm": 0.2580716907978058,
      "learning_rate": 2.22375e-05,
      "loss": 0.0016,
      "step": 66630
    },
    {
      "epoch": 4.442666666666667,
      "grad_norm": 0.08909866958856583,
      "learning_rate": 2.2233333333333333e-05,
      "loss": 0.0015,
      "step": 66640
    },
    {
      "epoch": 4.443333333333333,
      "grad_norm": 0.5287747979164124,
      "learning_rate": 2.2229166666666667e-05,
      "loss": 0.0025,
      "step": 66650
    },
    {
      "epoch": 4.444,
      "grad_norm": 0.3501206934452057,
      "learning_rate": 2.2225e-05,
      "loss": 0.0027,
      "step": 66660
    },
    {
      "epoch": 4.4446666666666665,
      "grad_norm": 0.06935904920101166,
      "learning_rate": 2.2220833333333336e-05,
      "loss": 0.0019,
      "step": 66670
    },
    {
      "epoch": 4.445333333333333,
      "grad_norm": 0.07855452597141266,
      "learning_rate": 2.221666666666667e-05,
      "loss": 0.0017,
      "step": 66680
    },
    {
      "epoch": 4.446,
      "grad_norm": 0.5891948938369751,
      "learning_rate": 2.22125e-05,
      "loss": 0.0024,
      "step": 66690
    },
    {
      "epoch": 4.446666666666666,
      "grad_norm": 0.8071298599243164,
      "learning_rate": 2.2208333333333332e-05,
      "loss": 0.0022,
      "step": 66700
    },
    {
      "epoch": 4.447333333333333,
      "grad_norm": 0.14176568388938904,
      "learning_rate": 2.2204166666666667e-05,
      "loss": 0.0018,
      "step": 66710
    },
    {
      "epoch": 4.448,
      "grad_norm": 0.7429777383804321,
      "learning_rate": 2.22e-05,
      "loss": 0.0023,
      "step": 66720
    },
    {
      "epoch": 4.448666666666667,
      "grad_norm": 0.38829505443573,
      "learning_rate": 2.2195833333333335e-05,
      "loss": 0.0018,
      "step": 66730
    },
    {
      "epoch": 4.449333333333334,
      "grad_norm": 0.24942798912525177,
      "learning_rate": 2.219166666666667e-05,
      "loss": 0.002,
      "step": 66740
    },
    {
      "epoch": 4.45,
      "grad_norm": 0.12011312693357468,
      "learning_rate": 2.21875e-05,
      "loss": 0.0019,
      "step": 66750
    },
    {
      "epoch": 4.450666666666667,
      "grad_norm": 0.6818978190422058,
      "learning_rate": 2.2183333333333335e-05,
      "loss": 0.0022,
      "step": 66760
    },
    {
      "epoch": 4.451333333333333,
      "grad_norm": 0.042800601571798325,
      "learning_rate": 2.2179166666666666e-05,
      "loss": 0.0022,
      "step": 66770
    },
    {
      "epoch": 4.452,
      "grad_norm": 0.28855276107788086,
      "learning_rate": 2.2175e-05,
      "loss": 0.0024,
      "step": 66780
    },
    {
      "epoch": 4.4526666666666666,
      "grad_norm": 0.6516331434249878,
      "learning_rate": 2.2170833333333335e-05,
      "loss": 0.0024,
      "step": 66790
    },
    {
      "epoch": 4.453333333333333,
      "grad_norm": 0.6186081767082214,
      "learning_rate": 2.216666666666667e-05,
      "loss": 0.0023,
      "step": 66800
    },
    {
      "epoch": 4.454,
      "grad_norm": 0.2906894087791443,
      "learning_rate": 2.21625e-05,
      "loss": 0.0018,
      "step": 66810
    },
    {
      "epoch": 4.454666666666666,
      "grad_norm": 0.40731146931648254,
      "learning_rate": 2.2158333333333334e-05,
      "loss": 0.0017,
      "step": 66820
    },
    {
      "epoch": 4.455333333333333,
      "grad_norm": 0.5050859451293945,
      "learning_rate": 2.215416666666667e-05,
      "loss": 0.0022,
      "step": 66830
    },
    {
      "epoch": 4.456,
      "grad_norm": 0.5299847722053528,
      "learning_rate": 2.215e-05,
      "loss": 0.0019,
      "step": 66840
    },
    {
      "epoch": 4.456666666666667,
      "grad_norm": 0.3546820282936096,
      "learning_rate": 2.2145833333333334e-05,
      "loss": 0.0015,
      "step": 66850
    },
    {
      "epoch": 4.457333333333334,
      "grad_norm": 0.279121994972229,
      "learning_rate": 2.214166666666667e-05,
      "loss": 0.002,
      "step": 66860
    },
    {
      "epoch": 4.458,
      "grad_norm": 0.32551655173301697,
      "learning_rate": 2.21375e-05,
      "loss": 0.0016,
      "step": 66870
    },
    {
      "epoch": 4.458666666666667,
      "grad_norm": 0.45842480659484863,
      "learning_rate": 2.2133333333333334e-05,
      "loss": 0.0026,
      "step": 66880
    },
    {
      "epoch": 4.459333333333333,
      "grad_norm": 0.6284292936325073,
      "learning_rate": 2.2129166666666668e-05,
      "loss": 0.0023,
      "step": 66890
    },
    {
      "epoch": 4.46,
      "grad_norm": 0.8194831013679504,
      "learning_rate": 2.2125000000000002e-05,
      "loss": 0.0028,
      "step": 66900
    },
    {
      "epoch": 4.460666666666667,
      "grad_norm": 0.4195970892906189,
      "learning_rate": 2.2120833333333333e-05,
      "loss": 0.002,
      "step": 66910
    },
    {
      "epoch": 4.461333333333333,
      "grad_norm": 0.1455598920583725,
      "learning_rate": 2.2116666666666668e-05,
      "loss": 0.0029,
      "step": 66920
    },
    {
      "epoch": 4.462,
      "grad_norm": 0.24616257846355438,
      "learning_rate": 2.21125e-05,
      "loss": 0.0027,
      "step": 66930
    },
    {
      "epoch": 4.462666666666666,
      "grad_norm": 0.029899798333644867,
      "learning_rate": 2.2108333333333333e-05,
      "loss": 0.0019,
      "step": 66940
    },
    {
      "epoch": 4.463333333333333,
      "grad_norm": 0.8465235233306885,
      "learning_rate": 2.2104166666666667e-05,
      "loss": 0.0018,
      "step": 66950
    },
    {
      "epoch": 4.464,
      "grad_norm": 1.0039608478546143,
      "learning_rate": 2.2100000000000002e-05,
      "loss": 0.0019,
      "step": 66960
    },
    {
      "epoch": 4.464666666666667,
      "grad_norm": 0.9739475846290588,
      "learning_rate": 2.2095833333333336e-05,
      "loss": 0.0019,
      "step": 66970
    },
    {
      "epoch": 4.465333333333334,
      "grad_norm": 0.21114490926265717,
      "learning_rate": 2.2091666666666667e-05,
      "loss": 0.0022,
      "step": 66980
    },
    {
      "epoch": 4.466,
      "grad_norm": 0.4569621980190277,
      "learning_rate": 2.2087499999999998e-05,
      "loss": 0.003,
      "step": 66990
    },
    {
      "epoch": 4.466666666666667,
      "grad_norm": 0.6119450330734253,
      "learning_rate": 2.2083333333333333e-05,
      "loss": 0.0021,
      "step": 67000
    },
    {
      "epoch": 4.467333333333333,
      "grad_norm": 0.7915737628936768,
      "learning_rate": 2.2079166666666667e-05,
      "loss": 0.0019,
      "step": 67010
    },
    {
      "epoch": 4.468,
      "grad_norm": 0.36764276027679443,
      "learning_rate": 2.2075e-05,
      "loss": 0.0022,
      "step": 67020
    },
    {
      "epoch": 4.468666666666667,
      "grad_norm": 0.0881696343421936,
      "learning_rate": 2.2070833333333336e-05,
      "loss": 0.0024,
      "step": 67030
    },
    {
      "epoch": 4.469333333333333,
      "grad_norm": 0.4219570755958557,
      "learning_rate": 2.206666666666667e-05,
      "loss": 0.0014,
      "step": 67040
    },
    {
      "epoch": 4.47,
      "grad_norm": 0.6354304552078247,
      "learning_rate": 2.20625e-05,
      "loss": 0.0021,
      "step": 67050
    },
    {
      "epoch": 4.470666666666666,
      "grad_norm": 0.6956749558448792,
      "learning_rate": 2.2058333333333335e-05,
      "loss": 0.0023,
      "step": 67060
    },
    {
      "epoch": 4.471333333333333,
      "grad_norm": 0.3518233299255371,
      "learning_rate": 2.2054166666666666e-05,
      "loss": 0.002,
      "step": 67070
    },
    {
      "epoch": 4.4719999999999995,
      "grad_norm": 0.047968581318855286,
      "learning_rate": 2.205e-05,
      "loss": 0.0014,
      "step": 67080
    },
    {
      "epoch": 4.472666666666667,
      "grad_norm": 0.206943079829216,
      "learning_rate": 2.2045833333333335e-05,
      "loss": 0.0014,
      "step": 67090
    },
    {
      "epoch": 4.473333333333334,
      "grad_norm": 0.28652074933052063,
      "learning_rate": 2.204166666666667e-05,
      "loss": 0.0015,
      "step": 67100
    },
    {
      "epoch": 4.474,
      "grad_norm": 0.5222396850585938,
      "learning_rate": 2.20375e-05,
      "loss": 0.0028,
      "step": 67110
    },
    {
      "epoch": 4.474666666666667,
      "grad_norm": 0.20627106726169586,
      "learning_rate": 2.2033333333333335e-05,
      "loss": 0.0022,
      "step": 67120
    },
    {
      "epoch": 4.475333333333333,
      "grad_norm": 0.17461827397346497,
      "learning_rate": 2.202916666666667e-05,
      "loss": 0.0028,
      "step": 67130
    },
    {
      "epoch": 4.476,
      "grad_norm": 0.3970247209072113,
      "learning_rate": 2.2025e-05,
      "loss": 0.0028,
      "step": 67140
    },
    {
      "epoch": 4.476666666666667,
      "grad_norm": 0.5237171053886414,
      "learning_rate": 2.2020833333333334e-05,
      "loss": 0.002,
      "step": 67150
    },
    {
      "epoch": 4.477333333333333,
      "grad_norm": 0.2113746702671051,
      "learning_rate": 2.201666666666667e-05,
      "loss": 0.0013,
      "step": 67160
    },
    {
      "epoch": 4.478,
      "grad_norm": 0.516420304775238,
      "learning_rate": 2.20125e-05,
      "loss": 0.0025,
      "step": 67170
    },
    {
      "epoch": 4.478666666666666,
      "grad_norm": 0.18379642069339752,
      "learning_rate": 2.2008333333333334e-05,
      "loss": 0.0012,
      "step": 67180
    },
    {
      "epoch": 4.479333333333333,
      "grad_norm": 0.3386915624141693,
      "learning_rate": 2.200416666666667e-05,
      "loss": 0.0019,
      "step": 67190
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.6241299510002136,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.0024,
      "step": 67200
    },
    {
      "epoch": 4.480666666666667,
      "grad_norm": 0.44567105174064636,
      "learning_rate": 2.1995833333333334e-05,
      "loss": 0.0028,
      "step": 67210
    },
    {
      "epoch": 4.481333333333334,
      "grad_norm": 0.14726856350898743,
      "learning_rate": 2.1991666666666668e-05,
      "loss": 0.0023,
      "step": 67220
    },
    {
      "epoch": 4.482,
      "grad_norm": 0.10504525899887085,
      "learning_rate": 2.19875e-05,
      "loss": 0.0019,
      "step": 67230
    },
    {
      "epoch": 4.482666666666667,
      "grad_norm": 0.28190356492996216,
      "learning_rate": 2.1983333333333333e-05,
      "loss": 0.0038,
      "step": 67240
    },
    {
      "epoch": 4.483333333333333,
      "grad_norm": 0.43542516231536865,
      "learning_rate": 2.1979166666666668e-05,
      "loss": 0.0013,
      "step": 67250
    },
    {
      "epoch": 4.484,
      "grad_norm": 0.0576665885746479,
      "learning_rate": 2.1975000000000002e-05,
      "loss": 0.0016,
      "step": 67260
    },
    {
      "epoch": 4.484666666666667,
      "grad_norm": 0.49084749817848206,
      "learning_rate": 2.1970833333333337e-05,
      "loss": 0.0025,
      "step": 67270
    },
    {
      "epoch": 4.485333333333333,
      "grad_norm": 0.3509286642074585,
      "learning_rate": 2.1966666666666668e-05,
      "loss": 0.0015,
      "step": 67280
    },
    {
      "epoch": 4.486,
      "grad_norm": 0.4712096154689789,
      "learning_rate": 2.19625e-05,
      "loss": 0.0023,
      "step": 67290
    },
    {
      "epoch": 4.486666666666666,
      "grad_norm": 0.139456644654274,
      "learning_rate": 2.1958333333333333e-05,
      "loss": 0.0015,
      "step": 67300
    },
    {
      "epoch": 4.487333333333333,
      "grad_norm": 0.05583319813013077,
      "learning_rate": 2.1954166666666667e-05,
      "loss": 0.002,
      "step": 67310
    },
    {
      "epoch": 4.4879999999999995,
      "grad_norm": 0.28132718801498413,
      "learning_rate": 2.195e-05,
      "loss": 0.0014,
      "step": 67320
    },
    {
      "epoch": 4.488666666666667,
      "grad_norm": 0.11770287156105042,
      "learning_rate": 2.1945833333333336e-05,
      "loss": 0.0021,
      "step": 67330
    },
    {
      "epoch": 4.489333333333334,
      "grad_norm": 0.14658169448375702,
      "learning_rate": 2.194166666666667e-05,
      "loss": 0.0021,
      "step": 67340
    },
    {
      "epoch": 4.49,
      "grad_norm": 0.11359124630689621,
      "learning_rate": 2.19375e-05,
      "loss": 0.0013,
      "step": 67350
    },
    {
      "epoch": 4.490666666666667,
      "grad_norm": 0.2935839593410492,
      "learning_rate": 2.1933333333333332e-05,
      "loss": 0.0028,
      "step": 67360
    },
    {
      "epoch": 4.491333333333333,
      "grad_norm": 0.0444253645837307,
      "learning_rate": 2.1929166666666667e-05,
      "loss": 0.0032,
      "step": 67370
    },
    {
      "epoch": 4.492,
      "grad_norm": 0.40069955587387085,
      "learning_rate": 2.1925e-05,
      "loss": 0.0015,
      "step": 67380
    },
    {
      "epoch": 4.492666666666667,
      "grad_norm": 0.32492348551750183,
      "learning_rate": 2.1920833333333335e-05,
      "loss": 0.002,
      "step": 67390
    },
    {
      "epoch": 4.493333333333333,
      "grad_norm": 0.049324654042720795,
      "learning_rate": 2.191666666666667e-05,
      "loss": 0.0015,
      "step": 67400
    },
    {
      "epoch": 4.494,
      "grad_norm": 0.3535342812538147,
      "learning_rate": 2.19125e-05,
      "loss": 0.0023,
      "step": 67410
    },
    {
      "epoch": 4.494666666666666,
      "grad_norm": 0.1192796379327774,
      "learning_rate": 2.1908333333333335e-05,
      "loss": 0.0014,
      "step": 67420
    },
    {
      "epoch": 4.495333333333333,
      "grad_norm": 0.04805748909711838,
      "learning_rate": 2.1904166666666666e-05,
      "loss": 0.0018,
      "step": 67430
    },
    {
      "epoch": 4.496,
      "grad_norm": 0.4908379316329956,
      "learning_rate": 2.19e-05,
      "loss": 0.0024,
      "step": 67440
    },
    {
      "epoch": 4.496666666666667,
      "grad_norm": 0.14150668680667877,
      "learning_rate": 2.1895833333333335e-05,
      "loss": 0.0014,
      "step": 67450
    },
    {
      "epoch": 4.497333333333334,
      "grad_norm": 0.34632784128189087,
      "learning_rate": 2.189166666666667e-05,
      "loss": 0.0015,
      "step": 67460
    },
    {
      "epoch": 4.498,
      "grad_norm": 0.3891047239303589,
      "learning_rate": 2.18875e-05,
      "loss": 0.0025,
      "step": 67470
    },
    {
      "epoch": 4.498666666666667,
      "grad_norm": 0.2532798647880554,
      "learning_rate": 2.1883333333333334e-05,
      "loss": 0.0018,
      "step": 67480
    },
    {
      "epoch": 4.499333333333333,
      "grad_norm": 0.14448226988315582,
      "learning_rate": 2.187916666666667e-05,
      "loss": 0.0019,
      "step": 67490
    },
    {
      "epoch": 4.5,
      "grad_norm": 0.04489125311374664,
      "learning_rate": 2.1875e-05,
      "loss": 0.0014,
      "step": 67500
    },
    {
      "epoch": 4.500666666666667,
      "grad_norm": 0.4221649467945099,
      "learning_rate": 2.1870833333333334e-05,
      "loss": 0.0026,
      "step": 67510
    },
    {
      "epoch": 4.501333333333333,
      "grad_norm": 0.5867124199867249,
      "learning_rate": 2.186666666666667e-05,
      "loss": 0.0018,
      "step": 67520
    },
    {
      "epoch": 4.502,
      "grad_norm": 0.3949173390865326,
      "learning_rate": 2.1862500000000003e-05,
      "loss": 0.0015,
      "step": 67530
    },
    {
      "epoch": 4.502666666666666,
      "grad_norm": 0.7018856406211853,
      "learning_rate": 2.1858333333333334e-05,
      "loss": 0.0012,
      "step": 67540
    },
    {
      "epoch": 4.503333333333333,
      "grad_norm": 0.3799908757209778,
      "learning_rate": 2.1854166666666668e-05,
      "loss": 0.0014,
      "step": 67550
    },
    {
      "epoch": 4.504,
      "grad_norm": 1.0146573781967163,
      "learning_rate": 2.1850000000000003e-05,
      "loss": 0.0014,
      "step": 67560
    },
    {
      "epoch": 4.504666666666667,
      "grad_norm": 0.4965851902961731,
      "learning_rate": 2.1845833333333334e-05,
      "loss": 0.0025,
      "step": 67570
    },
    {
      "epoch": 4.505333333333334,
      "grad_norm": 0.04525553435087204,
      "learning_rate": 2.1841666666666668e-05,
      "loss": 0.0015,
      "step": 67580
    },
    {
      "epoch": 4.506,
      "grad_norm": 0.0514458492398262,
      "learning_rate": 2.1837500000000002e-05,
      "loss": 0.0019,
      "step": 67590
    },
    {
      "epoch": 4.506666666666667,
      "grad_norm": 0.10543103516101837,
      "learning_rate": 2.1833333333333333e-05,
      "loss": 0.0026,
      "step": 67600
    },
    {
      "epoch": 4.507333333333333,
      "grad_norm": 0.3531632423400879,
      "learning_rate": 2.1829166666666668e-05,
      "loss": 0.0024,
      "step": 67610
    },
    {
      "epoch": 4.508,
      "grad_norm": 0.31349438428878784,
      "learning_rate": 2.1825000000000002e-05,
      "loss": 0.0021,
      "step": 67620
    },
    {
      "epoch": 4.508666666666667,
      "grad_norm": 0.03389029949903488,
      "learning_rate": 2.1820833333333336e-05,
      "loss": 0.002,
      "step": 67630
    },
    {
      "epoch": 4.509333333333333,
      "grad_norm": 0.13806554675102234,
      "learning_rate": 2.1816666666666667e-05,
      "loss": 0.0018,
      "step": 67640
    },
    {
      "epoch": 4.51,
      "grad_norm": 0.5198843479156494,
      "learning_rate": 2.18125e-05,
      "loss": 0.0019,
      "step": 67650
    },
    {
      "epoch": 4.510666666666666,
      "grad_norm": 0.35186368227005005,
      "learning_rate": 2.1808333333333333e-05,
      "loss": 0.0027,
      "step": 67660
    },
    {
      "epoch": 4.511333333333333,
      "grad_norm": 0.3217867314815521,
      "learning_rate": 2.1804166666666667e-05,
      "loss": 0.0018,
      "step": 67670
    },
    {
      "epoch": 4.5120000000000005,
      "grad_norm": 0.11237578094005585,
      "learning_rate": 2.18e-05,
      "loss": 0.0017,
      "step": 67680
    },
    {
      "epoch": 4.512666666666667,
      "grad_norm": 0.3211643397808075,
      "learning_rate": 2.1795833333333336e-05,
      "loss": 0.0024,
      "step": 67690
    },
    {
      "epoch": 4.513333333333334,
      "grad_norm": 0.21482042968273163,
      "learning_rate": 2.179166666666667e-05,
      "loss": 0.0022,
      "step": 67700
    },
    {
      "epoch": 4.514,
      "grad_norm": 0.3128397762775421,
      "learning_rate": 2.17875e-05,
      "loss": 0.0013,
      "step": 67710
    },
    {
      "epoch": 4.514666666666667,
      "grad_norm": 0.07907774299383163,
      "learning_rate": 2.1783333333333332e-05,
      "loss": 0.0015,
      "step": 67720
    },
    {
      "epoch": 4.515333333333333,
      "grad_norm": 0.08909633755683899,
      "learning_rate": 2.1779166666666666e-05,
      "loss": 0.0025,
      "step": 67730
    },
    {
      "epoch": 4.516,
      "grad_norm": 0.5927252173423767,
      "learning_rate": 2.1775e-05,
      "loss": 0.0019,
      "step": 67740
    },
    {
      "epoch": 4.516666666666667,
      "grad_norm": 0.15023422241210938,
      "learning_rate": 2.1770833333333335e-05,
      "loss": 0.0022,
      "step": 67750
    },
    {
      "epoch": 4.517333333333333,
      "grad_norm": 0.4225810468196869,
      "learning_rate": 2.176666666666667e-05,
      "loss": 0.0022,
      "step": 67760
    },
    {
      "epoch": 4.518,
      "grad_norm": 0.1713615208864212,
      "learning_rate": 2.1762500000000004e-05,
      "loss": 0.0019,
      "step": 67770
    },
    {
      "epoch": 4.518666666666666,
      "grad_norm": 0.11116145551204681,
      "learning_rate": 2.1758333333333335e-05,
      "loss": 0.0019,
      "step": 67780
    },
    {
      "epoch": 4.519333333333333,
      "grad_norm": 0.18262144923210144,
      "learning_rate": 2.1754166666666666e-05,
      "loss": 0.0021,
      "step": 67790
    },
    {
      "epoch": 4.52,
      "grad_norm": 0.26235711574554443,
      "learning_rate": 2.175e-05,
      "loss": 0.0015,
      "step": 67800
    },
    {
      "epoch": 4.520666666666667,
      "grad_norm": 0.5246598720550537,
      "learning_rate": 2.1745833333333334e-05,
      "loss": 0.0021,
      "step": 67810
    },
    {
      "epoch": 4.521333333333334,
      "grad_norm": 0.2538929879665375,
      "learning_rate": 2.174166666666667e-05,
      "loss": 0.0015,
      "step": 67820
    },
    {
      "epoch": 4.522,
      "grad_norm": 0.20758754014968872,
      "learning_rate": 2.1737500000000003e-05,
      "loss": 0.0022,
      "step": 67830
    },
    {
      "epoch": 4.522666666666667,
      "grad_norm": 0.525898277759552,
      "learning_rate": 2.1733333333333334e-05,
      "loss": 0.0021,
      "step": 67840
    },
    {
      "epoch": 4.523333333333333,
      "grad_norm": 0.5978204011917114,
      "learning_rate": 2.172916666666667e-05,
      "loss": 0.0013,
      "step": 67850
    },
    {
      "epoch": 4.524,
      "grad_norm": 0.17379595339298248,
      "learning_rate": 2.1725e-05,
      "loss": 0.002,
      "step": 67860
    },
    {
      "epoch": 4.524666666666667,
      "grad_norm": 0.06786876916885376,
      "learning_rate": 2.1720833333333334e-05,
      "loss": 0.0035,
      "step": 67870
    },
    {
      "epoch": 4.525333333333333,
      "grad_norm": 0.4198438823223114,
      "learning_rate": 2.1716666666666668e-05,
      "loss": 0.0024,
      "step": 67880
    },
    {
      "epoch": 4.526,
      "grad_norm": 0.6688812971115112,
      "learning_rate": 2.1712500000000003e-05,
      "loss": 0.0015,
      "step": 67890
    },
    {
      "epoch": 4.526666666666666,
      "grad_norm": 0.04058882221579552,
      "learning_rate": 2.1708333333333334e-05,
      "loss": 0.0014,
      "step": 67900
    },
    {
      "epoch": 4.527333333333333,
      "grad_norm": 0.4240618348121643,
      "learning_rate": 2.1704166666666668e-05,
      "loss": 0.0015,
      "step": 67910
    },
    {
      "epoch": 4.5280000000000005,
      "grad_norm": 1.0013298988342285,
      "learning_rate": 2.1700000000000002e-05,
      "loss": 0.0015,
      "step": 67920
    },
    {
      "epoch": 4.528666666666666,
      "grad_norm": 0.6257427930831909,
      "learning_rate": 2.1695833333333333e-05,
      "loss": 0.0021,
      "step": 67930
    },
    {
      "epoch": 4.529333333333334,
      "grad_norm": 0.2141554355621338,
      "learning_rate": 2.1691666666666668e-05,
      "loss": 0.0027,
      "step": 67940
    },
    {
      "epoch": 4.53,
      "grad_norm": 0.5460401177406311,
      "learning_rate": 2.1687500000000002e-05,
      "loss": 0.0033,
      "step": 67950
    },
    {
      "epoch": 4.530666666666667,
      "grad_norm": 0.5861421823501587,
      "learning_rate": 2.1683333333333333e-05,
      "loss": 0.0012,
      "step": 67960
    },
    {
      "epoch": 4.531333333333333,
      "grad_norm": 0.15170259773731232,
      "learning_rate": 2.1679166666666667e-05,
      "loss": 0.0026,
      "step": 67970
    },
    {
      "epoch": 4.532,
      "grad_norm": 0.24853920936584473,
      "learning_rate": 2.1675e-05,
      "loss": 0.0018,
      "step": 67980
    },
    {
      "epoch": 4.532666666666667,
      "grad_norm": 0.24754756689071655,
      "learning_rate": 2.1670833333333336e-05,
      "loss": 0.0025,
      "step": 67990
    },
    {
      "epoch": 4.533333333333333,
      "grad_norm": 0.08932452648878098,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 0.0022,
      "step": 68000
    },
    {
      "epoch": 4.534,
      "grad_norm": 0.1111711636185646,
      "learning_rate": 2.16625e-05,
      "loss": 0.0037,
      "step": 68010
    },
    {
      "epoch": 4.534666666666666,
      "grad_norm": 0.7345277667045593,
      "learning_rate": 2.1658333333333332e-05,
      "loss": 0.0016,
      "step": 68020
    },
    {
      "epoch": 4.535333333333333,
      "grad_norm": 0.5810695886611938,
      "learning_rate": 2.1654166666666667e-05,
      "loss": 0.0022,
      "step": 68030
    },
    {
      "epoch": 4.536,
      "grad_norm": 0.4609566628932953,
      "learning_rate": 2.165e-05,
      "loss": 0.0027,
      "step": 68040
    },
    {
      "epoch": 4.536666666666667,
      "grad_norm": 0.038500577211380005,
      "learning_rate": 2.1645833333333335e-05,
      "loss": 0.0031,
      "step": 68050
    },
    {
      "epoch": 4.537333333333334,
      "grad_norm": 0.2206299901008606,
      "learning_rate": 2.164166666666667e-05,
      "loss": 0.0022,
      "step": 68060
    },
    {
      "epoch": 4.538,
      "grad_norm": 0.2830336093902588,
      "learning_rate": 2.16375e-05,
      "loss": 0.0033,
      "step": 68070
    },
    {
      "epoch": 4.538666666666667,
      "grad_norm": 0.24121849238872528,
      "learning_rate": 2.1633333333333332e-05,
      "loss": 0.002,
      "step": 68080
    },
    {
      "epoch": 4.539333333333333,
      "grad_norm": 0.3198026120662689,
      "learning_rate": 2.1629166666666666e-05,
      "loss": 0.0017,
      "step": 68090
    },
    {
      "epoch": 4.54,
      "grad_norm": 0.08405563235282898,
      "learning_rate": 2.1625e-05,
      "loss": 0.0015,
      "step": 68100
    },
    {
      "epoch": 4.540666666666667,
      "grad_norm": 0.45021748542785645,
      "learning_rate": 2.1620833333333335e-05,
      "loss": 0.0015,
      "step": 68110
    },
    {
      "epoch": 4.541333333333333,
      "grad_norm": 0.11681470274925232,
      "learning_rate": 2.161666666666667e-05,
      "loss": 0.0023,
      "step": 68120
    },
    {
      "epoch": 4.542,
      "grad_norm": 0.12014120817184448,
      "learning_rate": 2.1612500000000004e-05,
      "loss": 0.0023,
      "step": 68130
    },
    {
      "epoch": 4.542666666666666,
      "grad_norm": 0.07914316654205322,
      "learning_rate": 2.1608333333333335e-05,
      "loss": 0.0024,
      "step": 68140
    },
    {
      "epoch": 4.543333333333333,
      "grad_norm": 0.5367917418479919,
      "learning_rate": 2.1604166666666666e-05,
      "loss": 0.0022,
      "step": 68150
    },
    {
      "epoch": 4.5440000000000005,
      "grad_norm": 0.30926719307899475,
      "learning_rate": 2.16e-05,
      "loss": 0.0018,
      "step": 68160
    },
    {
      "epoch": 4.544666666666666,
      "grad_norm": 0.5238626599311829,
      "learning_rate": 2.1595833333333334e-05,
      "loss": 0.0017,
      "step": 68170
    },
    {
      "epoch": 4.545333333333334,
      "grad_norm": 0.35523369908332825,
      "learning_rate": 2.159166666666667e-05,
      "loss": 0.0016,
      "step": 68180
    },
    {
      "epoch": 4.546,
      "grad_norm": 0.7699570059776306,
      "learning_rate": 2.1587500000000003e-05,
      "loss": 0.0016,
      "step": 68190
    },
    {
      "epoch": 4.546666666666667,
      "grad_norm": 0.300608366727829,
      "learning_rate": 2.1583333333333334e-05,
      "loss": 0.0031,
      "step": 68200
    },
    {
      "epoch": 4.5473333333333334,
      "grad_norm": 0.4768383800983429,
      "learning_rate": 2.1579166666666668e-05,
      "loss": 0.0018,
      "step": 68210
    },
    {
      "epoch": 4.548,
      "grad_norm": 0.18058174848556519,
      "learning_rate": 2.1575e-05,
      "loss": 0.002,
      "step": 68220
    },
    {
      "epoch": 4.548666666666667,
      "grad_norm": 0.21846014261245728,
      "learning_rate": 2.1570833333333334e-05,
      "loss": 0.0029,
      "step": 68230
    },
    {
      "epoch": 4.549333333333333,
      "grad_norm": 0.17791981995105743,
      "learning_rate": 2.1566666666666668e-05,
      "loss": 0.0014,
      "step": 68240
    },
    {
      "epoch": 4.55,
      "grad_norm": 0.618409276008606,
      "learning_rate": 2.1562500000000002e-05,
      "loss": 0.0019,
      "step": 68250
    },
    {
      "epoch": 4.550666666666666,
      "grad_norm": 0.24326607584953308,
      "learning_rate": 2.1558333333333333e-05,
      "loss": 0.0015,
      "step": 68260
    },
    {
      "epoch": 4.551333333333333,
      "grad_norm": 0.34983646869659424,
      "learning_rate": 2.1554166666666668e-05,
      "loss": 0.0022,
      "step": 68270
    },
    {
      "epoch": 4.552,
      "grad_norm": 0.5052183270454407,
      "learning_rate": 2.1550000000000002e-05,
      "loss": 0.0018,
      "step": 68280
    },
    {
      "epoch": 4.552666666666667,
      "grad_norm": 0.14014844596385956,
      "learning_rate": 2.1545833333333333e-05,
      "loss": 0.0023,
      "step": 68290
    },
    {
      "epoch": 4.553333333333334,
      "grad_norm": 0.09495556354522705,
      "learning_rate": 2.1541666666666667e-05,
      "loss": 0.0024,
      "step": 68300
    },
    {
      "epoch": 4.554,
      "grad_norm": 0.24635247886180878,
      "learning_rate": 2.1537500000000002e-05,
      "loss": 0.0027,
      "step": 68310
    },
    {
      "epoch": 4.554666666666667,
      "grad_norm": 0.5603087544441223,
      "learning_rate": 2.1533333333333333e-05,
      "loss": 0.0021,
      "step": 68320
    },
    {
      "epoch": 4.5553333333333335,
      "grad_norm": 0.24877652525901794,
      "learning_rate": 2.1529166666666667e-05,
      "loss": 0.0013,
      "step": 68330
    },
    {
      "epoch": 4.556,
      "grad_norm": 0.3565889596939087,
      "learning_rate": 2.1525e-05,
      "loss": 0.0023,
      "step": 68340
    },
    {
      "epoch": 4.556666666666667,
      "grad_norm": 0.5197141766548157,
      "learning_rate": 2.1520833333333336e-05,
      "loss": 0.0023,
      "step": 68350
    },
    {
      "epoch": 4.557333333333333,
      "grad_norm": 0.35137277841567993,
      "learning_rate": 2.1516666666666667e-05,
      "loss": 0.0017,
      "step": 68360
    },
    {
      "epoch": 4.558,
      "grad_norm": 0.32628750801086426,
      "learning_rate": 2.15125e-05,
      "loss": 0.002,
      "step": 68370
    },
    {
      "epoch": 4.558666666666666,
      "grad_norm": 0.01950696110725403,
      "learning_rate": 2.1508333333333332e-05,
      "loss": 0.003,
      "step": 68380
    },
    {
      "epoch": 4.559333333333333,
      "grad_norm": 0.08611510694026947,
      "learning_rate": 2.1504166666666666e-05,
      "loss": 0.0014,
      "step": 68390
    },
    {
      "epoch": 4.5600000000000005,
      "grad_norm": 0.4618869721889496,
      "learning_rate": 2.15e-05,
      "loss": 0.0018,
      "step": 68400
    },
    {
      "epoch": 4.560666666666666,
      "grad_norm": 0.48029467463493347,
      "learning_rate": 2.1495833333333335e-05,
      "loss": 0.0013,
      "step": 68410
    },
    {
      "epoch": 4.561333333333334,
      "grad_norm": 0.35292404890060425,
      "learning_rate": 2.149166666666667e-05,
      "loss": 0.0024,
      "step": 68420
    },
    {
      "epoch": 4.562,
      "grad_norm": 0.04165550693869591,
      "learning_rate": 2.14875e-05,
      "loss": 0.0015,
      "step": 68430
    },
    {
      "epoch": 4.562666666666667,
      "grad_norm": 0.18516339361667633,
      "learning_rate": 2.148333333333333e-05,
      "loss": 0.0023,
      "step": 68440
    },
    {
      "epoch": 4.5633333333333335,
      "grad_norm": 0.3567133843898773,
      "learning_rate": 2.1479166666666666e-05,
      "loss": 0.0025,
      "step": 68450
    },
    {
      "epoch": 4.564,
      "grad_norm": 0.05078157037496567,
      "learning_rate": 2.1475e-05,
      "loss": 0.0019,
      "step": 68460
    },
    {
      "epoch": 4.564666666666667,
      "grad_norm": 0.2149488478899002,
      "learning_rate": 2.1470833333333335e-05,
      "loss": 0.0018,
      "step": 68470
    },
    {
      "epoch": 4.565333333333333,
      "grad_norm": 0.41852277517318726,
      "learning_rate": 2.146666666666667e-05,
      "loss": 0.0016,
      "step": 68480
    },
    {
      "epoch": 4.566,
      "grad_norm": 0.11327153444290161,
      "learning_rate": 2.1462500000000003e-05,
      "loss": 0.0026,
      "step": 68490
    },
    {
      "epoch": 4.566666666666666,
      "grad_norm": 0.13970434665679932,
      "learning_rate": 2.1458333333333334e-05,
      "loss": 0.0017,
      "step": 68500
    },
    {
      "epoch": 4.567333333333333,
      "grad_norm": 0.21088482439517975,
      "learning_rate": 2.1454166666666665e-05,
      "loss": 0.0026,
      "step": 68510
    },
    {
      "epoch": 4.568,
      "grad_norm": 0.1784154623746872,
      "learning_rate": 2.145e-05,
      "loss": 0.0023,
      "step": 68520
    },
    {
      "epoch": 4.568666666666667,
      "grad_norm": 0.45422056317329407,
      "learning_rate": 2.1445833333333334e-05,
      "loss": 0.0027,
      "step": 68530
    },
    {
      "epoch": 4.569333333333334,
      "grad_norm": 0.18151350319385529,
      "learning_rate": 2.144166666666667e-05,
      "loss": 0.0017,
      "step": 68540
    },
    {
      "epoch": 4.57,
      "grad_norm": 0.21046102046966553,
      "learning_rate": 2.1437500000000003e-05,
      "loss": 0.0019,
      "step": 68550
    },
    {
      "epoch": 4.570666666666667,
      "grad_norm": 0.3527584373950958,
      "learning_rate": 2.1433333333333334e-05,
      "loss": 0.0015,
      "step": 68560
    },
    {
      "epoch": 4.5713333333333335,
      "grad_norm": 0.48680296540260315,
      "learning_rate": 2.1429166666666668e-05,
      "loss": 0.0024,
      "step": 68570
    },
    {
      "epoch": 4.572,
      "grad_norm": 0.74591064453125,
      "learning_rate": 2.1425e-05,
      "loss": 0.002,
      "step": 68580
    },
    {
      "epoch": 4.572666666666667,
      "grad_norm": 0.3315977156162262,
      "learning_rate": 2.1420833333333333e-05,
      "loss": 0.002,
      "step": 68590
    },
    {
      "epoch": 4.573333333333333,
      "grad_norm": 0.10771480202674866,
      "learning_rate": 2.1416666666666668e-05,
      "loss": 0.002,
      "step": 68600
    },
    {
      "epoch": 4.574,
      "grad_norm": 0.020374171435832977,
      "learning_rate": 2.1412500000000002e-05,
      "loss": 0.0039,
      "step": 68610
    },
    {
      "epoch": 4.574666666666666,
      "grad_norm": 0.4613399803638458,
      "learning_rate": 2.1408333333333333e-05,
      "loss": 0.0021,
      "step": 68620
    },
    {
      "epoch": 4.575333333333333,
      "grad_norm": 0.18016952276229858,
      "learning_rate": 2.1404166666666667e-05,
      "loss": 0.0017,
      "step": 68630
    },
    {
      "epoch": 4.576,
      "grad_norm": 0.12998053431510925,
      "learning_rate": 2.1400000000000002e-05,
      "loss": 0.002,
      "step": 68640
    },
    {
      "epoch": 4.576666666666666,
      "grad_norm": 0.35371309518814087,
      "learning_rate": 2.1395833333333333e-05,
      "loss": 0.0021,
      "step": 68650
    },
    {
      "epoch": 4.577333333333334,
      "grad_norm": 0.5673923492431641,
      "learning_rate": 2.1391666666666667e-05,
      "loss": 0.0017,
      "step": 68660
    },
    {
      "epoch": 4.578,
      "grad_norm": 0.6416381597518921,
      "learning_rate": 2.13875e-05,
      "loss": 0.0022,
      "step": 68670
    },
    {
      "epoch": 4.578666666666667,
      "grad_norm": 0.11012227088212967,
      "learning_rate": 2.1383333333333332e-05,
      "loss": 0.0018,
      "step": 68680
    },
    {
      "epoch": 4.5793333333333335,
      "grad_norm": 0.9048365950584412,
      "learning_rate": 2.1379166666666667e-05,
      "loss": 0.0021,
      "step": 68690
    },
    {
      "epoch": 4.58,
      "grad_norm": 0.8804258108139038,
      "learning_rate": 2.1375e-05,
      "loss": 0.0027,
      "step": 68700
    },
    {
      "epoch": 4.580666666666667,
      "grad_norm": 0.10892817378044128,
      "learning_rate": 2.1370833333333336e-05,
      "loss": 0.0016,
      "step": 68710
    },
    {
      "epoch": 4.581333333333333,
      "grad_norm": 0.4008343517780304,
      "learning_rate": 2.1366666666666667e-05,
      "loss": 0.0015,
      "step": 68720
    },
    {
      "epoch": 4.582,
      "grad_norm": 0.20896974205970764,
      "learning_rate": 2.13625e-05,
      "loss": 0.0019,
      "step": 68730
    },
    {
      "epoch": 4.582666666666666,
      "grad_norm": 0.2589515745639801,
      "learning_rate": 2.1358333333333332e-05,
      "loss": 0.0017,
      "step": 68740
    },
    {
      "epoch": 4.583333333333333,
      "grad_norm": 0.4634063243865967,
      "learning_rate": 2.1354166666666666e-05,
      "loss": 0.002,
      "step": 68750
    },
    {
      "epoch": 4.584,
      "grad_norm": 0.36020025610923767,
      "learning_rate": 2.135e-05,
      "loss": 0.0019,
      "step": 68760
    },
    {
      "epoch": 4.584666666666667,
      "grad_norm": 0.5560032725334167,
      "learning_rate": 2.1345833333333335e-05,
      "loss": 0.0024,
      "step": 68770
    },
    {
      "epoch": 4.585333333333334,
      "grad_norm": 0.14232859015464783,
      "learning_rate": 2.134166666666667e-05,
      "loss": 0.0017,
      "step": 68780
    },
    {
      "epoch": 4.586,
      "grad_norm": 0.06158261373639107,
      "learning_rate": 2.13375e-05,
      "loss": 0.0019,
      "step": 68790
    },
    {
      "epoch": 4.586666666666667,
      "grad_norm": 0.14388121664524078,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 0.0019,
      "step": 68800
    },
    {
      "epoch": 4.5873333333333335,
      "grad_norm": 0.0537412166595459,
      "learning_rate": 2.1329166666666666e-05,
      "loss": 0.0019,
      "step": 68810
    },
    {
      "epoch": 4.588,
      "grad_norm": 0.8072413206100464,
      "learning_rate": 2.1325e-05,
      "loss": 0.002,
      "step": 68820
    },
    {
      "epoch": 4.588666666666667,
      "grad_norm": 0.7474894523620605,
      "learning_rate": 2.1320833333333334e-05,
      "loss": 0.0021,
      "step": 68830
    },
    {
      "epoch": 4.589333333333333,
      "grad_norm": 0.24744270741939545,
      "learning_rate": 2.131666666666667e-05,
      "loss": 0.0018,
      "step": 68840
    },
    {
      "epoch": 4.59,
      "grad_norm": 0.036823637783527374,
      "learning_rate": 2.1312500000000003e-05,
      "loss": 0.0014,
      "step": 68850
    },
    {
      "epoch": 4.5906666666666665,
      "grad_norm": 0.21229338645935059,
      "learning_rate": 2.1308333333333334e-05,
      "loss": 0.0026,
      "step": 68860
    },
    {
      "epoch": 4.591333333333333,
      "grad_norm": 0.18449793756008148,
      "learning_rate": 2.130416666666667e-05,
      "loss": 0.003,
      "step": 68870
    },
    {
      "epoch": 4.592,
      "grad_norm": 0.07453647255897522,
      "learning_rate": 2.13e-05,
      "loss": 0.0028,
      "step": 68880
    },
    {
      "epoch": 4.592666666666666,
      "grad_norm": 0.5697202682495117,
      "learning_rate": 2.1295833333333334e-05,
      "loss": 0.0018,
      "step": 68890
    },
    {
      "epoch": 4.593333333333334,
      "grad_norm": 0.42566680908203125,
      "learning_rate": 2.1291666666666668e-05,
      "loss": 0.0023,
      "step": 68900
    },
    {
      "epoch": 4.594,
      "grad_norm": 0.10714618861675262,
      "learning_rate": 2.1287500000000002e-05,
      "loss": 0.002,
      "step": 68910
    },
    {
      "epoch": 4.594666666666667,
      "grad_norm": 0.21644338965415955,
      "learning_rate": 2.1283333333333337e-05,
      "loss": 0.0019,
      "step": 68920
    },
    {
      "epoch": 4.5953333333333335,
      "grad_norm": 0.31381499767303467,
      "learning_rate": 2.1279166666666668e-05,
      "loss": 0.0023,
      "step": 68930
    },
    {
      "epoch": 4.596,
      "grad_norm": 0.2524329125881195,
      "learning_rate": 2.1275000000000002e-05,
      "loss": 0.0015,
      "step": 68940
    },
    {
      "epoch": 4.596666666666667,
      "grad_norm": 0.04659733176231384,
      "learning_rate": 2.1270833333333333e-05,
      "loss": 0.0017,
      "step": 68950
    },
    {
      "epoch": 4.597333333333333,
      "grad_norm": 0.12063872814178467,
      "learning_rate": 2.1266666666666667e-05,
      "loss": 0.0014,
      "step": 68960
    },
    {
      "epoch": 4.598,
      "grad_norm": 0.040693048387765884,
      "learning_rate": 2.1262500000000002e-05,
      "loss": 0.002,
      "step": 68970
    },
    {
      "epoch": 4.5986666666666665,
      "grad_norm": 0.17769895493984222,
      "learning_rate": 2.1258333333333336e-05,
      "loss": 0.0024,
      "step": 68980
    },
    {
      "epoch": 4.599333333333333,
      "grad_norm": 0.03756546974182129,
      "learning_rate": 2.1254166666666667e-05,
      "loss": 0.0026,
      "step": 68990
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.5862587094306946,
      "learning_rate": 2.125e-05,
      "loss": 0.0021,
      "step": 69000
    },
    {
      "epoch": 4.600666666666667,
      "grad_norm": 0.08835846185684204,
      "learning_rate": 2.1245833333333336e-05,
      "loss": 0.002,
      "step": 69010
    },
    {
      "epoch": 4.601333333333334,
      "grad_norm": 1.0817426443099976,
      "learning_rate": 2.1241666666666667e-05,
      "loss": 0.0022,
      "step": 69020
    },
    {
      "epoch": 4.602,
      "grad_norm": 0.18348704278469086,
      "learning_rate": 2.12375e-05,
      "loss": 0.0011,
      "step": 69030
    },
    {
      "epoch": 4.602666666666667,
      "grad_norm": 0.7219980955123901,
      "learning_rate": 2.1233333333333336e-05,
      "loss": 0.0019,
      "step": 69040
    },
    {
      "epoch": 4.6033333333333335,
      "grad_norm": 0.6365123987197876,
      "learning_rate": 2.1229166666666667e-05,
      "loss": 0.0029,
      "step": 69050
    },
    {
      "epoch": 4.604,
      "grad_norm": 0.38392284512519836,
      "learning_rate": 2.1225e-05,
      "loss": 0.0025,
      "step": 69060
    },
    {
      "epoch": 4.604666666666667,
      "grad_norm": 0.1776246428489685,
      "learning_rate": 2.1220833333333335e-05,
      "loss": 0.0013,
      "step": 69070
    },
    {
      "epoch": 4.605333333333333,
      "grad_norm": 0.20714889466762543,
      "learning_rate": 2.121666666666667e-05,
      "loss": 0.002,
      "step": 69080
    },
    {
      "epoch": 4.606,
      "grad_norm": 0.522146463394165,
      "learning_rate": 2.12125e-05,
      "loss": 0.0018,
      "step": 69090
    },
    {
      "epoch": 4.6066666666666665,
      "grad_norm": 0.07598300278186798,
      "learning_rate": 2.1208333333333335e-05,
      "loss": 0.0016,
      "step": 69100
    },
    {
      "epoch": 4.607333333333333,
      "grad_norm": 0.6366775631904602,
      "learning_rate": 2.1204166666666666e-05,
      "loss": 0.0022,
      "step": 69110
    },
    {
      "epoch": 4.608,
      "grad_norm": 0.04579870402812958,
      "learning_rate": 2.12e-05,
      "loss": 0.0018,
      "step": 69120
    },
    {
      "epoch": 4.608666666666666,
      "grad_norm": 0.04091311991214752,
      "learning_rate": 2.1195833333333335e-05,
      "loss": 0.0017,
      "step": 69130
    },
    {
      "epoch": 4.609333333333334,
      "grad_norm": 0.18508055806159973,
      "learning_rate": 2.119166666666667e-05,
      "loss": 0.0022,
      "step": 69140
    },
    {
      "epoch": 4.61,
      "grad_norm": 0.35405850410461426,
      "learning_rate": 2.1187500000000003e-05,
      "loss": 0.0018,
      "step": 69150
    },
    {
      "epoch": 4.610666666666667,
      "grad_norm": 0.04750337824225426,
      "learning_rate": 2.1183333333333334e-05,
      "loss": 0.0018,
      "step": 69160
    },
    {
      "epoch": 4.6113333333333335,
      "grad_norm": 0.453060507774353,
      "learning_rate": 2.1179166666666665e-05,
      "loss": 0.0023,
      "step": 69170
    },
    {
      "epoch": 4.612,
      "grad_norm": 0.1805444359779358,
      "learning_rate": 2.1175e-05,
      "loss": 0.0019,
      "step": 69180
    },
    {
      "epoch": 4.612666666666667,
      "grad_norm": 0.20561574399471283,
      "learning_rate": 2.1170833333333334e-05,
      "loss": 0.0021,
      "step": 69190
    },
    {
      "epoch": 4.613333333333333,
      "grad_norm": 0.04533282667398453,
      "learning_rate": 2.116666666666667e-05,
      "loss": 0.002,
      "step": 69200
    },
    {
      "epoch": 4.614,
      "grad_norm": 0.2540162205696106,
      "learning_rate": 2.1162500000000003e-05,
      "loss": 0.0022,
      "step": 69210
    },
    {
      "epoch": 4.6146666666666665,
      "grad_norm": 0.3561328649520874,
      "learning_rate": 2.1158333333333337e-05,
      "loss": 0.0015,
      "step": 69220
    },
    {
      "epoch": 4.615333333333333,
      "grad_norm": 0.14257657527923584,
      "learning_rate": 2.1154166666666668e-05,
      "loss": 0.0022,
      "step": 69230
    },
    {
      "epoch": 4.616,
      "grad_norm": 0.4229448735713959,
      "learning_rate": 2.115e-05,
      "loss": 0.002,
      "step": 69240
    },
    {
      "epoch": 4.616666666666667,
      "grad_norm": 0.31490176916122437,
      "learning_rate": 2.1145833333333333e-05,
      "loss": 0.0018,
      "step": 69250
    },
    {
      "epoch": 4.617333333333333,
      "grad_norm": 0.27797988057136536,
      "learning_rate": 2.1141666666666668e-05,
      "loss": 0.0021,
      "step": 69260
    },
    {
      "epoch": 4.618,
      "grad_norm": 0.3811577260494232,
      "learning_rate": 2.1137500000000002e-05,
      "loss": 0.0018,
      "step": 69270
    },
    {
      "epoch": 4.618666666666667,
      "grad_norm": 0.18018239736557007,
      "learning_rate": 2.1133333333333337e-05,
      "loss": 0.0021,
      "step": 69280
    },
    {
      "epoch": 4.6193333333333335,
      "grad_norm": 0.49232205748558044,
      "learning_rate": 2.1129166666666668e-05,
      "loss": 0.0018,
      "step": 69290
    },
    {
      "epoch": 4.62,
      "grad_norm": 0.2457524538040161,
      "learning_rate": 2.1125000000000002e-05,
      "loss": 0.0022,
      "step": 69300
    },
    {
      "epoch": 4.620666666666667,
      "grad_norm": 0.1827521175146103,
      "learning_rate": 2.1120833333333333e-05,
      "loss": 0.0015,
      "step": 69310
    },
    {
      "epoch": 4.621333333333333,
      "grad_norm": 0.44745761156082153,
      "learning_rate": 2.1116666666666667e-05,
      "loss": 0.0015,
      "step": 69320
    },
    {
      "epoch": 4.622,
      "grad_norm": 0.10933611541986465,
      "learning_rate": 2.11125e-05,
      "loss": 0.0015,
      "step": 69330
    },
    {
      "epoch": 4.6226666666666665,
      "grad_norm": 0.06215101107954979,
      "learning_rate": 2.1108333333333336e-05,
      "loss": 0.0023,
      "step": 69340
    },
    {
      "epoch": 4.623333333333333,
      "grad_norm": 0.2463802993297577,
      "learning_rate": 2.1104166666666667e-05,
      "loss": 0.0019,
      "step": 69350
    },
    {
      "epoch": 4.624,
      "grad_norm": 0.5235629081726074,
      "learning_rate": 2.11e-05,
      "loss": 0.0012,
      "step": 69360
    },
    {
      "epoch": 4.624666666666666,
      "grad_norm": 0.6236512064933777,
      "learning_rate": 2.1095833333333336e-05,
      "loss": 0.0017,
      "step": 69370
    },
    {
      "epoch": 4.625333333333334,
      "grad_norm": 0.4518199563026428,
      "learning_rate": 2.1091666666666667e-05,
      "loss": 0.0019,
      "step": 69380
    },
    {
      "epoch": 4.626,
      "grad_norm": 0.20809383690357208,
      "learning_rate": 2.10875e-05,
      "loss": 0.0013,
      "step": 69390
    },
    {
      "epoch": 4.626666666666667,
      "grad_norm": 0.41730996966362,
      "learning_rate": 2.1083333333333335e-05,
      "loss": 0.0021,
      "step": 69400
    },
    {
      "epoch": 4.6273333333333335,
      "grad_norm": 0.1200999766588211,
      "learning_rate": 2.1079166666666666e-05,
      "loss": 0.0016,
      "step": 69410
    },
    {
      "epoch": 4.628,
      "grad_norm": 0.41738593578338623,
      "learning_rate": 2.1075e-05,
      "loss": 0.0018,
      "step": 69420
    },
    {
      "epoch": 4.628666666666667,
      "grad_norm": 0.373879998922348,
      "learning_rate": 2.1070833333333335e-05,
      "loss": 0.002,
      "step": 69430
    },
    {
      "epoch": 4.629333333333333,
      "grad_norm": 0.07540076971054077,
      "learning_rate": 2.106666666666667e-05,
      "loss": 0.0013,
      "step": 69440
    },
    {
      "epoch": 4.63,
      "grad_norm": 0.021270792931318283,
      "learning_rate": 2.10625e-05,
      "loss": 0.0025,
      "step": 69450
    },
    {
      "epoch": 4.6306666666666665,
      "grad_norm": 0.032842330634593964,
      "learning_rate": 2.1058333333333335e-05,
      "loss": 0.0019,
      "step": 69460
    },
    {
      "epoch": 4.631333333333333,
      "grad_norm": 0.1368582546710968,
      "learning_rate": 2.1054166666666666e-05,
      "loss": 0.0017,
      "step": 69470
    },
    {
      "epoch": 4.632,
      "grad_norm": 0.17708861827850342,
      "learning_rate": 2.105e-05,
      "loss": 0.0021,
      "step": 69480
    },
    {
      "epoch": 4.632666666666667,
      "grad_norm": 0.288443386554718,
      "learning_rate": 2.1045833333333334e-05,
      "loss": 0.0024,
      "step": 69490
    },
    {
      "epoch": 4.633333333333333,
      "grad_norm": 0.4866071343421936,
      "learning_rate": 2.104166666666667e-05,
      "loss": 0.0026,
      "step": 69500
    },
    {
      "epoch": 4.634,
      "grad_norm": 0.6245169043540955,
      "learning_rate": 2.1037500000000003e-05,
      "loss": 0.0014,
      "step": 69510
    },
    {
      "epoch": 4.634666666666667,
      "grad_norm": 0.46303796768188477,
      "learning_rate": 2.1033333333333334e-05,
      "loss": 0.0019,
      "step": 69520
    },
    {
      "epoch": 4.6353333333333335,
      "grad_norm": 0.4167811870574951,
      "learning_rate": 2.1029166666666665e-05,
      "loss": 0.0021,
      "step": 69530
    },
    {
      "epoch": 4.636,
      "grad_norm": 0.2072824090719223,
      "learning_rate": 2.1025e-05,
      "loss": 0.0022,
      "step": 69540
    },
    {
      "epoch": 4.636666666666667,
      "grad_norm": 0.1806715428829193,
      "learning_rate": 2.1020833333333334e-05,
      "loss": 0.0018,
      "step": 69550
    },
    {
      "epoch": 4.637333333333333,
      "grad_norm": 0.4237416982650757,
      "learning_rate": 2.1016666666666668e-05,
      "loss": 0.0017,
      "step": 69560
    },
    {
      "epoch": 4.638,
      "grad_norm": 0.6971522569656372,
      "learning_rate": 2.1012500000000003e-05,
      "loss": 0.0016,
      "step": 69570
    },
    {
      "epoch": 4.6386666666666665,
      "grad_norm": 0.02762775495648384,
      "learning_rate": 2.1008333333333337e-05,
      "loss": 0.0024,
      "step": 69580
    },
    {
      "epoch": 4.639333333333333,
      "grad_norm": 0.6245743632316589,
      "learning_rate": 2.1004166666666668e-05,
      "loss": 0.0019,
      "step": 69590
    },
    {
      "epoch": 4.64,
      "grad_norm": 0.17626163363456726,
      "learning_rate": 2.1e-05,
      "loss": 0.0024,
      "step": 69600
    },
    {
      "epoch": 4.640666666666666,
      "grad_norm": 0.4221856892108917,
      "learning_rate": 2.0995833333333333e-05,
      "loss": 0.0015,
      "step": 69610
    },
    {
      "epoch": 4.641333333333334,
      "grad_norm": 0.2387080192565918,
      "learning_rate": 2.0991666666666668e-05,
      "loss": 0.0023,
      "step": 69620
    },
    {
      "epoch": 4.642,
      "grad_norm": 0.1098741963505745,
      "learning_rate": 2.0987500000000002e-05,
      "loss": 0.0015,
      "step": 69630
    },
    {
      "epoch": 4.642666666666667,
      "grad_norm": 0.07292408496141434,
      "learning_rate": 2.0983333333333336e-05,
      "loss": 0.0022,
      "step": 69640
    },
    {
      "epoch": 4.6433333333333335,
      "grad_norm": 0.24124544858932495,
      "learning_rate": 2.0979166666666667e-05,
      "loss": 0.0026,
      "step": 69650
    },
    {
      "epoch": 4.644,
      "grad_norm": 0.175241619348526,
      "learning_rate": 2.0975e-05,
      "loss": 0.0022,
      "step": 69660
    },
    {
      "epoch": 4.644666666666667,
      "grad_norm": 0.0428834930062294,
      "learning_rate": 2.0970833333333333e-05,
      "loss": 0.0017,
      "step": 69670
    },
    {
      "epoch": 4.645333333333333,
      "grad_norm": 0.34697848558425903,
      "learning_rate": 2.0966666666666667e-05,
      "loss": 0.0018,
      "step": 69680
    },
    {
      "epoch": 4.646,
      "grad_norm": 0.4816490113735199,
      "learning_rate": 2.09625e-05,
      "loss": 0.0014,
      "step": 69690
    },
    {
      "epoch": 4.6466666666666665,
      "grad_norm": 0.3137878477573395,
      "learning_rate": 2.0958333333333336e-05,
      "loss": 0.0018,
      "step": 69700
    },
    {
      "epoch": 4.647333333333333,
      "grad_norm": 0.24907760322093964,
      "learning_rate": 2.0954166666666667e-05,
      "loss": 0.0026,
      "step": 69710
    },
    {
      "epoch": 4.648,
      "grad_norm": 0.9351640939712524,
      "learning_rate": 2.095e-05,
      "loss": 0.0023,
      "step": 69720
    },
    {
      "epoch": 4.648666666666666,
      "grad_norm": 0.08450721204280853,
      "learning_rate": 2.0945833333333335e-05,
      "loss": 0.0025,
      "step": 69730
    },
    {
      "epoch": 4.649333333333333,
      "grad_norm": 0.39367935061454773,
      "learning_rate": 2.0941666666666666e-05,
      "loss": 0.0022,
      "step": 69740
    },
    {
      "epoch": 4.65,
      "grad_norm": 0.3784826993942261,
      "learning_rate": 2.09375e-05,
      "loss": 0.0021,
      "step": 69750
    },
    {
      "epoch": 4.650666666666667,
      "grad_norm": 0.47157442569732666,
      "learning_rate": 2.0933333333333335e-05,
      "loss": 0.0015,
      "step": 69760
    },
    {
      "epoch": 4.6513333333333335,
      "grad_norm": 0.5281646251678467,
      "learning_rate": 2.0929166666666666e-05,
      "loss": 0.0019,
      "step": 69770
    },
    {
      "epoch": 4.652,
      "grad_norm": 0.11100678890943527,
      "learning_rate": 2.0925e-05,
      "loss": 0.0025,
      "step": 69780
    },
    {
      "epoch": 4.652666666666667,
      "grad_norm": 0.34336692094802856,
      "learning_rate": 2.0920833333333335e-05,
      "loss": 0.0016,
      "step": 69790
    },
    {
      "epoch": 4.653333333333333,
      "grad_norm": 0.21177399158477783,
      "learning_rate": 2.091666666666667e-05,
      "loss": 0.0015,
      "step": 69800
    },
    {
      "epoch": 4.654,
      "grad_norm": 0.08436508476734161,
      "learning_rate": 2.09125e-05,
      "loss": 0.0028,
      "step": 69810
    },
    {
      "epoch": 4.6546666666666665,
      "grad_norm": 0.3146103322505951,
      "learning_rate": 2.0908333333333334e-05,
      "loss": 0.0019,
      "step": 69820
    },
    {
      "epoch": 4.655333333333333,
      "grad_norm": 0.18009303510189056,
      "learning_rate": 2.0904166666666665e-05,
      "loss": 0.0019,
      "step": 69830
    },
    {
      "epoch": 4.656,
      "grad_norm": 0.10902480036020279,
      "learning_rate": 2.09e-05,
      "loss": 0.0022,
      "step": 69840
    },
    {
      "epoch": 4.656666666666666,
      "grad_norm": 0.276292085647583,
      "learning_rate": 2.0895833333333334e-05,
      "loss": 0.0016,
      "step": 69850
    },
    {
      "epoch": 4.657333333333334,
      "grad_norm": 0.4604266583919525,
      "learning_rate": 2.089166666666667e-05,
      "loss": 0.0021,
      "step": 69860
    },
    {
      "epoch": 4.658,
      "grad_norm": 0.3496546149253845,
      "learning_rate": 2.0887500000000003e-05,
      "loss": 0.002,
      "step": 69870
    },
    {
      "epoch": 4.658666666666667,
      "grad_norm": 0.14057688415050507,
      "learning_rate": 2.0883333333333334e-05,
      "loss": 0.0029,
      "step": 69880
    },
    {
      "epoch": 4.6593333333333335,
      "grad_norm": 0.44895026087760925,
      "learning_rate": 2.0879166666666665e-05,
      "loss": 0.0018,
      "step": 69890
    },
    {
      "epoch": 4.66,
      "grad_norm": 0.5216708183288574,
      "learning_rate": 2.0875e-05,
      "loss": 0.002,
      "step": 69900
    },
    {
      "epoch": 4.660666666666667,
      "grad_norm": 0.07166972011327744,
      "learning_rate": 2.0870833333333334e-05,
      "loss": 0.0012,
      "step": 69910
    },
    {
      "epoch": 4.661333333333333,
      "grad_norm": 0.11583781987428665,
      "learning_rate": 2.0866666666666668e-05,
      "loss": 0.0014,
      "step": 69920
    },
    {
      "epoch": 4.662,
      "grad_norm": 0.24692454934120178,
      "learning_rate": 2.0862500000000002e-05,
      "loss": 0.0017,
      "step": 69930
    },
    {
      "epoch": 4.6626666666666665,
      "grad_norm": 0.24338069558143616,
      "learning_rate": 2.0858333333333337e-05,
      "loss": 0.0015,
      "step": 69940
    },
    {
      "epoch": 4.663333333333333,
      "grad_norm": 0.17469532787799835,
      "learning_rate": 2.0854166666666668e-05,
      "loss": 0.0027,
      "step": 69950
    },
    {
      "epoch": 4.664,
      "grad_norm": 0.2431231588125229,
      "learning_rate": 2.085e-05,
      "loss": 0.0017,
      "step": 69960
    },
    {
      "epoch": 4.664666666666666,
      "grad_norm": 0.24385349452495575,
      "learning_rate": 2.0845833333333333e-05,
      "loss": 0.0015,
      "step": 69970
    },
    {
      "epoch": 4.665333333333333,
      "grad_norm": 0.11767182499170303,
      "learning_rate": 2.0841666666666667e-05,
      "loss": 0.0031,
      "step": 69980
    },
    {
      "epoch": 4.666,
      "grad_norm": 0.6199480891227722,
      "learning_rate": 2.08375e-05,
      "loss": 0.0022,
      "step": 69990
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 0.20915459096431732,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 0.0019,
      "step": 70000
    },
    {
      "epoch": 4.667333333333334,
      "grad_norm": 0.5352631211280823,
      "learning_rate": 2.082916666666667e-05,
      "loss": 0.0017,
      "step": 70010
    },
    {
      "epoch": 4.668,
      "grad_norm": 0.20182424783706665,
      "learning_rate": 2.0825e-05,
      "loss": 0.002,
      "step": 70020
    },
    {
      "epoch": 4.668666666666667,
      "grad_norm": 0.07951311767101288,
      "learning_rate": 2.0820833333333332e-05,
      "loss": 0.0023,
      "step": 70030
    },
    {
      "epoch": 4.669333333333333,
      "grad_norm": 0.7288084030151367,
      "learning_rate": 2.0816666666666667e-05,
      "loss": 0.0017,
      "step": 70040
    },
    {
      "epoch": 4.67,
      "grad_norm": 0.5840612053871155,
      "learning_rate": 2.08125e-05,
      "loss": 0.0029,
      "step": 70050
    },
    {
      "epoch": 4.6706666666666665,
      "grad_norm": 0.5151438117027283,
      "learning_rate": 2.0808333333333335e-05,
      "loss": 0.0024,
      "step": 70060
    },
    {
      "epoch": 4.671333333333333,
      "grad_norm": 0.1042819395661354,
      "learning_rate": 2.080416666666667e-05,
      "loss": 0.0014,
      "step": 70070
    },
    {
      "epoch": 4.672,
      "grad_norm": 0.14385303854942322,
      "learning_rate": 2.08e-05,
      "loss": 0.0012,
      "step": 70080
    },
    {
      "epoch": 4.672666666666666,
      "grad_norm": 0.3070545792579651,
      "learning_rate": 2.0795833333333335e-05,
      "loss": 0.0019,
      "step": 70090
    },
    {
      "epoch": 4.673333333333334,
      "grad_norm": 0.27575019001960754,
      "learning_rate": 2.0791666666666666e-05,
      "loss": 0.0027,
      "step": 70100
    },
    {
      "epoch": 4.674,
      "grad_norm": 0.3762710690498352,
      "learning_rate": 2.07875e-05,
      "loss": 0.0014,
      "step": 70110
    },
    {
      "epoch": 4.674666666666667,
      "grad_norm": 0.08987221121788025,
      "learning_rate": 2.0783333333333335e-05,
      "loss": 0.0021,
      "step": 70120
    },
    {
      "epoch": 4.675333333333334,
      "grad_norm": 0.14796896278858185,
      "learning_rate": 2.077916666666667e-05,
      "loss": 0.0018,
      "step": 70130
    },
    {
      "epoch": 4.676,
      "grad_norm": 0.1402864009141922,
      "learning_rate": 2.0775e-05,
      "loss": 0.0016,
      "step": 70140
    },
    {
      "epoch": 4.676666666666667,
      "grad_norm": 0.10807870328426361,
      "learning_rate": 2.0770833333333335e-05,
      "loss": 0.0023,
      "step": 70150
    },
    {
      "epoch": 4.677333333333333,
      "grad_norm": 0.693105161190033,
      "learning_rate": 2.076666666666667e-05,
      "loss": 0.0036,
      "step": 70160
    },
    {
      "epoch": 4.678,
      "grad_norm": 0.5909583568572998,
      "learning_rate": 2.07625e-05,
      "loss": 0.0028,
      "step": 70170
    },
    {
      "epoch": 4.6786666666666665,
      "grad_norm": 0.043488677591085434,
      "learning_rate": 2.0758333333333334e-05,
      "loss": 0.0018,
      "step": 70180
    },
    {
      "epoch": 4.679333333333333,
      "grad_norm": 0.3109200596809387,
      "learning_rate": 2.075416666666667e-05,
      "loss": 0.0015,
      "step": 70190
    },
    {
      "epoch": 4.68,
      "grad_norm": 0.4486478269100189,
      "learning_rate": 2.075e-05,
      "loss": 0.0021,
      "step": 70200
    },
    {
      "epoch": 4.680666666666666,
      "grad_norm": 0.42388948798179626,
      "learning_rate": 2.0745833333333334e-05,
      "loss": 0.0017,
      "step": 70210
    },
    {
      "epoch": 4.681333333333333,
      "grad_norm": 0.1458587646484375,
      "learning_rate": 2.0741666666666668e-05,
      "loss": 0.0015,
      "step": 70220
    },
    {
      "epoch": 4.682,
      "grad_norm": 0.10370334982872009,
      "learning_rate": 2.0737500000000003e-05,
      "loss": 0.0018,
      "step": 70230
    },
    {
      "epoch": 4.682666666666667,
      "grad_norm": 0.1430809050798416,
      "learning_rate": 2.0733333333333334e-05,
      "loss": 0.0028,
      "step": 70240
    },
    {
      "epoch": 4.683333333333334,
      "grad_norm": 0.30824288725852966,
      "learning_rate": 2.0729166666666668e-05,
      "loss": 0.0018,
      "step": 70250
    },
    {
      "epoch": 4.684,
      "grad_norm": 0.6559039950370789,
      "learning_rate": 2.0725e-05,
      "loss": 0.0014,
      "step": 70260
    },
    {
      "epoch": 4.684666666666667,
      "grad_norm": 0.21095259487628937,
      "learning_rate": 2.0720833333333333e-05,
      "loss": 0.0018,
      "step": 70270
    },
    {
      "epoch": 4.685333333333333,
      "grad_norm": 0.49246713519096375,
      "learning_rate": 2.0716666666666668e-05,
      "loss": 0.0025,
      "step": 70280
    },
    {
      "epoch": 4.686,
      "grad_norm": 0.11894965916872025,
      "learning_rate": 2.0712500000000002e-05,
      "loss": 0.0011,
      "step": 70290
    },
    {
      "epoch": 4.6866666666666665,
      "grad_norm": 0.05202497914433479,
      "learning_rate": 2.0708333333333336e-05,
      "loss": 0.0021,
      "step": 70300
    },
    {
      "epoch": 4.687333333333333,
      "grad_norm": 0.04070764407515526,
      "learning_rate": 2.0704166666666667e-05,
      "loss": 0.0028,
      "step": 70310
    },
    {
      "epoch": 4.688,
      "grad_norm": 0.13308489322662354,
      "learning_rate": 2.07e-05,
      "loss": 0.0015,
      "step": 70320
    },
    {
      "epoch": 4.688666666666666,
      "grad_norm": 0.4720331132411957,
      "learning_rate": 2.0695833333333333e-05,
      "loss": 0.0012,
      "step": 70330
    },
    {
      "epoch": 4.689333333333334,
      "grad_norm": 0.09055686742067337,
      "learning_rate": 2.0691666666666667e-05,
      "loss": 0.0022,
      "step": 70340
    },
    {
      "epoch": 4.6899999999999995,
      "grad_norm": 0.2785783112049103,
      "learning_rate": 2.06875e-05,
      "loss": 0.0019,
      "step": 70350
    },
    {
      "epoch": 4.690666666666667,
      "grad_norm": 0.669197678565979,
      "learning_rate": 2.0683333333333336e-05,
      "loss": 0.0013,
      "step": 70360
    },
    {
      "epoch": 4.691333333333334,
      "grad_norm": 0.44803330302238464,
      "learning_rate": 2.067916666666667e-05,
      "loss": 0.003,
      "step": 70370
    },
    {
      "epoch": 4.692,
      "grad_norm": 0.5136715173721313,
      "learning_rate": 2.0675e-05,
      "loss": 0.0023,
      "step": 70380
    },
    {
      "epoch": 4.692666666666667,
      "grad_norm": 0.2494022101163864,
      "learning_rate": 2.0670833333333332e-05,
      "loss": 0.0023,
      "step": 70390
    },
    {
      "epoch": 4.693333333333333,
      "grad_norm": 0.7333572506904602,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 0.0013,
      "step": 70400
    },
    {
      "epoch": 4.694,
      "grad_norm": 0.3557581603527069,
      "learning_rate": 2.06625e-05,
      "loss": 0.0016,
      "step": 70410
    },
    {
      "epoch": 4.6946666666666665,
      "grad_norm": 0.042115963995456696,
      "learning_rate": 2.0658333333333335e-05,
      "loss": 0.002,
      "step": 70420
    },
    {
      "epoch": 4.695333333333333,
      "grad_norm": 0.14749585092067719,
      "learning_rate": 2.065416666666667e-05,
      "loss": 0.0022,
      "step": 70430
    },
    {
      "epoch": 4.696,
      "grad_norm": 0.6200169920921326,
      "learning_rate": 2.065e-05,
      "loss": 0.0032,
      "step": 70440
    },
    {
      "epoch": 4.696666666666666,
      "grad_norm": 0.14596262574195862,
      "learning_rate": 2.0645833333333335e-05,
      "loss": 0.0017,
      "step": 70450
    },
    {
      "epoch": 4.697333333333333,
      "grad_norm": 0.6656178832054138,
      "learning_rate": 2.0641666666666666e-05,
      "loss": 0.0017,
      "step": 70460
    },
    {
      "epoch": 4.698,
      "grad_norm": 0.05780404061079025,
      "learning_rate": 2.06375e-05,
      "loss": 0.0021,
      "step": 70470
    },
    {
      "epoch": 4.698666666666667,
      "grad_norm": 0.6015344262123108,
      "learning_rate": 2.0633333333333335e-05,
      "loss": 0.0017,
      "step": 70480
    },
    {
      "epoch": 4.699333333333334,
      "grad_norm": 0.5249280333518982,
      "learning_rate": 2.062916666666667e-05,
      "loss": 0.0024,
      "step": 70490
    },
    {
      "epoch": 4.7,
      "grad_norm": 0.5476163625717163,
      "learning_rate": 2.0625e-05,
      "loss": 0.0023,
      "step": 70500
    },
    {
      "epoch": 4.700666666666667,
      "grad_norm": 0.18655090034008026,
      "learning_rate": 2.0620833333333334e-05,
      "loss": 0.0015,
      "step": 70510
    },
    {
      "epoch": 4.701333333333333,
      "grad_norm": 0.148128941655159,
      "learning_rate": 2.061666666666667e-05,
      "loss": 0.002,
      "step": 70520
    },
    {
      "epoch": 4.702,
      "grad_norm": 0.30027157068252563,
      "learning_rate": 2.06125e-05,
      "loss": 0.0017,
      "step": 70530
    },
    {
      "epoch": 4.7026666666666666,
      "grad_norm": 0.23968692123889923,
      "learning_rate": 2.0608333333333334e-05,
      "loss": 0.0031,
      "step": 70540
    },
    {
      "epoch": 4.703333333333333,
      "grad_norm": 0.315809428691864,
      "learning_rate": 2.060416666666667e-05,
      "loss": 0.0022,
      "step": 70550
    },
    {
      "epoch": 4.704,
      "grad_norm": 0.4853913486003876,
      "learning_rate": 2.06e-05,
      "loss": 0.0023,
      "step": 70560
    },
    {
      "epoch": 4.704666666666666,
      "grad_norm": 0.06773574650287628,
      "learning_rate": 2.0595833333333334e-05,
      "loss": 0.0018,
      "step": 70570
    },
    {
      "epoch": 4.705333333333334,
      "grad_norm": 0.15591581165790558,
      "learning_rate": 2.0591666666666668e-05,
      "loss": 0.0014,
      "step": 70580
    },
    {
      "epoch": 4.7059999999999995,
      "grad_norm": 0.14923058450222015,
      "learning_rate": 2.0587500000000002e-05,
      "loss": 0.0018,
      "step": 70590
    },
    {
      "epoch": 4.706666666666667,
      "grad_norm": 0.8315793871879578,
      "learning_rate": 2.0583333333333333e-05,
      "loss": 0.0023,
      "step": 70600
    },
    {
      "epoch": 4.707333333333334,
      "grad_norm": 0.9304313063621521,
      "learning_rate": 2.0579166666666668e-05,
      "loss": 0.0023,
      "step": 70610
    },
    {
      "epoch": 4.708,
      "grad_norm": 0.580253005027771,
      "learning_rate": 2.0575e-05,
      "loss": 0.0029,
      "step": 70620
    },
    {
      "epoch": 4.708666666666667,
      "grad_norm": 0.11186474561691284,
      "learning_rate": 2.0570833333333333e-05,
      "loss": 0.0022,
      "step": 70630
    },
    {
      "epoch": 4.709333333333333,
      "grad_norm": 0.12181004881858826,
      "learning_rate": 2.0566666666666667e-05,
      "loss": 0.0018,
      "step": 70640
    },
    {
      "epoch": 4.71,
      "grad_norm": 0.588537335395813,
      "learning_rate": 2.0562500000000002e-05,
      "loss": 0.0025,
      "step": 70650
    },
    {
      "epoch": 4.710666666666667,
      "grad_norm": 0.1469668298959732,
      "learning_rate": 2.0558333333333336e-05,
      "loss": 0.0024,
      "step": 70660
    },
    {
      "epoch": 4.711333333333333,
      "grad_norm": 0.26553863286972046,
      "learning_rate": 2.055416666666667e-05,
      "loss": 0.0017,
      "step": 70670
    },
    {
      "epoch": 4.712,
      "grad_norm": 0.3525444269180298,
      "learning_rate": 2.055e-05,
      "loss": 0.0014,
      "step": 70680
    },
    {
      "epoch": 4.712666666666666,
      "grad_norm": 0.18389511108398438,
      "learning_rate": 2.0545833333333332e-05,
      "loss": 0.0018,
      "step": 70690
    },
    {
      "epoch": 4.713333333333333,
      "grad_norm": 0.5329665541648865,
      "learning_rate": 2.0541666666666667e-05,
      "loss": 0.002,
      "step": 70700
    },
    {
      "epoch": 4.714,
      "grad_norm": 0.17097832262516022,
      "learning_rate": 2.05375e-05,
      "loss": 0.0021,
      "step": 70710
    },
    {
      "epoch": 4.714666666666667,
      "grad_norm": 0.08102016150951385,
      "learning_rate": 2.0533333333333336e-05,
      "loss": 0.0015,
      "step": 70720
    },
    {
      "epoch": 4.715333333333334,
      "grad_norm": 0.3730883002281189,
      "learning_rate": 2.052916666666667e-05,
      "loss": 0.0016,
      "step": 70730
    },
    {
      "epoch": 4.716,
      "grad_norm": 0.04519972205162048,
      "learning_rate": 2.0525e-05,
      "loss": 0.0018,
      "step": 70740
    },
    {
      "epoch": 4.716666666666667,
      "grad_norm": 0.5505803227424622,
      "learning_rate": 2.0520833333333335e-05,
      "loss": 0.0019,
      "step": 70750
    },
    {
      "epoch": 4.717333333333333,
      "grad_norm": 0.2825128734111786,
      "learning_rate": 2.0516666666666666e-05,
      "loss": 0.0013,
      "step": 70760
    },
    {
      "epoch": 4.718,
      "grad_norm": 0.29801443219184875,
      "learning_rate": 2.05125e-05,
      "loss": 0.0019,
      "step": 70770
    },
    {
      "epoch": 4.718666666666667,
      "grad_norm": 0.14500851929187775,
      "learning_rate": 2.0508333333333335e-05,
      "loss": 0.0016,
      "step": 70780
    },
    {
      "epoch": 4.719333333333333,
      "grad_norm": 0.49585092067718506,
      "learning_rate": 2.050416666666667e-05,
      "loss": 0.0018,
      "step": 70790
    },
    {
      "epoch": 4.72,
      "grad_norm": 0.10570070892572403,
      "learning_rate": 2.05e-05,
      "loss": 0.0021,
      "step": 70800
    },
    {
      "epoch": 4.720666666666666,
      "grad_norm": 0.6625528335571289,
      "learning_rate": 2.0495833333333335e-05,
      "loss": 0.0013,
      "step": 70810
    },
    {
      "epoch": 4.721333333333334,
      "grad_norm": 0.45370611548423767,
      "learning_rate": 2.049166666666667e-05,
      "loss": 0.0018,
      "step": 70820
    },
    {
      "epoch": 4.7219999999999995,
      "grad_norm": 0.48922666907310486,
      "learning_rate": 2.04875e-05,
      "loss": 0.0017,
      "step": 70830
    },
    {
      "epoch": 4.722666666666667,
      "grad_norm": 0.025079835206270218,
      "learning_rate": 2.0483333333333334e-05,
      "loss": 0.0024,
      "step": 70840
    },
    {
      "epoch": 4.723333333333334,
      "grad_norm": 0.2497519552707672,
      "learning_rate": 2.047916666666667e-05,
      "loss": 0.0019,
      "step": 70850
    },
    {
      "epoch": 4.724,
      "grad_norm": 0.7386074066162109,
      "learning_rate": 2.0475e-05,
      "loss": 0.0018,
      "step": 70860
    },
    {
      "epoch": 4.724666666666667,
      "grad_norm": 0.49798256158828735,
      "learning_rate": 2.0470833333333334e-05,
      "loss": 0.0026,
      "step": 70870
    },
    {
      "epoch": 4.725333333333333,
      "grad_norm": 0.7023126482963562,
      "learning_rate": 2.046666666666667e-05,
      "loss": 0.0022,
      "step": 70880
    },
    {
      "epoch": 4.726,
      "grad_norm": 0.07182043045759201,
      "learning_rate": 2.0462500000000003e-05,
      "loss": 0.0015,
      "step": 70890
    },
    {
      "epoch": 4.726666666666667,
      "grad_norm": 0.31545060873031616,
      "learning_rate": 2.0458333333333334e-05,
      "loss": 0.0022,
      "step": 70900
    },
    {
      "epoch": 4.727333333333333,
      "grad_norm": 0.30578237771987915,
      "learning_rate": 2.0454166666666668e-05,
      "loss": 0.0025,
      "step": 70910
    },
    {
      "epoch": 4.728,
      "grad_norm": 0.21187809109687805,
      "learning_rate": 2.045e-05,
      "loss": 0.0019,
      "step": 70920
    },
    {
      "epoch": 4.728666666666666,
      "grad_norm": 0.24256670475006104,
      "learning_rate": 2.0445833333333333e-05,
      "loss": 0.0023,
      "step": 70930
    },
    {
      "epoch": 4.729333333333333,
      "grad_norm": 0.3554186224937439,
      "learning_rate": 2.0441666666666668e-05,
      "loss": 0.0029,
      "step": 70940
    },
    {
      "epoch": 4.73,
      "grad_norm": 0.382539838552475,
      "learning_rate": 2.0437500000000002e-05,
      "loss": 0.0016,
      "step": 70950
    },
    {
      "epoch": 4.730666666666667,
      "grad_norm": 0.5205091238021851,
      "learning_rate": 2.0433333333333336e-05,
      "loss": 0.0014,
      "step": 70960
    },
    {
      "epoch": 4.731333333333334,
      "grad_norm": 0.47960373759269714,
      "learning_rate": 2.0429166666666667e-05,
      "loss": 0.0019,
      "step": 70970
    },
    {
      "epoch": 4.732,
      "grad_norm": 0.4492037296295166,
      "learning_rate": 2.0425e-05,
      "loss": 0.0015,
      "step": 70980
    },
    {
      "epoch": 4.732666666666667,
      "grad_norm": 0.34523844718933105,
      "learning_rate": 2.0420833333333333e-05,
      "loss": 0.0024,
      "step": 70990
    },
    {
      "epoch": 4.733333333333333,
      "grad_norm": 0.2575298249721527,
      "learning_rate": 2.0416666666666667e-05,
      "loss": 0.0028,
      "step": 71000
    },
    {
      "epoch": 4.734,
      "grad_norm": 0.3806067705154419,
      "learning_rate": 2.04125e-05,
      "loss": 0.0018,
      "step": 71010
    },
    {
      "epoch": 4.734666666666667,
      "grad_norm": 0.6207689046859741,
      "learning_rate": 2.0408333333333336e-05,
      "loss": 0.002,
      "step": 71020
    },
    {
      "epoch": 4.735333333333333,
      "grad_norm": 0.06274206936359406,
      "learning_rate": 2.040416666666667e-05,
      "loss": 0.0021,
      "step": 71030
    },
    {
      "epoch": 4.736,
      "grad_norm": 0.2068493664264679,
      "learning_rate": 2.04e-05,
      "loss": 0.0015,
      "step": 71040
    },
    {
      "epoch": 4.736666666666666,
      "grad_norm": 0.14243817329406738,
      "learning_rate": 2.0395833333333332e-05,
      "loss": 0.0017,
      "step": 71050
    },
    {
      "epoch": 4.737333333333333,
      "grad_norm": 0.4557109773159027,
      "learning_rate": 2.0391666666666667e-05,
      "loss": 0.0024,
      "step": 71060
    },
    {
      "epoch": 4.7379999999999995,
      "grad_norm": 0.22503039240837097,
      "learning_rate": 2.03875e-05,
      "loss": 0.0027,
      "step": 71070
    },
    {
      "epoch": 4.738666666666667,
      "grad_norm": 0.08220915496349335,
      "learning_rate": 2.0383333333333335e-05,
      "loss": 0.0022,
      "step": 71080
    },
    {
      "epoch": 4.739333333333334,
      "grad_norm": 0.45521485805511475,
      "learning_rate": 2.037916666666667e-05,
      "loss": 0.0021,
      "step": 71090
    },
    {
      "epoch": 4.74,
      "grad_norm": 0.7605046033859253,
      "learning_rate": 2.0375e-05,
      "loss": 0.0011,
      "step": 71100
    },
    {
      "epoch": 4.740666666666667,
      "grad_norm": 0.41783303022384644,
      "learning_rate": 2.0370833333333335e-05,
      "loss": 0.0025,
      "step": 71110
    },
    {
      "epoch": 4.741333333333333,
      "grad_norm": 0.06913942098617554,
      "learning_rate": 2.0366666666666666e-05,
      "loss": 0.0018,
      "step": 71120
    },
    {
      "epoch": 4.742,
      "grad_norm": 0.3133385181427002,
      "learning_rate": 2.03625e-05,
      "loss": 0.0016,
      "step": 71130
    },
    {
      "epoch": 4.742666666666667,
      "grad_norm": 0.14878615736961365,
      "learning_rate": 2.0358333333333335e-05,
      "loss": 0.0028,
      "step": 71140
    },
    {
      "epoch": 4.743333333333333,
      "grad_norm": 0.10439339280128479,
      "learning_rate": 2.035416666666667e-05,
      "loss": 0.0012,
      "step": 71150
    },
    {
      "epoch": 4.744,
      "grad_norm": 0.07676587253808975,
      "learning_rate": 2.035e-05,
      "loss": 0.0027,
      "step": 71160
    },
    {
      "epoch": 4.744666666666666,
      "grad_norm": 0.2099050134420395,
      "learning_rate": 2.0345833333333334e-05,
      "loss": 0.0025,
      "step": 71170
    },
    {
      "epoch": 4.745333333333333,
      "grad_norm": 0.234071746468544,
      "learning_rate": 2.034166666666667e-05,
      "loss": 0.0031,
      "step": 71180
    },
    {
      "epoch": 4.746,
      "grad_norm": 0.7062488198280334,
      "learning_rate": 2.03375e-05,
      "loss": 0.0015,
      "step": 71190
    },
    {
      "epoch": 4.746666666666667,
      "grad_norm": 0.4157847464084625,
      "learning_rate": 2.0333333333333334e-05,
      "loss": 0.0028,
      "step": 71200
    },
    {
      "epoch": 4.747333333333334,
      "grad_norm": 0.7235481142997742,
      "learning_rate": 2.032916666666667e-05,
      "loss": 0.0015,
      "step": 71210
    },
    {
      "epoch": 4.748,
      "grad_norm": 0.8789626359939575,
      "learning_rate": 2.0325e-05,
      "loss": 0.0016,
      "step": 71220
    },
    {
      "epoch": 4.748666666666667,
      "grad_norm": 0.4464489817619324,
      "learning_rate": 2.0320833333333334e-05,
      "loss": 0.0019,
      "step": 71230
    },
    {
      "epoch": 4.749333333333333,
      "grad_norm": 0.3604017198085785,
      "learning_rate": 2.0316666666666668e-05,
      "loss": 0.002,
      "step": 71240
    },
    {
      "epoch": 4.75,
      "grad_norm": 0.3150443434715271,
      "learning_rate": 2.0312500000000002e-05,
      "loss": 0.0026,
      "step": 71250
    },
    {
      "epoch": 4.750666666666667,
      "grad_norm": 0.20635439455509186,
      "learning_rate": 2.0308333333333333e-05,
      "loss": 0.0017,
      "step": 71260
    },
    {
      "epoch": 4.751333333333333,
      "grad_norm": 0.5170080661773682,
      "learning_rate": 2.0304166666666668e-05,
      "loss": 0.0019,
      "step": 71270
    },
    {
      "epoch": 4.752,
      "grad_norm": 0.1483101099729538,
      "learning_rate": 2.0300000000000002e-05,
      "loss": 0.0017,
      "step": 71280
    },
    {
      "epoch": 4.752666666666666,
      "grad_norm": 0.11543165147304535,
      "learning_rate": 2.0295833333333333e-05,
      "loss": 0.0014,
      "step": 71290
    },
    {
      "epoch": 4.753333333333333,
      "grad_norm": 0.16977541148662567,
      "learning_rate": 2.0291666666666667e-05,
      "loss": 0.0026,
      "step": 71300
    },
    {
      "epoch": 4.754,
      "grad_norm": 0.2056855857372284,
      "learning_rate": 2.0287500000000002e-05,
      "loss": 0.002,
      "step": 71310
    },
    {
      "epoch": 4.754666666666667,
      "grad_norm": 0.547818124294281,
      "learning_rate": 2.0283333333333336e-05,
      "loss": 0.0022,
      "step": 71320
    },
    {
      "epoch": 4.755333333333334,
      "grad_norm": 0.5351659059524536,
      "learning_rate": 2.0279166666666667e-05,
      "loss": 0.0017,
      "step": 71330
    },
    {
      "epoch": 4.756,
      "grad_norm": 0.0695706382393837,
      "learning_rate": 2.0275e-05,
      "loss": 0.0015,
      "step": 71340
    },
    {
      "epoch": 4.756666666666667,
      "grad_norm": 0.5631872415542603,
      "learning_rate": 2.0270833333333333e-05,
      "loss": 0.0024,
      "step": 71350
    },
    {
      "epoch": 4.757333333333333,
      "grad_norm": 0.4504788815975189,
      "learning_rate": 2.0266666666666667e-05,
      "loss": 0.002,
      "step": 71360
    },
    {
      "epoch": 4.758,
      "grad_norm": 1.0072168111801147,
      "learning_rate": 2.02625e-05,
      "loss": 0.0017,
      "step": 71370
    },
    {
      "epoch": 4.758666666666667,
      "grad_norm": 0.07176486402750015,
      "learning_rate": 2.0258333333333336e-05,
      "loss": 0.0016,
      "step": 71380
    },
    {
      "epoch": 4.759333333333333,
      "grad_norm": 0.06825635582208633,
      "learning_rate": 2.025416666666667e-05,
      "loss": 0.002,
      "step": 71390
    },
    {
      "epoch": 4.76,
      "grad_norm": 0.4959454834461212,
      "learning_rate": 2.025e-05,
      "loss": 0.0014,
      "step": 71400
    },
    {
      "epoch": 4.760666666666666,
      "grad_norm": 0.19430454075336456,
      "learning_rate": 2.0245833333333332e-05,
      "loss": 0.0017,
      "step": 71410
    },
    {
      "epoch": 4.761333333333333,
      "grad_norm": 0.35060492157936096,
      "learning_rate": 2.0241666666666666e-05,
      "loss": 0.0021,
      "step": 71420
    },
    {
      "epoch": 4.7620000000000005,
      "grad_norm": 0.3884698450565338,
      "learning_rate": 2.02375e-05,
      "loss": 0.002,
      "step": 71430
    },
    {
      "epoch": 4.762666666666667,
      "grad_norm": 0.20825695991516113,
      "learning_rate": 2.0233333333333335e-05,
      "loss": 0.0022,
      "step": 71440
    },
    {
      "epoch": 4.763333333333334,
      "grad_norm": 0.28682735562324524,
      "learning_rate": 2.022916666666667e-05,
      "loss": 0.0015,
      "step": 71450
    },
    {
      "epoch": 4.764,
      "grad_norm": 0.03112301230430603,
      "learning_rate": 2.0225000000000004e-05,
      "loss": 0.0023,
      "step": 71460
    },
    {
      "epoch": 4.764666666666667,
      "grad_norm": 0.3153534531593323,
      "learning_rate": 2.0220833333333335e-05,
      "loss": 0.0022,
      "step": 71470
    },
    {
      "epoch": 4.765333333333333,
      "grad_norm": 0.10341810435056686,
      "learning_rate": 2.0216666666666666e-05,
      "loss": 0.0013,
      "step": 71480
    },
    {
      "epoch": 4.766,
      "grad_norm": 0.05595477670431137,
      "learning_rate": 2.02125e-05,
      "loss": 0.0018,
      "step": 71490
    },
    {
      "epoch": 4.766666666666667,
      "grad_norm": 0.24521490931510925,
      "learning_rate": 2.0208333333333334e-05,
      "loss": 0.0018,
      "step": 71500
    },
    {
      "epoch": 4.767333333333333,
      "grad_norm": 0.24291066825389862,
      "learning_rate": 2.020416666666667e-05,
      "loss": 0.0021,
      "step": 71510
    },
    {
      "epoch": 4.768,
      "grad_norm": 0.07202433794736862,
      "learning_rate": 2.0200000000000003e-05,
      "loss": 0.0024,
      "step": 71520
    },
    {
      "epoch": 4.768666666666666,
      "grad_norm": 0.27579861879348755,
      "learning_rate": 2.0195833333333334e-05,
      "loss": 0.0016,
      "step": 71530
    },
    {
      "epoch": 4.769333333333333,
      "grad_norm": 0.24873942136764526,
      "learning_rate": 2.019166666666667e-05,
      "loss": 0.0028,
      "step": 71540
    },
    {
      "epoch": 4.77,
      "grad_norm": 0.3103426396846771,
      "learning_rate": 2.01875e-05,
      "loss": 0.0019,
      "step": 71550
    },
    {
      "epoch": 4.770666666666667,
      "grad_norm": 0.2510954439640045,
      "learning_rate": 2.0183333333333334e-05,
      "loss": 0.0015,
      "step": 71560
    },
    {
      "epoch": 4.771333333333334,
      "grad_norm": 0.34854093194007874,
      "learning_rate": 2.0179166666666668e-05,
      "loss": 0.0011,
      "step": 71570
    },
    {
      "epoch": 4.772,
      "grad_norm": 0.6304577589035034,
      "learning_rate": 2.0175000000000003e-05,
      "loss": 0.0018,
      "step": 71580
    },
    {
      "epoch": 4.772666666666667,
      "grad_norm": 0.10603101551532745,
      "learning_rate": 2.0170833333333333e-05,
      "loss": 0.0016,
      "step": 71590
    },
    {
      "epoch": 4.773333333333333,
      "grad_norm": 0.10913044214248657,
      "learning_rate": 2.0166666666666668e-05,
      "loss": 0.0021,
      "step": 71600
    },
    {
      "epoch": 4.774,
      "grad_norm": 0.34076377749443054,
      "learning_rate": 2.0162500000000002e-05,
      "loss": 0.0012,
      "step": 71610
    },
    {
      "epoch": 4.774666666666667,
      "grad_norm": 0.17522411048412323,
      "learning_rate": 2.0158333333333333e-05,
      "loss": 0.0021,
      "step": 71620
    },
    {
      "epoch": 4.775333333333333,
      "grad_norm": 0.10917074233293533,
      "learning_rate": 2.0154166666666668e-05,
      "loss": 0.0025,
      "step": 71630
    },
    {
      "epoch": 4.776,
      "grad_norm": 0.613658607006073,
      "learning_rate": 2.0150000000000002e-05,
      "loss": 0.0016,
      "step": 71640
    },
    {
      "epoch": 4.776666666666666,
      "grad_norm": 0.061750367283821106,
      "learning_rate": 2.0145833333333333e-05,
      "loss": 0.002,
      "step": 71650
    },
    {
      "epoch": 4.777333333333333,
      "grad_norm": 0.10605363547801971,
      "learning_rate": 2.0141666666666667e-05,
      "loss": 0.0014,
      "step": 71660
    },
    {
      "epoch": 4.7780000000000005,
      "grad_norm": 0.5455164313316345,
      "learning_rate": 2.01375e-05,
      "loss": 0.0019,
      "step": 71670
    },
    {
      "epoch": 4.778666666666666,
      "grad_norm": 0.273850679397583,
      "learning_rate": 2.0133333333333336e-05,
      "loss": 0.0015,
      "step": 71680
    },
    {
      "epoch": 4.779333333333334,
      "grad_norm": 0.2763197124004364,
      "learning_rate": 2.0129166666666667e-05,
      "loss": 0.0023,
      "step": 71690
    },
    {
      "epoch": 4.78,
      "grad_norm": 0.7569087743759155,
      "learning_rate": 2.0125e-05,
      "loss": 0.0022,
      "step": 71700
    },
    {
      "epoch": 4.780666666666667,
      "grad_norm": 1.1015305519104004,
      "learning_rate": 2.0120833333333332e-05,
      "loss": 0.0017,
      "step": 71710
    },
    {
      "epoch": 4.781333333333333,
      "grad_norm": 0.20652945339679718,
      "learning_rate": 2.0116666666666667e-05,
      "loss": 0.0027,
      "step": 71720
    },
    {
      "epoch": 4.782,
      "grad_norm": 0.4933476150035858,
      "learning_rate": 2.01125e-05,
      "loss": 0.0019,
      "step": 71730
    },
    {
      "epoch": 4.782666666666667,
      "grad_norm": 0.23941172659397125,
      "learning_rate": 2.0108333333333335e-05,
      "loss": 0.0016,
      "step": 71740
    },
    {
      "epoch": 4.783333333333333,
      "grad_norm": 0.17496034502983093,
      "learning_rate": 2.010416666666667e-05,
      "loss": 0.0019,
      "step": 71750
    },
    {
      "epoch": 4.784,
      "grad_norm": 0.5099595189094543,
      "learning_rate": 2.01e-05,
      "loss": 0.0024,
      "step": 71760
    },
    {
      "epoch": 4.784666666666666,
      "grad_norm": 0.31100228428840637,
      "learning_rate": 2.009583333333333e-05,
      "loss": 0.0017,
      "step": 71770
    },
    {
      "epoch": 4.785333333333333,
      "grad_norm": 0.31710824370384216,
      "learning_rate": 2.0091666666666666e-05,
      "loss": 0.0015,
      "step": 71780
    },
    {
      "epoch": 4.786,
      "grad_norm": 0.3140283226966858,
      "learning_rate": 2.00875e-05,
      "loss": 0.0015,
      "step": 71790
    },
    {
      "epoch": 4.786666666666667,
      "grad_norm": 0.4832676351070404,
      "learning_rate": 2.0083333333333335e-05,
      "loss": 0.0019,
      "step": 71800
    },
    {
      "epoch": 4.787333333333334,
      "grad_norm": 0.5949804782867432,
      "learning_rate": 2.007916666666667e-05,
      "loss": 0.0025,
      "step": 71810
    },
    {
      "epoch": 4.788,
      "grad_norm": 0.0786261111497879,
      "learning_rate": 2.0075000000000003e-05,
      "loss": 0.0019,
      "step": 71820
    },
    {
      "epoch": 4.788666666666667,
      "grad_norm": 0.24476109445095062,
      "learning_rate": 2.0070833333333334e-05,
      "loss": 0.0025,
      "step": 71830
    },
    {
      "epoch": 4.789333333333333,
      "grad_norm": 0.10406789183616638,
      "learning_rate": 2.0066666666666665e-05,
      "loss": 0.0025,
      "step": 71840
    },
    {
      "epoch": 4.79,
      "grad_norm": 0.26145118474960327,
      "learning_rate": 2.00625e-05,
      "loss": 0.0025,
      "step": 71850
    },
    {
      "epoch": 4.790666666666667,
      "grad_norm": 0.04067098721861839,
      "learning_rate": 2.0058333333333334e-05,
      "loss": 0.0019,
      "step": 71860
    },
    {
      "epoch": 4.791333333333333,
      "grad_norm": 0.31203514337539673,
      "learning_rate": 2.005416666666667e-05,
      "loss": 0.0016,
      "step": 71870
    },
    {
      "epoch": 4.792,
      "grad_norm": 0.26968973875045776,
      "learning_rate": 2.0050000000000003e-05,
      "loss": 0.0021,
      "step": 71880
    },
    {
      "epoch": 4.792666666666666,
      "grad_norm": 0.3812204897403717,
      "learning_rate": 2.0045833333333334e-05,
      "loss": 0.0023,
      "step": 71890
    },
    {
      "epoch": 4.793333333333333,
      "grad_norm": 0.2746393382549286,
      "learning_rate": 2.0041666666666668e-05,
      "loss": 0.002,
      "step": 71900
    },
    {
      "epoch": 4.7940000000000005,
      "grad_norm": 0.41050058603286743,
      "learning_rate": 2.00375e-05,
      "loss": 0.0016,
      "step": 71910
    },
    {
      "epoch": 4.794666666666666,
      "grad_norm": 0.2814495265483856,
      "learning_rate": 2.0033333333333334e-05,
      "loss": 0.0016,
      "step": 71920
    },
    {
      "epoch": 4.795333333333334,
      "grad_norm": 0.24997813999652863,
      "learning_rate": 2.0029166666666668e-05,
      "loss": 0.0015,
      "step": 71930
    },
    {
      "epoch": 4.796,
      "grad_norm": 0.07643207162618637,
      "learning_rate": 2.0025000000000002e-05,
      "loss": 0.0018,
      "step": 71940
    },
    {
      "epoch": 4.796666666666667,
      "grad_norm": 0.37356966733932495,
      "learning_rate": 2.0020833333333333e-05,
      "loss": 0.0028,
      "step": 71950
    },
    {
      "epoch": 4.7973333333333334,
      "grad_norm": 0.2479751706123352,
      "learning_rate": 2.0016666666666668e-05,
      "loss": 0.0015,
      "step": 71960
    },
    {
      "epoch": 4.798,
      "grad_norm": 0.3503655791282654,
      "learning_rate": 2.0012500000000002e-05,
      "loss": 0.0015,
      "step": 71970
    },
    {
      "epoch": 4.798666666666667,
      "grad_norm": 0.5599088668823242,
      "learning_rate": 2.0008333333333333e-05,
      "loss": 0.0019,
      "step": 71980
    },
    {
      "epoch": 4.799333333333333,
      "grad_norm": 0.24720925092697144,
      "learning_rate": 2.0004166666666667e-05,
      "loss": 0.0027,
      "step": 71990
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.32312068343162537,
      "learning_rate": 2e-05,
      "loss": 0.0013,
      "step": 72000
    },
    {
      "epoch": 4.800666666666666,
      "grad_norm": 0.5883823037147522,
      "learning_rate": 1.9995833333333333e-05,
      "loss": 0.0019,
      "step": 72010
    },
    {
      "epoch": 4.801333333333333,
      "grad_norm": 0.7882413268089294,
      "learning_rate": 1.9991666666666667e-05,
      "loss": 0.0027,
      "step": 72020
    },
    {
      "epoch": 4.802,
      "grad_norm": 0.4955446720123291,
      "learning_rate": 1.99875e-05,
      "loss": 0.0031,
      "step": 72030
    },
    {
      "epoch": 4.802666666666667,
      "grad_norm": 0.6176210641860962,
      "learning_rate": 1.9983333333333336e-05,
      "loss": 0.0024,
      "step": 72040
    },
    {
      "epoch": 4.803333333333334,
      "grad_norm": 0.555239737033844,
      "learning_rate": 1.9979166666666667e-05,
      "loss": 0.0025,
      "step": 72050
    },
    {
      "epoch": 4.804,
      "grad_norm": 0.41482022404670715,
      "learning_rate": 1.9975e-05,
      "loss": 0.0023,
      "step": 72060
    },
    {
      "epoch": 4.804666666666667,
      "grad_norm": 0.17271097004413605,
      "learning_rate": 1.9970833333333332e-05,
      "loss": 0.002,
      "step": 72070
    },
    {
      "epoch": 4.8053333333333335,
      "grad_norm": 0.5181224942207336,
      "learning_rate": 1.9966666666666666e-05,
      "loss": 0.0021,
      "step": 72080
    },
    {
      "epoch": 4.806,
      "grad_norm": 0.3745153248310089,
      "learning_rate": 1.99625e-05,
      "loss": 0.0021,
      "step": 72090
    },
    {
      "epoch": 4.806666666666667,
      "grad_norm": 0.3570917546749115,
      "learning_rate": 1.9958333333333335e-05,
      "loss": 0.0014,
      "step": 72100
    },
    {
      "epoch": 4.807333333333333,
      "grad_norm": 0.05103713274002075,
      "learning_rate": 1.995416666666667e-05,
      "loss": 0.0022,
      "step": 72110
    },
    {
      "epoch": 4.808,
      "grad_norm": 0.3259449303150177,
      "learning_rate": 1.995e-05,
      "loss": 0.0021,
      "step": 72120
    },
    {
      "epoch": 4.808666666666666,
      "grad_norm": 0.2139701545238495,
      "learning_rate": 1.994583333333333e-05,
      "loss": 0.0018,
      "step": 72130
    },
    {
      "epoch": 4.809333333333333,
      "grad_norm": 0.3526514768600464,
      "learning_rate": 1.9941666666666666e-05,
      "loss": 0.0022,
      "step": 72140
    },
    {
      "epoch": 4.8100000000000005,
      "grad_norm": 0.48667407035827637,
      "learning_rate": 1.99375e-05,
      "loss": 0.0017,
      "step": 72150
    },
    {
      "epoch": 4.810666666666666,
      "grad_norm": 0.3220071792602539,
      "learning_rate": 1.9933333333333334e-05,
      "loss": 0.0022,
      "step": 72160
    },
    {
      "epoch": 4.811333333333334,
      "grad_norm": 0.1770128607749939,
      "learning_rate": 1.992916666666667e-05,
      "loss": 0.0018,
      "step": 72170
    },
    {
      "epoch": 4.812,
      "grad_norm": 0.31275755167007446,
      "learning_rate": 1.9925000000000003e-05,
      "loss": 0.0023,
      "step": 72180
    },
    {
      "epoch": 4.812666666666667,
      "grad_norm": 0.6512459516525269,
      "learning_rate": 1.9920833333333334e-05,
      "loss": 0.0018,
      "step": 72190
    },
    {
      "epoch": 4.8133333333333335,
      "grad_norm": 0.9136344194412231,
      "learning_rate": 1.9916666666666665e-05,
      "loss": 0.0019,
      "step": 72200
    },
    {
      "epoch": 4.814,
      "grad_norm": 0.049432966858148575,
      "learning_rate": 1.99125e-05,
      "loss": 0.0029,
      "step": 72210
    },
    {
      "epoch": 4.814666666666667,
      "grad_norm": 0.3500761091709137,
      "learning_rate": 1.9908333333333334e-05,
      "loss": 0.0015,
      "step": 72220
    },
    {
      "epoch": 4.815333333333333,
      "grad_norm": 0.35082024335861206,
      "learning_rate": 1.9904166666666668e-05,
      "loss": 0.0024,
      "step": 72230
    },
    {
      "epoch": 4.816,
      "grad_norm": 0.18075372278690338,
      "learning_rate": 1.9900000000000003e-05,
      "loss": 0.0018,
      "step": 72240
    },
    {
      "epoch": 4.816666666666666,
      "grad_norm": 0.04175560548901558,
      "learning_rate": 1.9895833333333334e-05,
      "loss": 0.0029,
      "step": 72250
    },
    {
      "epoch": 4.817333333333333,
      "grad_norm": 0.1355670988559723,
      "learning_rate": 1.9891666666666668e-05,
      "loss": 0.0024,
      "step": 72260
    },
    {
      "epoch": 4.818,
      "grad_norm": 0.06748195737600327,
      "learning_rate": 1.98875e-05,
      "loss": 0.0018,
      "step": 72270
    },
    {
      "epoch": 4.818666666666667,
      "grad_norm": 0.04264857620000839,
      "learning_rate": 1.9883333333333333e-05,
      "loss": 0.0021,
      "step": 72280
    },
    {
      "epoch": 4.819333333333334,
      "grad_norm": 0.13732609152793884,
      "learning_rate": 1.9879166666666668e-05,
      "loss": 0.0022,
      "step": 72290
    },
    {
      "epoch": 4.82,
      "grad_norm": 0.207377091050148,
      "learning_rate": 1.9875000000000002e-05,
      "loss": 0.0022,
      "step": 72300
    },
    {
      "epoch": 4.820666666666667,
      "grad_norm": 0.4871528148651123,
      "learning_rate": 1.9870833333333333e-05,
      "loss": 0.0017,
      "step": 72310
    },
    {
      "epoch": 4.8213333333333335,
      "grad_norm": 0.025974968448281288,
      "learning_rate": 1.9866666666666667e-05,
      "loss": 0.0033,
      "step": 72320
    },
    {
      "epoch": 4.822,
      "grad_norm": 0.2476411759853363,
      "learning_rate": 1.98625e-05,
      "loss": 0.0017,
      "step": 72330
    },
    {
      "epoch": 4.822666666666667,
      "grad_norm": 0.17668016254901886,
      "learning_rate": 1.9858333333333333e-05,
      "loss": 0.0016,
      "step": 72340
    },
    {
      "epoch": 4.823333333333333,
      "grad_norm": 0.45430877804756165,
      "learning_rate": 1.9854166666666667e-05,
      "loss": 0.0013,
      "step": 72350
    },
    {
      "epoch": 4.824,
      "grad_norm": 0.053366489708423615,
      "learning_rate": 1.985e-05,
      "loss": 0.0024,
      "step": 72360
    },
    {
      "epoch": 4.824666666666666,
      "grad_norm": 0.1784350872039795,
      "learning_rate": 1.9845833333333332e-05,
      "loss": 0.0021,
      "step": 72370
    },
    {
      "epoch": 4.825333333333333,
      "grad_norm": 0.5325729846954346,
      "learning_rate": 1.9841666666666667e-05,
      "loss": 0.003,
      "step": 72380
    },
    {
      "epoch": 4.826,
      "grad_norm": 0.6507669687271118,
      "learning_rate": 1.98375e-05,
      "loss": 0.0022,
      "step": 72390
    },
    {
      "epoch": 4.826666666666666,
      "grad_norm": 0.18151701986789703,
      "learning_rate": 1.9833333333333335e-05,
      "loss": 0.002,
      "step": 72400
    },
    {
      "epoch": 4.827333333333334,
      "grad_norm": 0.4882820248603821,
      "learning_rate": 1.982916666666667e-05,
      "loss": 0.0029,
      "step": 72410
    },
    {
      "epoch": 4.828,
      "grad_norm": 0.21582967042922974,
      "learning_rate": 1.9825e-05,
      "loss": 0.0021,
      "step": 72420
    },
    {
      "epoch": 4.828666666666667,
      "grad_norm": 0.245183527469635,
      "learning_rate": 1.9820833333333332e-05,
      "loss": 0.0016,
      "step": 72430
    },
    {
      "epoch": 4.8293333333333335,
      "grad_norm": 0.23034532368183136,
      "learning_rate": 1.9816666666666666e-05,
      "loss": 0.0017,
      "step": 72440
    },
    {
      "epoch": 4.83,
      "grad_norm": 0.4561005234718323,
      "learning_rate": 1.98125e-05,
      "loss": 0.0013,
      "step": 72450
    },
    {
      "epoch": 4.830666666666667,
      "grad_norm": 0.21427308022975922,
      "learning_rate": 1.9808333333333335e-05,
      "loss": 0.0017,
      "step": 72460
    },
    {
      "epoch": 4.831333333333333,
      "grad_norm": 0.05268895998597145,
      "learning_rate": 1.980416666666667e-05,
      "loss": 0.0027,
      "step": 72470
    },
    {
      "epoch": 4.832,
      "grad_norm": 0.46873629093170166,
      "learning_rate": 1.9800000000000004e-05,
      "loss": 0.0016,
      "step": 72480
    },
    {
      "epoch": 4.832666666666666,
      "grad_norm": 0.17666932940483093,
      "learning_rate": 1.9795833333333335e-05,
      "loss": 0.0023,
      "step": 72490
    },
    {
      "epoch": 4.833333333333333,
      "grad_norm": 0.5200340747833252,
      "learning_rate": 1.9791666666666665e-05,
      "loss": 0.0014,
      "step": 72500
    },
    {
      "epoch": 4.834,
      "grad_norm": 0.24930129945278168,
      "learning_rate": 1.97875e-05,
      "loss": 0.0017,
      "step": 72510
    },
    {
      "epoch": 4.834666666666667,
      "grad_norm": 0.745164155960083,
      "learning_rate": 1.9783333333333334e-05,
      "loss": 0.0016,
      "step": 72520
    },
    {
      "epoch": 4.835333333333334,
      "grad_norm": 0.10502798110246658,
      "learning_rate": 1.977916666666667e-05,
      "loss": 0.0022,
      "step": 72530
    },
    {
      "epoch": 4.836,
      "grad_norm": 0.35112136602401733,
      "learning_rate": 1.9775000000000003e-05,
      "loss": 0.002,
      "step": 72540
    },
    {
      "epoch": 4.836666666666667,
      "grad_norm": 0.055817995220422745,
      "learning_rate": 1.9770833333333337e-05,
      "loss": 0.0017,
      "step": 72550
    },
    {
      "epoch": 4.8373333333333335,
      "grad_norm": 0.04671463742852211,
      "learning_rate": 1.9766666666666668e-05,
      "loss": 0.002,
      "step": 72560
    },
    {
      "epoch": 4.838,
      "grad_norm": 0.6195678114891052,
      "learning_rate": 1.97625e-05,
      "loss": 0.0026,
      "step": 72570
    },
    {
      "epoch": 4.838666666666667,
      "grad_norm": 0.5478006601333618,
      "learning_rate": 1.9758333333333334e-05,
      "loss": 0.0018,
      "step": 72580
    },
    {
      "epoch": 4.839333333333333,
      "grad_norm": 0.05593264475464821,
      "learning_rate": 1.9754166666666668e-05,
      "loss": 0.0014,
      "step": 72590
    },
    {
      "epoch": 4.84,
      "grad_norm": 0.08843293786048889,
      "learning_rate": 1.9750000000000002e-05,
      "loss": 0.0022,
      "step": 72600
    },
    {
      "epoch": 4.8406666666666665,
      "grad_norm": 0.5865519046783447,
      "learning_rate": 1.9745833333333337e-05,
      "loss": 0.0023,
      "step": 72610
    },
    {
      "epoch": 4.841333333333333,
      "grad_norm": 0.49608463048934937,
      "learning_rate": 1.9741666666666668e-05,
      "loss": 0.0027,
      "step": 72620
    },
    {
      "epoch": 4.842,
      "grad_norm": 0.44997072219848633,
      "learning_rate": 1.9737500000000002e-05,
      "loss": 0.003,
      "step": 72630
    },
    {
      "epoch": 4.842666666666666,
      "grad_norm": 0.17744405567646027,
      "learning_rate": 1.9733333333333333e-05,
      "loss": 0.0012,
      "step": 72640
    },
    {
      "epoch": 4.843333333333334,
      "grad_norm": 0.044581446796655655,
      "learning_rate": 1.9729166666666667e-05,
      "loss": 0.0023,
      "step": 72650
    },
    {
      "epoch": 4.844,
      "grad_norm": 0.10494759678840637,
      "learning_rate": 1.9725000000000002e-05,
      "loss": 0.0023,
      "step": 72660
    },
    {
      "epoch": 4.844666666666667,
      "grad_norm": 0.38245466351509094,
      "learning_rate": 1.9720833333333336e-05,
      "loss": 0.0023,
      "step": 72670
    },
    {
      "epoch": 4.8453333333333335,
      "grad_norm": 0.45767998695373535,
      "learning_rate": 1.9716666666666667e-05,
      "loss": 0.0016,
      "step": 72680
    },
    {
      "epoch": 4.846,
      "grad_norm": 0.18920361995697021,
      "learning_rate": 1.97125e-05,
      "loss": 0.0016,
      "step": 72690
    },
    {
      "epoch": 4.846666666666667,
      "grad_norm": 0.17781829833984375,
      "learning_rate": 1.9708333333333336e-05,
      "loss": 0.0013,
      "step": 72700
    },
    {
      "epoch": 4.847333333333333,
      "grad_norm": 0.06273836642503738,
      "learning_rate": 1.9704166666666667e-05,
      "loss": 0.0028,
      "step": 72710
    },
    {
      "epoch": 4.848,
      "grad_norm": 0.23765364289283752,
      "learning_rate": 1.97e-05,
      "loss": 0.0012,
      "step": 72720
    },
    {
      "epoch": 4.8486666666666665,
      "grad_norm": 0.5914527773857117,
      "learning_rate": 1.9695833333333335e-05,
      "loss": 0.0027,
      "step": 72730
    },
    {
      "epoch": 4.849333333333333,
      "grad_norm": 0.5539475679397583,
      "learning_rate": 1.9691666666666666e-05,
      "loss": 0.0019,
      "step": 72740
    },
    {
      "epoch": 4.85,
      "grad_norm": 0.20624256134033203,
      "learning_rate": 1.96875e-05,
      "loss": 0.0019,
      "step": 72750
    },
    {
      "epoch": 4.850666666666667,
      "grad_norm": 0.2132442593574524,
      "learning_rate": 1.9683333333333335e-05,
      "loss": 0.002,
      "step": 72760
    },
    {
      "epoch": 4.851333333333334,
      "grad_norm": 0.4792420268058777,
      "learning_rate": 1.967916666666667e-05,
      "loss": 0.0016,
      "step": 72770
    },
    {
      "epoch": 4.852,
      "grad_norm": 0.37894171476364136,
      "learning_rate": 1.9675e-05,
      "loss": 0.0016,
      "step": 72780
    },
    {
      "epoch": 4.852666666666667,
      "grad_norm": 0.5886090993881226,
      "learning_rate": 1.9670833333333335e-05,
      "loss": 0.0014,
      "step": 72790
    },
    {
      "epoch": 4.8533333333333335,
      "grad_norm": 0.37822139263153076,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 0.0015,
      "step": 72800
    },
    {
      "epoch": 4.854,
      "grad_norm": 0.09696552157402039,
      "learning_rate": 1.96625e-05,
      "loss": 0.0018,
      "step": 72810
    },
    {
      "epoch": 4.854666666666667,
      "grad_norm": 0.3417017459869385,
      "learning_rate": 1.9658333333333335e-05,
      "loss": 0.002,
      "step": 72820
    },
    {
      "epoch": 4.855333333333333,
      "grad_norm": 0.5467817187309265,
      "learning_rate": 1.965416666666667e-05,
      "loss": 0.0022,
      "step": 72830
    },
    {
      "epoch": 4.856,
      "grad_norm": 0.4187266528606415,
      "learning_rate": 1.9650000000000003e-05,
      "loss": 0.0016,
      "step": 72840
    },
    {
      "epoch": 4.8566666666666665,
      "grad_norm": 0.27640262246131897,
      "learning_rate": 1.9645833333333334e-05,
      "loss": 0.0025,
      "step": 72850
    },
    {
      "epoch": 4.857333333333333,
      "grad_norm": 0.3770182728767395,
      "learning_rate": 1.9641666666666665e-05,
      "loss": 0.0026,
      "step": 72860
    },
    {
      "epoch": 4.858,
      "grad_norm": 0.14713427424430847,
      "learning_rate": 1.96375e-05,
      "loss": 0.0023,
      "step": 72870
    },
    {
      "epoch": 4.858666666666666,
      "grad_norm": 0.06412956863641739,
      "learning_rate": 1.9633333333333334e-05,
      "loss": 0.0015,
      "step": 72880
    },
    {
      "epoch": 4.859333333333334,
      "grad_norm": 0.6512230634689331,
      "learning_rate": 1.962916666666667e-05,
      "loss": 0.0021,
      "step": 72890
    },
    {
      "epoch": 4.86,
      "grad_norm": 0.7605252265930176,
      "learning_rate": 1.9625000000000003e-05,
      "loss": 0.0018,
      "step": 72900
    },
    {
      "epoch": 4.860666666666667,
      "grad_norm": 0.4746936559677124,
      "learning_rate": 1.9620833333333337e-05,
      "loss": 0.0024,
      "step": 72910
    },
    {
      "epoch": 4.8613333333333335,
      "grad_norm": 0.4174632430076599,
      "learning_rate": 1.9616666666666668e-05,
      "loss": 0.0016,
      "step": 72920
    },
    {
      "epoch": 4.862,
      "grad_norm": 0.8991504907608032,
      "learning_rate": 1.96125e-05,
      "loss": 0.0018,
      "step": 72930
    },
    {
      "epoch": 4.862666666666667,
      "grad_norm": 0.18352924287319183,
      "learning_rate": 1.9608333333333333e-05,
      "loss": 0.0035,
      "step": 72940
    },
    {
      "epoch": 4.863333333333333,
      "grad_norm": 0.4767129719257355,
      "learning_rate": 1.9604166666666668e-05,
      "loss": 0.0015,
      "step": 72950
    },
    {
      "epoch": 4.864,
      "grad_norm": 0.24860309064388275,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.0018,
      "step": 72960
    },
    {
      "epoch": 4.8646666666666665,
      "grad_norm": 0.7255223393440247,
      "learning_rate": 1.9595833333333336e-05,
      "loss": 0.0014,
      "step": 72970
    },
    {
      "epoch": 4.865333333333333,
      "grad_norm": 0.4882875382900238,
      "learning_rate": 1.9591666666666667e-05,
      "loss": 0.0017,
      "step": 72980
    },
    {
      "epoch": 4.866,
      "grad_norm": 0.04947974532842636,
      "learning_rate": 1.9587500000000002e-05,
      "loss": 0.0025,
      "step": 72990
    },
    {
      "epoch": 4.866666666666667,
      "grad_norm": 0.3491002023220062,
      "learning_rate": 1.9583333333333333e-05,
      "loss": 0.0015,
      "step": 73000
    },
    {
      "epoch": 4.867333333333333,
      "grad_norm": 0.3261584937572479,
      "learning_rate": 1.9579166666666667e-05,
      "loss": 0.002,
      "step": 73010
    },
    {
      "epoch": 4.868,
      "grad_norm": 0.5549048781394958,
      "learning_rate": 1.9575e-05,
      "loss": 0.0021,
      "step": 73020
    },
    {
      "epoch": 4.868666666666667,
      "grad_norm": 0.20082682371139526,
      "learning_rate": 1.9570833333333336e-05,
      "loss": 0.0015,
      "step": 73030
    },
    {
      "epoch": 4.8693333333333335,
      "grad_norm": 0.09493272751569748,
      "learning_rate": 1.9566666666666667e-05,
      "loss": 0.0022,
      "step": 73040
    },
    {
      "epoch": 4.87,
      "grad_norm": 0.5496967434883118,
      "learning_rate": 1.95625e-05,
      "loss": 0.0024,
      "step": 73050
    },
    {
      "epoch": 4.870666666666667,
      "grad_norm": 0.3124925196170807,
      "learning_rate": 1.9558333333333336e-05,
      "loss": 0.0017,
      "step": 73060
    },
    {
      "epoch": 4.871333333333333,
      "grad_norm": 0.07180316001176834,
      "learning_rate": 1.9554166666666667e-05,
      "loss": 0.0017,
      "step": 73070
    },
    {
      "epoch": 4.872,
      "grad_norm": 0.2118789702653885,
      "learning_rate": 1.955e-05,
      "loss": 0.0018,
      "step": 73080
    },
    {
      "epoch": 4.8726666666666665,
      "grad_norm": 0.0736636370420456,
      "learning_rate": 1.9545833333333335e-05,
      "loss": 0.0019,
      "step": 73090
    },
    {
      "epoch": 4.873333333333333,
      "grad_norm": 0.6632149815559387,
      "learning_rate": 1.9541666666666666e-05,
      "loss": 0.0028,
      "step": 73100
    },
    {
      "epoch": 4.874,
      "grad_norm": 0.755918025970459,
      "learning_rate": 1.95375e-05,
      "loss": 0.0018,
      "step": 73110
    },
    {
      "epoch": 4.874666666666666,
      "grad_norm": 0.09590853750705719,
      "learning_rate": 1.9533333333333335e-05,
      "loss": 0.0019,
      "step": 73120
    },
    {
      "epoch": 4.875333333333334,
      "grad_norm": 0.24243998527526855,
      "learning_rate": 1.952916666666667e-05,
      "loss": 0.0015,
      "step": 73130
    },
    {
      "epoch": 4.876,
      "grad_norm": 0.2416723668575287,
      "learning_rate": 1.9525e-05,
      "loss": 0.002,
      "step": 73140
    },
    {
      "epoch": 4.876666666666667,
      "grad_norm": 0.40371161699295044,
      "learning_rate": 1.9520833333333335e-05,
      "loss": 0.0012,
      "step": 73150
    },
    {
      "epoch": 4.8773333333333335,
      "grad_norm": 0.46000489592552185,
      "learning_rate": 1.9516666666666666e-05,
      "loss": 0.0026,
      "step": 73160
    },
    {
      "epoch": 4.878,
      "grad_norm": 0.25600260496139526,
      "learning_rate": 1.95125e-05,
      "loss": 0.0021,
      "step": 73170
    },
    {
      "epoch": 4.878666666666667,
      "grad_norm": 0.5813702940940857,
      "learning_rate": 1.9508333333333334e-05,
      "loss": 0.0014,
      "step": 73180
    },
    {
      "epoch": 4.879333333333333,
      "grad_norm": 0.10351745784282684,
      "learning_rate": 1.950416666666667e-05,
      "loss": 0.0024,
      "step": 73190
    },
    {
      "epoch": 4.88,
      "grad_norm": 0.41639649868011475,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 0.0015,
      "step": 73200
    },
    {
      "epoch": 4.8806666666666665,
      "grad_norm": 0.34603872895240784,
      "learning_rate": 1.9495833333333334e-05,
      "loss": 0.0014,
      "step": 73210
    },
    {
      "epoch": 4.881333333333333,
      "grad_norm": 0.0442916639149189,
      "learning_rate": 1.9491666666666665e-05,
      "loss": 0.0018,
      "step": 73220
    },
    {
      "epoch": 4.882,
      "grad_norm": 0.3132938742637634,
      "learning_rate": 1.94875e-05,
      "loss": 0.002,
      "step": 73230
    },
    {
      "epoch": 4.882666666666667,
      "grad_norm": 0.02633565478026867,
      "learning_rate": 1.9483333333333334e-05,
      "loss": 0.0026,
      "step": 73240
    },
    {
      "epoch": 4.883333333333333,
      "grad_norm": 0.38615018129348755,
      "learning_rate": 1.9479166666666668e-05,
      "loss": 0.0022,
      "step": 73250
    },
    {
      "epoch": 4.884,
      "grad_norm": 0.5184490084648132,
      "learning_rate": 1.9475000000000002e-05,
      "loss": 0.0024,
      "step": 73260
    },
    {
      "epoch": 4.884666666666667,
      "grad_norm": 0.5177555680274963,
      "learning_rate": 1.9470833333333337e-05,
      "loss": 0.0028,
      "step": 73270
    },
    {
      "epoch": 4.8853333333333335,
      "grad_norm": 0.21104796230793,
      "learning_rate": 1.9466666666666668e-05,
      "loss": 0.0021,
      "step": 73280
    },
    {
      "epoch": 4.886,
      "grad_norm": 0.1882191002368927,
      "learning_rate": 1.94625e-05,
      "loss": 0.0015,
      "step": 73290
    },
    {
      "epoch": 4.886666666666667,
      "grad_norm": 0.31414613127708435,
      "learning_rate": 1.9458333333333333e-05,
      "loss": 0.0018,
      "step": 73300
    },
    {
      "epoch": 4.887333333333333,
      "grad_norm": 0.34277114272117615,
      "learning_rate": 1.9454166666666667e-05,
      "loss": 0.0017,
      "step": 73310
    },
    {
      "epoch": 4.888,
      "grad_norm": 0.524181604385376,
      "learning_rate": 1.9450000000000002e-05,
      "loss": 0.0021,
      "step": 73320
    },
    {
      "epoch": 4.8886666666666665,
      "grad_norm": 0.06069299206137657,
      "learning_rate": 1.9445833333333336e-05,
      "loss": 0.0019,
      "step": 73330
    },
    {
      "epoch": 4.889333333333333,
      "grad_norm": 0.44970592856407166,
      "learning_rate": 1.9441666666666667e-05,
      "loss": 0.0016,
      "step": 73340
    },
    {
      "epoch": 4.89,
      "grad_norm": 0.10953191667795181,
      "learning_rate": 1.94375e-05,
      "loss": 0.0015,
      "step": 73350
    },
    {
      "epoch": 4.890666666666666,
      "grad_norm": 0.3669135570526123,
      "learning_rate": 1.9433333333333332e-05,
      "loss": 0.0033,
      "step": 73360
    },
    {
      "epoch": 4.891333333333334,
      "grad_norm": 0.21421945095062256,
      "learning_rate": 1.9429166666666667e-05,
      "loss": 0.0027,
      "step": 73370
    },
    {
      "epoch": 4.892,
      "grad_norm": 0.468779981136322,
      "learning_rate": 1.9425e-05,
      "loss": 0.0022,
      "step": 73380
    },
    {
      "epoch": 4.892666666666667,
      "grad_norm": 0.3814973533153534,
      "learning_rate": 1.9420833333333336e-05,
      "loss": 0.0014,
      "step": 73390
    },
    {
      "epoch": 4.8933333333333335,
      "grad_norm": 0.03253183141350746,
      "learning_rate": 1.9416666666666667e-05,
      "loss": 0.0024,
      "step": 73400
    },
    {
      "epoch": 4.894,
      "grad_norm": 0.3430575728416443,
      "learning_rate": 1.94125e-05,
      "loss": 0.0017,
      "step": 73410
    },
    {
      "epoch": 4.894666666666667,
      "grad_norm": 0.10888256877660751,
      "learning_rate": 1.9408333333333335e-05,
      "loss": 0.0017,
      "step": 73420
    },
    {
      "epoch": 4.895333333333333,
      "grad_norm": 0.28378504514694214,
      "learning_rate": 1.9404166666666666e-05,
      "loss": 0.0015,
      "step": 73430
    },
    {
      "epoch": 4.896,
      "grad_norm": 0.14798840880393982,
      "learning_rate": 1.94e-05,
      "loss": 0.0015,
      "step": 73440
    },
    {
      "epoch": 4.8966666666666665,
      "grad_norm": 0.057550638914108276,
      "learning_rate": 1.9395833333333335e-05,
      "loss": 0.0018,
      "step": 73450
    },
    {
      "epoch": 4.897333333333333,
      "grad_norm": 0.7426463961601257,
      "learning_rate": 1.9391666666666666e-05,
      "loss": 0.0015,
      "step": 73460
    },
    {
      "epoch": 4.898,
      "grad_norm": 0.22140847146511078,
      "learning_rate": 1.93875e-05,
      "loss": 0.0015,
      "step": 73470
    },
    {
      "epoch": 4.898666666666666,
      "grad_norm": 0.552219808101654,
      "learning_rate": 1.9383333333333335e-05,
      "loss": 0.0019,
      "step": 73480
    },
    {
      "epoch": 4.899333333333333,
      "grad_norm": 0.18227529525756836,
      "learning_rate": 1.937916666666667e-05,
      "loss": 0.0022,
      "step": 73490
    },
    {
      "epoch": 4.9,
      "grad_norm": 0.3124188482761383,
      "learning_rate": 1.9375e-05,
      "loss": 0.0029,
      "step": 73500
    },
    {
      "epoch": 4.900666666666667,
      "grad_norm": 0.048124004155397415,
      "learning_rate": 1.9370833333333334e-05,
      "loss": 0.0019,
      "step": 73510
    },
    {
      "epoch": 4.9013333333333335,
      "grad_norm": 0.24442315101623535,
      "learning_rate": 1.9366666666666665e-05,
      "loss": 0.002,
      "step": 73520
    },
    {
      "epoch": 4.902,
      "grad_norm": 0.07390318810939789,
      "learning_rate": 1.93625e-05,
      "loss": 0.002,
      "step": 73530
    },
    {
      "epoch": 4.902666666666667,
      "grad_norm": 0.24089333415031433,
      "learning_rate": 1.9358333333333334e-05,
      "loss": 0.0021,
      "step": 73540
    },
    {
      "epoch": 4.903333333333333,
      "grad_norm": 0.11160171031951904,
      "learning_rate": 1.935416666666667e-05,
      "loss": 0.002,
      "step": 73550
    },
    {
      "epoch": 4.904,
      "grad_norm": 0.2773750424385071,
      "learning_rate": 1.9350000000000003e-05,
      "loss": 0.0023,
      "step": 73560
    },
    {
      "epoch": 4.9046666666666665,
      "grad_norm": 1.0001615285873413,
      "learning_rate": 1.9345833333333334e-05,
      "loss": 0.0012,
      "step": 73570
    },
    {
      "epoch": 4.905333333333333,
      "grad_norm": 0.045655664056539536,
      "learning_rate": 1.9341666666666665e-05,
      "loss": 0.0029,
      "step": 73580
    },
    {
      "epoch": 4.906,
      "grad_norm": 0.17004626989364624,
      "learning_rate": 1.93375e-05,
      "loss": 0.0021,
      "step": 73590
    },
    {
      "epoch": 4.906666666666666,
      "grad_norm": 0.2745516002178192,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 0.0017,
      "step": 73600
    },
    {
      "epoch": 4.907333333333334,
      "grad_norm": 0.10681337863206863,
      "learning_rate": 1.9329166666666668e-05,
      "loss": 0.002,
      "step": 73610
    },
    {
      "epoch": 4.908,
      "grad_norm": 0.4772956073284149,
      "learning_rate": 1.9325000000000002e-05,
      "loss": 0.002,
      "step": 73620
    },
    {
      "epoch": 4.908666666666667,
      "grad_norm": 0.11200466006994247,
      "learning_rate": 1.9320833333333337e-05,
      "loss": 0.0015,
      "step": 73630
    },
    {
      "epoch": 4.9093333333333335,
      "grad_norm": 0.3118642568588257,
      "learning_rate": 1.9316666666666668e-05,
      "loss": 0.0016,
      "step": 73640
    },
    {
      "epoch": 4.91,
      "grad_norm": 0.2116995006799698,
      "learning_rate": 1.93125e-05,
      "loss": 0.0028,
      "step": 73650
    },
    {
      "epoch": 4.910666666666667,
      "grad_norm": 0.4904644191265106,
      "learning_rate": 1.9308333333333333e-05,
      "loss": 0.0019,
      "step": 73660
    },
    {
      "epoch": 4.911333333333333,
      "grad_norm": 0.366598904132843,
      "learning_rate": 1.9304166666666667e-05,
      "loss": 0.0018,
      "step": 73670
    },
    {
      "epoch": 4.912,
      "grad_norm": 0.2737370431423187,
      "learning_rate": 1.93e-05,
      "loss": 0.0029,
      "step": 73680
    },
    {
      "epoch": 4.9126666666666665,
      "grad_norm": 0.8451033234596252,
      "learning_rate": 1.9295833333333336e-05,
      "loss": 0.0014,
      "step": 73690
    },
    {
      "epoch": 4.913333333333333,
      "grad_norm": 0.14460891485214233,
      "learning_rate": 1.9291666666666667e-05,
      "loss": 0.0024,
      "step": 73700
    },
    {
      "epoch": 4.914,
      "grad_norm": 0.035502947866916656,
      "learning_rate": 1.92875e-05,
      "loss": 0.0027,
      "step": 73710
    },
    {
      "epoch": 4.914666666666666,
      "grad_norm": 0.31503307819366455,
      "learning_rate": 1.9283333333333332e-05,
      "loss": 0.0019,
      "step": 73720
    },
    {
      "epoch": 4.915333333333333,
      "grad_norm": 0.11133620142936707,
      "learning_rate": 1.9279166666666667e-05,
      "loss": 0.0019,
      "step": 73730
    },
    {
      "epoch": 4.916,
      "grad_norm": 0.288045197725296,
      "learning_rate": 1.9275e-05,
      "loss": 0.0027,
      "step": 73740
    },
    {
      "epoch": 4.916666666666667,
      "grad_norm": 0.14491011202335358,
      "learning_rate": 1.9270833333333335e-05,
      "loss": 0.0023,
      "step": 73750
    },
    {
      "epoch": 4.917333333333334,
      "grad_norm": 0.24765363335609436,
      "learning_rate": 1.926666666666667e-05,
      "loss": 0.0023,
      "step": 73760
    },
    {
      "epoch": 4.918,
      "grad_norm": 0.05434787645936012,
      "learning_rate": 1.92625e-05,
      "loss": 0.0028,
      "step": 73770
    },
    {
      "epoch": 4.918666666666667,
      "grad_norm": 0.11175905913114548,
      "learning_rate": 1.9258333333333335e-05,
      "loss": 0.002,
      "step": 73780
    },
    {
      "epoch": 4.919333333333333,
      "grad_norm": 0.523946225643158,
      "learning_rate": 1.9254166666666666e-05,
      "loss": 0.002,
      "step": 73790
    },
    {
      "epoch": 4.92,
      "grad_norm": 0.23188382387161255,
      "learning_rate": 1.925e-05,
      "loss": 0.0013,
      "step": 73800
    },
    {
      "epoch": 4.9206666666666665,
      "grad_norm": 0.6184602379798889,
      "learning_rate": 1.9245833333333335e-05,
      "loss": 0.0019,
      "step": 73810
    },
    {
      "epoch": 4.921333333333333,
      "grad_norm": 0.27203240990638733,
      "learning_rate": 1.924166666666667e-05,
      "loss": 0.0024,
      "step": 73820
    },
    {
      "epoch": 4.922,
      "grad_norm": 0.6282888054847717,
      "learning_rate": 1.92375e-05,
      "loss": 0.0023,
      "step": 73830
    },
    {
      "epoch": 4.922666666666666,
      "grad_norm": 0.2015872746706009,
      "learning_rate": 1.9233333333333334e-05,
      "loss": 0.0021,
      "step": 73840
    },
    {
      "epoch": 4.923333333333334,
      "grad_norm": 0.44637659192085266,
      "learning_rate": 1.922916666666667e-05,
      "loss": 0.002,
      "step": 73850
    },
    {
      "epoch": 4.924,
      "grad_norm": 0.04861215874552727,
      "learning_rate": 1.9225e-05,
      "loss": 0.0025,
      "step": 73860
    },
    {
      "epoch": 4.924666666666667,
      "grad_norm": 0.1774195432662964,
      "learning_rate": 1.9220833333333334e-05,
      "loss": 0.0015,
      "step": 73870
    },
    {
      "epoch": 4.925333333333334,
      "grad_norm": 0.17068897187709808,
      "learning_rate": 1.921666666666667e-05,
      "loss": 0.0017,
      "step": 73880
    },
    {
      "epoch": 4.926,
      "grad_norm": 0.036847397685050964,
      "learning_rate": 1.92125e-05,
      "loss": 0.0036,
      "step": 73890
    },
    {
      "epoch": 4.926666666666667,
      "grad_norm": 0.3072998523712158,
      "learning_rate": 1.9208333333333334e-05,
      "loss": 0.0023,
      "step": 73900
    },
    {
      "epoch": 4.927333333333333,
      "grad_norm": 0.13789254426956177,
      "learning_rate": 1.9204166666666668e-05,
      "loss": 0.0023,
      "step": 73910
    },
    {
      "epoch": 4.928,
      "grad_norm": 0.24340999126434326,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.0019,
      "step": 73920
    },
    {
      "epoch": 4.9286666666666665,
      "grad_norm": 0.18057756125926971,
      "learning_rate": 1.9195833333333333e-05,
      "loss": 0.0019,
      "step": 73930
    },
    {
      "epoch": 4.929333333333333,
      "grad_norm": 0.23080912232398987,
      "learning_rate": 1.9191666666666668e-05,
      "loss": 0.0014,
      "step": 73940
    },
    {
      "epoch": 4.93,
      "grad_norm": 0.038601722568273544,
      "learning_rate": 1.91875e-05,
      "loss": 0.0024,
      "step": 73950
    },
    {
      "epoch": 4.930666666666666,
      "grad_norm": 0.4719089865684509,
      "learning_rate": 1.9183333333333333e-05,
      "loss": 0.0027,
      "step": 73960
    },
    {
      "epoch": 4.931333333333333,
      "grad_norm": 0.211741104722023,
      "learning_rate": 1.9179166666666668e-05,
      "loss": 0.0011,
      "step": 73970
    },
    {
      "epoch": 4.932,
      "grad_norm": 0.2768059968948364,
      "learning_rate": 1.9175000000000002e-05,
      "loss": 0.0014,
      "step": 73980
    },
    {
      "epoch": 4.932666666666667,
      "grad_norm": 0.18823659420013428,
      "learning_rate": 1.9170833333333336e-05,
      "loss": 0.0025,
      "step": 73990
    },
    {
      "epoch": 4.933333333333334,
      "grad_norm": 0.25400447845458984,
      "learning_rate": 1.9166666666666667e-05,
      "loss": 0.0032,
      "step": 74000
    },
    {
      "epoch": 4.934,
      "grad_norm": 0.10435480624437332,
      "learning_rate": 1.9162499999999998e-05,
      "loss": 0.0027,
      "step": 74010
    },
    {
      "epoch": 4.934666666666667,
      "grad_norm": 0.2108105719089508,
      "learning_rate": 1.9158333333333333e-05,
      "loss": 0.0016,
      "step": 74020
    },
    {
      "epoch": 4.935333333333333,
      "grad_norm": 0.14366455376148224,
      "learning_rate": 1.9154166666666667e-05,
      "loss": 0.0015,
      "step": 74030
    },
    {
      "epoch": 4.936,
      "grad_norm": 0.18282298743724823,
      "learning_rate": 1.915e-05,
      "loss": 0.0018,
      "step": 74040
    },
    {
      "epoch": 4.9366666666666665,
      "grad_norm": 0.08269470930099487,
      "learning_rate": 1.9145833333333336e-05,
      "loss": 0.0028,
      "step": 74050
    },
    {
      "epoch": 4.937333333333333,
      "grad_norm": 0.6988974213600159,
      "learning_rate": 1.914166666666667e-05,
      "loss": 0.0021,
      "step": 74060
    },
    {
      "epoch": 4.938,
      "grad_norm": 0.34695154428482056,
      "learning_rate": 1.91375e-05,
      "loss": 0.0033,
      "step": 74070
    },
    {
      "epoch": 4.938666666666666,
      "grad_norm": 0.17453621327877045,
      "learning_rate": 1.9133333333333332e-05,
      "loss": 0.0016,
      "step": 74080
    },
    {
      "epoch": 4.939333333333334,
      "grad_norm": 0.6898081302642822,
      "learning_rate": 1.9129166666666666e-05,
      "loss": 0.002,
      "step": 74090
    },
    {
      "epoch": 4.9399999999999995,
      "grad_norm": 0.7643939852714539,
      "learning_rate": 1.9125e-05,
      "loss": 0.0016,
      "step": 74100
    },
    {
      "epoch": 4.940666666666667,
      "grad_norm": 0.33395642042160034,
      "learning_rate": 1.9120833333333335e-05,
      "loss": 0.0027,
      "step": 74110
    },
    {
      "epoch": 4.941333333333334,
      "grad_norm": 0.20472091436386108,
      "learning_rate": 1.911666666666667e-05,
      "loss": 0.0019,
      "step": 74120
    },
    {
      "epoch": 4.942,
      "grad_norm": 0.11389833688735962,
      "learning_rate": 1.91125e-05,
      "loss": 0.0022,
      "step": 74130
    },
    {
      "epoch": 4.942666666666667,
      "grad_norm": 0.17730076611042023,
      "learning_rate": 1.9108333333333335e-05,
      "loss": 0.0011,
      "step": 74140
    },
    {
      "epoch": 4.943333333333333,
      "grad_norm": 0.1370386928319931,
      "learning_rate": 1.910416666666667e-05,
      "loss": 0.0023,
      "step": 74150
    },
    {
      "epoch": 4.944,
      "grad_norm": 0.18106600642204285,
      "learning_rate": 1.91e-05,
      "loss": 0.0023,
      "step": 74160
    },
    {
      "epoch": 4.9446666666666665,
      "grad_norm": 0.5490069389343262,
      "learning_rate": 1.9095833333333334e-05,
      "loss": 0.0023,
      "step": 74170
    },
    {
      "epoch": 4.945333333333333,
      "grad_norm": 0.06453651934862137,
      "learning_rate": 1.909166666666667e-05,
      "loss": 0.0026,
      "step": 74180
    },
    {
      "epoch": 4.946,
      "grad_norm": 0.4368059039115906,
      "learning_rate": 1.90875e-05,
      "loss": 0.0022,
      "step": 74190
    },
    {
      "epoch": 4.946666666666666,
      "grad_norm": 0.272724449634552,
      "learning_rate": 1.9083333333333334e-05,
      "loss": 0.0019,
      "step": 74200
    },
    {
      "epoch": 4.947333333333333,
      "grad_norm": 0.5242933034896851,
      "learning_rate": 1.907916666666667e-05,
      "loss": 0.0014,
      "step": 74210
    },
    {
      "epoch": 4.948,
      "grad_norm": 0.1497010886669159,
      "learning_rate": 1.9075000000000003e-05,
      "loss": 0.0015,
      "step": 74220
    },
    {
      "epoch": 4.948666666666667,
      "grad_norm": 0.28445860743522644,
      "learning_rate": 1.9070833333333334e-05,
      "loss": 0.002,
      "step": 74230
    },
    {
      "epoch": 4.949333333333334,
      "grad_norm": 0.3739440143108368,
      "learning_rate": 1.9066666666666668e-05,
      "loss": 0.0019,
      "step": 74240
    },
    {
      "epoch": 4.95,
      "grad_norm": 0.48687562346458435,
      "learning_rate": 1.90625e-05,
      "loss": 0.0022,
      "step": 74250
    },
    {
      "epoch": 4.950666666666667,
      "grad_norm": 0.2772388458251953,
      "learning_rate": 1.9058333333333334e-05,
      "loss": 0.0017,
      "step": 74260
    },
    {
      "epoch": 4.951333333333333,
      "grad_norm": 0.2111884206533432,
      "learning_rate": 1.9054166666666668e-05,
      "loss": 0.0016,
      "step": 74270
    },
    {
      "epoch": 4.952,
      "grad_norm": 0.027504079043865204,
      "learning_rate": 1.9050000000000002e-05,
      "loss": 0.0021,
      "step": 74280
    },
    {
      "epoch": 4.9526666666666666,
      "grad_norm": 0.4841252863407135,
      "learning_rate": 1.9045833333333337e-05,
      "loss": 0.0016,
      "step": 74290
    },
    {
      "epoch": 4.953333333333333,
      "grad_norm": 0.18663963675498962,
      "learning_rate": 1.9041666666666668e-05,
      "loss": 0.0019,
      "step": 74300
    },
    {
      "epoch": 4.954,
      "grad_norm": 0.05960666015744209,
      "learning_rate": 1.90375e-05,
      "loss": 0.0027,
      "step": 74310
    },
    {
      "epoch": 4.954666666666666,
      "grad_norm": 0.41779595613479614,
      "learning_rate": 1.9033333333333333e-05,
      "loss": 0.0022,
      "step": 74320
    },
    {
      "epoch": 4.955333333333334,
      "grad_norm": 0.5212723612785339,
      "learning_rate": 1.9029166666666667e-05,
      "loss": 0.0028,
      "step": 74330
    },
    {
      "epoch": 4.9559999999999995,
      "grad_norm": 0.09135662764310837,
      "learning_rate": 1.9025e-05,
      "loss": 0.0015,
      "step": 74340
    },
    {
      "epoch": 4.956666666666667,
      "grad_norm": 0.17787185311317444,
      "learning_rate": 1.9020833333333336e-05,
      "loss": 0.0023,
      "step": 74350
    },
    {
      "epoch": 4.957333333333334,
      "grad_norm": 0.8333752155303955,
      "learning_rate": 1.901666666666667e-05,
      "loss": 0.0022,
      "step": 74360
    },
    {
      "epoch": 4.958,
      "grad_norm": 0.14458835124969482,
      "learning_rate": 1.90125e-05,
      "loss": 0.0018,
      "step": 74370
    },
    {
      "epoch": 4.958666666666667,
      "grad_norm": 0.1423518806695938,
      "learning_rate": 1.9008333333333332e-05,
      "loss": 0.002,
      "step": 74380
    },
    {
      "epoch": 4.959333333333333,
      "grad_norm": 0.5773481130599976,
      "learning_rate": 1.9004166666666667e-05,
      "loss": 0.0023,
      "step": 74390
    },
    {
      "epoch": 4.96,
      "grad_norm": 0.13791701197624207,
      "learning_rate": 1.9e-05,
      "loss": 0.0015,
      "step": 74400
    },
    {
      "epoch": 4.960666666666667,
      "grad_norm": 0.08446619659662247,
      "learning_rate": 1.8995833333333335e-05,
      "loss": 0.0023,
      "step": 74410
    },
    {
      "epoch": 4.961333333333333,
      "grad_norm": 0.24086721241474152,
      "learning_rate": 1.899166666666667e-05,
      "loss": 0.0019,
      "step": 74420
    },
    {
      "epoch": 4.962,
      "grad_norm": 0.4533977508544922,
      "learning_rate": 1.89875e-05,
      "loss": 0.0027,
      "step": 74430
    },
    {
      "epoch": 4.962666666666666,
      "grad_norm": 0.1826266199350357,
      "learning_rate": 1.8983333333333335e-05,
      "loss": 0.0024,
      "step": 74440
    },
    {
      "epoch": 4.963333333333333,
      "grad_norm": 0.32752731442451477,
      "learning_rate": 1.8979166666666666e-05,
      "loss": 0.0021,
      "step": 74450
    },
    {
      "epoch": 4.964,
      "grad_norm": 0.24861495196819305,
      "learning_rate": 1.8975e-05,
      "loss": 0.0014,
      "step": 74460
    },
    {
      "epoch": 4.964666666666667,
      "grad_norm": 0.27739953994750977,
      "learning_rate": 1.8970833333333335e-05,
      "loss": 0.0016,
      "step": 74470
    },
    {
      "epoch": 4.965333333333334,
      "grad_norm": 0.2586808502674103,
      "learning_rate": 1.896666666666667e-05,
      "loss": 0.0015,
      "step": 74480
    },
    {
      "epoch": 4.966,
      "grad_norm": 0.05656588077545166,
      "learning_rate": 1.89625e-05,
      "loss": 0.0012,
      "step": 74490
    },
    {
      "epoch": 4.966666666666667,
      "grad_norm": 0.3544688820838928,
      "learning_rate": 1.8958333333333334e-05,
      "loss": 0.0015,
      "step": 74500
    },
    {
      "epoch": 4.967333333333333,
      "grad_norm": 0.11030780524015427,
      "learning_rate": 1.895416666666667e-05,
      "loss": 0.0027,
      "step": 74510
    },
    {
      "epoch": 4.968,
      "grad_norm": 0.2758854031562805,
      "learning_rate": 1.895e-05,
      "loss": 0.0018,
      "step": 74520
    },
    {
      "epoch": 4.968666666666667,
      "grad_norm": 0.18388022482395172,
      "learning_rate": 1.8945833333333334e-05,
      "loss": 0.0021,
      "step": 74530
    },
    {
      "epoch": 4.969333333333333,
      "grad_norm": 0.1127001941204071,
      "learning_rate": 1.894166666666667e-05,
      "loss": 0.0014,
      "step": 74540
    },
    {
      "epoch": 4.97,
      "grad_norm": 0.180572509765625,
      "learning_rate": 1.89375e-05,
      "loss": 0.0014,
      "step": 74550
    },
    {
      "epoch": 4.970666666666666,
      "grad_norm": 0.6560266613960266,
      "learning_rate": 1.8933333333333334e-05,
      "loss": 0.0018,
      "step": 74560
    },
    {
      "epoch": 4.971333333333334,
      "grad_norm": 0.3833698332309723,
      "learning_rate": 1.8929166666666668e-05,
      "loss": 0.0013,
      "step": 74570
    },
    {
      "epoch": 4.9719999999999995,
      "grad_norm": 0.3898612856864929,
      "learning_rate": 1.8925000000000003e-05,
      "loss": 0.0017,
      "step": 74580
    },
    {
      "epoch": 4.972666666666667,
      "grad_norm": 0.27562215924263,
      "learning_rate": 1.8920833333333334e-05,
      "loss": 0.0023,
      "step": 74590
    },
    {
      "epoch": 4.973333333333334,
      "grad_norm": 0.34293848276138306,
      "learning_rate": 1.8916666666666668e-05,
      "loss": 0.0017,
      "step": 74600
    },
    {
      "epoch": 4.974,
      "grad_norm": 0.4540422260761261,
      "learning_rate": 1.89125e-05,
      "loss": 0.0033,
      "step": 74610
    },
    {
      "epoch": 4.974666666666667,
      "grad_norm": 0.08527292311191559,
      "learning_rate": 1.8908333333333333e-05,
      "loss": 0.0019,
      "step": 74620
    },
    {
      "epoch": 4.975333333333333,
      "grad_norm": 0.3519841432571411,
      "learning_rate": 1.8904166666666668e-05,
      "loss": 0.0018,
      "step": 74630
    },
    {
      "epoch": 4.976,
      "grad_norm": 0.6534625887870789,
      "learning_rate": 1.8900000000000002e-05,
      "loss": 0.0016,
      "step": 74640
    },
    {
      "epoch": 4.976666666666667,
      "grad_norm": 0.05603783205151558,
      "learning_rate": 1.8895833333333336e-05,
      "loss": 0.0016,
      "step": 74650
    },
    {
      "epoch": 4.977333333333333,
      "grad_norm": 0.49448099732398987,
      "learning_rate": 1.8891666666666667e-05,
      "loss": 0.0017,
      "step": 74660
    },
    {
      "epoch": 4.978,
      "grad_norm": 0.825992226600647,
      "learning_rate": 1.88875e-05,
      "loss": 0.0024,
      "step": 74670
    },
    {
      "epoch": 4.978666666666666,
      "grad_norm": 0.2793090045452118,
      "learning_rate": 1.8883333333333333e-05,
      "loss": 0.0019,
      "step": 74680
    },
    {
      "epoch": 4.979333333333333,
      "grad_norm": 0.1430693417787552,
      "learning_rate": 1.8879166666666667e-05,
      "loss": 0.0029,
      "step": 74690
    },
    {
      "epoch": 4.98,
      "grad_norm": 0.4094374179840088,
      "learning_rate": 1.8875e-05,
      "loss": 0.0029,
      "step": 74700
    },
    {
      "epoch": 4.980666666666667,
      "grad_norm": 0.053465113043785095,
      "learning_rate": 1.8870833333333336e-05,
      "loss": 0.0014,
      "step": 74710
    },
    {
      "epoch": 4.981333333333334,
      "grad_norm": 0.5319461822509766,
      "learning_rate": 1.886666666666667e-05,
      "loss": 0.0015,
      "step": 74720
    },
    {
      "epoch": 4.982,
      "grad_norm": 0.3743034601211548,
      "learning_rate": 1.88625e-05,
      "loss": 0.0012,
      "step": 74730
    },
    {
      "epoch": 4.982666666666667,
      "grad_norm": 0.5495322346687317,
      "learning_rate": 1.8858333333333332e-05,
      "loss": 0.0016,
      "step": 74740
    },
    {
      "epoch": 4.983333333333333,
      "grad_norm": 0.02323274500668049,
      "learning_rate": 1.8854166666666666e-05,
      "loss": 0.0017,
      "step": 74750
    },
    {
      "epoch": 4.984,
      "grad_norm": 0.2100016176700592,
      "learning_rate": 1.885e-05,
      "loss": 0.0019,
      "step": 74760
    },
    {
      "epoch": 4.984666666666667,
      "grad_norm": 0.20834758877754211,
      "learning_rate": 1.8845833333333335e-05,
      "loss": 0.0017,
      "step": 74770
    },
    {
      "epoch": 4.985333333333333,
      "grad_norm": 0.14005644619464874,
      "learning_rate": 1.884166666666667e-05,
      "loss": 0.0013,
      "step": 74780
    },
    {
      "epoch": 4.986,
      "grad_norm": 0.4309738576412201,
      "learning_rate": 1.88375e-05,
      "loss": 0.0018,
      "step": 74790
    },
    {
      "epoch": 4.986666666666666,
      "grad_norm": 0.46573716402053833,
      "learning_rate": 1.8833333333333335e-05,
      "loss": 0.0018,
      "step": 74800
    },
    {
      "epoch": 4.987333333333333,
      "grad_norm": 0.5787120461463928,
      "learning_rate": 1.8829166666666666e-05,
      "loss": 0.0015,
      "step": 74810
    },
    {
      "epoch": 4.9879999999999995,
      "grad_norm": 0.5955823659896851,
      "learning_rate": 1.8825e-05,
      "loss": 0.0016,
      "step": 74820
    },
    {
      "epoch": 4.988666666666667,
      "grad_norm": 0.4191916584968567,
      "learning_rate": 1.8820833333333335e-05,
      "loss": 0.0016,
      "step": 74830
    },
    {
      "epoch": 4.989333333333334,
      "grad_norm": 0.17889522016048431,
      "learning_rate": 1.881666666666667e-05,
      "loss": 0.0016,
      "step": 74840
    },
    {
      "epoch": 4.99,
      "grad_norm": 0.5545528531074524,
      "learning_rate": 1.88125e-05,
      "loss": 0.0022,
      "step": 74850
    },
    {
      "epoch": 4.990666666666667,
      "grad_norm": 0.6620852947235107,
      "learning_rate": 1.8808333333333334e-05,
      "loss": 0.0021,
      "step": 74860
    },
    {
      "epoch": 4.991333333333333,
      "grad_norm": 0.415554016828537,
      "learning_rate": 1.880416666666667e-05,
      "loss": 0.0022,
      "step": 74870
    },
    {
      "epoch": 4.992,
      "grad_norm": 0.16963613033294678,
      "learning_rate": 1.88e-05,
      "loss": 0.0018,
      "step": 74880
    },
    {
      "epoch": 4.992666666666667,
      "grad_norm": 0.10234784334897995,
      "learning_rate": 1.8795833333333334e-05,
      "loss": 0.0024,
      "step": 74890
    },
    {
      "epoch": 4.993333333333333,
      "grad_norm": 0.24301116168498993,
      "learning_rate": 1.8791666666666668e-05,
      "loss": 0.0018,
      "step": 74900
    },
    {
      "epoch": 4.994,
      "grad_norm": 0.16978491842746735,
      "learning_rate": 1.87875e-05,
      "loss": 0.0019,
      "step": 74910
    },
    {
      "epoch": 4.994666666666666,
      "grad_norm": 0.2599879801273346,
      "learning_rate": 1.8783333333333334e-05,
      "loss": 0.0016,
      "step": 74920
    },
    {
      "epoch": 4.995333333333333,
      "grad_norm": 0.18056438863277435,
      "learning_rate": 1.8779166666666668e-05,
      "loss": 0.0019,
      "step": 74930
    },
    {
      "epoch": 4.996,
      "grad_norm": 0.13849931955337524,
      "learning_rate": 1.8775000000000002e-05,
      "loss": 0.0017,
      "step": 74940
    },
    {
      "epoch": 4.996666666666667,
      "grad_norm": 0.4473256766796112,
      "learning_rate": 1.8770833333333333e-05,
      "loss": 0.0027,
      "step": 74950
    },
    {
      "epoch": 4.997333333333334,
      "grad_norm": 0.2520206570625305,
      "learning_rate": 1.8766666666666668e-05,
      "loss": 0.0017,
      "step": 74960
    },
    {
      "epoch": 4.998,
      "grad_norm": 0.5389222502708435,
      "learning_rate": 1.87625e-05,
      "loss": 0.0017,
      "step": 74970
    },
    {
      "epoch": 4.998666666666667,
      "grad_norm": 0.789552628993988,
      "learning_rate": 1.8758333333333333e-05,
      "loss": 0.0024,
      "step": 74980
    },
    {
      "epoch": 4.999333333333333,
      "grad_norm": 0.0859348401427269,
      "learning_rate": 1.8754166666666667e-05,
      "loss": 0.0025,
      "step": 74990
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.07701396942138672,
      "learning_rate": 1.8750000000000002e-05,
      "loss": 0.0018,
      "step": 75000
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.0019185383571311831,
      "eval_runtime": 164.0253,
      "eval_samples_per_second": 1219.324,
      "eval_steps_per_second": 30.483,
      "step": 75000
    },
    {
      "epoch": 5.000666666666667,
      "grad_norm": 0.17719584703445435,
      "learning_rate": 1.8745833333333336e-05,
      "loss": 0.0015,
      "step": 75010
    },
    {
      "epoch": 5.001333333333333,
      "grad_norm": 0.044255003333091736,
      "learning_rate": 1.8741666666666667e-05,
      "loss": 0.0014,
      "step": 75020
    },
    {
      "epoch": 5.002,
      "grad_norm": 0.1813596934080124,
      "learning_rate": 1.87375e-05,
      "loss": 0.0015,
      "step": 75030
    },
    {
      "epoch": 5.002666666666666,
      "grad_norm": 0.1458134651184082,
      "learning_rate": 1.8733333333333332e-05,
      "loss": 0.0015,
      "step": 75040
    },
    {
      "epoch": 5.003333333333333,
      "grad_norm": 0.317781537771225,
      "learning_rate": 1.8729166666666667e-05,
      "loss": 0.0017,
      "step": 75050
    },
    {
      "epoch": 5.004,
      "grad_norm": 0.5146853923797607,
      "learning_rate": 1.8725e-05,
      "loss": 0.0019,
      "step": 75060
    },
    {
      "epoch": 5.004666666666667,
      "grad_norm": 0.3158819377422333,
      "learning_rate": 1.8720833333333335e-05,
      "loss": 0.0017,
      "step": 75070
    },
    {
      "epoch": 5.005333333333334,
      "grad_norm": 0.04479852691292763,
      "learning_rate": 1.871666666666667e-05,
      "loss": 0.0039,
      "step": 75080
    },
    {
      "epoch": 5.006,
      "grad_norm": 0.4292072653770447,
      "learning_rate": 1.87125e-05,
      "loss": 0.002,
      "step": 75090
    },
    {
      "epoch": 5.006666666666667,
      "grad_norm": 0.14509792625904083,
      "learning_rate": 1.8708333333333332e-05,
      "loss": 0.0013,
      "step": 75100
    },
    {
      "epoch": 5.007333333333333,
      "grad_norm": 0.18956758081912994,
      "learning_rate": 1.8704166666666666e-05,
      "loss": 0.0014,
      "step": 75110
    },
    {
      "epoch": 5.008,
      "grad_norm": 0.6616425514221191,
      "learning_rate": 1.87e-05,
      "loss": 0.0016,
      "step": 75120
    },
    {
      "epoch": 5.008666666666667,
      "grad_norm": 0.2896532118320465,
      "learning_rate": 1.8695833333333335e-05,
      "loss": 0.0023,
      "step": 75130
    },
    {
      "epoch": 5.009333333333333,
      "grad_norm": 0.07744346559047699,
      "learning_rate": 1.869166666666667e-05,
      "loss": 0.0019,
      "step": 75140
    },
    {
      "epoch": 5.01,
      "grad_norm": 0.07966680824756622,
      "learning_rate": 1.8687500000000004e-05,
      "loss": 0.0018,
      "step": 75150
    },
    {
      "epoch": 5.010666666666666,
      "grad_norm": 0.3850344121456146,
      "learning_rate": 1.8683333333333335e-05,
      "loss": 0.0019,
      "step": 75160
    },
    {
      "epoch": 5.011333333333333,
      "grad_norm": 0.5996816754341125,
      "learning_rate": 1.8679166666666666e-05,
      "loss": 0.0024,
      "step": 75170
    },
    {
      "epoch": 5.012,
      "grad_norm": 0.4888420104980469,
      "learning_rate": 1.8675e-05,
      "loss": 0.0017,
      "step": 75180
    },
    {
      "epoch": 5.012666666666667,
      "grad_norm": 0.28053542971611023,
      "learning_rate": 1.8670833333333334e-05,
      "loss": 0.0017,
      "step": 75190
    },
    {
      "epoch": 5.013333333333334,
      "grad_norm": 0.9117767214775085,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.0022,
      "step": 75200
    },
    {
      "epoch": 5.014,
      "grad_norm": 0.04661436006426811,
      "learning_rate": 1.8662500000000003e-05,
      "loss": 0.0019,
      "step": 75210
    },
    {
      "epoch": 5.014666666666667,
      "grad_norm": 0.0758243054151535,
      "learning_rate": 1.8658333333333334e-05,
      "loss": 0.0019,
      "step": 75220
    },
    {
      "epoch": 5.015333333333333,
      "grad_norm": 0.05230887979269028,
      "learning_rate": 1.865416666666667e-05,
      "loss": 0.0015,
      "step": 75230
    },
    {
      "epoch": 5.016,
      "grad_norm": 0.2698083221912384,
      "learning_rate": 1.865e-05,
      "loss": 0.0026,
      "step": 75240
    },
    {
      "epoch": 5.016666666666667,
      "grad_norm": 0.45866256952285767,
      "learning_rate": 1.8645833333333334e-05,
      "loss": 0.0019,
      "step": 75250
    },
    {
      "epoch": 5.017333333333333,
      "grad_norm": 0.2947344481945038,
      "learning_rate": 1.8641666666666668e-05,
      "loss": 0.0013,
      "step": 75260
    },
    {
      "epoch": 5.018,
      "grad_norm": 0.378451943397522,
      "learning_rate": 1.8637500000000002e-05,
      "loss": 0.0016,
      "step": 75270
    },
    {
      "epoch": 5.018666666666666,
      "grad_norm": 0.44872918725013733,
      "learning_rate": 1.8633333333333333e-05,
      "loss": 0.002,
      "step": 75280
    },
    {
      "epoch": 5.019333333333333,
      "grad_norm": 0.8393168449401855,
      "learning_rate": 1.8629166666666668e-05,
      "loss": 0.0019,
      "step": 75290
    },
    {
      "epoch": 5.02,
      "grad_norm": 0.3883149027824402,
      "learning_rate": 1.8625000000000002e-05,
      "loss": 0.0014,
      "step": 75300
    },
    {
      "epoch": 5.020666666666667,
      "grad_norm": 0.02400953695178032,
      "learning_rate": 1.8620833333333333e-05,
      "loss": 0.0019,
      "step": 75310
    },
    {
      "epoch": 5.021333333333334,
      "grad_norm": 0.03271808102726936,
      "learning_rate": 1.8616666666666667e-05,
      "loss": 0.0024,
      "step": 75320
    },
    {
      "epoch": 5.022,
      "grad_norm": 0.1044437512755394,
      "learning_rate": 1.8612500000000002e-05,
      "loss": 0.0027,
      "step": 75330
    },
    {
      "epoch": 5.022666666666667,
      "grad_norm": 0.6234539747238159,
      "learning_rate": 1.8608333333333333e-05,
      "loss": 0.0014,
      "step": 75340
    },
    {
      "epoch": 5.023333333333333,
      "grad_norm": 0.13679224252700806,
      "learning_rate": 1.8604166666666667e-05,
      "loss": 0.0023,
      "step": 75350
    },
    {
      "epoch": 5.024,
      "grad_norm": 0.143258735537529,
      "learning_rate": 1.86e-05,
      "loss": 0.0018,
      "step": 75360
    },
    {
      "epoch": 5.024666666666667,
      "grad_norm": 0.7992309927940369,
      "learning_rate": 1.8595833333333336e-05,
      "loss": 0.0015,
      "step": 75370
    },
    {
      "epoch": 5.025333333333333,
      "grad_norm": 0.6863312125205994,
      "learning_rate": 1.8591666666666667e-05,
      "loss": 0.0014,
      "step": 75380
    },
    {
      "epoch": 5.026,
      "grad_norm": 0.7672367691993713,
      "learning_rate": 1.85875e-05,
      "loss": 0.0018,
      "step": 75390
    },
    {
      "epoch": 5.026666666666666,
      "grad_norm": 0.07703179866075516,
      "learning_rate": 1.8583333333333332e-05,
      "loss": 0.0015,
      "step": 75400
    },
    {
      "epoch": 5.027333333333333,
      "grad_norm": 0.7708991169929504,
      "learning_rate": 1.8579166666666667e-05,
      "loss": 0.0022,
      "step": 75410
    },
    {
      "epoch": 5.028,
      "grad_norm": 0.6020393371582031,
      "learning_rate": 1.8575e-05,
      "loss": 0.0023,
      "step": 75420
    },
    {
      "epoch": 5.028666666666667,
      "grad_norm": 0.47033581137657166,
      "learning_rate": 1.8570833333333335e-05,
      "loss": 0.0019,
      "step": 75430
    },
    {
      "epoch": 5.029333333333334,
      "grad_norm": 0.3531140089035034,
      "learning_rate": 1.856666666666667e-05,
      "loss": 0.0021,
      "step": 75440
    },
    {
      "epoch": 5.03,
      "grad_norm": 0.024127451702952385,
      "learning_rate": 1.85625e-05,
      "loss": 0.0021,
      "step": 75450
    },
    {
      "epoch": 5.030666666666667,
      "grad_norm": 0.6231304407119751,
      "learning_rate": 1.855833333333333e-05,
      "loss": 0.0018,
      "step": 75460
    },
    {
      "epoch": 5.031333333333333,
      "grad_norm": 0.07478507608175278,
      "learning_rate": 1.8554166666666666e-05,
      "loss": 0.002,
      "step": 75470
    },
    {
      "epoch": 5.032,
      "grad_norm": 0.02904967963695526,
      "learning_rate": 1.855e-05,
      "loss": 0.0023,
      "step": 75480
    },
    {
      "epoch": 5.032666666666667,
      "grad_norm": 0.042732737958431244,
      "learning_rate": 1.8545833333333335e-05,
      "loss": 0.0025,
      "step": 75490
    },
    {
      "epoch": 5.033333333333333,
      "grad_norm": 0.24582523107528687,
      "learning_rate": 1.854166666666667e-05,
      "loss": 0.0016,
      "step": 75500
    },
    {
      "epoch": 5.034,
      "grad_norm": 0.42350852489471436,
      "learning_rate": 1.8537500000000003e-05,
      "loss": 0.0015,
      "step": 75510
    },
    {
      "epoch": 5.034666666666666,
      "grad_norm": 0.18153922259807587,
      "learning_rate": 1.8533333333333334e-05,
      "loss": 0.0024,
      "step": 75520
    },
    {
      "epoch": 5.035333333333333,
      "grad_norm": 0.2078886479139328,
      "learning_rate": 1.8529166666666665e-05,
      "loss": 0.0034,
      "step": 75530
    },
    {
      "epoch": 5.036,
      "grad_norm": 0.14288920164108276,
      "learning_rate": 1.8525e-05,
      "loss": 0.0018,
      "step": 75540
    },
    {
      "epoch": 5.036666666666667,
      "grad_norm": 0.05160605534911156,
      "learning_rate": 1.8520833333333334e-05,
      "loss": 0.0035,
      "step": 75550
    },
    {
      "epoch": 5.037333333333334,
      "grad_norm": 0.5950658917427063,
      "learning_rate": 1.851666666666667e-05,
      "loss": 0.0022,
      "step": 75560
    },
    {
      "epoch": 5.038,
      "grad_norm": 0.6246338486671448,
      "learning_rate": 1.8512500000000003e-05,
      "loss": 0.0025,
      "step": 75570
    },
    {
      "epoch": 5.038666666666667,
      "grad_norm": 0.09097610414028168,
      "learning_rate": 1.8508333333333334e-05,
      "loss": 0.0028,
      "step": 75580
    },
    {
      "epoch": 5.039333333333333,
      "grad_norm": 0.137929305434227,
      "learning_rate": 1.8504166666666668e-05,
      "loss": 0.0025,
      "step": 75590
    },
    {
      "epoch": 5.04,
      "grad_norm": 0.15930022299289703,
      "learning_rate": 1.85e-05,
      "loss": 0.0029,
      "step": 75600
    },
    {
      "epoch": 5.040666666666667,
      "grad_norm": 0.4222133159637451,
      "learning_rate": 1.8495833333333333e-05,
      "loss": 0.0019,
      "step": 75610
    },
    {
      "epoch": 5.041333333333333,
      "grad_norm": 0.32079923152923584,
      "learning_rate": 1.8491666666666668e-05,
      "loss": 0.0019,
      "step": 75620
    },
    {
      "epoch": 5.042,
      "grad_norm": 0.0443120077252388,
      "learning_rate": 1.8487500000000002e-05,
      "loss": 0.0023,
      "step": 75630
    },
    {
      "epoch": 5.042666666666666,
      "grad_norm": 0.14058071374893188,
      "learning_rate": 1.8483333333333333e-05,
      "loss": 0.0014,
      "step": 75640
    },
    {
      "epoch": 5.043333333333333,
      "grad_norm": 0.1695803850889206,
      "learning_rate": 1.8479166666666667e-05,
      "loss": 0.0019,
      "step": 75650
    },
    {
      "epoch": 5.044,
      "grad_norm": 0.38735681772232056,
      "learning_rate": 1.8475000000000002e-05,
      "loss": 0.0016,
      "step": 75660
    },
    {
      "epoch": 5.044666666666667,
      "grad_norm": 0.36311304569244385,
      "learning_rate": 1.8470833333333333e-05,
      "loss": 0.0013,
      "step": 75670
    },
    {
      "epoch": 5.045333333333334,
      "grad_norm": 0.08209548145532608,
      "learning_rate": 1.8466666666666667e-05,
      "loss": 0.0025,
      "step": 75680
    },
    {
      "epoch": 5.046,
      "grad_norm": 0.8886422514915466,
      "learning_rate": 1.84625e-05,
      "loss": 0.0023,
      "step": 75690
    },
    {
      "epoch": 5.046666666666667,
      "grad_norm": 0.22539100050926208,
      "learning_rate": 1.8458333333333333e-05,
      "loss": 0.0016,
      "step": 75700
    },
    {
      "epoch": 5.0473333333333334,
      "grad_norm": 0.2840726375579834,
      "learning_rate": 1.8454166666666667e-05,
      "loss": 0.0019,
      "step": 75710
    },
    {
      "epoch": 5.048,
      "grad_norm": 0.680779218673706,
      "learning_rate": 1.845e-05,
      "loss": 0.0014,
      "step": 75720
    },
    {
      "epoch": 5.048666666666667,
      "grad_norm": 0.14135940372943878,
      "learning_rate": 1.8445833333333336e-05,
      "loss": 0.0016,
      "step": 75730
    },
    {
      "epoch": 5.049333333333333,
      "grad_norm": 0.25456029176712036,
      "learning_rate": 1.8441666666666667e-05,
      "loss": 0.0026,
      "step": 75740
    },
    {
      "epoch": 5.05,
      "grad_norm": 0.037031520158052444,
      "learning_rate": 1.84375e-05,
      "loss": 0.0022,
      "step": 75750
    },
    {
      "epoch": 5.050666666666666,
      "grad_norm": 0.26793068647384644,
      "learning_rate": 1.8433333333333332e-05,
      "loss": 0.0024,
      "step": 75760
    },
    {
      "epoch": 5.051333333333333,
      "grad_norm": 0.5897119641304016,
      "learning_rate": 1.8429166666666666e-05,
      "loss": 0.0014,
      "step": 75770
    },
    {
      "epoch": 5.052,
      "grad_norm": 0.08090851455926895,
      "learning_rate": 1.8425e-05,
      "loss": 0.0022,
      "step": 75780
    },
    {
      "epoch": 5.052666666666667,
      "grad_norm": 0.3844758868217468,
      "learning_rate": 1.8420833333333335e-05,
      "loss": 0.0031,
      "step": 75790
    },
    {
      "epoch": 5.053333333333334,
      "grad_norm": 0.04643987491726875,
      "learning_rate": 1.841666666666667e-05,
      "loss": 0.0016,
      "step": 75800
    },
    {
      "epoch": 5.054,
      "grad_norm": 0.14036965370178223,
      "learning_rate": 1.84125e-05,
      "loss": 0.0018,
      "step": 75810
    },
    {
      "epoch": 5.054666666666667,
      "grad_norm": 0.0552406832575798,
      "learning_rate": 1.8408333333333335e-05,
      "loss": 0.0018,
      "step": 75820
    },
    {
      "epoch": 5.0553333333333335,
      "grad_norm": 0.06771532446146011,
      "learning_rate": 1.8404166666666666e-05,
      "loss": 0.0035,
      "step": 75830
    },
    {
      "epoch": 5.056,
      "grad_norm": 0.2665361762046814,
      "learning_rate": 1.84e-05,
      "loss": 0.0018,
      "step": 75840
    },
    {
      "epoch": 5.056666666666667,
      "grad_norm": 0.15793129801750183,
      "learning_rate": 1.8395833333333334e-05,
      "loss": 0.0018,
      "step": 75850
    },
    {
      "epoch": 5.057333333333333,
      "grad_norm": 0.41672268509864807,
      "learning_rate": 1.839166666666667e-05,
      "loss": 0.0016,
      "step": 75860
    },
    {
      "epoch": 5.058,
      "grad_norm": 0.3390256464481354,
      "learning_rate": 1.8387500000000003e-05,
      "loss": 0.0023,
      "step": 75870
    },
    {
      "epoch": 5.058666666666666,
      "grad_norm": 0.4110569953918457,
      "learning_rate": 1.8383333333333334e-05,
      "loss": 0.0026,
      "step": 75880
    },
    {
      "epoch": 5.059333333333333,
      "grad_norm": 0.0790485143661499,
      "learning_rate": 1.837916666666667e-05,
      "loss": 0.0023,
      "step": 75890
    },
    {
      "epoch": 5.06,
      "grad_norm": 0.14908882975578308,
      "learning_rate": 1.8375e-05,
      "loss": 0.0017,
      "step": 75900
    },
    {
      "epoch": 5.060666666666667,
      "grad_norm": 0.40263426303863525,
      "learning_rate": 1.8370833333333334e-05,
      "loss": 0.0016,
      "step": 75910
    },
    {
      "epoch": 5.061333333333334,
      "grad_norm": 0.14082573354244232,
      "learning_rate": 1.8366666666666668e-05,
      "loss": 0.0017,
      "step": 75920
    },
    {
      "epoch": 5.062,
      "grad_norm": 0.3349483907222748,
      "learning_rate": 1.8362500000000002e-05,
      "loss": 0.0024,
      "step": 75930
    },
    {
      "epoch": 5.062666666666667,
      "grad_norm": 0.06820471584796906,
      "learning_rate": 1.8358333333333333e-05,
      "loss": 0.0028,
      "step": 75940
    },
    {
      "epoch": 5.0633333333333335,
      "grad_norm": 0.10657501220703125,
      "learning_rate": 1.8354166666666668e-05,
      "loss": 0.0013,
      "step": 75950
    },
    {
      "epoch": 5.064,
      "grad_norm": 0.6529917120933533,
      "learning_rate": 1.8350000000000002e-05,
      "loss": 0.0023,
      "step": 75960
    },
    {
      "epoch": 5.064666666666667,
      "grad_norm": 0.0646730288863182,
      "learning_rate": 1.8345833333333333e-05,
      "loss": 0.0017,
      "step": 75970
    },
    {
      "epoch": 5.065333333333333,
      "grad_norm": 0.46574655175209045,
      "learning_rate": 1.8341666666666668e-05,
      "loss": 0.0014,
      "step": 75980
    },
    {
      "epoch": 5.066,
      "grad_norm": 0.1151011735200882,
      "learning_rate": 1.8337500000000002e-05,
      "loss": 0.0015,
      "step": 75990
    },
    {
      "epoch": 5.066666666666666,
      "grad_norm": 0.2107362449169159,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.0026,
      "step": 76000
    },
    {
      "epoch": 5.067333333333333,
      "grad_norm": 0.48383238911628723,
      "learning_rate": 1.8329166666666667e-05,
      "loss": 0.0032,
      "step": 76010
    },
    {
      "epoch": 5.068,
      "grad_norm": 0.11046721041202545,
      "learning_rate": 1.8325e-05,
      "loss": 0.0021,
      "step": 76020
    },
    {
      "epoch": 5.068666666666667,
      "grad_norm": 0.747550904750824,
      "learning_rate": 1.8320833333333336e-05,
      "loss": 0.002,
      "step": 76030
    },
    {
      "epoch": 5.069333333333334,
      "grad_norm": 0.1428566575050354,
      "learning_rate": 1.8316666666666667e-05,
      "loss": 0.0014,
      "step": 76040
    },
    {
      "epoch": 5.07,
      "grad_norm": 0.5259117484092712,
      "learning_rate": 1.83125e-05,
      "loss": 0.0014,
      "step": 76050
    },
    {
      "epoch": 5.070666666666667,
      "grad_norm": 0.4483865797519684,
      "learning_rate": 1.8308333333333332e-05,
      "loss": 0.0019,
      "step": 76060
    },
    {
      "epoch": 5.0713333333333335,
      "grad_norm": 0.4274185597896576,
      "learning_rate": 1.8304166666666667e-05,
      "loss": 0.0015,
      "step": 76070
    },
    {
      "epoch": 5.072,
      "grad_norm": 0.27821066975593567,
      "learning_rate": 1.83e-05,
      "loss": 0.0023,
      "step": 76080
    },
    {
      "epoch": 5.072666666666667,
      "grad_norm": 0.14029976725578308,
      "learning_rate": 1.8295833333333335e-05,
      "loss": 0.0022,
      "step": 76090
    },
    {
      "epoch": 5.073333333333333,
      "grad_norm": 0.6349471807479858,
      "learning_rate": 1.829166666666667e-05,
      "loss": 0.004,
      "step": 76100
    },
    {
      "epoch": 5.074,
      "grad_norm": 0.4438493549823761,
      "learning_rate": 1.82875e-05,
      "loss": 0.0019,
      "step": 76110
    },
    {
      "epoch": 5.074666666666666,
      "grad_norm": 0.3568609952926636,
      "learning_rate": 1.828333333333333e-05,
      "loss": 0.0017,
      "step": 76120
    },
    {
      "epoch": 5.075333333333333,
      "grad_norm": 0.05416567996144295,
      "learning_rate": 1.8279166666666666e-05,
      "loss": 0.0016,
      "step": 76130
    },
    {
      "epoch": 5.076,
      "grad_norm": 0.46661582589149475,
      "learning_rate": 1.8275e-05,
      "loss": 0.0028,
      "step": 76140
    },
    {
      "epoch": 5.076666666666666,
      "grad_norm": 0.24763448536396027,
      "learning_rate": 1.8270833333333335e-05,
      "loss": 0.0015,
      "step": 76150
    },
    {
      "epoch": 5.077333333333334,
      "grad_norm": 0.1153925210237503,
      "learning_rate": 1.826666666666667e-05,
      "loss": 0.0018,
      "step": 76160
    },
    {
      "epoch": 5.078,
      "grad_norm": 0.6649519205093384,
      "learning_rate": 1.8262500000000003e-05,
      "loss": 0.0023,
      "step": 76170
    },
    {
      "epoch": 5.078666666666667,
      "grad_norm": 0.6551198959350586,
      "learning_rate": 1.8258333333333334e-05,
      "loss": 0.0022,
      "step": 76180
    },
    {
      "epoch": 5.0793333333333335,
      "grad_norm": 0.7453837394714355,
      "learning_rate": 1.8254166666666665e-05,
      "loss": 0.0025,
      "step": 76190
    },
    {
      "epoch": 5.08,
      "grad_norm": 0.18129201233386993,
      "learning_rate": 1.825e-05,
      "loss": 0.0015,
      "step": 76200
    },
    {
      "epoch": 5.080666666666667,
      "grad_norm": 0.3546513319015503,
      "learning_rate": 1.8245833333333334e-05,
      "loss": 0.002,
      "step": 76210
    },
    {
      "epoch": 5.081333333333333,
      "grad_norm": 0.7626695036888123,
      "learning_rate": 1.824166666666667e-05,
      "loss": 0.0023,
      "step": 76220
    },
    {
      "epoch": 5.082,
      "grad_norm": 0.35406774282455444,
      "learning_rate": 1.8237500000000003e-05,
      "loss": 0.0021,
      "step": 76230
    },
    {
      "epoch": 5.082666666666666,
      "grad_norm": 0.315315842628479,
      "learning_rate": 1.8233333333333334e-05,
      "loss": 0.0023,
      "step": 76240
    },
    {
      "epoch": 5.083333333333333,
      "grad_norm": 0.5279049873352051,
      "learning_rate": 1.8229166666666668e-05,
      "loss": 0.0017,
      "step": 76250
    },
    {
      "epoch": 5.084,
      "grad_norm": 0.4479116201400757,
      "learning_rate": 1.8225e-05,
      "loss": 0.0013,
      "step": 76260
    },
    {
      "epoch": 5.084666666666667,
      "grad_norm": 0.31268444657325745,
      "learning_rate": 1.8220833333333334e-05,
      "loss": 0.002,
      "step": 76270
    },
    {
      "epoch": 5.085333333333334,
      "grad_norm": 0.2756480574607849,
      "learning_rate": 1.8216666666666668e-05,
      "loss": 0.0026,
      "step": 76280
    },
    {
      "epoch": 5.086,
      "grad_norm": 0.0850030854344368,
      "learning_rate": 1.8212500000000002e-05,
      "loss": 0.0015,
      "step": 76290
    },
    {
      "epoch": 5.086666666666667,
      "grad_norm": 0.09558887034654617,
      "learning_rate": 1.8208333333333337e-05,
      "loss": 0.0016,
      "step": 76300
    },
    {
      "epoch": 5.0873333333333335,
      "grad_norm": 0.6495308876037598,
      "learning_rate": 1.8204166666666668e-05,
      "loss": 0.0019,
      "step": 76310
    },
    {
      "epoch": 5.088,
      "grad_norm": 0.1457361876964569,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 0.0021,
      "step": 76320
    },
    {
      "epoch": 5.088666666666667,
      "grad_norm": 0.38763558864593506,
      "learning_rate": 1.8195833333333333e-05,
      "loss": 0.0021,
      "step": 76330
    },
    {
      "epoch": 5.089333333333333,
      "grad_norm": 0.05171988531947136,
      "learning_rate": 1.8191666666666667e-05,
      "loss": 0.0015,
      "step": 76340
    },
    {
      "epoch": 5.09,
      "grad_norm": 0.1374596357345581,
      "learning_rate": 1.81875e-05,
      "loss": 0.0023,
      "step": 76350
    },
    {
      "epoch": 5.0906666666666665,
      "grad_norm": 0.7317814230918884,
      "learning_rate": 1.8183333333333336e-05,
      "loss": 0.0014,
      "step": 76360
    },
    {
      "epoch": 5.091333333333333,
      "grad_norm": 0.1956179291009903,
      "learning_rate": 1.8179166666666667e-05,
      "loss": 0.0022,
      "step": 76370
    },
    {
      "epoch": 5.092,
      "grad_norm": 0.13898921012878418,
      "learning_rate": 1.8175e-05,
      "loss": 0.0022,
      "step": 76380
    },
    {
      "epoch": 5.092666666666666,
      "grad_norm": 0.2422715574502945,
      "learning_rate": 1.8170833333333336e-05,
      "loss": 0.0025,
      "step": 76390
    },
    {
      "epoch": 5.093333333333334,
      "grad_norm": 0.38605302572250366,
      "learning_rate": 1.8166666666666667e-05,
      "loss": 0.0022,
      "step": 76400
    },
    {
      "epoch": 5.094,
      "grad_norm": 0.699053943157196,
      "learning_rate": 1.81625e-05,
      "loss": 0.0026,
      "step": 76410
    },
    {
      "epoch": 5.094666666666667,
      "grad_norm": 0.41503646969795227,
      "learning_rate": 1.8158333333333335e-05,
      "loss": 0.0018,
      "step": 76420
    },
    {
      "epoch": 5.0953333333333335,
      "grad_norm": 0.28147122263908386,
      "learning_rate": 1.8154166666666666e-05,
      "loss": 0.0023,
      "step": 76430
    },
    {
      "epoch": 5.096,
      "grad_norm": 0.17525191605091095,
      "learning_rate": 1.815e-05,
      "loss": 0.0014,
      "step": 76440
    },
    {
      "epoch": 5.096666666666667,
      "grad_norm": 0.09245861321687698,
      "learning_rate": 1.8145833333333335e-05,
      "loss": 0.0014,
      "step": 76450
    },
    {
      "epoch": 5.097333333333333,
      "grad_norm": 0.48444536328315735,
      "learning_rate": 1.814166666666667e-05,
      "loss": 0.0014,
      "step": 76460
    },
    {
      "epoch": 5.098,
      "grad_norm": 0.26668214797973633,
      "learning_rate": 1.81375e-05,
      "loss": 0.0015,
      "step": 76470
    },
    {
      "epoch": 5.0986666666666665,
      "grad_norm": 0.107342928647995,
      "learning_rate": 1.8133333333333335e-05,
      "loss": 0.0023,
      "step": 76480
    },
    {
      "epoch": 5.099333333333333,
      "grad_norm": 0.4377174377441406,
      "learning_rate": 1.8129166666666666e-05,
      "loss": 0.0018,
      "step": 76490
    },
    {
      "epoch": 5.1,
      "grad_norm": 0.5787705183029175,
      "learning_rate": 1.8125e-05,
      "loss": 0.0019,
      "step": 76500
    },
    {
      "epoch": 5.100666666666666,
      "grad_norm": 0.8655755519866943,
      "learning_rate": 1.8120833333333334e-05,
      "loss": 0.0036,
      "step": 76510
    },
    {
      "epoch": 5.101333333333334,
      "grad_norm": 0.044949643313884735,
      "learning_rate": 1.811666666666667e-05,
      "loss": 0.0021,
      "step": 76520
    },
    {
      "epoch": 5.102,
      "grad_norm": 0.8310818076133728,
      "learning_rate": 1.8112500000000003e-05,
      "loss": 0.002,
      "step": 76530
    },
    {
      "epoch": 5.102666666666667,
      "grad_norm": 0.8317182660102844,
      "learning_rate": 1.8108333333333334e-05,
      "loss": 0.0017,
      "step": 76540
    },
    {
      "epoch": 5.1033333333333335,
      "grad_norm": 0.6534667015075684,
      "learning_rate": 1.8104166666666665e-05,
      "loss": 0.002,
      "step": 76550
    },
    {
      "epoch": 5.104,
      "grad_norm": 0.6606939435005188,
      "learning_rate": 1.81e-05,
      "loss": 0.0014,
      "step": 76560
    },
    {
      "epoch": 5.104666666666667,
      "grad_norm": 0.22701407968997955,
      "learning_rate": 1.8095833333333334e-05,
      "loss": 0.0013,
      "step": 76570
    },
    {
      "epoch": 5.105333333333333,
      "grad_norm": 0.6584557890892029,
      "learning_rate": 1.8091666666666668e-05,
      "loss": 0.0017,
      "step": 76580
    },
    {
      "epoch": 5.106,
      "grad_norm": 0.11540435254573822,
      "learning_rate": 1.8087500000000003e-05,
      "loss": 0.002,
      "step": 76590
    },
    {
      "epoch": 5.1066666666666665,
      "grad_norm": 0.381761372089386,
      "learning_rate": 1.8083333333333337e-05,
      "loss": 0.0017,
      "step": 76600
    },
    {
      "epoch": 5.107333333333333,
      "grad_norm": 0.06902103126049042,
      "learning_rate": 1.8079166666666668e-05,
      "loss": 0.0014,
      "step": 76610
    },
    {
      "epoch": 5.108,
      "grad_norm": 0.10873763263225555,
      "learning_rate": 1.8075e-05,
      "loss": 0.0025,
      "step": 76620
    },
    {
      "epoch": 5.108666666666666,
      "grad_norm": 0.2713788151741028,
      "learning_rate": 1.8070833333333333e-05,
      "loss": 0.0015,
      "step": 76630
    },
    {
      "epoch": 5.109333333333334,
      "grad_norm": 0.485254168510437,
      "learning_rate": 1.8066666666666668e-05,
      "loss": 0.0013,
      "step": 76640
    },
    {
      "epoch": 5.11,
      "grad_norm": 0.10663603991270065,
      "learning_rate": 1.8062500000000002e-05,
      "loss": 0.0026,
      "step": 76650
    },
    {
      "epoch": 5.110666666666667,
      "grad_norm": 0.038232289254665375,
      "learning_rate": 1.8058333333333336e-05,
      "loss": 0.0014,
      "step": 76660
    },
    {
      "epoch": 5.1113333333333335,
      "grad_norm": 0.47469425201416016,
      "learning_rate": 1.8054166666666667e-05,
      "loss": 0.0027,
      "step": 76670
    },
    {
      "epoch": 5.112,
      "grad_norm": 0.1400923877954483,
      "learning_rate": 1.805e-05,
      "loss": 0.0027,
      "step": 76680
    },
    {
      "epoch": 5.112666666666667,
      "grad_norm": 0.30984005331993103,
      "learning_rate": 1.8045833333333333e-05,
      "loss": 0.002,
      "step": 76690
    },
    {
      "epoch": 5.113333333333333,
      "grad_norm": 0.3399110734462738,
      "learning_rate": 1.8041666666666667e-05,
      "loss": 0.0019,
      "step": 76700
    },
    {
      "epoch": 5.114,
      "grad_norm": 0.07995115220546722,
      "learning_rate": 1.80375e-05,
      "loss": 0.0025,
      "step": 76710
    },
    {
      "epoch": 5.1146666666666665,
      "grad_norm": 0.10409621149301529,
      "learning_rate": 1.8033333333333336e-05,
      "loss": 0.0014,
      "step": 76720
    },
    {
      "epoch": 5.115333333333333,
      "grad_norm": 0.4155232012271881,
      "learning_rate": 1.8029166666666667e-05,
      "loss": 0.0017,
      "step": 76730
    },
    {
      "epoch": 5.116,
      "grad_norm": 0.48520466685295105,
      "learning_rate": 1.8025e-05,
      "loss": 0.0015,
      "step": 76740
    },
    {
      "epoch": 5.116666666666666,
      "grad_norm": 0.3811640441417694,
      "learning_rate": 1.8020833333333335e-05,
      "loss": 0.0015,
      "step": 76750
    },
    {
      "epoch": 5.117333333333334,
      "grad_norm": 0.24554374814033508,
      "learning_rate": 1.8016666666666666e-05,
      "loss": 0.0024,
      "step": 76760
    },
    {
      "epoch": 5.118,
      "grad_norm": 0.31126880645751953,
      "learning_rate": 1.80125e-05,
      "loss": 0.0015,
      "step": 76770
    },
    {
      "epoch": 5.118666666666667,
      "grad_norm": 0.7117419242858887,
      "learning_rate": 1.8008333333333335e-05,
      "loss": 0.0015,
      "step": 76780
    },
    {
      "epoch": 5.1193333333333335,
      "grad_norm": 0.5278793573379517,
      "learning_rate": 1.8004166666666666e-05,
      "loss": 0.0018,
      "step": 76790
    },
    {
      "epoch": 5.12,
      "grad_norm": 0.715783417224884,
      "learning_rate": 1.8e-05,
      "loss": 0.0016,
      "step": 76800
    },
    {
      "epoch": 5.120666666666667,
      "grad_norm": 0.1070057675242424,
      "learning_rate": 1.7995833333333335e-05,
      "loss": 0.0018,
      "step": 76810
    },
    {
      "epoch": 5.121333333333333,
      "grad_norm": 0.04133016988635063,
      "learning_rate": 1.799166666666667e-05,
      "loss": 0.0021,
      "step": 76820
    },
    {
      "epoch": 5.122,
      "grad_norm": 0.8474938869476318,
      "learning_rate": 1.79875e-05,
      "loss": 0.0011,
      "step": 76830
    },
    {
      "epoch": 5.1226666666666665,
      "grad_norm": 0.0207833144813776,
      "learning_rate": 1.7983333333333335e-05,
      "loss": 0.0014,
      "step": 76840
    },
    {
      "epoch": 5.123333333333333,
      "grad_norm": 0.4098476767539978,
      "learning_rate": 1.7979166666666665e-05,
      "loss": 0.0021,
      "step": 76850
    },
    {
      "epoch": 5.124,
      "grad_norm": 0.14424510300159454,
      "learning_rate": 1.7975e-05,
      "loss": 0.0022,
      "step": 76860
    },
    {
      "epoch": 5.124666666666666,
      "grad_norm": 0.8270213603973389,
      "learning_rate": 1.7970833333333334e-05,
      "loss": 0.002,
      "step": 76870
    },
    {
      "epoch": 5.125333333333334,
      "grad_norm": 0.3518127202987671,
      "learning_rate": 1.796666666666667e-05,
      "loss": 0.0013,
      "step": 76880
    },
    {
      "epoch": 5.126,
      "grad_norm": 0.052472855895757675,
      "learning_rate": 1.7962500000000003e-05,
      "loss": 0.0019,
      "step": 76890
    },
    {
      "epoch": 5.126666666666667,
      "grad_norm": 0.30822017788887024,
      "learning_rate": 1.7958333333333334e-05,
      "loss": 0.0016,
      "step": 76900
    },
    {
      "epoch": 5.1273333333333335,
      "grad_norm": 0.07469209283590317,
      "learning_rate": 1.7954166666666665e-05,
      "loss": 0.0026,
      "step": 76910
    },
    {
      "epoch": 5.128,
      "grad_norm": 0.4459245204925537,
      "learning_rate": 1.795e-05,
      "loss": 0.0021,
      "step": 76920
    },
    {
      "epoch": 5.128666666666667,
      "grad_norm": 0.4949970841407776,
      "learning_rate": 1.7945833333333334e-05,
      "loss": 0.0018,
      "step": 76930
    },
    {
      "epoch": 5.129333333333333,
      "grad_norm": 0.6536946296691895,
      "learning_rate": 1.7941666666666668e-05,
      "loss": 0.0016,
      "step": 76940
    },
    {
      "epoch": 5.13,
      "grad_norm": 0.14525608718395233,
      "learning_rate": 1.7937500000000002e-05,
      "loss": 0.0028,
      "step": 76950
    },
    {
      "epoch": 5.1306666666666665,
      "grad_norm": 0.3808932304382324,
      "learning_rate": 1.7933333333333337e-05,
      "loss": 0.0018,
      "step": 76960
    },
    {
      "epoch": 5.131333333333333,
      "grad_norm": 0.3254282474517822,
      "learning_rate": 1.7929166666666668e-05,
      "loss": 0.0013,
      "step": 76970
    },
    {
      "epoch": 5.132,
      "grad_norm": 0.5494861602783203,
      "learning_rate": 1.7925e-05,
      "loss": 0.0021,
      "step": 76980
    },
    {
      "epoch": 5.132666666666666,
      "grad_norm": 0.3143826723098755,
      "learning_rate": 1.7920833333333333e-05,
      "loss": 0.0019,
      "step": 76990
    },
    {
      "epoch": 5.133333333333334,
      "grad_norm": 0.022253766655921936,
      "learning_rate": 1.7916666666666667e-05,
      "loss": 0.0017,
      "step": 77000
    },
    {
      "epoch": 5.134,
      "grad_norm": 0.051224466413259506,
      "learning_rate": 1.7912500000000002e-05,
      "loss": 0.0024,
      "step": 77010
    },
    {
      "epoch": 5.134666666666667,
      "grad_norm": 0.04198950156569481,
      "learning_rate": 1.7908333333333336e-05,
      "loss": 0.0011,
      "step": 77020
    },
    {
      "epoch": 5.1353333333333335,
      "grad_norm": 0.4833866059780121,
      "learning_rate": 1.7904166666666667e-05,
      "loss": 0.0016,
      "step": 77030
    },
    {
      "epoch": 5.136,
      "grad_norm": 0.033565182238817215,
      "learning_rate": 1.79e-05,
      "loss": 0.0013,
      "step": 77040
    },
    {
      "epoch": 5.136666666666667,
      "grad_norm": 0.37938377261161804,
      "learning_rate": 1.7895833333333332e-05,
      "loss": 0.0013,
      "step": 77050
    },
    {
      "epoch": 5.137333333333333,
      "grad_norm": 0.270330011844635,
      "learning_rate": 1.7891666666666667e-05,
      "loss": 0.0014,
      "step": 77060
    },
    {
      "epoch": 5.138,
      "grad_norm": 0.27872487902641296,
      "learning_rate": 1.78875e-05,
      "loss": 0.0022,
      "step": 77070
    },
    {
      "epoch": 5.1386666666666665,
      "grad_norm": 0.7548266053199768,
      "learning_rate": 1.7883333333333335e-05,
      "loss": 0.002,
      "step": 77080
    },
    {
      "epoch": 5.139333333333333,
      "grad_norm": 0.38138726353645325,
      "learning_rate": 1.7879166666666666e-05,
      "loss": 0.0019,
      "step": 77090
    },
    {
      "epoch": 5.14,
      "grad_norm": 0.5184882283210754,
      "learning_rate": 1.7875e-05,
      "loss": 0.002,
      "step": 77100
    },
    {
      "epoch": 5.140666666666666,
      "grad_norm": 0.3097951412200928,
      "learning_rate": 1.7870833333333335e-05,
      "loss": 0.0018,
      "step": 77110
    },
    {
      "epoch": 5.141333333333334,
      "grad_norm": 0.2959190309047699,
      "learning_rate": 1.7866666666666666e-05,
      "loss": 0.0014,
      "step": 77120
    },
    {
      "epoch": 5.142,
      "grad_norm": 0.5497540235519409,
      "learning_rate": 1.78625e-05,
      "loss": 0.0018,
      "step": 77130
    },
    {
      "epoch": 5.142666666666667,
      "grad_norm": 0.034420378506183624,
      "learning_rate": 1.7858333333333335e-05,
      "loss": 0.0016,
      "step": 77140
    },
    {
      "epoch": 5.1433333333333335,
      "grad_norm": 0.14545854926109314,
      "learning_rate": 1.7854166666666666e-05,
      "loss": 0.0026,
      "step": 77150
    },
    {
      "epoch": 5.144,
      "grad_norm": 0.22102895379066467,
      "learning_rate": 1.785e-05,
      "loss": 0.0016,
      "step": 77160
    },
    {
      "epoch": 5.144666666666667,
      "grad_norm": 0.35533133149147034,
      "learning_rate": 1.7845833333333335e-05,
      "loss": 0.0014,
      "step": 77170
    },
    {
      "epoch": 5.145333333333333,
      "grad_norm": 0.2988981306552887,
      "learning_rate": 1.784166666666667e-05,
      "loss": 0.0014,
      "step": 77180
    },
    {
      "epoch": 5.146,
      "grad_norm": 0.17611552774906158,
      "learning_rate": 1.78375e-05,
      "loss": 0.0018,
      "step": 77190
    },
    {
      "epoch": 5.1466666666666665,
      "grad_norm": 0.0755649134516716,
      "learning_rate": 1.7833333333333334e-05,
      "loss": 0.0023,
      "step": 77200
    },
    {
      "epoch": 5.147333333333333,
      "grad_norm": 0.2468687891960144,
      "learning_rate": 1.7829166666666665e-05,
      "loss": 0.0019,
      "step": 77210
    },
    {
      "epoch": 5.148,
      "grad_norm": 0.10162700712680817,
      "learning_rate": 1.7825e-05,
      "loss": 0.0019,
      "step": 77220
    },
    {
      "epoch": 5.148666666666666,
      "grad_norm": 0.34669041633605957,
      "learning_rate": 1.7820833333333334e-05,
      "loss": 0.0015,
      "step": 77230
    },
    {
      "epoch": 5.149333333333334,
      "grad_norm": 0.4535762369632721,
      "learning_rate": 1.781666666666667e-05,
      "loss": 0.0023,
      "step": 77240
    },
    {
      "epoch": 5.15,
      "grad_norm": 0.43996700644493103,
      "learning_rate": 1.7812500000000003e-05,
      "loss": 0.0024,
      "step": 77250
    },
    {
      "epoch": 5.150666666666667,
      "grad_norm": 0.7271476984024048,
      "learning_rate": 1.7808333333333334e-05,
      "loss": 0.0017,
      "step": 77260
    },
    {
      "epoch": 5.1513333333333335,
      "grad_norm": 0.48482367396354675,
      "learning_rate": 1.7804166666666665e-05,
      "loss": 0.0016,
      "step": 77270
    },
    {
      "epoch": 5.152,
      "grad_norm": 0.28062257170677185,
      "learning_rate": 1.78e-05,
      "loss": 0.0023,
      "step": 77280
    },
    {
      "epoch": 5.152666666666667,
      "grad_norm": 0.0642147958278656,
      "learning_rate": 1.7795833333333333e-05,
      "loss": 0.0019,
      "step": 77290
    },
    {
      "epoch": 5.153333333333333,
      "grad_norm": 0.07538297772407532,
      "learning_rate": 1.7791666666666668e-05,
      "loss": 0.0013,
      "step": 77300
    },
    {
      "epoch": 5.154,
      "grad_norm": 0.4749543070793152,
      "learning_rate": 1.7787500000000002e-05,
      "loss": 0.0027,
      "step": 77310
    },
    {
      "epoch": 5.1546666666666665,
      "grad_norm": 0.4784448444843292,
      "learning_rate": 1.7783333333333336e-05,
      "loss": 0.0015,
      "step": 77320
    },
    {
      "epoch": 5.155333333333333,
      "grad_norm": 0.6914973855018616,
      "learning_rate": 1.7779166666666667e-05,
      "loss": 0.0019,
      "step": 77330
    },
    {
      "epoch": 5.156,
      "grad_norm": 0.2070191353559494,
      "learning_rate": 1.7775e-05,
      "loss": 0.0031,
      "step": 77340
    },
    {
      "epoch": 5.156666666666666,
      "grad_norm": 0.43693721294403076,
      "learning_rate": 1.7770833333333333e-05,
      "loss": 0.0018,
      "step": 77350
    },
    {
      "epoch": 5.157333333333334,
      "grad_norm": 0.052421245723962784,
      "learning_rate": 1.7766666666666667e-05,
      "loss": 0.0013,
      "step": 77360
    },
    {
      "epoch": 5.158,
      "grad_norm": 0.24953575432300568,
      "learning_rate": 1.77625e-05,
      "loss": 0.002,
      "step": 77370
    },
    {
      "epoch": 5.158666666666667,
      "grad_norm": 0.06033169850707054,
      "learning_rate": 1.7758333333333336e-05,
      "loss": 0.0017,
      "step": 77380
    },
    {
      "epoch": 5.1593333333333335,
      "grad_norm": 0.5457585453987122,
      "learning_rate": 1.7754166666666667e-05,
      "loss": 0.0014,
      "step": 77390
    },
    {
      "epoch": 5.16,
      "grad_norm": 0.46583160758018494,
      "learning_rate": 1.775e-05,
      "loss": 0.0024,
      "step": 77400
    },
    {
      "epoch": 5.160666666666667,
      "grad_norm": 0.20814473927021027,
      "learning_rate": 1.7745833333333332e-05,
      "loss": 0.0014,
      "step": 77410
    },
    {
      "epoch": 5.161333333333333,
      "grad_norm": 0.30167317390441895,
      "learning_rate": 1.7741666666666666e-05,
      "loss": 0.002,
      "step": 77420
    },
    {
      "epoch": 5.162,
      "grad_norm": 0.04514693841338158,
      "learning_rate": 1.77375e-05,
      "loss": 0.0015,
      "step": 77430
    },
    {
      "epoch": 5.1626666666666665,
      "grad_norm": 0.2833358943462372,
      "learning_rate": 1.7733333333333335e-05,
      "loss": 0.0017,
      "step": 77440
    },
    {
      "epoch": 5.163333333333333,
      "grad_norm": 0.3716200888156891,
      "learning_rate": 1.7729166666666666e-05,
      "loss": 0.0024,
      "step": 77450
    },
    {
      "epoch": 5.164,
      "grad_norm": 0.20463484525680542,
      "learning_rate": 1.7725e-05,
      "loss": 0.0022,
      "step": 77460
    },
    {
      "epoch": 5.164666666666666,
      "grad_norm": 0.6228215098381042,
      "learning_rate": 1.7720833333333335e-05,
      "loss": 0.0019,
      "step": 77470
    },
    {
      "epoch": 5.165333333333333,
      "grad_norm": 0.28572890162467957,
      "learning_rate": 1.7716666666666666e-05,
      "loss": 0.0017,
      "step": 77480
    },
    {
      "epoch": 5.166,
      "grad_norm": 0.07386776804924011,
      "learning_rate": 1.77125e-05,
      "loss": 0.0022,
      "step": 77490
    },
    {
      "epoch": 5.166666666666667,
      "grad_norm": 0.46040433645248413,
      "learning_rate": 1.7708333333333335e-05,
      "loss": 0.0017,
      "step": 77500
    },
    {
      "epoch": 5.167333333333334,
      "grad_norm": 0.021548332646489143,
      "learning_rate": 1.770416666666667e-05,
      "loss": 0.0017,
      "step": 77510
    },
    {
      "epoch": 5.168,
      "grad_norm": 0.4509623348712921,
      "learning_rate": 1.77e-05,
      "loss": 0.002,
      "step": 77520
    },
    {
      "epoch": 5.168666666666667,
      "grad_norm": 0.2465094029903412,
      "learning_rate": 1.7695833333333334e-05,
      "loss": 0.0018,
      "step": 77530
    },
    {
      "epoch": 5.169333333333333,
      "grad_norm": 0.07750419527292252,
      "learning_rate": 1.769166666666667e-05,
      "loss": 0.0023,
      "step": 77540
    },
    {
      "epoch": 5.17,
      "grad_norm": 0.8544251918792725,
      "learning_rate": 1.76875e-05,
      "loss": 0.002,
      "step": 77550
    },
    {
      "epoch": 5.1706666666666665,
      "grad_norm": 0.17173919081687927,
      "learning_rate": 1.7683333333333334e-05,
      "loss": 0.0021,
      "step": 77560
    },
    {
      "epoch": 5.171333333333333,
      "grad_norm": 0.04521799460053444,
      "learning_rate": 1.767916666666667e-05,
      "loss": 0.0026,
      "step": 77570
    },
    {
      "epoch": 5.172,
      "grad_norm": 0.1787824034690857,
      "learning_rate": 1.7675e-05,
      "loss": 0.0019,
      "step": 77580
    },
    {
      "epoch": 5.172666666666666,
      "grad_norm": 0.3842962980270386,
      "learning_rate": 1.7670833333333334e-05,
      "loss": 0.002,
      "step": 77590
    },
    {
      "epoch": 5.173333333333334,
      "grad_norm": 0.2866643965244293,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 0.0018,
      "step": 77600
    },
    {
      "epoch": 5.174,
      "grad_norm": 0.0742696151137352,
      "learning_rate": 1.7662500000000002e-05,
      "loss": 0.0016,
      "step": 77610
    },
    {
      "epoch": 5.174666666666667,
      "grad_norm": 0.058191683143377304,
      "learning_rate": 1.7658333333333333e-05,
      "loss": 0.0014,
      "step": 77620
    },
    {
      "epoch": 5.175333333333334,
      "grad_norm": 0.1420213133096695,
      "learning_rate": 1.7654166666666668e-05,
      "loss": 0.0016,
      "step": 77630
    },
    {
      "epoch": 5.176,
      "grad_norm": 0.5558640360832214,
      "learning_rate": 1.765e-05,
      "loss": 0.0017,
      "step": 77640
    },
    {
      "epoch": 5.176666666666667,
      "grad_norm": 0.4849450886249542,
      "learning_rate": 1.7645833333333333e-05,
      "loss": 0.002,
      "step": 77650
    },
    {
      "epoch": 5.177333333333333,
      "grad_norm": 0.2839124798774719,
      "learning_rate": 1.7641666666666667e-05,
      "loss": 0.0018,
      "step": 77660
    },
    {
      "epoch": 5.178,
      "grad_norm": 0.4864223003387451,
      "learning_rate": 1.7637500000000002e-05,
      "loss": 0.0019,
      "step": 77670
    },
    {
      "epoch": 5.1786666666666665,
      "grad_norm": 0.07621605694293976,
      "learning_rate": 1.7633333333333336e-05,
      "loss": 0.0016,
      "step": 77680
    },
    {
      "epoch": 5.179333333333333,
      "grad_norm": 0.060260970145463943,
      "learning_rate": 1.7629166666666667e-05,
      "loss": 0.0016,
      "step": 77690
    },
    {
      "epoch": 5.18,
      "grad_norm": 0.09127620607614517,
      "learning_rate": 1.7625e-05,
      "loss": 0.0018,
      "step": 77700
    },
    {
      "epoch": 5.180666666666666,
      "grad_norm": 0.05447972193360329,
      "learning_rate": 1.7620833333333332e-05,
      "loss": 0.0017,
      "step": 77710
    },
    {
      "epoch": 5.181333333333333,
      "grad_norm": 0.3475383520126343,
      "learning_rate": 1.7616666666666667e-05,
      "loss": 0.0024,
      "step": 77720
    },
    {
      "epoch": 5.182,
      "grad_norm": 0.32236117124557495,
      "learning_rate": 1.76125e-05,
      "loss": 0.0015,
      "step": 77730
    },
    {
      "epoch": 5.182666666666667,
      "grad_norm": 0.11630652099847794,
      "learning_rate": 1.7608333333333336e-05,
      "loss": 0.0016,
      "step": 77740
    },
    {
      "epoch": 5.183333333333334,
      "grad_norm": 0.17854607105255127,
      "learning_rate": 1.760416666666667e-05,
      "loss": 0.0015,
      "step": 77750
    },
    {
      "epoch": 5.184,
      "grad_norm": 0.07374116033315659,
      "learning_rate": 1.76e-05,
      "loss": 0.0028,
      "step": 77760
    },
    {
      "epoch": 5.184666666666667,
      "grad_norm": 0.5457219481468201,
      "learning_rate": 1.7595833333333335e-05,
      "loss": 0.002,
      "step": 77770
    },
    {
      "epoch": 5.185333333333333,
      "grad_norm": 0.07843589037656784,
      "learning_rate": 1.7591666666666666e-05,
      "loss": 0.0013,
      "step": 77780
    },
    {
      "epoch": 5.186,
      "grad_norm": 0.43108129501342773,
      "learning_rate": 1.75875e-05,
      "loss": 0.003,
      "step": 77790
    },
    {
      "epoch": 5.1866666666666665,
      "grad_norm": 0.07082471996545792,
      "learning_rate": 1.7583333333333335e-05,
      "loss": 0.0029,
      "step": 77800
    },
    {
      "epoch": 5.187333333333333,
      "grad_norm": 0.10623366385698318,
      "learning_rate": 1.757916666666667e-05,
      "loss": 0.0018,
      "step": 77810
    },
    {
      "epoch": 5.188,
      "grad_norm": 0.10658691078424454,
      "learning_rate": 1.7575e-05,
      "loss": 0.0023,
      "step": 77820
    },
    {
      "epoch": 5.188666666666666,
      "grad_norm": 0.07309247553348541,
      "learning_rate": 1.7570833333333335e-05,
      "loss": 0.0027,
      "step": 77830
    },
    {
      "epoch": 5.189333333333333,
      "grad_norm": 0.49930813908576965,
      "learning_rate": 1.756666666666667e-05,
      "loss": 0.0018,
      "step": 77840
    },
    {
      "epoch": 5.19,
      "grad_norm": 0.4414329528808594,
      "learning_rate": 1.75625e-05,
      "loss": 0.0018,
      "step": 77850
    },
    {
      "epoch": 5.190666666666667,
      "grad_norm": 0.3471645712852478,
      "learning_rate": 1.7558333333333334e-05,
      "loss": 0.0015,
      "step": 77860
    },
    {
      "epoch": 5.191333333333334,
      "grad_norm": 0.49129366874694824,
      "learning_rate": 1.755416666666667e-05,
      "loss": 0.002,
      "step": 77870
    },
    {
      "epoch": 5.192,
      "grad_norm": 0.0778508260846138,
      "learning_rate": 1.755e-05,
      "loss": 0.0021,
      "step": 77880
    },
    {
      "epoch": 5.192666666666667,
      "grad_norm": 0.24754339456558228,
      "learning_rate": 1.7545833333333334e-05,
      "loss": 0.0018,
      "step": 77890
    },
    {
      "epoch": 5.193333333333333,
      "grad_norm": 0.27919408679008484,
      "learning_rate": 1.754166666666667e-05,
      "loss": 0.0032,
      "step": 77900
    },
    {
      "epoch": 5.194,
      "grad_norm": 0.11116835474967957,
      "learning_rate": 1.7537500000000003e-05,
      "loss": 0.0015,
      "step": 77910
    },
    {
      "epoch": 5.1946666666666665,
      "grad_norm": 0.0767216682434082,
      "learning_rate": 1.7533333333333334e-05,
      "loss": 0.002,
      "step": 77920
    },
    {
      "epoch": 5.195333333333333,
      "grad_norm": 0.24082696437835693,
      "learning_rate": 1.7529166666666668e-05,
      "loss": 0.0026,
      "step": 77930
    },
    {
      "epoch": 5.196,
      "grad_norm": 0.23609226942062378,
      "learning_rate": 1.7525e-05,
      "loss": 0.0018,
      "step": 77940
    },
    {
      "epoch": 5.196666666666666,
      "grad_norm": 0.20568886399269104,
      "learning_rate": 1.7520833333333333e-05,
      "loss": 0.0012,
      "step": 77950
    },
    {
      "epoch": 5.197333333333333,
      "grad_norm": 0.10968343913555145,
      "learning_rate": 1.7516666666666668e-05,
      "loss": 0.0014,
      "step": 77960
    },
    {
      "epoch": 5.198,
      "grad_norm": 0.4489775002002716,
      "learning_rate": 1.7512500000000002e-05,
      "loss": 0.0014,
      "step": 77970
    },
    {
      "epoch": 5.198666666666667,
      "grad_norm": 0.5537351965904236,
      "learning_rate": 1.7508333333333337e-05,
      "loss": 0.0014,
      "step": 77980
    },
    {
      "epoch": 5.199333333333334,
      "grad_norm": 0.11712773144245148,
      "learning_rate": 1.7504166666666667e-05,
      "loss": 0.0018,
      "step": 77990
    },
    {
      "epoch": 5.2,
      "grad_norm": 0.1737688034772873,
      "learning_rate": 1.75e-05,
      "loss": 0.0022,
      "step": 78000
    },
    {
      "epoch": 5.200666666666667,
      "grad_norm": 0.14113055169582367,
      "learning_rate": 1.7495833333333333e-05,
      "loss": 0.0016,
      "step": 78010
    },
    {
      "epoch": 5.201333333333333,
      "grad_norm": 0.34855207800865173,
      "learning_rate": 1.7491666666666667e-05,
      "loss": 0.0015,
      "step": 78020
    },
    {
      "epoch": 5.202,
      "grad_norm": 0.13903430104255676,
      "learning_rate": 1.74875e-05,
      "loss": 0.0023,
      "step": 78030
    },
    {
      "epoch": 5.2026666666666666,
      "grad_norm": 0.20909708738327026,
      "learning_rate": 1.7483333333333336e-05,
      "loss": 0.0016,
      "step": 78040
    },
    {
      "epoch": 5.203333333333333,
      "grad_norm": 0.05204152688384056,
      "learning_rate": 1.747916666666667e-05,
      "loss": 0.0017,
      "step": 78050
    },
    {
      "epoch": 5.204,
      "grad_norm": 0.243826225399971,
      "learning_rate": 1.7475e-05,
      "loss": 0.0024,
      "step": 78060
    },
    {
      "epoch": 5.204666666666666,
      "grad_norm": 0.513399600982666,
      "learning_rate": 1.7470833333333332e-05,
      "loss": 0.0021,
      "step": 78070
    },
    {
      "epoch": 5.205333333333333,
      "grad_norm": 0.45469388365745544,
      "learning_rate": 1.7466666666666667e-05,
      "loss": 0.0017,
      "step": 78080
    },
    {
      "epoch": 5.206,
      "grad_norm": 0.37452828884124756,
      "learning_rate": 1.74625e-05,
      "loss": 0.0016,
      "step": 78090
    },
    {
      "epoch": 5.206666666666667,
      "grad_norm": 0.3091288208961487,
      "learning_rate": 1.7458333333333335e-05,
      "loss": 0.0016,
      "step": 78100
    },
    {
      "epoch": 5.207333333333334,
      "grad_norm": 0.34941163659095764,
      "learning_rate": 1.745416666666667e-05,
      "loss": 0.0024,
      "step": 78110
    },
    {
      "epoch": 5.208,
      "grad_norm": 0.7633681297302246,
      "learning_rate": 1.745e-05,
      "loss": 0.0031,
      "step": 78120
    },
    {
      "epoch": 5.208666666666667,
      "grad_norm": 0.11070375144481659,
      "learning_rate": 1.7445833333333335e-05,
      "loss": 0.003,
      "step": 78130
    },
    {
      "epoch": 5.209333333333333,
      "grad_norm": 0.17884579300880432,
      "learning_rate": 1.7441666666666666e-05,
      "loss": 0.0023,
      "step": 78140
    },
    {
      "epoch": 5.21,
      "grad_norm": 0.4163539409637451,
      "learning_rate": 1.74375e-05,
      "loss": 0.0025,
      "step": 78150
    },
    {
      "epoch": 5.210666666666667,
      "grad_norm": 0.1435631960630417,
      "learning_rate": 1.7433333333333335e-05,
      "loss": 0.0018,
      "step": 78160
    },
    {
      "epoch": 5.211333333333333,
      "grad_norm": 0.14170685410499573,
      "learning_rate": 1.742916666666667e-05,
      "loss": 0.0012,
      "step": 78170
    },
    {
      "epoch": 5.212,
      "grad_norm": 0.1392790973186493,
      "learning_rate": 1.7425e-05,
      "loss": 0.0013,
      "step": 78180
    },
    {
      "epoch": 5.212666666666666,
      "grad_norm": 0.4150184392929077,
      "learning_rate": 1.7420833333333334e-05,
      "loss": 0.0028,
      "step": 78190
    },
    {
      "epoch": 5.213333333333333,
      "grad_norm": 0.8270242810249329,
      "learning_rate": 1.741666666666667e-05,
      "loss": 0.0019,
      "step": 78200
    },
    {
      "epoch": 5.214,
      "grad_norm": 0.07486022263765335,
      "learning_rate": 1.74125e-05,
      "loss": 0.0032,
      "step": 78210
    },
    {
      "epoch": 5.214666666666667,
      "grad_norm": 0.1481952667236328,
      "learning_rate": 1.7408333333333334e-05,
      "loss": 0.0013,
      "step": 78220
    },
    {
      "epoch": 5.215333333333334,
      "grad_norm": 0.2844541370868683,
      "learning_rate": 1.740416666666667e-05,
      "loss": 0.0015,
      "step": 78230
    },
    {
      "epoch": 5.216,
      "grad_norm": 0.04014747589826584,
      "learning_rate": 1.74e-05,
      "loss": 0.0017,
      "step": 78240
    },
    {
      "epoch": 5.216666666666667,
      "grad_norm": 0.2748248875141144,
      "learning_rate": 1.7395833333333334e-05,
      "loss": 0.002,
      "step": 78250
    },
    {
      "epoch": 5.217333333333333,
      "grad_norm": 0.14427268505096436,
      "learning_rate": 1.7391666666666668e-05,
      "loss": 0.0028,
      "step": 78260
    },
    {
      "epoch": 5.218,
      "grad_norm": 0.026450494304299355,
      "learning_rate": 1.7387500000000003e-05,
      "loss": 0.0013,
      "step": 78270
    },
    {
      "epoch": 5.218666666666667,
      "grad_norm": 0.24534612894058228,
      "learning_rate": 1.7383333333333333e-05,
      "loss": 0.002,
      "step": 78280
    },
    {
      "epoch": 5.219333333333333,
      "grad_norm": 0.4616779685020447,
      "learning_rate": 1.7379166666666668e-05,
      "loss": 0.0027,
      "step": 78290
    },
    {
      "epoch": 5.22,
      "grad_norm": 0.9202225804328918,
      "learning_rate": 1.7375e-05,
      "loss": 0.0022,
      "step": 78300
    },
    {
      "epoch": 5.220666666666666,
      "grad_norm": 0.04062792658805847,
      "learning_rate": 1.7370833333333333e-05,
      "loss": 0.0022,
      "step": 78310
    },
    {
      "epoch": 5.221333333333333,
      "grad_norm": 0.7833281755447388,
      "learning_rate": 1.7366666666666668e-05,
      "loss": 0.0017,
      "step": 78320
    },
    {
      "epoch": 5.222,
      "grad_norm": 0.09119749814271927,
      "learning_rate": 1.7362500000000002e-05,
      "loss": 0.0015,
      "step": 78330
    },
    {
      "epoch": 5.222666666666667,
      "grad_norm": 0.2759723365306854,
      "learning_rate": 1.7358333333333336e-05,
      "loss": 0.0018,
      "step": 78340
    },
    {
      "epoch": 5.223333333333334,
      "grad_norm": 0.35293930768966675,
      "learning_rate": 1.7354166666666667e-05,
      "loss": 0.0022,
      "step": 78350
    },
    {
      "epoch": 5.224,
      "grad_norm": 0.04069918394088745,
      "learning_rate": 1.7349999999999998e-05,
      "loss": 0.0028,
      "step": 78360
    },
    {
      "epoch": 5.224666666666667,
      "grad_norm": 0.10627976059913635,
      "learning_rate": 1.7345833333333333e-05,
      "loss": 0.0021,
      "step": 78370
    },
    {
      "epoch": 5.225333333333333,
      "grad_norm": 0.2786114513874054,
      "learning_rate": 1.7341666666666667e-05,
      "loss": 0.0018,
      "step": 78380
    },
    {
      "epoch": 5.226,
      "grad_norm": 0.2102459967136383,
      "learning_rate": 1.73375e-05,
      "loss": 0.0012,
      "step": 78390
    },
    {
      "epoch": 5.226666666666667,
      "grad_norm": 0.24485410749912262,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.0015,
      "step": 78400
    },
    {
      "epoch": 5.227333333333333,
      "grad_norm": 0.6861979365348816,
      "learning_rate": 1.732916666666667e-05,
      "loss": 0.0026,
      "step": 78410
    },
    {
      "epoch": 5.228,
      "grad_norm": 0.4533824026584625,
      "learning_rate": 1.7325e-05,
      "loss": 0.0023,
      "step": 78420
    },
    {
      "epoch": 5.228666666666666,
      "grad_norm": 0.2736963629722595,
      "learning_rate": 1.7320833333333332e-05,
      "loss": 0.0018,
      "step": 78430
    },
    {
      "epoch": 5.229333333333333,
      "grad_norm": 0.5264125466346741,
      "learning_rate": 1.7316666666666666e-05,
      "loss": 0.0026,
      "step": 78440
    },
    {
      "epoch": 5.23,
      "grad_norm": 0.04665343835949898,
      "learning_rate": 1.73125e-05,
      "loss": 0.0018,
      "step": 78450
    },
    {
      "epoch": 5.230666666666667,
      "grad_norm": 0.4849863648414612,
      "learning_rate": 1.7308333333333335e-05,
      "loss": 0.0019,
      "step": 78460
    },
    {
      "epoch": 5.231333333333334,
      "grad_norm": 0.20104937255382538,
      "learning_rate": 1.730416666666667e-05,
      "loss": 0.0024,
      "step": 78470
    },
    {
      "epoch": 5.232,
      "grad_norm": 0.2688741683959961,
      "learning_rate": 1.73e-05,
      "loss": 0.003,
      "step": 78480
    },
    {
      "epoch": 5.232666666666667,
      "grad_norm": 0.056390274316072464,
      "learning_rate": 1.7295833333333335e-05,
      "loss": 0.0037,
      "step": 78490
    },
    {
      "epoch": 5.233333333333333,
      "grad_norm": 0.20740239322185516,
      "learning_rate": 1.7291666666666666e-05,
      "loss": 0.0023,
      "step": 78500
    },
    {
      "epoch": 5.234,
      "grad_norm": 0.1411505490541458,
      "learning_rate": 1.72875e-05,
      "loss": 0.0018,
      "step": 78510
    },
    {
      "epoch": 5.234666666666667,
      "grad_norm": 0.8165350556373596,
      "learning_rate": 1.7283333333333334e-05,
      "loss": 0.0027,
      "step": 78520
    },
    {
      "epoch": 5.235333333333333,
      "grad_norm": 0.48067471385002136,
      "learning_rate": 1.727916666666667e-05,
      "loss": 0.0017,
      "step": 78530
    },
    {
      "epoch": 5.236,
      "grad_norm": 0.4065902829170227,
      "learning_rate": 1.7275e-05,
      "loss": 0.0021,
      "step": 78540
    },
    {
      "epoch": 5.236666666666666,
      "grad_norm": 0.5242759585380554,
      "learning_rate": 1.7270833333333334e-05,
      "loss": 0.0026,
      "step": 78550
    },
    {
      "epoch": 5.237333333333333,
      "grad_norm": 0.24220092594623566,
      "learning_rate": 1.726666666666667e-05,
      "loss": 0.0018,
      "step": 78560
    },
    {
      "epoch": 5.2379999999999995,
      "grad_norm": 0.6977439522743225,
      "learning_rate": 1.72625e-05,
      "loss": 0.002,
      "step": 78570
    },
    {
      "epoch": 5.238666666666667,
      "grad_norm": 0.4633851647377014,
      "learning_rate": 1.7258333333333334e-05,
      "loss": 0.0015,
      "step": 78580
    },
    {
      "epoch": 5.239333333333334,
      "grad_norm": 0.45830127596855164,
      "learning_rate": 1.7254166666666668e-05,
      "loss": 0.0019,
      "step": 78590
    },
    {
      "epoch": 5.24,
      "grad_norm": 0.30907154083251953,
      "learning_rate": 1.725e-05,
      "loss": 0.0019,
      "step": 78600
    },
    {
      "epoch": 5.240666666666667,
      "grad_norm": 0.6883370876312256,
      "learning_rate": 1.7245833333333334e-05,
      "loss": 0.0019,
      "step": 78610
    },
    {
      "epoch": 5.241333333333333,
      "grad_norm": 0.42337626218795776,
      "learning_rate": 1.7241666666666668e-05,
      "loss": 0.0029,
      "step": 78620
    },
    {
      "epoch": 5.242,
      "grad_norm": 0.613281786441803,
      "learning_rate": 1.7237500000000002e-05,
      "loss": 0.0023,
      "step": 78630
    },
    {
      "epoch": 5.242666666666667,
      "grad_norm": 0.41601815819740295,
      "learning_rate": 1.7233333333333333e-05,
      "loss": 0.0024,
      "step": 78640
    },
    {
      "epoch": 5.243333333333333,
      "grad_norm": 0.3824959099292755,
      "learning_rate": 1.7229166666666668e-05,
      "loss": 0.0018,
      "step": 78650
    },
    {
      "epoch": 5.244,
      "grad_norm": 0.1420254409313202,
      "learning_rate": 1.7225e-05,
      "loss": 0.0023,
      "step": 78660
    },
    {
      "epoch": 5.244666666666666,
      "grad_norm": 0.27767473459243774,
      "learning_rate": 1.7220833333333333e-05,
      "loss": 0.0022,
      "step": 78670
    },
    {
      "epoch": 5.245333333333333,
      "grad_norm": 0.45107969641685486,
      "learning_rate": 1.7216666666666667e-05,
      "loss": 0.0022,
      "step": 78680
    },
    {
      "epoch": 5.246,
      "grad_norm": 0.20914803445339203,
      "learning_rate": 1.72125e-05,
      "loss": 0.0019,
      "step": 78690
    },
    {
      "epoch": 5.246666666666667,
      "grad_norm": 0.567566990852356,
      "learning_rate": 1.7208333333333336e-05,
      "loss": 0.0023,
      "step": 78700
    },
    {
      "epoch": 5.247333333333334,
      "grad_norm": 0.21406707167625427,
      "learning_rate": 1.7204166666666667e-05,
      "loss": 0.0025,
      "step": 78710
    },
    {
      "epoch": 5.248,
      "grad_norm": 0.38435304164886475,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 0.0014,
      "step": 78720
    },
    {
      "epoch": 5.248666666666667,
      "grad_norm": 0.20617495477199554,
      "learning_rate": 1.7195833333333332e-05,
      "loss": 0.0018,
      "step": 78730
    },
    {
      "epoch": 5.249333333333333,
      "grad_norm": 0.3769226372241974,
      "learning_rate": 1.7191666666666667e-05,
      "loss": 0.0018,
      "step": 78740
    },
    {
      "epoch": 5.25,
      "grad_norm": 0.5053198933601379,
      "learning_rate": 1.71875e-05,
      "loss": 0.0021,
      "step": 78750
    },
    {
      "epoch": 5.250666666666667,
      "grad_norm": 0.2756102383136749,
      "learning_rate": 1.7183333333333335e-05,
      "loss": 0.0021,
      "step": 78760
    },
    {
      "epoch": 5.251333333333333,
      "grad_norm": 0.04976872354745865,
      "learning_rate": 1.717916666666667e-05,
      "loss": 0.0018,
      "step": 78770
    },
    {
      "epoch": 5.252,
      "grad_norm": 0.5070144534111023,
      "learning_rate": 1.7175e-05,
      "loss": 0.0018,
      "step": 78780
    },
    {
      "epoch": 5.252666666666666,
      "grad_norm": 0.5596389174461365,
      "learning_rate": 1.7170833333333332e-05,
      "loss": 0.0022,
      "step": 78790
    },
    {
      "epoch": 5.253333333333333,
      "grad_norm": 0.2131490856409073,
      "learning_rate": 1.7166666666666666e-05,
      "loss": 0.002,
      "step": 78800
    },
    {
      "epoch": 5.254,
      "grad_norm": 0.08513614535331726,
      "learning_rate": 1.71625e-05,
      "loss": 0.002,
      "step": 78810
    },
    {
      "epoch": 5.254666666666667,
      "grad_norm": 0.5309062004089355,
      "learning_rate": 1.7158333333333335e-05,
      "loss": 0.0015,
      "step": 78820
    },
    {
      "epoch": 5.255333333333334,
      "grad_norm": 0.6016790866851807,
      "learning_rate": 1.715416666666667e-05,
      "loss": 0.0016,
      "step": 78830
    },
    {
      "epoch": 5.256,
      "grad_norm": 0.7161412239074707,
      "learning_rate": 1.7150000000000004e-05,
      "loss": 0.0019,
      "step": 78840
    },
    {
      "epoch": 5.256666666666667,
      "grad_norm": 0.2753717005252838,
      "learning_rate": 1.7145833333333334e-05,
      "loss": 0.0018,
      "step": 78850
    },
    {
      "epoch": 5.257333333333333,
      "grad_norm": 0.4151168465614319,
      "learning_rate": 1.7141666666666665e-05,
      "loss": 0.0018,
      "step": 78860
    },
    {
      "epoch": 5.258,
      "grad_norm": 0.4237997233867645,
      "learning_rate": 1.71375e-05,
      "loss": 0.0016,
      "step": 78870
    },
    {
      "epoch": 5.258666666666667,
      "grad_norm": 0.3085744380950928,
      "learning_rate": 1.7133333333333334e-05,
      "loss": 0.0016,
      "step": 78880
    },
    {
      "epoch": 5.259333333333333,
      "grad_norm": 0.0509183406829834,
      "learning_rate": 1.712916666666667e-05,
      "loss": 0.0023,
      "step": 78890
    },
    {
      "epoch": 5.26,
      "grad_norm": 0.050724681466817856,
      "learning_rate": 1.7125000000000003e-05,
      "loss": 0.0018,
      "step": 78900
    },
    {
      "epoch": 5.260666666666666,
      "grad_norm": 0.030293967574834824,
      "learning_rate": 1.7120833333333334e-05,
      "loss": 0.0025,
      "step": 78910
    },
    {
      "epoch": 5.261333333333333,
      "grad_norm": 0.5332542657852173,
      "learning_rate": 1.7116666666666668e-05,
      "loss": 0.0015,
      "step": 78920
    },
    {
      "epoch": 5.2620000000000005,
      "grad_norm": 0.20880360901355743,
      "learning_rate": 1.71125e-05,
      "loss": 0.0012,
      "step": 78930
    },
    {
      "epoch": 5.262666666666667,
      "grad_norm": 0.20467659831047058,
      "learning_rate": 1.7108333333333334e-05,
      "loss": 0.002,
      "step": 78940
    },
    {
      "epoch": 5.263333333333334,
      "grad_norm": 0.08031953871250153,
      "learning_rate": 1.7104166666666668e-05,
      "loss": 0.0014,
      "step": 78950
    },
    {
      "epoch": 5.264,
      "grad_norm": 0.0735325813293457,
      "learning_rate": 1.7100000000000002e-05,
      "loss": 0.0022,
      "step": 78960
    },
    {
      "epoch": 5.264666666666667,
      "grad_norm": 0.07402953505516052,
      "learning_rate": 1.7095833333333333e-05,
      "loss": 0.0015,
      "step": 78970
    },
    {
      "epoch": 5.265333333333333,
      "grad_norm": 0.35439977049827576,
      "learning_rate": 1.7091666666666668e-05,
      "loss": 0.0019,
      "step": 78980
    },
    {
      "epoch": 5.266,
      "grad_norm": 0.0822528675198555,
      "learning_rate": 1.7087500000000002e-05,
      "loss": 0.0016,
      "step": 78990
    },
    {
      "epoch": 5.266666666666667,
      "grad_norm": 0.6528159976005554,
      "learning_rate": 1.7083333333333333e-05,
      "loss": 0.0021,
      "step": 79000
    },
    {
      "epoch": 5.267333333333333,
      "grad_norm": 0.45350730419158936,
      "learning_rate": 1.7079166666666667e-05,
      "loss": 0.0017,
      "step": 79010
    },
    {
      "epoch": 5.268,
      "grad_norm": 0.16592876613140106,
      "learning_rate": 1.7075e-05,
      "loss": 0.0028,
      "step": 79020
    },
    {
      "epoch": 5.268666666666666,
      "grad_norm": 0.24342194199562073,
      "learning_rate": 1.7070833333333333e-05,
      "loss": 0.0015,
      "step": 79030
    },
    {
      "epoch": 5.269333333333333,
      "grad_norm": 0.14994783699512482,
      "learning_rate": 1.7066666666666667e-05,
      "loss": 0.0028,
      "step": 79040
    },
    {
      "epoch": 5.27,
      "grad_norm": 0.2802903950214386,
      "learning_rate": 1.70625e-05,
      "loss": 0.0017,
      "step": 79050
    },
    {
      "epoch": 5.270666666666667,
      "grad_norm": 0.07776311784982681,
      "learning_rate": 1.7058333333333336e-05,
      "loss": 0.002,
      "step": 79060
    },
    {
      "epoch": 5.271333333333334,
      "grad_norm": 0.10748115181922913,
      "learning_rate": 1.7054166666666667e-05,
      "loss": 0.0018,
      "step": 79070
    },
    {
      "epoch": 5.272,
      "grad_norm": 0.46830862760543823,
      "learning_rate": 1.705e-05,
      "loss": 0.0014,
      "step": 79080
    },
    {
      "epoch": 5.272666666666667,
      "grad_norm": 0.04481415078043938,
      "learning_rate": 1.7045833333333332e-05,
      "loss": 0.0021,
      "step": 79090
    },
    {
      "epoch": 5.273333333333333,
      "grad_norm": 0.20326080918312073,
      "learning_rate": 1.7041666666666666e-05,
      "loss": 0.0012,
      "step": 79100
    },
    {
      "epoch": 5.274,
      "grad_norm": 0.48458626866340637,
      "learning_rate": 1.70375e-05,
      "loss": 0.0022,
      "step": 79110
    },
    {
      "epoch": 5.274666666666667,
      "grad_norm": 0.2780493199825287,
      "learning_rate": 1.7033333333333335e-05,
      "loss": 0.0018,
      "step": 79120
    },
    {
      "epoch": 5.275333333333333,
      "grad_norm": 0.24203136563301086,
      "learning_rate": 1.702916666666667e-05,
      "loss": 0.0019,
      "step": 79130
    },
    {
      "epoch": 5.276,
      "grad_norm": 0.6515993475914001,
      "learning_rate": 1.7025e-05,
      "loss": 0.0019,
      "step": 79140
    },
    {
      "epoch": 5.276666666666666,
      "grad_norm": 0.17366115748882294,
      "learning_rate": 1.702083333333333e-05,
      "loss": 0.0018,
      "step": 79150
    },
    {
      "epoch": 5.277333333333333,
      "grad_norm": 0.2777828276157379,
      "learning_rate": 1.7016666666666666e-05,
      "loss": 0.0016,
      "step": 79160
    },
    {
      "epoch": 5.2780000000000005,
      "grad_norm": 0.045966386795043945,
      "learning_rate": 1.70125e-05,
      "loss": 0.0015,
      "step": 79170
    },
    {
      "epoch": 5.278666666666667,
      "grad_norm": 0.14897866547107697,
      "learning_rate": 1.7008333333333335e-05,
      "loss": 0.0014,
      "step": 79180
    },
    {
      "epoch": 5.279333333333334,
      "grad_norm": 0.18191136419773102,
      "learning_rate": 1.700416666666667e-05,
      "loss": 0.0035,
      "step": 79190
    },
    {
      "epoch": 5.28,
      "grad_norm": 0.08156567811965942,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.0017,
      "step": 79200
    },
    {
      "epoch": 5.280666666666667,
      "grad_norm": 0.21195733547210693,
      "learning_rate": 1.6995833333333334e-05,
      "loss": 0.0026,
      "step": 79210
    },
    {
      "epoch": 5.281333333333333,
      "grad_norm": 0.2534274458885193,
      "learning_rate": 1.6991666666666665e-05,
      "loss": 0.0016,
      "step": 79220
    },
    {
      "epoch": 5.282,
      "grad_norm": 0.29962992668151855,
      "learning_rate": 1.69875e-05,
      "loss": 0.0024,
      "step": 79230
    },
    {
      "epoch": 5.282666666666667,
      "grad_norm": 0.07838569581508636,
      "learning_rate": 1.6983333333333334e-05,
      "loss": 0.0017,
      "step": 79240
    },
    {
      "epoch": 5.283333333333333,
      "grad_norm": 0.7562574744224548,
      "learning_rate": 1.6979166666666668e-05,
      "loss": 0.0014,
      "step": 79250
    },
    {
      "epoch": 5.284,
      "grad_norm": 0.10943286120891571,
      "learning_rate": 1.6975000000000003e-05,
      "loss": 0.0013,
      "step": 79260
    },
    {
      "epoch": 5.284666666666666,
      "grad_norm": 0.14671114087104797,
      "learning_rate": 1.6970833333333334e-05,
      "loss": 0.0016,
      "step": 79270
    },
    {
      "epoch": 5.285333333333333,
      "grad_norm": 0.27249789237976074,
      "learning_rate": 1.6966666666666668e-05,
      "loss": 0.0021,
      "step": 79280
    },
    {
      "epoch": 5.286,
      "grad_norm": 0.10832273215055466,
      "learning_rate": 1.69625e-05,
      "loss": 0.0018,
      "step": 79290
    },
    {
      "epoch": 5.286666666666667,
      "grad_norm": 0.3556758463382721,
      "learning_rate": 1.6958333333333333e-05,
      "loss": 0.0025,
      "step": 79300
    },
    {
      "epoch": 5.287333333333334,
      "grad_norm": 0.5594225525856018,
      "learning_rate": 1.6954166666666668e-05,
      "loss": 0.0026,
      "step": 79310
    },
    {
      "epoch": 5.288,
      "grad_norm": 0.24102286994457245,
      "learning_rate": 1.6950000000000002e-05,
      "loss": 0.0014,
      "step": 79320
    },
    {
      "epoch": 5.288666666666667,
      "grad_norm": 0.4162871539592743,
      "learning_rate": 1.6945833333333333e-05,
      "loss": 0.0029,
      "step": 79330
    },
    {
      "epoch": 5.289333333333333,
      "grad_norm": 0.21161885559558868,
      "learning_rate": 1.6941666666666667e-05,
      "loss": 0.0022,
      "step": 79340
    },
    {
      "epoch": 5.29,
      "grad_norm": 0.11031752079725266,
      "learning_rate": 1.6937500000000002e-05,
      "loss": 0.0016,
      "step": 79350
    },
    {
      "epoch": 5.290666666666667,
      "grad_norm": 0.6960742473602295,
      "learning_rate": 1.6933333333333333e-05,
      "loss": 0.0027,
      "step": 79360
    },
    {
      "epoch": 5.291333333333333,
      "grad_norm": 0.2074444741010666,
      "learning_rate": 1.6929166666666667e-05,
      "loss": 0.0011,
      "step": 79370
    },
    {
      "epoch": 5.292,
      "grad_norm": 0.21253474056720734,
      "learning_rate": 1.6925e-05,
      "loss": 0.0028,
      "step": 79380
    },
    {
      "epoch": 5.292666666666666,
      "grad_norm": 0.21104462444782257,
      "learning_rate": 1.6920833333333332e-05,
      "loss": 0.0017,
      "step": 79390
    },
    {
      "epoch": 5.293333333333333,
      "grad_norm": 0.3427543640136719,
      "learning_rate": 1.6916666666666667e-05,
      "loss": 0.0014,
      "step": 79400
    },
    {
      "epoch": 5.294,
      "grad_norm": 0.3213527798652649,
      "learning_rate": 1.69125e-05,
      "loss": 0.0012,
      "step": 79410
    },
    {
      "epoch": 5.294666666666667,
      "grad_norm": 0.25276458263397217,
      "learning_rate": 1.6908333333333335e-05,
      "loss": 0.0014,
      "step": 79420
    },
    {
      "epoch": 5.295333333333334,
      "grad_norm": 0.14166440069675446,
      "learning_rate": 1.6904166666666666e-05,
      "loss": 0.0021,
      "step": 79430
    },
    {
      "epoch": 5.296,
      "grad_norm": 0.7281689047813416,
      "learning_rate": 1.69e-05,
      "loss": 0.0017,
      "step": 79440
    },
    {
      "epoch": 5.296666666666667,
      "grad_norm": 0.24036704003810883,
      "learning_rate": 1.6895833333333332e-05,
      "loss": 0.0015,
      "step": 79450
    },
    {
      "epoch": 5.2973333333333334,
      "grad_norm": 0.4352417588233948,
      "learning_rate": 1.6891666666666666e-05,
      "loss": 0.0027,
      "step": 79460
    },
    {
      "epoch": 5.298,
      "grad_norm": 0.14039787650108337,
      "learning_rate": 1.68875e-05,
      "loss": 0.0014,
      "step": 79470
    },
    {
      "epoch": 5.298666666666667,
      "grad_norm": 0.37417900562286377,
      "learning_rate": 1.6883333333333335e-05,
      "loss": 0.0018,
      "step": 79480
    },
    {
      "epoch": 5.299333333333333,
      "grad_norm": 0.4182742238044739,
      "learning_rate": 1.687916666666667e-05,
      "loss": 0.0027,
      "step": 79490
    },
    {
      "epoch": 5.3,
      "grad_norm": 0.1438063532114029,
      "learning_rate": 1.6875000000000004e-05,
      "loss": 0.0011,
      "step": 79500
    },
    {
      "epoch": 5.300666666666666,
      "grad_norm": 0.1086919754743576,
      "learning_rate": 1.6870833333333335e-05,
      "loss": 0.0019,
      "step": 79510
    },
    {
      "epoch": 5.301333333333333,
      "grad_norm": 0.05436095595359802,
      "learning_rate": 1.6866666666666666e-05,
      "loss": 0.0015,
      "step": 79520
    },
    {
      "epoch": 5.302,
      "grad_norm": 0.49698764085769653,
      "learning_rate": 1.68625e-05,
      "loss": 0.0013,
      "step": 79530
    },
    {
      "epoch": 5.302666666666667,
      "grad_norm": 0.33789005875587463,
      "learning_rate": 1.6858333333333334e-05,
      "loss": 0.0026,
      "step": 79540
    },
    {
      "epoch": 5.303333333333334,
      "grad_norm": 0.2710135877132416,
      "learning_rate": 1.685416666666667e-05,
      "loss": 0.0014,
      "step": 79550
    },
    {
      "epoch": 5.304,
      "grad_norm": 0.0469500906765461,
      "learning_rate": 1.6850000000000003e-05,
      "loss": 0.0019,
      "step": 79560
    },
    {
      "epoch": 5.304666666666667,
      "grad_norm": 0.06524395197629929,
      "learning_rate": 1.6845833333333334e-05,
      "loss": 0.0018,
      "step": 79570
    },
    {
      "epoch": 5.3053333333333335,
      "grad_norm": 0.5143661499023438,
      "learning_rate": 1.684166666666667e-05,
      "loss": 0.003,
      "step": 79580
    },
    {
      "epoch": 5.306,
      "grad_norm": 0.24156352877616882,
      "learning_rate": 1.68375e-05,
      "loss": 0.0013,
      "step": 79590
    },
    {
      "epoch": 5.306666666666667,
      "grad_norm": 0.11082066595554352,
      "learning_rate": 1.6833333333333334e-05,
      "loss": 0.0013,
      "step": 79600
    },
    {
      "epoch": 5.307333333333333,
      "grad_norm": 0.14628510177135468,
      "learning_rate": 1.6829166666666668e-05,
      "loss": 0.0014,
      "step": 79610
    },
    {
      "epoch": 5.308,
      "grad_norm": 0.20769186317920685,
      "learning_rate": 1.6825000000000002e-05,
      "loss": 0.0015,
      "step": 79620
    },
    {
      "epoch": 5.308666666666666,
      "grad_norm": 0.5866788625717163,
      "learning_rate": 1.6820833333333333e-05,
      "loss": 0.0023,
      "step": 79630
    },
    {
      "epoch": 5.309333333333333,
      "grad_norm": 0.11140095442533493,
      "learning_rate": 1.6816666666666668e-05,
      "loss": 0.0022,
      "step": 79640
    },
    {
      "epoch": 5.31,
      "grad_norm": 0.39840906858444214,
      "learning_rate": 1.6812500000000002e-05,
      "loss": 0.0016,
      "step": 79650
    },
    {
      "epoch": 5.310666666666666,
      "grad_norm": 0.3095126152038574,
      "learning_rate": 1.6808333333333333e-05,
      "loss": 0.0015,
      "step": 79660
    },
    {
      "epoch": 5.311333333333334,
      "grad_norm": 0.20655237138271332,
      "learning_rate": 1.6804166666666667e-05,
      "loss": 0.0017,
      "step": 79670
    },
    {
      "epoch": 5.312,
      "grad_norm": 0.24087105691432953,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.0019,
      "step": 79680
    },
    {
      "epoch": 5.312666666666667,
      "grad_norm": 0.0375337228178978,
      "learning_rate": 1.6795833333333333e-05,
      "loss": 0.0021,
      "step": 79690
    },
    {
      "epoch": 5.3133333333333335,
      "grad_norm": 0.20254288613796234,
      "learning_rate": 1.6791666666666667e-05,
      "loss": 0.0028,
      "step": 79700
    },
    {
      "epoch": 5.314,
      "grad_norm": 0.2846945524215698,
      "learning_rate": 1.67875e-05,
      "loss": 0.0022,
      "step": 79710
    },
    {
      "epoch": 5.314666666666667,
      "grad_norm": 0.06747518479824066,
      "learning_rate": 1.6783333333333336e-05,
      "loss": 0.0032,
      "step": 79720
    },
    {
      "epoch": 5.315333333333333,
      "grad_norm": 0.1011047214269638,
      "learning_rate": 1.6779166666666667e-05,
      "loss": 0.0018,
      "step": 79730
    },
    {
      "epoch": 5.316,
      "grad_norm": 0.20593346655368805,
      "learning_rate": 1.6775e-05,
      "loss": 0.0012,
      "step": 79740
    },
    {
      "epoch": 5.316666666666666,
      "grad_norm": 0.24179482460021973,
      "learning_rate": 1.6770833333333332e-05,
      "loss": 0.0013,
      "step": 79750
    },
    {
      "epoch": 5.317333333333333,
      "grad_norm": 0.20845583081245422,
      "learning_rate": 1.6766666666666667e-05,
      "loss": 0.0017,
      "step": 79760
    },
    {
      "epoch": 5.318,
      "grad_norm": 0.5591667294502258,
      "learning_rate": 1.67625e-05,
      "loss": 0.0014,
      "step": 79770
    },
    {
      "epoch": 5.318666666666667,
      "grad_norm": 0.04620525613427162,
      "learning_rate": 1.6758333333333335e-05,
      "loss": 0.0014,
      "step": 79780
    },
    {
      "epoch": 5.319333333333334,
      "grad_norm": 0.49734407663345337,
      "learning_rate": 1.675416666666667e-05,
      "loss": 0.0014,
      "step": 79790
    },
    {
      "epoch": 5.32,
      "grad_norm": 0.5393528342247009,
      "learning_rate": 1.675e-05,
      "loss": 0.0022,
      "step": 79800
    },
    {
      "epoch": 5.320666666666667,
      "grad_norm": 0.3812258839607239,
      "learning_rate": 1.674583333333333e-05,
      "loss": 0.0026,
      "step": 79810
    },
    {
      "epoch": 5.3213333333333335,
      "grad_norm": 0.13410556316375732,
      "learning_rate": 1.6741666666666666e-05,
      "loss": 0.0014,
      "step": 79820
    },
    {
      "epoch": 5.322,
      "grad_norm": 0.280937135219574,
      "learning_rate": 1.67375e-05,
      "loss": 0.0016,
      "step": 79830
    },
    {
      "epoch": 5.322666666666667,
      "grad_norm": 0.1457124948501587,
      "learning_rate": 1.6733333333333335e-05,
      "loss": 0.0018,
      "step": 79840
    },
    {
      "epoch": 5.323333333333333,
      "grad_norm": 0.5419887900352478,
      "learning_rate": 1.672916666666667e-05,
      "loss": 0.002,
      "step": 79850
    },
    {
      "epoch": 5.324,
      "grad_norm": 0.24205806851387024,
      "learning_rate": 1.6725000000000003e-05,
      "loss": 0.002,
      "step": 79860
    },
    {
      "epoch": 5.324666666666666,
      "grad_norm": 0.24509939551353455,
      "learning_rate": 1.6720833333333334e-05,
      "loss": 0.0015,
      "step": 79870
    },
    {
      "epoch": 5.325333333333333,
      "grad_norm": 0.5476914048194885,
      "learning_rate": 1.6716666666666665e-05,
      "loss": 0.0013,
      "step": 79880
    },
    {
      "epoch": 5.326,
      "grad_norm": 0.5488162040710449,
      "learning_rate": 1.67125e-05,
      "loss": 0.0019,
      "step": 79890
    },
    {
      "epoch": 5.326666666666666,
      "grad_norm": 0.14124800264835358,
      "learning_rate": 1.6708333333333334e-05,
      "loss": 0.0018,
      "step": 79900
    },
    {
      "epoch": 5.327333333333334,
      "grad_norm": 0.4106327295303345,
      "learning_rate": 1.670416666666667e-05,
      "loss": 0.0024,
      "step": 79910
    },
    {
      "epoch": 5.328,
      "grad_norm": 0.4795682430267334,
      "learning_rate": 1.6700000000000003e-05,
      "loss": 0.0021,
      "step": 79920
    },
    {
      "epoch": 5.328666666666667,
      "grad_norm": 0.1749027818441391,
      "learning_rate": 1.6695833333333334e-05,
      "loss": 0.0017,
      "step": 79930
    },
    {
      "epoch": 5.3293333333333335,
      "grad_norm": 0.7505708932876587,
      "learning_rate": 1.6691666666666668e-05,
      "loss": 0.002,
      "step": 79940
    },
    {
      "epoch": 5.33,
      "grad_norm": 0.03947971761226654,
      "learning_rate": 1.66875e-05,
      "loss": 0.0017,
      "step": 79950
    },
    {
      "epoch": 5.330666666666667,
      "grad_norm": 0.3108236789703369,
      "learning_rate": 1.6683333333333333e-05,
      "loss": 0.0013,
      "step": 79960
    },
    {
      "epoch": 5.331333333333333,
      "grad_norm": 0.3780450224876404,
      "learning_rate": 1.6679166666666668e-05,
      "loss": 0.002,
      "step": 79970
    },
    {
      "epoch": 5.332,
      "grad_norm": 0.12149316072463989,
      "learning_rate": 1.6675000000000002e-05,
      "loss": 0.0019,
      "step": 79980
    },
    {
      "epoch": 5.332666666666666,
      "grad_norm": 0.5471545457839966,
      "learning_rate": 1.6670833333333333e-05,
      "loss": 0.0019,
      "step": 79990
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 0.26432353258132935,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.0018,
      "step": 80000
    },
    {
      "epoch": 5.334,
      "grad_norm": 0.1379138082265854,
      "learning_rate": 1.6662500000000002e-05,
      "loss": 0.0014,
      "step": 80010
    },
    {
      "epoch": 5.334666666666667,
      "grad_norm": 0.3459531366825104,
      "learning_rate": 1.6658333333333333e-05,
      "loss": 0.0014,
      "step": 80020
    },
    {
      "epoch": 5.335333333333334,
      "grad_norm": 0.07763247191905975,
      "learning_rate": 1.6654166666666667e-05,
      "loss": 0.0021,
      "step": 80030
    },
    {
      "epoch": 5.336,
      "grad_norm": 0.24520310759544373,
      "learning_rate": 1.665e-05,
      "loss": 0.0019,
      "step": 80040
    },
    {
      "epoch": 5.336666666666667,
      "grad_norm": 0.2771044075489044,
      "learning_rate": 1.6645833333333336e-05,
      "loss": 0.0015,
      "step": 80050
    },
    {
      "epoch": 5.3373333333333335,
      "grad_norm": 0.44136038422584534,
      "learning_rate": 1.6641666666666667e-05,
      "loss": 0.0015,
      "step": 80060
    },
    {
      "epoch": 5.338,
      "grad_norm": 0.5939284563064575,
      "learning_rate": 1.66375e-05,
      "loss": 0.0015,
      "step": 80070
    },
    {
      "epoch": 5.338666666666667,
      "grad_norm": 0.674874484539032,
      "learning_rate": 1.6633333333333336e-05,
      "loss": 0.0016,
      "step": 80080
    },
    {
      "epoch": 5.339333333333333,
      "grad_norm": 0.18622028827667236,
      "learning_rate": 1.6629166666666667e-05,
      "loss": 0.0018,
      "step": 80090
    },
    {
      "epoch": 5.34,
      "grad_norm": 0.2713317573070526,
      "learning_rate": 1.6625e-05,
      "loss": 0.0021,
      "step": 80100
    },
    {
      "epoch": 5.3406666666666665,
      "grad_norm": 0.12277143448591232,
      "learning_rate": 1.6620833333333335e-05,
      "loss": 0.0021,
      "step": 80110
    },
    {
      "epoch": 5.341333333333333,
      "grad_norm": 0.21236400306224823,
      "learning_rate": 1.6616666666666666e-05,
      "loss": 0.0016,
      "step": 80120
    },
    {
      "epoch": 5.342,
      "grad_norm": 0.35098594427108765,
      "learning_rate": 1.66125e-05,
      "loss": 0.0021,
      "step": 80130
    },
    {
      "epoch": 5.342666666666666,
      "grad_norm": 0.3714831471443176,
      "learning_rate": 1.6608333333333335e-05,
      "loss": 0.0024,
      "step": 80140
    },
    {
      "epoch": 5.343333333333334,
      "grad_norm": 0.38142532110214233,
      "learning_rate": 1.660416666666667e-05,
      "loss": 0.0027,
      "step": 80150
    },
    {
      "epoch": 5.344,
      "grad_norm": 0.044296860694885254,
      "learning_rate": 1.66e-05,
      "loss": 0.002,
      "step": 80160
    },
    {
      "epoch": 5.344666666666667,
      "grad_norm": 0.33464571833610535,
      "learning_rate": 1.6595833333333335e-05,
      "loss": 0.0015,
      "step": 80170
    },
    {
      "epoch": 5.3453333333333335,
      "grad_norm": 0.21028362214565277,
      "learning_rate": 1.6591666666666666e-05,
      "loss": 0.0023,
      "step": 80180
    },
    {
      "epoch": 5.346,
      "grad_norm": 0.5168473124504089,
      "learning_rate": 1.65875e-05,
      "loss": 0.0021,
      "step": 80190
    },
    {
      "epoch": 5.346666666666667,
      "grad_norm": 0.2768239974975586,
      "learning_rate": 1.6583333333333334e-05,
      "loss": 0.0019,
      "step": 80200
    },
    {
      "epoch": 5.347333333333333,
      "grad_norm": 0.3097624182701111,
      "learning_rate": 1.657916666666667e-05,
      "loss": 0.0016,
      "step": 80210
    },
    {
      "epoch": 5.348,
      "grad_norm": 0.6026505827903748,
      "learning_rate": 1.6575000000000003e-05,
      "loss": 0.0018,
      "step": 80220
    },
    {
      "epoch": 5.3486666666666665,
      "grad_norm": 0.48753154277801514,
      "learning_rate": 1.6570833333333334e-05,
      "loss": 0.0018,
      "step": 80230
    },
    {
      "epoch": 5.349333333333333,
      "grad_norm": 0.24354064464569092,
      "learning_rate": 1.6566666666666665e-05,
      "loss": 0.0015,
      "step": 80240
    },
    {
      "epoch": 5.35,
      "grad_norm": 0.28751838207244873,
      "learning_rate": 1.65625e-05,
      "loss": 0.0015,
      "step": 80250
    },
    {
      "epoch": 5.350666666666667,
      "grad_norm": 0.07485541701316833,
      "learning_rate": 1.6558333333333334e-05,
      "loss": 0.0013,
      "step": 80260
    },
    {
      "epoch": 5.351333333333334,
      "grad_norm": 0.07733026891946793,
      "learning_rate": 1.6554166666666668e-05,
      "loss": 0.0019,
      "step": 80270
    },
    {
      "epoch": 5.352,
      "grad_norm": 0.1791267842054367,
      "learning_rate": 1.6550000000000002e-05,
      "loss": 0.0017,
      "step": 80280
    },
    {
      "epoch": 5.352666666666667,
      "grad_norm": 0.3515447676181793,
      "learning_rate": 1.6545833333333337e-05,
      "loss": 0.0018,
      "step": 80290
    },
    {
      "epoch": 5.3533333333333335,
      "grad_norm": 0.5181725025177002,
      "learning_rate": 1.6541666666666668e-05,
      "loss": 0.0018,
      "step": 80300
    },
    {
      "epoch": 5.354,
      "grad_norm": 0.5597392916679382,
      "learning_rate": 1.65375e-05,
      "loss": 0.0018,
      "step": 80310
    },
    {
      "epoch": 5.354666666666667,
      "grad_norm": 0.766392171382904,
      "learning_rate": 1.6533333333333333e-05,
      "loss": 0.0023,
      "step": 80320
    },
    {
      "epoch": 5.355333333333333,
      "grad_norm": 0.34673458337783813,
      "learning_rate": 1.6529166666666668e-05,
      "loss": 0.0027,
      "step": 80330
    },
    {
      "epoch": 5.356,
      "grad_norm": 0.6013833284378052,
      "learning_rate": 1.6525000000000002e-05,
      "loss": 0.0017,
      "step": 80340
    },
    {
      "epoch": 5.3566666666666665,
      "grad_norm": 0.7505995631217957,
      "learning_rate": 1.6520833333333336e-05,
      "loss": 0.0016,
      "step": 80350
    },
    {
      "epoch": 5.357333333333333,
      "grad_norm": 0.07390587776899338,
      "learning_rate": 1.6516666666666667e-05,
      "loss": 0.0014,
      "step": 80360
    },
    {
      "epoch": 5.358,
      "grad_norm": 0.7487180233001709,
      "learning_rate": 1.65125e-05,
      "loss": 0.002,
      "step": 80370
    },
    {
      "epoch": 5.358666666666666,
      "grad_norm": 0.6832888722419739,
      "learning_rate": 1.6508333333333333e-05,
      "loss": 0.0023,
      "step": 80380
    },
    {
      "epoch": 5.359333333333334,
      "grad_norm": 0.3131922483444214,
      "learning_rate": 1.6504166666666667e-05,
      "loss": 0.0018,
      "step": 80390
    },
    {
      "epoch": 5.36,
      "grad_norm": 0.2430444359779358,
      "learning_rate": 1.65e-05,
      "loss": 0.002,
      "step": 80400
    },
    {
      "epoch": 5.360666666666667,
      "grad_norm": 0.44294655323028564,
      "learning_rate": 1.6495833333333336e-05,
      "loss": 0.0019,
      "step": 80410
    },
    {
      "epoch": 5.3613333333333335,
      "grad_norm": 0.30775925517082214,
      "learning_rate": 1.6491666666666667e-05,
      "loss": 0.002,
      "step": 80420
    },
    {
      "epoch": 5.362,
      "grad_norm": 0.11126160621643066,
      "learning_rate": 1.64875e-05,
      "loss": 0.004,
      "step": 80430
    },
    {
      "epoch": 5.362666666666667,
      "grad_norm": 0.07417922466993332,
      "learning_rate": 1.6483333333333335e-05,
      "loss": 0.0013,
      "step": 80440
    },
    {
      "epoch": 5.363333333333333,
      "grad_norm": 0.1491633504629135,
      "learning_rate": 1.6479166666666666e-05,
      "loss": 0.002,
      "step": 80450
    },
    {
      "epoch": 5.364,
      "grad_norm": 0.3192296326160431,
      "learning_rate": 1.6475e-05,
      "loss": 0.0032,
      "step": 80460
    },
    {
      "epoch": 5.3646666666666665,
      "grad_norm": 0.3135792911052704,
      "learning_rate": 1.6470833333333335e-05,
      "loss": 0.0022,
      "step": 80470
    },
    {
      "epoch": 5.365333333333333,
      "grad_norm": 0.799743115901947,
      "learning_rate": 1.6466666666666666e-05,
      "loss": 0.0026,
      "step": 80480
    },
    {
      "epoch": 5.366,
      "grad_norm": 0.17525401711463928,
      "learning_rate": 1.64625e-05,
      "loss": 0.0027,
      "step": 80490
    },
    {
      "epoch": 5.366666666666666,
      "grad_norm": 0.07077161222696304,
      "learning_rate": 1.6458333333333335e-05,
      "loss": 0.0019,
      "step": 80500
    },
    {
      "epoch": 5.367333333333334,
      "grad_norm": 0.6542664766311646,
      "learning_rate": 1.645416666666667e-05,
      "loss": 0.0029,
      "step": 80510
    },
    {
      "epoch": 5.368,
      "grad_norm": 0.31552624702453613,
      "learning_rate": 1.645e-05,
      "loss": 0.0029,
      "step": 80520
    },
    {
      "epoch": 5.368666666666667,
      "grad_norm": 0.10621905326843262,
      "learning_rate": 1.6445833333333334e-05,
      "loss": 0.0012,
      "step": 80530
    },
    {
      "epoch": 5.3693333333333335,
      "grad_norm": 0.4685753881931305,
      "learning_rate": 1.6441666666666665e-05,
      "loss": 0.0031,
      "step": 80540
    },
    {
      "epoch": 5.37,
      "grad_norm": 0.5556049346923828,
      "learning_rate": 1.64375e-05,
      "loss": 0.0018,
      "step": 80550
    },
    {
      "epoch": 5.370666666666667,
      "grad_norm": 0.10764303803443909,
      "learning_rate": 1.6433333333333334e-05,
      "loss": 0.0021,
      "step": 80560
    },
    {
      "epoch": 5.371333333333333,
      "grad_norm": 0.2473975569009781,
      "learning_rate": 1.642916666666667e-05,
      "loss": 0.002,
      "step": 80570
    },
    {
      "epoch": 5.372,
      "grad_norm": 0.02578473649919033,
      "learning_rate": 1.6425000000000003e-05,
      "loss": 0.0016,
      "step": 80580
    },
    {
      "epoch": 5.3726666666666665,
      "grad_norm": 0.3984549343585968,
      "learning_rate": 1.6420833333333334e-05,
      "loss": 0.0019,
      "step": 80590
    },
    {
      "epoch": 5.373333333333333,
      "grad_norm": 0.32702067494392395,
      "learning_rate": 1.6416666666666665e-05,
      "loss": 0.002,
      "step": 80600
    },
    {
      "epoch": 5.374,
      "grad_norm": 0.4513941705226898,
      "learning_rate": 1.64125e-05,
      "loss": 0.0027,
      "step": 80610
    },
    {
      "epoch": 5.374666666666666,
      "grad_norm": 0.46538984775543213,
      "learning_rate": 1.6408333333333333e-05,
      "loss": 0.0016,
      "step": 80620
    },
    {
      "epoch": 5.375333333333334,
      "grad_norm": 0.24383248388767242,
      "learning_rate": 1.6404166666666668e-05,
      "loss": 0.0021,
      "step": 80630
    },
    {
      "epoch": 5.376,
      "grad_norm": 0.3120267391204834,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 0.0017,
      "step": 80640
    },
    {
      "epoch": 5.376666666666667,
      "grad_norm": 0.2383192926645279,
      "learning_rate": 1.6395833333333337e-05,
      "loss": 0.0014,
      "step": 80650
    },
    {
      "epoch": 5.3773333333333335,
      "grad_norm": 0.610801637172699,
      "learning_rate": 1.6391666666666668e-05,
      "loss": 0.0026,
      "step": 80660
    },
    {
      "epoch": 5.378,
      "grad_norm": 0.14772263169288635,
      "learning_rate": 1.63875e-05,
      "loss": 0.0033,
      "step": 80670
    },
    {
      "epoch": 5.378666666666667,
      "grad_norm": 0.3217679262161255,
      "learning_rate": 1.6383333333333333e-05,
      "loss": 0.0024,
      "step": 80680
    },
    {
      "epoch": 5.379333333333333,
      "grad_norm": 0.3506717383861542,
      "learning_rate": 1.6379166666666667e-05,
      "loss": 0.002,
      "step": 80690
    },
    {
      "epoch": 5.38,
      "grad_norm": 0.17334820330142975,
      "learning_rate": 1.6375e-05,
      "loss": 0.0023,
      "step": 80700
    },
    {
      "epoch": 5.3806666666666665,
      "grad_norm": 0.3505750596523285,
      "learning_rate": 1.6370833333333336e-05,
      "loss": 0.0018,
      "step": 80710
    },
    {
      "epoch": 5.381333333333333,
      "grad_norm": 1.0319656133651733,
      "learning_rate": 1.6366666666666667e-05,
      "loss": 0.0034,
      "step": 80720
    },
    {
      "epoch": 5.382,
      "grad_norm": 0.1999714970588684,
      "learning_rate": 1.63625e-05,
      "loss": 0.0013,
      "step": 80730
    },
    {
      "epoch": 5.382666666666666,
      "grad_norm": 0.04239732772111893,
      "learning_rate": 1.6358333333333332e-05,
      "loss": 0.0019,
      "step": 80740
    },
    {
      "epoch": 5.383333333333334,
      "grad_norm": 0.389774888753891,
      "learning_rate": 1.6354166666666667e-05,
      "loss": 0.0017,
      "step": 80750
    },
    {
      "epoch": 5.384,
      "grad_norm": 0.1439221352338791,
      "learning_rate": 1.635e-05,
      "loss": 0.0018,
      "step": 80760
    },
    {
      "epoch": 5.384666666666667,
      "grad_norm": 0.2726755440235138,
      "learning_rate": 1.6345833333333335e-05,
      "loss": 0.0019,
      "step": 80770
    },
    {
      "epoch": 5.3853333333333335,
      "grad_norm": 0.044557373970746994,
      "learning_rate": 1.6341666666666666e-05,
      "loss": 0.0015,
      "step": 80780
    },
    {
      "epoch": 5.386,
      "grad_norm": 0.05047198385000229,
      "learning_rate": 1.63375e-05,
      "loss": 0.0023,
      "step": 80790
    },
    {
      "epoch": 5.386666666666667,
      "grad_norm": 0.34302693605422974,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 0.0015,
      "step": 80800
    },
    {
      "epoch": 5.387333333333333,
      "grad_norm": 0.07406765222549438,
      "learning_rate": 1.6329166666666666e-05,
      "loss": 0.002,
      "step": 80810
    },
    {
      "epoch": 5.388,
      "grad_norm": 0.21136434376239777,
      "learning_rate": 1.6325e-05,
      "loss": 0.0025,
      "step": 80820
    },
    {
      "epoch": 5.3886666666666665,
      "grad_norm": 0.1749524176120758,
      "learning_rate": 1.6320833333333335e-05,
      "loss": 0.0021,
      "step": 80830
    },
    {
      "epoch": 5.389333333333333,
      "grad_norm": 0.42986875772476196,
      "learning_rate": 1.6316666666666666e-05,
      "loss": 0.0027,
      "step": 80840
    },
    {
      "epoch": 5.39,
      "grad_norm": 0.39569398760795593,
      "learning_rate": 1.63125e-05,
      "loss": 0.002,
      "step": 80850
    },
    {
      "epoch": 5.390666666666666,
      "grad_norm": 0.4452166259288788,
      "learning_rate": 1.6308333333333334e-05,
      "loss": 0.0028,
      "step": 80860
    },
    {
      "epoch": 5.391333333333334,
      "grad_norm": 0.2748839259147644,
      "learning_rate": 1.630416666666667e-05,
      "loss": 0.0029,
      "step": 80870
    },
    {
      "epoch": 5.392,
      "grad_norm": 0.35619649291038513,
      "learning_rate": 1.63e-05,
      "loss": 0.0012,
      "step": 80880
    },
    {
      "epoch": 5.392666666666667,
      "grad_norm": 0.2400214672088623,
      "learning_rate": 1.6295833333333334e-05,
      "loss": 0.0014,
      "step": 80890
    },
    {
      "epoch": 5.3933333333333335,
      "grad_norm": 0.27949732542037964,
      "learning_rate": 1.6291666666666665e-05,
      "loss": 0.0014,
      "step": 80900
    },
    {
      "epoch": 5.394,
      "grad_norm": 0.3792911767959595,
      "learning_rate": 1.62875e-05,
      "loss": 0.0025,
      "step": 80910
    },
    {
      "epoch": 5.394666666666667,
      "grad_norm": 0.03342998027801514,
      "learning_rate": 1.6283333333333334e-05,
      "loss": 0.0013,
      "step": 80920
    },
    {
      "epoch": 5.395333333333333,
      "grad_norm": 0.4858092665672302,
      "learning_rate": 1.6279166666666668e-05,
      "loss": 0.0018,
      "step": 80930
    },
    {
      "epoch": 5.396,
      "grad_norm": 0.3800256550312042,
      "learning_rate": 1.6275000000000003e-05,
      "loss": 0.0019,
      "step": 80940
    },
    {
      "epoch": 5.3966666666666665,
      "grad_norm": 0.1683523803949356,
      "learning_rate": 1.6270833333333334e-05,
      "loss": 0.0018,
      "step": 80950
    },
    {
      "epoch": 5.397333333333333,
      "grad_norm": 0.07170045375823975,
      "learning_rate": 1.6266666666666665e-05,
      "loss": 0.0023,
      "step": 80960
    },
    {
      "epoch": 5.398,
      "grad_norm": 0.277830570936203,
      "learning_rate": 1.62625e-05,
      "loss": 0.0018,
      "step": 80970
    },
    {
      "epoch": 5.398666666666666,
      "grad_norm": 0.17484693229198456,
      "learning_rate": 1.6258333333333333e-05,
      "loss": 0.0022,
      "step": 80980
    },
    {
      "epoch": 5.399333333333333,
      "grad_norm": 0.2139507234096527,
      "learning_rate": 1.6254166666666668e-05,
      "loss": 0.0012,
      "step": 80990
    },
    {
      "epoch": 5.4,
      "grad_norm": 0.35053253173828125,
      "learning_rate": 1.6250000000000002e-05,
      "loss": 0.0012,
      "step": 81000
    },
    {
      "epoch": 5.400666666666667,
      "grad_norm": 0.6272772550582886,
      "learning_rate": 1.6245833333333336e-05,
      "loss": 0.0015,
      "step": 81010
    },
    {
      "epoch": 5.4013333333333335,
      "grad_norm": 0.17394192516803741,
      "learning_rate": 1.6241666666666667e-05,
      "loss": 0.0027,
      "step": 81020
    },
    {
      "epoch": 5.402,
      "grad_norm": 0.24419055879116058,
      "learning_rate": 1.6237499999999998e-05,
      "loss": 0.0012,
      "step": 81030
    },
    {
      "epoch": 5.402666666666667,
      "grad_norm": 0.07940225303173065,
      "learning_rate": 1.6233333333333333e-05,
      "loss": 0.0021,
      "step": 81040
    },
    {
      "epoch": 5.403333333333333,
      "grad_norm": 0.5440821051597595,
      "learning_rate": 1.6229166666666667e-05,
      "loss": 0.0023,
      "step": 81050
    },
    {
      "epoch": 5.404,
      "grad_norm": 1.1014052629470825,
      "learning_rate": 1.6225e-05,
      "loss": 0.0022,
      "step": 81060
    },
    {
      "epoch": 5.4046666666666665,
      "grad_norm": 0.14048413932323456,
      "learning_rate": 1.6220833333333336e-05,
      "loss": 0.0014,
      "step": 81070
    },
    {
      "epoch": 5.405333333333333,
      "grad_norm": 0.41166406869888306,
      "learning_rate": 1.6216666666666667e-05,
      "loss": 0.0023,
      "step": 81080
    },
    {
      "epoch": 5.406,
      "grad_norm": 0.6517693996429443,
      "learning_rate": 1.62125e-05,
      "loss": 0.0017,
      "step": 81090
    },
    {
      "epoch": 5.406666666666666,
      "grad_norm": 0.21203966438770294,
      "learning_rate": 1.6208333333333332e-05,
      "loss": 0.002,
      "step": 81100
    },
    {
      "epoch": 5.407333333333334,
      "grad_norm": 0.4585808217525482,
      "learning_rate": 1.6204166666666666e-05,
      "loss": 0.0015,
      "step": 81110
    },
    {
      "epoch": 5.408,
      "grad_norm": 0.027061566710472107,
      "learning_rate": 1.62e-05,
      "loss": 0.002,
      "step": 81120
    },
    {
      "epoch": 5.408666666666667,
      "grad_norm": 0.6184254884719849,
      "learning_rate": 1.6195833333333335e-05,
      "loss": 0.0019,
      "step": 81130
    },
    {
      "epoch": 5.4093333333333335,
      "grad_norm": 0.3717764616012573,
      "learning_rate": 1.6191666666666666e-05,
      "loss": 0.0028,
      "step": 81140
    },
    {
      "epoch": 5.41,
      "grad_norm": 0.6172633171081543,
      "learning_rate": 1.61875e-05,
      "loss": 0.0018,
      "step": 81150
    },
    {
      "epoch": 5.410666666666667,
      "grad_norm": 0.9199333786964417,
      "learning_rate": 1.6183333333333335e-05,
      "loss": 0.0019,
      "step": 81160
    },
    {
      "epoch": 5.411333333333333,
      "grad_norm": 0.6417794823646545,
      "learning_rate": 1.6179166666666666e-05,
      "loss": 0.0022,
      "step": 81170
    },
    {
      "epoch": 5.412,
      "grad_norm": 0.5495762228965759,
      "learning_rate": 1.6175e-05,
      "loss": 0.0016,
      "step": 81180
    },
    {
      "epoch": 5.4126666666666665,
      "grad_norm": 0.3116598427295685,
      "learning_rate": 1.6170833333333335e-05,
      "loss": 0.0019,
      "step": 81190
    },
    {
      "epoch": 5.413333333333333,
      "grad_norm": 0.7548391819000244,
      "learning_rate": 1.6166666666666665e-05,
      "loss": 0.0015,
      "step": 81200
    },
    {
      "epoch": 5.414,
      "grad_norm": 0.046909939497709274,
      "learning_rate": 1.61625e-05,
      "loss": 0.0014,
      "step": 81210
    },
    {
      "epoch": 5.414666666666666,
      "grad_norm": 0.14977489411830902,
      "learning_rate": 1.6158333333333334e-05,
      "loss": 0.0026,
      "step": 81220
    },
    {
      "epoch": 5.415333333333333,
      "grad_norm": 0.314488023519516,
      "learning_rate": 1.615416666666667e-05,
      "loss": 0.0024,
      "step": 81230
    },
    {
      "epoch": 5.416,
      "grad_norm": 0.2057560384273529,
      "learning_rate": 1.6150000000000003e-05,
      "loss": 0.0019,
      "step": 81240
    },
    {
      "epoch": 5.416666666666667,
      "grad_norm": 0.4214160740375519,
      "learning_rate": 1.6145833333333334e-05,
      "loss": 0.0026,
      "step": 81250
    },
    {
      "epoch": 5.417333333333334,
      "grad_norm": 0.07916762679815292,
      "learning_rate": 1.6141666666666668e-05,
      "loss": 0.0018,
      "step": 81260
    },
    {
      "epoch": 5.418,
      "grad_norm": 0.5525437593460083,
      "learning_rate": 1.61375e-05,
      "loss": 0.0016,
      "step": 81270
    },
    {
      "epoch": 5.418666666666667,
      "grad_norm": 0.1367267370223999,
      "learning_rate": 1.6133333333333334e-05,
      "loss": 0.0023,
      "step": 81280
    },
    {
      "epoch": 5.419333333333333,
      "grad_norm": 0.1452505886554718,
      "learning_rate": 1.6129166666666668e-05,
      "loss": 0.0015,
      "step": 81290
    },
    {
      "epoch": 5.42,
      "grad_norm": 0.16983312368392944,
      "learning_rate": 1.6125000000000002e-05,
      "loss": 0.0019,
      "step": 81300
    },
    {
      "epoch": 5.4206666666666665,
      "grad_norm": 0.13851892948150635,
      "learning_rate": 1.6120833333333337e-05,
      "loss": 0.0015,
      "step": 81310
    },
    {
      "epoch": 5.421333333333333,
      "grad_norm": 0.5805819630622864,
      "learning_rate": 1.6116666666666668e-05,
      "loss": 0.0016,
      "step": 81320
    },
    {
      "epoch": 5.422,
      "grad_norm": 0.34187841415405273,
      "learning_rate": 1.61125e-05,
      "loss": 0.002,
      "step": 81330
    },
    {
      "epoch": 5.422666666666666,
      "grad_norm": 0.1793605536222458,
      "learning_rate": 1.6108333333333333e-05,
      "loss": 0.0019,
      "step": 81340
    },
    {
      "epoch": 5.423333333333334,
      "grad_norm": 0.19306406378746033,
      "learning_rate": 1.6104166666666667e-05,
      "loss": 0.0025,
      "step": 81350
    },
    {
      "epoch": 5.424,
      "grad_norm": 0.5239915251731873,
      "learning_rate": 1.6100000000000002e-05,
      "loss": 0.0021,
      "step": 81360
    },
    {
      "epoch": 5.424666666666667,
      "grad_norm": 0.17990627884864807,
      "learning_rate": 1.6095833333333336e-05,
      "loss": 0.002,
      "step": 81370
    },
    {
      "epoch": 5.425333333333334,
      "grad_norm": 0.40752875804901123,
      "learning_rate": 1.609166666666667e-05,
      "loss": 0.003,
      "step": 81380
    },
    {
      "epoch": 5.426,
      "grad_norm": 0.4808390438556671,
      "learning_rate": 1.60875e-05,
      "loss": 0.0016,
      "step": 81390
    },
    {
      "epoch": 5.426666666666667,
      "grad_norm": 0.15205906331539154,
      "learning_rate": 1.6083333333333332e-05,
      "loss": 0.002,
      "step": 81400
    },
    {
      "epoch": 5.427333333333333,
      "grad_norm": 0.2784973680973053,
      "learning_rate": 1.6079166666666667e-05,
      "loss": 0.0019,
      "step": 81410
    },
    {
      "epoch": 5.428,
      "grad_norm": 0.3780505061149597,
      "learning_rate": 1.6075e-05,
      "loss": 0.0016,
      "step": 81420
    },
    {
      "epoch": 5.4286666666666665,
      "grad_norm": 0.23736028373241425,
      "learning_rate": 1.6070833333333335e-05,
      "loss": 0.0019,
      "step": 81430
    },
    {
      "epoch": 5.429333333333333,
      "grad_norm": 0.03488313406705856,
      "learning_rate": 1.606666666666667e-05,
      "loss": 0.0018,
      "step": 81440
    },
    {
      "epoch": 5.43,
      "grad_norm": 0.374710351228714,
      "learning_rate": 1.60625e-05,
      "loss": 0.0029,
      "step": 81450
    },
    {
      "epoch": 5.430666666666666,
      "grad_norm": 0.6535243391990662,
      "learning_rate": 1.6058333333333335e-05,
      "loss": 0.0026,
      "step": 81460
    },
    {
      "epoch": 5.431333333333333,
      "grad_norm": 0.4338773488998413,
      "learning_rate": 1.6054166666666666e-05,
      "loss": 0.002,
      "step": 81470
    },
    {
      "epoch": 5.432,
      "grad_norm": 0.21488991379737854,
      "learning_rate": 1.605e-05,
      "loss": 0.0015,
      "step": 81480
    },
    {
      "epoch": 5.432666666666667,
      "grad_norm": 0.23815150558948517,
      "learning_rate": 1.6045833333333335e-05,
      "loss": 0.002,
      "step": 81490
    },
    {
      "epoch": 5.433333333333334,
      "grad_norm": 0.13726691901683807,
      "learning_rate": 1.604166666666667e-05,
      "loss": 0.0029,
      "step": 81500
    },
    {
      "epoch": 5.434,
      "grad_norm": 0.27914589643478394,
      "learning_rate": 1.60375e-05,
      "loss": 0.0027,
      "step": 81510
    },
    {
      "epoch": 5.434666666666667,
      "grad_norm": 0.03934688866138458,
      "learning_rate": 1.6033333333333335e-05,
      "loss": 0.0014,
      "step": 81520
    },
    {
      "epoch": 5.435333333333333,
      "grad_norm": 0.2812737822532654,
      "learning_rate": 1.602916666666667e-05,
      "loss": 0.0013,
      "step": 81530
    },
    {
      "epoch": 5.436,
      "grad_norm": 0.30652087926864624,
      "learning_rate": 1.6025e-05,
      "loss": 0.0013,
      "step": 81540
    },
    {
      "epoch": 5.4366666666666665,
      "grad_norm": 0.4405665993690491,
      "learning_rate": 1.6020833333333334e-05,
      "loss": 0.0021,
      "step": 81550
    },
    {
      "epoch": 5.437333333333333,
      "grad_norm": 0.049489814788103104,
      "learning_rate": 1.601666666666667e-05,
      "loss": 0.0018,
      "step": 81560
    },
    {
      "epoch": 5.438,
      "grad_norm": 0.03499378263950348,
      "learning_rate": 1.60125e-05,
      "loss": 0.0018,
      "step": 81570
    },
    {
      "epoch": 5.438666666666666,
      "grad_norm": 0.3431240916252136,
      "learning_rate": 1.6008333333333334e-05,
      "loss": 0.0014,
      "step": 81580
    },
    {
      "epoch": 5.439333333333334,
      "grad_norm": 0.11038931459188461,
      "learning_rate": 1.6004166666666668e-05,
      "loss": 0.0016,
      "step": 81590
    },
    {
      "epoch": 5.44,
      "grad_norm": 0.04630999639630318,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.0014,
      "step": 81600
    },
    {
      "epoch": 5.440666666666667,
      "grad_norm": 0.1717197448015213,
      "learning_rate": 1.5995833333333334e-05,
      "loss": 0.0017,
      "step": 81610
    },
    {
      "epoch": 5.441333333333334,
      "grad_norm": 0.321310430765152,
      "learning_rate": 1.5991666666666668e-05,
      "loss": 0.0024,
      "step": 81620
    },
    {
      "epoch": 5.442,
      "grad_norm": 0.5289410948753357,
      "learning_rate": 1.59875e-05,
      "loss": 0.002,
      "step": 81630
    },
    {
      "epoch": 5.442666666666667,
      "grad_norm": 0.13608451187610626,
      "learning_rate": 1.5983333333333333e-05,
      "loss": 0.002,
      "step": 81640
    },
    {
      "epoch": 5.443333333333333,
      "grad_norm": 0.05397326871752739,
      "learning_rate": 1.5979166666666668e-05,
      "loss": 0.0021,
      "step": 81650
    },
    {
      "epoch": 5.444,
      "grad_norm": 0.3059805929660797,
      "learning_rate": 1.5975000000000002e-05,
      "loss": 0.002,
      "step": 81660
    },
    {
      "epoch": 5.4446666666666665,
      "grad_norm": 0.2833007872104645,
      "learning_rate": 1.5970833333333336e-05,
      "loss": 0.0017,
      "step": 81670
    },
    {
      "epoch": 5.445333333333333,
      "grad_norm": 0.4792134165763855,
      "learning_rate": 1.5966666666666667e-05,
      "loss": 0.0015,
      "step": 81680
    },
    {
      "epoch": 5.446,
      "grad_norm": 0.5811663866043091,
      "learning_rate": 1.59625e-05,
      "loss": 0.0025,
      "step": 81690
    },
    {
      "epoch": 5.446666666666666,
      "grad_norm": 0.33596938848495483,
      "learning_rate": 1.5958333333333333e-05,
      "loss": 0.0015,
      "step": 81700
    },
    {
      "epoch": 5.447333333333333,
      "grad_norm": 0.34747105836868286,
      "learning_rate": 1.5954166666666667e-05,
      "loss": 0.0022,
      "step": 81710
    },
    {
      "epoch": 5.448,
      "grad_norm": 0.2734539806842804,
      "learning_rate": 1.595e-05,
      "loss": 0.0026,
      "step": 81720
    },
    {
      "epoch": 5.448666666666667,
      "grad_norm": 0.3691410422325134,
      "learning_rate": 1.5945833333333336e-05,
      "loss": 0.0015,
      "step": 81730
    },
    {
      "epoch": 5.449333333333334,
      "grad_norm": 0.2589503526687622,
      "learning_rate": 1.594166666666667e-05,
      "loss": 0.0017,
      "step": 81740
    },
    {
      "epoch": 5.45,
      "grad_norm": 0.17305630445480347,
      "learning_rate": 1.59375e-05,
      "loss": 0.0014,
      "step": 81750
    },
    {
      "epoch": 5.450666666666667,
      "grad_norm": 0.30809181928634644,
      "learning_rate": 1.5933333333333332e-05,
      "loss": 0.0018,
      "step": 81760
    },
    {
      "epoch": 5.451333333333333,
      "grad_norm": 0.11297748982906342,
      "learning_rate": 1.5929166666666666e-05,
      "loss": 0.0022,
      "step": 81770
    },
    {
      "epoch": 5.452,
      "grad_norm": 0.3389538824558258,
      "learning_rate": 1.5925e-05,
      "loss": 0.002,
      "step": 81780
    },
    {
      "epoch": 5.4526666666666666,
      "grad_norm": 0.307018518447876,
      "learning_rate": 1.5920833333333335e-05,
      "loss": 0.002,
      "step": 81790
    },
    {
      "epoch": 5.453333333333333,
      "grad_norm": 0.27094000577926636,
      "learning_rate": 1.591666666666667e-05,
      "loss": 0.0014,
      "step": 81800
    },
    {
      "epoch": 5.454,
      "grad_norm": 0.05646267533302307,
      "learning_rate": 1.59125e-05,
      "loss": 0.0022,
      "step": 81810
    },
    {
      "epoch": 5.454666666666666,
      "grad_norm": 0.10103591531515121,
      "learning_rate": 1.5908333333333335e-05,
      "loss": 0.0018,
      "step": 81820
    },
    {
      "epoch": 5.455333333333333,
      "grad_norm": 0.3152237832546234,
      "learning_rate": 1.5904166666666666e-05,
      "loss": 0.0016,
      "step": 81830
    },
    {
      "epoch": 5.456,
      "grad_norm": 0.021760400384664536,
      "learning_rate": 1.59e-05,
      "loss": 0.0028,
      "step": 81840
    },
    {
      "epoch": 5.456666666666667,
      "grad_norm": 0.06127321720123291,
      "learning_rate": 1.5895833333333335e-05,
      "loss": 0.002,
      "step": 81850
    },
    {
      "epoch": 5.457333333333334,
      "grad_norm": 0.5122628211975098,
      "learning_rate": 1.589166666666667e-05,
      "loss": 0.0028,
      "step": 81860
    },
    {
      "epoch": 5.458,
      "grad_norm": 0.14695343375205994,
      "learning_rate": 1.58875e-05,
      "loss": 0.0014,
      "step": 81870
    },
    {
      "epoch": 5.458666666666667,
      "grad_norm": 0.40565431118011475,
      "learning_rate": 1.5883333333333334e-05,
      "loss": 0.0015,
      "step": 81880
    },
    {
      "epoch": 5.459333333333333,
      "grad_norm": 0.16998964548110962,
      "learning_rate": 1.587916666666667e-05,
      "loss": 0.0019,
      "step": 81890
    },
    {
      "epoch": 5.46,
      "grad_norm": 1.0512323379516602,
      "learning_rate": 1.5875e-05,
      "loss": 0.0026,
      "step": 81900
    },
    {
      "epoch": 5.460666666666667,
      "grad_norm": 0.1073174998164177,
      "learning_rate": 1.5870833333333334e-05,
      "loss": 0.0021,
      "step": 81910
    },
    {
      "epoch": 5.461333333333333,
      "grad_norm": 0.7491023540496826,
      "learning_rate": 1.586666666666667e-05,
      "loss": 0.0028,
      "step": 81920
    },
    {
      "epoch": 5.462,
      "grad_norm": 0.3992805480957031,
      "learning_rate": 1.58625e-05,
      "loss": 0.0024,
      "step": 81930
    },
    {
      "epoch": 5.462666666666666,
      "grad_norm": 0.13942289352416992,
      "learning_rate": 1.5858333333333334e-05,
      "loss": 0.0018,
      "step": 81940
    },
    {
      "epoch": 5.463333333333333,
      "grad_norm": 0.41531383991241455,
      "learning_rate": 1.5854166666666668e-05,
      "loss": 0.0017,
      "step": 81950
    },
    {
      "epoch": 5.464,
      "grad_norm": 0.5464056730270386,
      "learning_rate": 1.5850000000000002e-05,
      "loss": 0.0014,
      "step": 81960
    },
    {
      "epoch": 5.464666666666667,
      "grad_norm": 0.30690672993659973,
      "learning_rate": 1.5845833333333333e-05,
      "loss": 0.0016,
      "step": 81970
    },
    {
      "epoch": 5.465333333333334,
      "grad_norm": 0.5047389268875122,
      "learning_rate": 1.5841666666666668e-05,
      "loss": 0.0018,
      "step": 81980
    },
    {
      "epoch": 5.466,
      "grad_norm": 0.27766042947769165,
      "learning_rate": 1.58375e-05,
      "loss": 0.0013,
      "step": 81990
    },
    {
      "epoch": 5.466666666666667,
      "grad_norm": 0.0765031948685646,
      "learning_rate": 1.5833333333333333e-05,
      "loss": 0.002,
      "step": 82000
    },
    {
      "epoch": 5.467333333333333,
      "grad_norm": 0.08231557160615921,
      "learning_rate": 1.5829166666666667e-05,
      "loss": 0.0024,
      "step": 82010
    },
    {
      "epoch": 5.468,
      "grad_norm": 0.17035675048828125,
      "learning_rate": 1.5825000000000002e-05,
      "loss": 0.0016,
      "step": 82020
    },
    {
      "epoch": 5.468666666666667,
      "grad_norm": 0.3979732096195221,
      "learning_rate": 1.5820833333333336e-05,
      "loss": 0.0018,
      "step": 82030
    },
    {
      "epoch": 5.469333333333333,
      "grad_norm": 0.37872564792633057,
      "learning_rate": 1.5816666666666667e-05,
      "loss": 0.0018,
      "step": 82040
    },
    {
      "epoch": 5.47,
      "grad_norm": 0.5891017913818359,
      "learning_rate": 1.5812499999999998e-05,
      "loss": 0.002,
      "step": 82050
    },
    {
      "epoch": 5.470666666666666,
      "grad_norm": 0.5429881811141968,
      "learning_rate": 1.5808333333333332e-05,
      "loss": 0.0026,
      "step": 82060
    },
    {
      "epoch": 5.471333333333333,
      "grad_norm": 0.07227572053670883,
      "learning_rate": 1.5804166666666667e-05,
      "loss": 0.0012,
      "step": 82070
    },
    {
      "epoch": 5.4719999999999995,
      "grad_norm": 0.17472021281719208,
      "learning_rate": 1.58e-05,
      "loss": 0.0022,
      "step": 82080
    },
    {
      "epoch": 5.472666666666667,
      "grad_norm": 0.44910362362861633,
      "learning_rate": 1.5795833333333336e-05,
      "loss": 0.0017,
      "step": 82090
    },
    {
      "epoch": 5.473333333333334,
      "grad_norm": 0.0825364887714386,
      "learning_rate": 1.579166666666667e-05,
      "loss": 0.0016,
      "step": 82100
    },
    {
      "epoch": 5.474,
      "grad_norm": 0.44071295857429504,
      "learning_rate": 1.57875e-05,
      "loss": 0.0015,
      "step": 82110
    },
    {
      "epoch": 5.474666666666667,
      "grad_norm": 0.08724664151668549,
      "learning_rate": 1.5783333333333332e-05,
      "loss": 0.0013,
      "step": 82120
    },
    {
      "epoch": 5.475333333333333,
      "grad_norm": 0.13917243480682373,
      "learning_rate": 1.5779166666666666e-05,
      "loss": 0.0018,
      "step": 82130
    },
    {
      "epoch": 5.476,
      "grad_norm": 0.028056463226675987,
      "learning_rate": 1.5775e-05,
      "loss": 0.0013,
      "step": 82140
    },
    {
      "epoch": 5.476666666666667,
      "grad_norm": 0.0774446651339531,
      "learning_rate": 1.5770833333333335e-05,
      "loss": 0.0013,
      "step": 82150
    },
    {
      "epoch": 5.477333333333333,
      "grad_norm": 0.11897106468677521,
      "learning_rate": 1.576666666666667e-05,
      "loss": 0.0017,
      "step": 82160
    },
    {
      "epoch": 5.478,
      "grad_norm": 0.8516314029693604,
      "learning_rate": 1.57625e-05,
      "loss": 0.0027,
      "step": 82170
    },
    {
      "epoch": 5.478666666666666,
      "grad_norm": 0.17634740471839905,
      "learning_rate": 1.5758333333333335e-05,
      "loss": 0.002,
      "step": 82180
    },
    {
      "epoch": 5.479333333333333,
      "grad_norm": 0.04490891471505165,
      "learning_rate": 1.5754166666666666e-05,
      "loss": 0.0022,
      "step": 82190
    },
    {
      "epoch": 5.48,
      "grad_norm": 0.2699761986732483,
      "learning_rate": 1.575e-05,
      "loss": 0.0013,
      "step": 82200
    },
    {
      "epoch": 5.480666666666667,
      "grad_norm": 0.29567819833755493,
      "learning_rate": 1.5745833333333334e-05,
      "loss": 0.0022,
      "step": 82210
    },
    {
      "epoch": 5.481333333333334,
      "grad_norm": 0.24500763416290283,
      "learning_rate": 1.574166666666667e-05,
      "loss": 0.0013,
      "step": 82220
    },
    {
      "epoch": 5.482,
      "grad_norm": 0.9092229008674622,
      "learning_rate": 1.57375e-05,
      "loss": 0.0013,
      "step": 82230
    },
    {
      "epoch": 5.482666666666667,
      "grad_norm": 0.44060957431793213,
      "learning_rate": 1.5733333333333334e-05,
      "loss": 0.0014,
      "step": 82240
    },
    {
      "epoch": 5.483333333333333,
      "grad_norm": 0.14686383306980133,
      "learning_rate": 1.572916666666667e-05,
      "loss": 0.0022,
      "step": 82250
    },
    {
      "epoch": 5.484,
      "grad_norm": 0.4131773114204407,
      "learning_rate": 1.5725e-05,
      "loss": 0.0018,
      "step": 82260
    },
    {
      "epoch": 5.484666666666667,
      "grad_norm": 0.21173709630966187,
      "learning_rate": 1.5720833333333334e-05,
      "loss": 0.0016,
      "step": 82270
    },
    {
      "epoch": 5.485333333333333,
      "grad_norm": 0.06665036827325821,
      "learning_rate": 1.5716666666666668e-05,
      "loss": 0.0015,
      "step": 82280
    },
    {
      "epoch": 5.486,
      "grad_norm": 0.3001764118671417,
      "learning_rate": 1.57125e-05,
      "loss": 0.0018,
      "step": 82290
    },
    {
      "epoch": 5.486666666666666,
      "grad_norm": 0.37122365832328796,
      "learning_rate": 1.5708333333333333e-05,
      "loss": 0.0023,
      "step": 82300
    },
    {
      "epoch": 5.487333333333333,
      "grad_norm": 0.23487235605716705,
      "learning_rate": 1.5704166666666668e-05,
      "loss": 0.0019,
      "step": 82310
    },
    {
      "epoch": 5.4879999999999995,
      "grad_norm": 0.38101792335510254,
      "learning_rate": 1.5700000000000002e-05,
      "loss": 0.0017,
      "step": 82320
    },
    {
      "epoch": 5.488666666666667,
      "grad_norm": 0.10886017233133316,
      "learning_rate": 1.5695833333333333e-05,
      "loss": 0.0018,
      "step": 82330
    },
    {
      "epoch": 5.489333333333334,
      "grad_norm": 0.42943495512008667,
      "learning_rate": 1.5691666666666667e-05,
      "loss": 0.0021,
      "step": 82340
    },
    {
      "epoch": 5.49,
      "grad_norm": 0.13315071165561676,
      "learning_rate": 1.56875e-05,
      "loss": 0.0013,
      "step": 82350
    },
    {
      "epoch": 5.490666666666667,
      "grad_norm": 0.23209691047668457,
      "learning_rate": 1.5683333333333333e-05,
      "loss": 0.0018,
      "step": 82360
    },
    {
      "epoch": 5.491333333333333,
      "grad_norm": 0.20202824473381042,
      "learning_rate": 1.5679166666666667e-05,
      "loss": 0.0022,
      "step": 82370
    },
    {
      "epoch": 5.492,
      "grad_norm": 0.20654641091823578,
      "learning_rate": 1.5675e-05,
      "loss": 0.0016,
      "step": 82380
    },
    {
      "epoch": 5.492666666666667,
      "grad_norm": 0.4219881594181061,
      "learning_rate": 1.5670833333333336e-05,
      "loss": 0.0019,
      "step": 82390
    },
    {
      "epoch": 5.493333333333333,
      "grad_norm": 0.18527983129024506,
      "learning_rate": 1.5666666666666667e-05,
      "loss": 0.0022,
      "step": 82400
    },
    {
      "epoch": 5.494,
      "grad_norm": 0.8251178860664368,
      "learning_rate": 1.5662499999999998e-05,
      "loss": 0.0019,
      "step": 82410
    },
    {
      "epoch": 5.494666666666666,
      "grad_norm": 0.18412818014621735,
      "learning_rate": 1.5658333333333332e-05,
      "loss": 0.0015,
      "step": 82420
    },
    {
      "epoch": 5.495333333333333,
      "grad_norm": 0.33438920974731445,
      "learning_rate": 1.5654166666666667e-05,
      "loss": 0.003,
      "step": 82430
    },
    {
      "epoch": 5.496,
      "grad_norm": 0.1812412291765213,
      "learning_rate": 1.565e-05,
      "loss": 0.0014,
      "step": 82440
    },
    {
      "epoch": 5.496666666666667,
      "grad_norm": 0.5498253107070923,
      "learning_rate": 1.5645833333333335e-05,
      "loss": 0.0017,
      "step": 82450
    },
    {
      "epoch": 5.497333333333334,
      "grad_norm": 0.6354074478149414,
      "learning_rate": 1.564166666666667e-05,
      "loss": 0.0016,
      "step": 82460
    },
    {
      "epoch": 5.498,
      "grad_norm": 0.2850131094455719,
      "learning_rate": 1.56375e-05,
      "loss": 0.0022,
      "step": 82470
    },
    {
      "epoch": 5.498666666666667,
      "grad_norm": 0.1377296894788742,
      "learning_rate": 1.563333333333333e-05,
      "loss": 0.0021,
      "step": 82480
    },
    {
      "epoch": 5.499333333333333,
      "grad_norm": 0.0397566556930542,
      "learning_rate": 1.5629166666666666e-05,
      "loss": 0.0018,
      "step": 82490
    },
    {
      "epoch": 5.5,
      "grad_norm": 0.6051900386810303,
      "learning_rate": 1.5625e-05,
      "loss": 0.0017,
      "step": 82500
    },
    {
      "epoch": 5.500666666666667,
      "grad_norm": 0.3284662365913391,
      "learning_rate": 1.5620833333333335e-05,
      "loss": 0.0026,
      "step": 82510
    },
    {
      "epoch": 5.501333333333333,
      "grad_norm": 0.4753672182559967,
      "learning_rate": 1.561666666666667e-05,
      "loss": 0.0017,
      "step": 82520
    },
    {
      "epoch": 5.502,
      "grad_norm": 0.5242230892181396,
      "learning_rate": 1.5612500000000003e-05,
      "loss": 0.0019,
      "step": 82530
    },
    {
      "epoch": 5.502666666666666,
      "grad_norm": 0.10539284348487854,
      "learning_rate": 1.5608333333333334e-05,
      "loss": 0.0026,
      "step": 82540
    },
    {
      "epoch": 5.503333333333333,
      "grad_norm": 0.27517426013946533,
      "learning_rate": 1.5604166666666665e-05,
      "loss": 0.0022,
      "step": 82550
    },
    {
      "epoch": 5.504,
      "grad_norm": 0.702163815498352,
      "learning_rate": 1.56e-05,
      "loss": 0.0023,
      "step": 82560
    },
    {
      "epoch": 5.504666666666667,
      "grad_norm": 0.029485885053873062,
      "learning_rate": 1.5595833333333334e-05,
      "loss": 0.002,
      "step": 82570
    },
    {
      "epoch": 5.505333333333334,
      "grad_norm": 0.6855788230895996,
      "learning_rate": 1.559166666666667e-05,
      "loss": 0.0017,
      "step": 82580
    },
    {
      "epoch": 5.506,
      "grad_norm": 0.06882874667644501,
      "learning_rate": 1.5587500000000003e-05,
      "loss": 0.0018,
      "step": 82590
    },
    {
      "epoch": 5.506666666666667,
      "grad_norm": 0.27447375655174255,
      "learning_rate": 1.5583333333333334e-05,
      "loss": 0.002,
      "step": 82600
    },
    {
      "epoch": 5.507333333333333,
      "grad_norm": 0.44827908277511597,
      "learning_rate": 1.5579166666666668e-05,
      "loss": 0.0017,
      "step": 82610
    },
    {
      "epoch": 5.508,
      "grad_norm": 0.10522910207509995,
      "learning_rate": 1.5575e-05,
      "loss": 0.0014,
      "step": 82620
    },
    {
      "epoch": 5.508666666666667,
      "grad_norm": 0.04746521636843681,
      "learning_rate": 1.5570833333333333e-05,
      "loss": 0.0023,
      "step": 82630
    },
    {
      "epoch": 5.509333333333333,
      "grad_norm": 0.30079275369644165,
      "learning_rate": 1.5566666666666668e-05,
      "loss": 0.0016,
      "step": 82640
    },
    {
      "epoch": 5.51,
      "grad_norm": 0.44758841395378113,
      "learning_rate": 1.5562500000000002e-05,
      "loss": 0.0013,
      "step": 82650
    },
    {
      "epoch": 5.510666666666666,
      "grad_norm": 0.24328017234802246,
      "learning_rate": 1.5558333333333333e-05,
      "loss": 0.002,
      "step": 82660
    },
    {
      "epoch": 5.511333333333333,
      "grad_norm": 0.1726980209350586,
      "learning_rate": 1.5554166666666668e-05,
      "loss": 0.002,
      "step": 82670
    },
    {
      "epoch": 5.5120000000000005,
      "grad_norm": 0.1709090918302536,
      "learning_rate": 1.5550000000000002e-05,
      "loss": 0.0015,
      "step": 82680
    },
    {
      "epoch": 5.512666666666667,
      "grad_norm": 0.10912618041038513,
      "learning_rate": 1.5545833333333333e-05,
      "loss": 0.0012,
      "step": 82690
    },
    {
      "epoch": 5.513333333333334,
      "grad_norm": 0.4156462848186493,
      "learning_rate": 1.5541666666666667e-05,
      "loss": 0.0018,
      "step": 82700
    },
    {
      "epoch": 5.514,
      "grad_norm": 0.09162318706512451,
      "learning_rate": 1.55375e-05,
      "loss": 0.0018,
      "step": 82710
    },
    {
      "epoch": 5.514666666666667,
      "grad_norm": 0.27673494815826416,
      "learning_rate": 1.5533333333333333e-05,
      "loss": 0.0021,
      "step": 82720
    },
    {
      "epoch": 5.515333333333333,
      "grad_norm": 0.4188939034938812,
      "learning_rate": 1.5529166666666667e-05,
      "loss": 0.003,
      "step": 82730
    },
    {
      "epoch": 5.516,
      "grad_norm": 0.3846983015537262,
      "learning_rate": 1.5525e-05,
      "loss": 0.0015,
      "step": 82740
    },
    {
      "epoch": 5.516666666666667,
      "grad_norm": 0.48594939708709717,
      "learning_rate": 1.5520833333333336e-05,
      "loss": 0.0016,
      "step": 82750
    },
    {
      "epoch": 5.517333333333333,
      "grad_norm": 0.24298103153705597,
      "learning_rate": 1.5516666666666667e-05,
      "loss": 0.0019,
      "step": 82760
    },
    {
      "epoch": 5.518,
      "grad_norm": 0.249071404337883,
      "learning_rate": 1.55125e-05,
      "loss": 0.0019,
      "step": 82770
    },
    {
      "epoch": 5.518666666666666,
      "grad_norm": 0.3177272379398346,
      "learning_rate": 1.5508333333333332e-05,
      "loss": 0.0034,
      "step": 82780
    },
    {
      "epoch": 5.519333333333333,
      "grad_norm": 0.41687336564064026,
      "learning_rate": 1.5504166666666666e-05,
      "loss": 0.0018,
      "step": 82790
    },
    {
      "epoch": 5.52,
      "grad_norm": 0.36647239327430725,
      "learning_rate": 1.55e-05,
      "loss": 0.0015,
      "step": 82800
    },
    {
      "epoch": 5.520666666666667,
      "grad_norm": 0.34255558252334595,
      "learning_rate": 1.5495833333333335e-05,
      "loss": 0.0017,
      "step": 82810
    },
    {
      "epoch": 5.521333333333334,
      "grad_norm": 0.21361394226551056,
      "learning_rate": 1.549166666666667e-05,
      "loss": 0.0024,
      "step": 82820
    },
    {
      "epoch": 5.522,
      "grad_norm": 0.2408704310655594,
      "learning_rate": 1.54875e-05,
      "loss": 0.0021,
      "step": 82830
    },
    {
      "epoch": 5.522666666666667,
      "grad_norm": 0.0785270631313324,
      "learning_rate": 1.548333333333333e-05,
      "loss": 0.0014,
      "step": 82840
    },
    {
      "epoch": 5.523333333333333,
      "grad_norm": 0.04464618116617203,
      "learning_rate": 1.5479166666666666e-05,
      "loss": 0.0018,
      "step": 82850
    },
    {
      "epoch": 5.524,
      "grad_norm": 0.9124588370323181,
      "learning_rate": 1.5475e-05,
      "loss": 0.0015,
      "step": 82860
    },
    {
      "epoch": 5.524666666666667,
      "grad_norm": 0.11951607465744019,
      "learning_rate": 1.5470833333333334e-05,
      "loss": 0.0025,
      "step": 82870
    },
    {
      "epoch": 5.525333333333333,
      "grad_norm": 0.2064449042081833,
      "learning_rate": 1.546666666666667e-05,
      "loss": 0.0017,
      "step": 82880
    },
    {
      "epoch": 5.526,
      "grad_norm": 0.40191444754600525,
      "learning_rate": 1.5462500000000003e-05,
      "loss": 0.0016,
      "step": 82890
    },
    {
      "epoch": 5.526666666666666,
      "grad_norm": 0.20356082916259766,
      "learning_rate": 1.5458333333333334e-05,
      "loss": 0.0019,
      "step": 82900
    },
    {
      "epoch": 5.527333333333333,
      "grad_norm": 0.043118733912706375,
      "learning_rate": 1.545416666666667e-05,
      "loss": 0.0019,
      "step": 82910
    },
    {
      "epoch": 5.5280000000000005,
      "grad_norm": 0.07892097532749176,
      "learning_rate": 1.545e-05,
      "loss": 0.0014,
      "step": 82920
    },
    {
      "epoch": 5.528666666666666,
      "grad_norm": 0.17082221806049347,
      "learning_rate": 1.5445833333333334e-05,
      "loss": 0.0016,
      "step": 82930
    },
    {
      "epoch": 5.529333333333334,
      "grad_norm": 0.38105034828186035,
      "learning_rate": 1.5441666666666668e-05,
      "loss": 0.0024,
      "step": 82940
    },
    {
      "epoch": 5.53,
      "grad_norm": 0.2827121913433075,
      "learning_rate": 1.5437500000000003e-05,
      "loss": 0.0016,
      "step": 82950
    },
    {
      "epoch": 5.530666666666667,
      "grad_norm": 0.08101943135261536,
      "learning_rate": 1.5433333333333334e-05,
      "loss": 0.0013,
      "step": 82960
    },
    {
      "epoch": 5.531333333333333,
      "grad_norm": 0.517129123210907,
      "learning_rate": 1.5429166666666668e-05,
      "loss": 0.0026,
      "step": 82970
    },
    {
      "epoch": 5.532,
      "grad_norm": 0.42752981185913086,
      "learning_rate": 1.5425000000000002e-05,
      "loss": 0.0014,
      "step": 82980
    },
    {
      "epoch": 5.532666666666667,
      "grad_norm": 0.4443626403808594,
      "learning_rate": 1.5420833333333333e-05,
      "loss": 0.0018,
      "step": 82990
    },
    {
      "epoch": 5.533333333333333,
      "grad_norm": 0.05351338908076286,
      "learning_rate": 1.5416666666666668e-05,
      "loss": 0.0016,
      "step": 83000
    },
    {
      "epoch": 5.534,
      "grad_norm": 0.38989490270614624,
      "learning_rate": 1.5412500000000002e-05,
      "loss": 0.0021,
      "step": 83010
    },
    {
      "epoch": 5.534666666666666,
      "grad_norm": 0.09346307814121246,
      "learning_rate": 1.5408333333333333e-05,
      "loss": 0.0016,
      "step": 83020
    },
    {
      "epoch": 5.535333333333333,
      "grad_norm": 0.27306777238845825,
      "learning_rate": 1.5404166666666667e-05,
      "loss": 0.0019,
      "step": 83030
    },
    {
      "epoch": 5.536,
      "grad_norm": 0.4123378098011017,
      "learning_rate": 1.54e-05,
      "loss": 0.0015,
      "step": 83040
    },
    {
      "epoch": 5.536666666666667,
      "grad_norm": 0.24067005515098572,
      "learning_rate": 1.5395833333333336e-05,
      "loss": 0.0025,
      "step": 83050
    },
    {
      "epoch": 5.537333333333334,
      "grad_norm": 0.6205375790596008,
      "learning_rate": 1.5391666666666667e-05,
      "loss": 0.0029,
      "step": 83060
    },
    {
      "epoch": 5.538,
      "grad_norm": 0.1107051745057106,
      "learning_rate": 1.53875e-05,
      "loss": 0.0017,
      "step": 83070
    },
    {
      "epoch": 5.538666666666667,
      "grad_norm": 0.8330649137496948,
      "learning_rate": 1.5383333333333332e-05,
      "loss": 0.0021,
      "step": 83080
    },
    {
      "epoch": 5.539333333333333,
      "grad_norm": 0.207153782248497,
      "learning_rate": 1.5379166666666667e-05,
      "loss": 0.0021,
      "step": 83090
    },
    {
      "epoch": 5.54,
      "grad_norm": 0.026073329150676727,
      "learning_rate": 1.5375e-05,
      "loss": 0.0017,
      "step": 83100
    },
    {
      "epoch": 5.540666666666667,
      "grad_norm": 0.07867437601089478,
      "learning_rate": 1.5370833333333335e-05,
      "loss": 0.002,
      "step": 83110
    },
    {
      "epoch": 5.541333333333333,
      "grad_norm": 0.08208778500556946,
      "learning_rate": 1.536666666666667e-05,
      "loss": 0.0013,
      "step": 83120
    },
    {
      "epoch": 5.542,
      "grad_norm": 0.34726670384407043,
      "learning_rate": 1.53625e-05,
      "loss": 0.0017,
      "step": 83130
    },
    {
      "epoch": 5.542666666666666,
      "grad_norm": 0.27082687616348267,
      "learning_rate": 1.5358333333333332e-05,
      "loss": 0.0027,
      "step": 83140
    },
    {
      "epoch": 5.543333333333333,
      "grad_norm": 0.4239438772201538,
      "learning_rate": 1.5354166666666666e-05,
      "loss": 0.0022,
      "step": 83150
    },
    {
      "epoch": 5.5440000000000005,
      "grad_norm": 0.08362884074449539,
      "learning_rate": 1.535e-05,
      "loss": 0.0017,
      "step": 83160
    },
    {
      "epoch": 5.544666666666666,
      "grad_norm": 0.17845426499843597,
      "learning_rate": 1.5345833333333335e-05,
      "loss": 0.0014,
      "step": 83170
    },
    {
      "epoch": 5.545333333333334,
      "grad_norm": 0.4373365044593811,
      "learning_rate": 1.534166666666667e-05,
      "loss": 0.0017,
      "step": 83180
    },
    {
      "epoch": 5.546,
      "grad_norm": 0.6756090521812439,
      "learning_rate": 1.5337500000000004e-05,
      "loss": 0.0019,
      "step": 83190
    },
    {
      "epoch": 5.546666666666667,
      "grad_norm": 0.07317560911178589,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 0.0021,
      "step": 83200
    },
    {
      "epoch": 5.5473333333333334,
      "grad_norm": 0.44679659605026245,
      "learning_rate": 1.5329166666666665e-05,
      "loss": 0.0019,
      "step": 83210
    },
    {
      "epoch": 5.548,
      "grad_norm": 0.7106494903564453,
      "learning_rate": 1.5325e-05,
      "loss": 0.0014,
      "step": 83220
    },
    {
      "epoch": 5.548666666666667,
      "grad_norm": 0.2725234925746918,
      "learning_rate": 1.5320833333333334e-05,
      "loss": 0.0019,
      "step": 83230
    },
    {
      "epoch": 5.549333333333333,
      "grad_norm": 0.5862873792648315,
      "learning_rate": 1.531666666666667e-05,
      "loss": 0.0023,
      "step": 83240
    },
    {
      "epoch": 5.55,
      "grad_norm": 0.05288419872522354,
      "learning_rate": 1.5312500000000003e-05,
      "loss": 0.0013,
      "step": 83250
    },
    {
      "epoch": 5.550666666666666,
      "grad_norm": 0.598056972026825,
      "learning_rate": 1.5308333333333334e-05,
      "loss": 0.002,
      "step": 83260
    },
    {
      "epoch": 5.551333333333333,
      "grad_norm": 0.11009488254785538,
      "learning_rate": 1.5304166666666668e-05,
      "loss": 0.0019,
      "step": 83270
    },
    {
      "epoch": 5.552,
      "grad_norm": 0.14410622417926788,
      "learning_rate": 1.53e-05,
      "loss": 0.0018,
      "step": 83280
    },
    {
      "epoch": 5.552666666666667,
      "grad_norm": 0.33030998706817627,
      "learning_rate": 1.5295833333333334e-05,
      "loss": 0.0029,
      "step": 83290
    },
    {
      "epoch": 5.553333333333334,
      "grad_norm": 0.078239306807518,
      "learning_rate": 1.5291666666666668e-05,
      "loss": 0.0016,
      "step": 83300
    },
    {
      "epoch": 5.554,
      "grad_norm": 0.2943391501903534,
      "learning_rate": 1.5287500000000002e-05,
      "loss": 0.0028,
      "step": 83310
    },
    {
      "epoch": 5.554666666666667,
      "grad_norm": 0.6133897304534912,
      "learning_rate": 1.5283333333333333e-05,
      "loss": 0.0024,
      "step": 83320
    },
    {
      "epoch": 5.5553333333333335,
      "grad_norm": 0.20937730371952057,
      "learning_rate": 1.5279166666666668e-05,
      "loss": 0.0014,
      "step": 83330
    },
    {
      "epoch": 5.556,
      "grad_norm": 0.182346373796463,
      "learning_rate": 1.5275000000000002e-05,
      "loss": 0.0018,
      "step": 83340
    },
    {
      "epoch": 5.556666666666667,
      "grad_norm": 0.5764452815055847,
      "learning_rate": 1.5270833333333333e-05,
      "loss": 0.0022,
      "step": 83350
    },
    {
      "epoch": 5.557333333333333,
      "grad_norm": 0.08732166141271591,
      "learning_rate": 1.5266666666666667e-05,
      "loss": 0.0023,
      "step": 83360
    },
    {
      "epoch": 5.558,
      "grad_norm": 0.20551513135433197,
      "learning_rate": 1.52625e-05,
      "loss": 0.0012,
      "step": 83370
    },
    {
      "epoch": 5.558666666666666,
      "grad_norm": 0.21217457950115204,
      "learning_rate": 1.5258333333333333e-05,
      "loss": 0.0017,
      "step": 83380
    },
    {
      "epoch": 5.559333333333333,
      "grad_norm": 0.6469098329544067,
      "learning_rate": 1.5254166666666667e-05,
      "loss": 0.0016,
      "step": 83390
    },
    {
      "epoch": 5.5600000000000005,
      "grad_norm": 0.9587492942810059,
      "learning_rate": 1.525e-05,
      "loss": 0.0023,
      "step": 83400
    },
    {
      "epoch": 5.560666666666666,
      "grad_norm": 0.24639885127544403,
      "learning_rate": 1.5245833333333334e-05,
      "loss": 0.0019,
      "step": 83410
    },
    {
      "epoch": 5.561333333333334,
      "grad_norm": 0.37353232502937317,
      "learning_rate": 1.5241666666666668e-05,
      "loss": 0.0018,
      "step": 83420
    },
    {
      "epoch": 5.562,
      "grad_norm": 0.10077130794525146,
      "learning_rate": 1.5237500000000001e-05,
      "loss": 0.0013,
      "step": 83430
    },
    {
      "epoch": 5.562666666666667,
      "grad_norm": 0.5392456650733948,
      "learning_rate": 1.5233333333333332e-05,
      "loss": 0.0025,
      "step": 83440
    },
    {
      "epoch": 5.5633333333333335,
      "grad_norm": 0.1658208668231964,
      "learning_rate": 1.5229166666666666e-05,
      "loss": 0.0019,
      "step": 83450
    },
    {
      "epoch": 5.564,
      "grad_norm": 0.17611955106258392,
      "learning_rate": 1.5225e-05,
      "loss": 0.0018,
      "step": 83460
    },
    {
      "epoch": 5.564666666666667,
      "grad_norm": 0.4730316996574402,
      "learning_rate": 1.5220833333333333e-05,
      "loss": 0.0014,
      "step": 83470
    },
    {
      "epoch": 5.565333333333333,
      "grad_norm": 0.4783763885498047,
      "learning_rate": 1.5216666666666668e-05,
      "loss": 0.0018,
      "step": 83480
    },
    {
      "epoch": 5.566,
      "grad_norm": 0.06299816817045212,
      "learning_rate": 1.5212500000000002e-05,
      "loss": 0.0015,
      "step": 83490
    },
    {
      "epoch": 5.566666666666666,
      "grad_norm": 0.3101215064525604,
      "learning_rate": 1.5208333333333333e-05,
      "loss": 0.0015,
      "step": 83500
    },
    {
      "epoch": 5.567333333333333,
      "grad_norm": 0.24341073632240295,
      "learning_rate": 1.5204166666666666e-05,
      "loss": 0.0015,
      "step": 83510
    },
    {
      "epoch": 5.568,
      "grad_norm": 0.9255280494689941,
      "learning_rate": 1.52e-05,
      "loss": 0.0024,
      "step": 83520
    },
    {
      "epoch": 5.568666666666667,
      "grad_norm": 0.6163442134857178,
      "learning_rate": 1.5195833333333335e-05,
      "loss": 0.0026,
      "step": 83530
    },
    {
      "epoch": 5.569333333333334,
      "grad_norm": 0.5577672719955444,
      "learning_rate": 1.5191666666666667e-05,
      "loss": 0.0015,
      "step": 83540
    },
    {
      "epoch": 5.57,
      "grad_norm": 0.0742267593741417,
      "learning_rate": 1.5187500000000002e-05,
      "loss": 0.0019,
      "step": 83550
    },
    {
      "epoch": 5.570666666666667,
      "grad_norm": 0.12840089201927185,
      "learning_rate": 1.5183333333333333e-05,
      "loss": 0.0022,
      "step": 83560
    },
    {
      "epoch": 5.5713333333333335,
      "grad_norm": 0.41054585576057434,
      "learning_rate": 1.5179166666666667e-05,
      "loss": 0.0018,
      "step": 83570
    },
    {
      "epoch": 5.572,
      "grad_norm": 0.08275202661752701,
      "learning_rate": 1.5175e-05,
      "loss": 0.0015,
      "step": 83580
    },
    {
      "epoch": 5.572666666666667,
      "grad_norm": 0.10602477937936783,
      "learning_rate": 1.5170833333333334e-05,
      "loss": 0.0021,
      "step": 83590
    },
    {
      "epoch": 5.573333333333333,
      "grad_norm": 0.2755497395992279,
      "learning_rate": 1.5166666666666668e-05,
      "loss": 0.0013,
      "step": 83600
    },
    {
      "epoch": 5.574,
      "grad_norm": 0.5650944113731384,
      "learning_rate": 1.5162500000000001e-05,
      "loss": 0.0019,
      "step": 83610
    },
    {
      "epoch": 5.574666666666666,
      "grad_norm": 0.17033517360687256,
      "learning_rate": 1.5158333333333332e-05,
      "loss": 0.0017,
      "step": 83620
    },
    {
      "epoch": 5.575333333333333,
      "grad_norm": 0.5838408470153809,
      "learning_rate": 1.5154166666666666e-05,
      "loss": 0.002,
      "step": 83630
    },
    {
      "epoch": 5.576,
      "grad_norm": 0.08133131265640259,
      "learning_rate": 1.515e-05,
      "loss": 0.0016,
      "step": 83640
    },
    {
      "epoch": 5.576666666666666,
      "grad_norm": 0.3417540490627289,
      "learning_rate": 1.5145833333333333e-05,
      "loss": 0.0034,
      "step": 83650
    },
    {
      "epoch": 5.577333333333334,
      "grad_norm": 0.34794530272483826,
      "learning_rate": 1.5141666666666668e-05,
      "loss": 0.0016,
      "step": 83660
    },
    {
      "epoch": 5.578,
      "grad_norm": 0.21091344952583313,
      "learning_rate": 1.5137500000000002e-05,
      "loss": 0.0015,
      "step": 83670
    },
    {
      "epoch": 5.578666666666667,
      "grad_norm": 0.34508323669433594,
      "learning_rate": 1.5133333333333333e-05,
      "loss": 0.0016,
      "step": 83680
    },
    {
      "epoch": 5.5793333333333335,
      "grad_norm": 0.05381658673286438,
      "learning_rate": 1.5129166666666666e-05,
      "loss": 0.0018,
      "step": 83690
    },
    {
      "epoch": 5.58,
      "grad_norm": 0.39038747549057007,
      "learning_rate": 1.5125e-05,
      "loss": 0.0024,
      "step": 83700
    },
    {
      "epoch": 5.580666666666667,
      "grad_norm": 0.17991213500499725,
      "learning_rate": 1.5120833333333334e-05,
      "loss": 0.0014,
      "step": 83710
    },
    {
      "epoch": 5.581333333333333,
      "grad_norm": 0.22168226540088654,
      "learning_rate": 1.5116666666666667e-05,
      "loss": 0.0018,
      "step": 83720
    },
    {
      "epoch": 5.582,
      "grad_norm": 0.34487342834472656,
      "learning_rate": 1.5112500000000001e-05,
      "loss": 0.0018,
      "step": 83730
    },
    {
      "epoch": 5.582666666666666,
      "grad_norm": 0.05959140136837959,
      "learning_rate": 1.5108333333333332e-05,
      "loss": 0.0019,
      "step": 83740
    },
    {
      "epoch": 5.583333333333333,
      "grad_norm": 0.17327241599559784,
      "learning_rate": 1.5104166666666667e-05,
      "loss": 0.0015,
      "step": 83750
    },
    {
      "epoch": 5.584,
      "grad_norm": 0.20349235832691193,
      "learning_rate": 1.51e-05,
      "loss": 0.0012,
      "step": 83760
    },
    {
      "epoch": 5.584666666666667,
      "grad_norm": 0.34692686796188354,
      "learning_rate": 1.5095833333333334e-05,
      "loss": 0.0013,
      "step": 83770
    },
    {
      "epoch": 5.585333333333334,
      "grad_norm": 0.07706485688686371,
      "learning_rate": 1.5091666666666668e-05,
      "loss": 0.0016,
      "step": 83780
    },
    {
      "epoch": 5.586,
      "grad_norm": 0.2428695559501648,
      "learning_rate": 1.50875e-05,
      "loss": 0.0013,
      "step": 83790
    },
    {
      "epoch": 5.586666666666667,
      "grad_norm": 0.14457982778549194,
      "learning_rate": 1.5083333333333335e-05,
      "loss": 0.0022,
      "step": 83800
    },
    {
      "epoch": 5.5873333333333335,
      "grad_norm": 0.7150150537490845,
      "learning_rate": 1.5079166666666666e-05,
      "loss": 0.0022,
      "step": 83810
    },
    {
      "epoch": 5.588,
      "grad_norm": 0.3434201180934906,
      "learning_rate": 1.5075e-05,
      "loss": 0.0012,
      "step": 83820
    },
    {
      "epoch": 5.588666666666667,
      "grad_norm": 0.04612383246421814,
      "learning_rate": 1.5070833333333335e-05,
      "loss": 0.0019,
      "step": 83830
    },
    {
      "epoch": 5.589333333333333,
      "grad_norm": 0.2426958829164505,
      "learning_rate": 1.5066666666666668e-05,
      "loss": 0.0022,
      "step": 83840
    },
    {
      "epoch": 5.59,
      "grad_norm": 0.47310787439346313,
      "learning_rate": 1.5062500000000002e-05,
      "loss": 0.0021,
      "step": 83850
    },
    {
      "epoch": 5.5906666666666665,
      "grad_norm": 0.2058878242969513,
      "learning_rate": 1.5058333333333335e-05,
      "loss": 0.0016,
      "step": 83860
    },
    {
      "epoch": 5.591333333333333,
      "grad_norm": 0.1423605978488922,
      "learning_rate": 1.5054166666666667e-05,
      "loss": 0.0015,
      "step": 83870
    },
    {
      "epoch": 5.592,
      "grad_norm": 0.3331233263015747,
      "learning_rate": 1.505e-05,
      "loss": 0.0017,
      "step": 83880
    },
    {
      "epoch": 5.592666666666666,
      "grad_norm": 0.17282089591026306,
      "learning_rate": 1.5045833333333334e-05,
      "loss": 0.0015,
      "step": 83890
    },
    {
      "epoch": 5.593333333333334,
      "grad_norm": 0.12134355306625366,
      "learning_rate": 1.5041666666666669e-05,
      "loss": 0.002,
      "step": 83900
    },
    {
      "epoch": 5.594,
      "grad_norm": 0.27503615617752075,
      "learning_rate": 1.5037500000000001e-05,
      "loss": 0.0012,
      "step": 83910
    },
    {
      "epoch": 5.594666666666667,
      "grad_norm": 0.47849205136299133,
      "learning_rate": 1.5033333333333336e-05,
      "loss": 0.0023,
      "step": 83920
    },
    {
      "epoch": 5.5953333333333335,
      "grad_norm": 0.17442405223846436,
      "learning_rate": 1.5029166666666667e-05,
      "loss": 0.0018,
      "step": 83930
    },
    {
      "epoch": 5.596,
      "grad_norm": 0.10845919698476791,
      "learning_rate": 1.5025000000000001e-05,
      "loss": 0.0021,
      "step": 83940
    },
    {
      "epoch": 5.596666666666667,
      "grad_norm": 0.10237406939268112,
      "learning_rate": 1.5020833333333334e-05,
      "loss": 0.0021,
      "step": 83950
    },
    {
      "epoch": 5.597333333333333,
      "grad_norm": 0.5095317363739014,
      "learning_rate": 1.5016666666666668e-05,
      "loss": 0.0019,
      "step": 83960
    },
    {
      "epoch": 5.598,
      "grad_norm": 0.07254869490861893,
      "learning_rate": 1.5012500000000002e-05,
      "loss": 0.0017,
      "step": 83970
    },
    {
      "epoch": 5.5986666666666665,
      "grad_norm": 0.31449708342552185,
      "learning_rate": 1.5008333333333335e-05,
      "loss": 0.0018,
      "step": 83980
    },
    {
      "epoch": 5.599333333333333,
      "grad_norm": 0.027486223727464676,
      "learning_rate": 1.5004166666666666e-05,
      "loss": 0.0016,
      "step": 83990
    },
    {
      "epoch": 5.6,
      "grad_norm": 0.21137191355228424,
      "learning_rate": 1.5e-05,
      "loss": 0.0014,
      "step": 84000
    },
    {
      "epoch": 5.600666666666667,
      "grad_norm": 0.07763510197401047,
      "learning_rate": 1.4995833333333335e-05,
      "loss": 0.0017,
      "step": 84010
    },
    {
      "epoch": 5.601333333333334,
      "grad_norm": 0.35010820627212524,
      "learning_rate": 1.4991666666666667e-05,
      "loss": 0.0027,
      "step": 84020
    },
    {
      "epoch": 5.602,
      "grad_norm": 0.13335663080215454,
      "learning_rate": 1.4987500000000002e-05,
      "loss": 0.0026,
      "step": 84030
    },
    {
      "epoch": 5.602666666666667,
      "grad_norm": 0.1837151050567627,
      "learning_rate": 1.4983333333333336e-05,
      "loss": 0.0019,
      "step": 84040
    },
    {
      "epoch": 5.6033333333333335,
      "grad_norm": 0.3139292001724243,
      "learning_rate": 1.4979166666666667e-05,
      "loss": 0.0021,
      "step": 84050
    },
    {
      "epoch": 5.604,
      "grad_norm": 0.7819375395774841,
      "learning_rate": 1.4975e-05,
      "loss": 0.0023,
      "step": 84060
    },
    {
      "epoch": 5.604666666666667,
      "grad_norm": 0.04249181970953941,
      "learning_rate": 1.4970833333333334e-05,
      "loss": 0.0017,
      "step": 84070
    },
    {
      "epoch": 5.605333333333333,
      "grad_norm": 0.11449532955884933,
      "learning_rate": 1.4966666666666668e-05,
      "loss": 0.0033,
      "step": 84080
    },
    {
      "epoch": 5.606,
      "grad_norm": 0.42534810304641724,
      "learning_rate": 1.4962500000000001e-05,
      "loss": 0.0015,
      "step": 84090
    },
    {
      "epoch": 5.6066666666666665,
      "grad_norm": 0.034782618284225464,
      "learning_rate": 1.4958333333333336e-05,
      "loss": 0.0016,
      "step": 84100
    },
    {
      "epoch": 5.607333333333333,
      "grad_norm": 0.44999387860298157,
      "learning_rate": 1.4954166666666666e-05,
      "loss": 0.0021,
      "step": 84110
    },
    {
      "epoch": 5.608,
      "grad_norm": 0.20828531682491302,
      "learning_rate": 1.4950000000000001e-05,
      "loss": 0.0014,
      "step": 84120
    },
    {
      "epoch": 5.608666666666666,
      "grad_norm": 0.342413067817688,
      "learning_rate": 1.4945833333333334e-05,
      "loss": 0.0014,
      "step": 84130
    },
    {
      "epoch": 5.609333333333334,
      "grad_norm": 0.07626122981309891,
      "learning_rate": 1.4941666666666668e-05,
      "loss": 0.0019,
      "step": 84140
    },
    {
      "epoch": 5.61,
      "grad_norm": 0.11088917404413223,
      "learning_rate": 1.4937500000000002e-05,
      "loss": 0.0018,
      "step": 84150
    },
    {
      "epoch": 5.610666666666667,
      "grad_norm": 0.555374264717102,
      "learning_rate": 1.4933333333333335e-05,
      "loss": 0.0023,
      "step": 84160
    },
    {
      "epoch": 5.6113333333333335,
      "grad_norm": 0.34443554282188416,
      "learning_rate": 1.4929166666666666e-05,
      "loss": 0.002,
      "step": 84170
    },
    {
      "epoch": 5.612,
      "grad_norm": 0.10767756402492523,
      "learning_rate": 1.4925e-05,
      "loss": 0.0022,
      "step": 84180
    },
    {
      "epoch": 5.612666666666667,
      "grad_norm": 0.1518218070268631,
      "learning_rate": 1.4920833333333335e-05,
      "loss": 0.0017,
      "step": 84190
    },
    {
      "epoch": 5.613333333333333,
      "grad_norm": 0.07628383487462997,
      "learning_rate": 1.4916666666666667e-05,
      "loss": 0.0024,
      "step": 84200
    },
    {
      "epoch": 5.614,
      "grad_norm": 0.42837613821029663,
      "learning_rate": 1.4912500000000002e-05,
      "loss": 0.002,
      "step": 84210
    },
    {
      "epoch": 5.6146666666666665,
      "grad_norm": 0.047992635518312454,
      "learning_rate": 1.4908333333333336e-05,
      "loss": 0.0022,
      "step": 84220
    },
    {
      "epoch": 5.615333333333333,
      "grad_norm": 0.20480895042419434,
      "learning_rate": 1.4904166666666667e-05,
      "loss": 0.0025,
      "step": 84230
    },
    {
      "epoch": 5.616,
      "grad_norm": 0.5762712359428406,
      "learning_rate": 1.49e-05,
      "loss": 0.002,
      "step": 84240
    },
    {
      "epoch": 5.616666666666667,
      "grad_norm": 0.11545003205537796,
      "learning_rate": 1.4895833333333334e-05,
      "loss": 0.0024,
      "step": 84250
    },
    {
      "epoch": 5.617333333333333,
      "grad_norm": 0.5515265464782715,
      "learning_rate": 1.4891666666666668e-05,
      "loss": 0.0015,
      "step": 84260
    },
    {
      "epoch": 5.618,
      "grad_norm": 0.11032185703516006,
      "learning_rate": 1.4887500000000001e-05,
      "loss": 0.0015,
      "step": 84270
    },
    {
      "epoch": 5.618666666666667,
      "grad_norm": 1.4108918905258179,
      "learning_rate": 1.4883333333333335e-05,
      "loss": 0.0023,
      "step": 84280
    },
    {
      "epoch": 5.6193333333333335,
      "grad_norm": 0.34204861521720886,
      "learning_rate": 1.4879166666666666e-05,
      "loss": 0.002,
      "step": 84290
    },
    {
      "epoch": 5.62,
      "grad_norm": 0.5906534194946289,
      "learning_rate": 1.4875e-05,
      "loss": 0.0018,
      "step": 84300
    },
    {
      "epoch": 5.620666666666667,
      "grad_norm": 0.05738465487957001,
      "learning_rate": 1.4870833333333333e-05,
      "loss": 0.0017,
      "step": 84310
    },
    {
      "epoch": 5.621333333333333,
      "grad_norm": 0.3381960391998291,
      "learning_rate": 1.4866666666666668e-05,
      "loss": 0.0017,
      "step": 84320
    },
    {
      "epoch": 5.622,
      "grad_norm": 0.2504388391971588,
      "learning_rate": 1.4862500000000002e-05,
      "loss": 0.0026,
      "step": 84330
    },
    {
      "epoch": 5.6226666666666665,
      "grad_norm": 0.3396795392036438,
      "learning_rate": 1.4858333333333335e-05,
      "loss": 0.0019,
      "step": 84340
    },
    {
      "epoch": 5.623333333333333,
      "grad_norm": 0.24406418204307556,
      "learning_rate": 1.4854166666666666e-05,
      "loss": 0.0017,
      "step": 84350
    },
    {
      "epoch": 5.624,
      "grad_norm": 0.2758679687976837,
      "learning_rate": 1.485e-05,
      "loss": 0.0018,
      "step": 84360
    },
    {
      "epoch": 5.624666666666666,
      "grad_norm": 0.24356546998023987,
      "learning_rate": 1.4845833333333334e-05,
      "loss": 0.0016,
      "step": 84370
    },
    {
      "epoch": 5.625333333333334,
      "grad_norm": 0.2777448892593384,
      "learning_rate": 1.4841666666666667e-05,
      "loss": 0.0014,
      "step": 84380
    },
    {
      "epoch": 5.626,
      "grad_norm": 0.8566067218780518,
      "learning_rate": 1.4837500000000002e-05,
      "loss": 0.0021,
      "step": 84390
    },
    {
      "epoch": 5.626666666666667,
      "grad_norm": 0.3474748134613037,
      "learning_rate": 1.4833333333333336e-05,
      "loss": 0.0021,
      "step": 84400
    },
    {
      "epoch": 5.6273333333333335,
      "grad_norm": 0.4797135889530182,
      "learning_rate": 1.4829166666666667e-05,
      "loss": 0.0018,
      "step": 84410
    },
    {
      "epoch": 5.628,
      "grad_norm": 0.05447957292199135,
      "learning_rate": 1.4825e-05,
      "loss": 0.0023,
      "step": 84420
    },
    {
      "epoch": 5.628666666666667,
      "grad_norm": 0.23917756974697113,
      "learning_rate": 1.4820833333333334e-05,
      "loss": 0.0016,
      "step": 84430
    },
    {
      "epoch": 5.629333333333333,
      "grad_norm": 0.05557379499077797,
      "learning_rate": 1.4816666666666668e-05,
      "loss": 0.0018,
      "step": 84440
    },
    {
      "epoch": 5.63,
      "grad_norm": 0.41050752997398376,
      "learning_rate": 1.4812500000000001e-05,
      "loss": 0.0017,
      "step": 84450
    },
    {
      "epoch": 5.6306666666666665,
      "grad_norm": 0.6263129711151123,
      "learning_rate": 1.4808333333333335e-05,
      "loss": 0.0015,
      "step": 84460
    },
    {
      "epoch": 5.631333333333333,
      "grad_norm": 0.36634549498558044,
      "learning_rate": 1.4804166666666666e-05,
      "loss": 0.0022,
      "step": 84470
    },
    {
      "epoch": 5.632,
      "grad_norm": 0.07049999386072159,
      "learning_rate": 1.48e-05,
      "loss": 0.0024,
      "step": 84480
    },
    {
      "epoch": 5.632666666666667,
      "grad_norm": 0.2389976680278778,
      "learning_rate": 1.4795833333333333e-05,
      "loss": 0.0014,
      "step": 84490
    },
    {
      "epoch": 5.633333333333333,
      "grad_norm": 0.7127824425697327,
      "learning_rate": 1.4791666666666668e-05,
      "loss": 0.0012,
      "step": 84500
    },
    {
      "epoch": 5.634,
      "grad_norm": 0.04590822011232376,
      "learning_rate": 1.4787500000000002e-05,
      "loss": 0.0022,
      "step": 84510
    },
    {
      "epoch": 5.634666666666667,
      "grad_norm": 0.271844744682312,
      "learning_rate": 1.4783333333333335e-05,
      "loss": 0.0019,
      "step": 84520
    },
    {
      "epoch": 5.6353333333333335,
      "grad_norm": 0.21284472942352295,
      "learning_rate": 1.4779166666666666e-05,
      "loss": 0.0018,
      "step": 84530
    },
    {
      "epoch": 5.636,
      "grad_norm": 0.10365375131368637,
      "learning_rate": 1.4775e-05,
      "loss": 0.0016,
      "step": 84540
    },
    {
      "epoch": 5.636666666666667,
      "grad_norm": 0.17515940964221954,
      "learning_rate": 1.4770833333333334e-05,
      "loss": 0.0025,
      "step": 84550
    },
    {
      "epoch": 5.637333333333333,
      "grad_norm": 0.41877371072769165,
      "learning_rate": 1.4766666666666667e-05,
      "loss": 0.0025,
      "step": 84560
    },
    {
      "epoch": 5.638,
      "grad_norm": 0.7918789982795715,
      "learning_rate": 1.4762500000000001e-05,
      "loss": 0.0023,
      "step": 84570
    },
    {
      "epoch": 5.6386666666666665,
      "grad_norm": 0.38159018754959106,
      "learning_rate": 1.4758333333333336e-05,
      "loss": 0.0012,
      "step": 84580
    },
    {
      "epoch": 5.639333333333333,
      "grad_norm": 0.04598112404346466,
      "learning_rate": 1.4754166666666667e-05,
      "loss": 0.0025,
      "step": 84590
    },
    {
      "epoch": 5.64,
      "grad_norm": 0.11384234577417374,
      "learning_rate": 1.475e-05,
      "loss": 0.0034,
      "step": 84600
    },
    {
      "epoch": 5.640666666666666,
      "grad_norm": 0.4914129972457886,
      "learning_rate": 1.4745833333333334e-05,
      "loss": 0.0017,
      "step": 84610
    },
    {
      "epoch": 5.641333333333334,
      "grad_norm": 0.4439042806625366,
      "learning_rate": 1.4741666666666668e-05,
      "loss": 0.0016,
      "step": 84620
    },
    {
      "epoch": 5.642,
      "grad_norm": 0.10739726573228836,
      "learning_rate": 1.47375e-05,
      "loss": 0.0027,
      "step": 84630
    },
    {
      "epoch": 5.642666666666667,
      "grad_norm": 0.04648979753255844,
      "learning_rate": 1.4733333333333335e-05,
      "loss": 0.0024,
      "step": 84640
    },
    {
      "epoch": 5.6433333333333335,
      "grad_norm": 0.4072911739349365,
      "learning_rate": 1.4729166666666666e-05,
      "loss": 0.0029,
      "step": 84650
    },
    {
      "epoch": 5.644,
      "grad_norm": 0.24157939851284027,
      "learning_rate": 1.4725e-05,
      "loss": 0.0017,
      "step": 84660
    },
    {
      "epoch": 5.644666666666667,
      "grad_norm": 0.47789910435676575,
      "learning_rate": 1.4720833333333333e-05,
      "loss": 0.0017,
      "step": 84670
    },
    {
      "epoch": 5.645333333333333,
      "grad_norm": 0.041946861892938614,
      "learning_rate": 1.4716666666666668e-05,
      "loss": 0.0023,
      "step": 84680
    },
    {
      "epoch": 5.646,
      "grad_norm": 0.08685947209596634,
      "learning_rate": 1.4712500000000002e-05,
      "loss": 0.0017,
      "step": 84690
    },
    {
      "epoch": 5.6466666666666665,
      "grad_norm": 0.11446025967597961,
      "learning_rate": 1.4708333333333335e-05,
      "loss": 0.0018,
      "step": 84700
    },
    {
      "epoch": 5.647333333333333,
      "grad_norm": 0.21908339858055115,
      "learning_rate": 1.4704166666666666e-05,
      "loss": 0.0015,
      "step": 84710
    },
    {
      "epoch": 5.648,
      "grad_norm": 0.6495753526687622,
      "learning_rate": 1.47e-05,
      "loss": 0.0023,
      "step": 84720
    },
    {
      "epoch": 5.648666666666666,
      "grad_norm": 0.3127190172672272,
      "learning_rate": 1.4695833333333334e-05,
      "loss": 0.003,
      "step": 84730
    },
    {
      "epoch": 5.649333333333333,
      "grad_norm": 0.18241442739963531,
      "learning_rate": 1.4691666666666667e-05,
      "loss": 0.0018,
      "step": 84740
    },
    {
      "epoch": 5.65,
      "grad_norm": 0.13993842899799347,
      "learning_rate": 1.4687500000000001e-05,
      "loss": 0.0018,
      "step": 84750
    },
    {
      "epoch": 5.650666666666667,
      "grad_norm": 0.7191390991210938,
      "learning_rate": 1.4683333333333336e-05,
      "loss": 0.0025,
      "step": 84760
    },
    {
      "epoch": 5.6513333333333335,
      "grad_norm": 0.3855842053890228,
      "learning_rate": 1.4679166666666667e-05,
      "loss": 0.0015,
      "step": 84770
    },
    {
      "epoch": 5.652,
      "grad_norm": 0.12153079360723495,
      "learning_rate": 1.4675e-05,
      "loss": 0.0025,
      "step": 84780
    },
    {
      "epoch": 5.652666666666667,
      "grad_norm": 0.0435483455657959,
      "learning_rate": 1.4670833333333334e-05,
      "loss": 0.0019,
      "step": 84790
    },
    {
      "epoch": 5.653333333333333,
      "grad_norm": 0.6165642738342285,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 0.0019,
      "step": 84800
    },
    {
      "epoch": 5.654,
      "grad_norm": 0.5036242008209229,
      "learning_rate": 1.46625e-05,
      "loss": 0.0023,
      "step": 84810
    },
    {
      "epoch": 5.6546666666666665,
      "grad_norm": 0.27220284938812256,
      "learning_rate": 1.4658333333333335e-05,
      "loss": 0.0024,
      "step": 84820
    },
    {
      "epoch": 5.655333333333333,
      "grad_norm": 0.37150874733924866,
      "learning_rate": 1.4654166666666666e-05,
      "loss": 0.0024,
      "step": 84830
    },
    {
      "epoch": 5.656,
      "grad_norm": 0.13675270974636078,
      "learning_rate": 1.465e-05,
      "loss": 0.0015,
      "step": 84840
    },
    {
      "epoch": 5.656666666666666,
      "grad_norm": 0.24487856030464172,
      "learning_rate": 1.4645833333333333e-05,
      "loss": 0.0025,
      "step": 84850
    },
    {
      "epoch": 5.657333333333334,
      "grad_norm": 0.11065194010734558,
      "learning_rate": 1.4641666666666667e-05,
      "loss": 0.002,
      "step": 84860
    },
    {
      "epoch": 5.658,
      "grad_norm": 0.13374461233615875,
      "learning_rate": 1.4637500000000002e-05,
      "loss": 0.0021,
      "step": 84870
    },
    {
      "epoch": 5.658666666666667,
      "grad_norm": 0.18201802670955658,
      "learning_rate": 1.4633333333333334e-05,
      "loss": 0.0019,
      "step": 84880
    },
    {
      "epoch": 5.6593333333333335,
      "grad_norm": 0.13124842941761017,
      "learning_rate": 1.4629166666666665e-05,
      "loss": 0.0024,
      "step": 84890
    },
    {
      "epoch": 5.66,
      "grad_norm": 0.06860917806625366,
      "learning_rate": 1.4625e-05,
      "loss": 0.002,
      "step": 84900
    },
    {
      "epoch": 5.660666666666667,
      "grad_norm": 0.04488081485033035,
      "learning_rate": 1.4620833333333334e-05,
      "loss": 0.0022,
      "step": 84910
    },
    {
      "epoch": 5.661333333333333,
      "grad_norm": 0.07899723201990128,
      "learning_rate": 1.4616666666666667e-05,
      "loss": 0.0015,
      "step": 84920
    },
    {
      "epoch": 5.662,
      "grad_norm": 0.22646643221378326,
      "learning_rate": 1.4612500000000001e-05,
      "loss": 0.0017,
      "step": 84930
    },
    {
      "epoch": 5.6626666666666665,
      "grad_norm": 0.2819618284702301,
      "learning_rate": 1.4608333333333335e-05,
      "loss": 0.0027,
      "step": 84940
    },
    {
      "epoch": 5.663333333333333,
      "grad_norm": 0.4074126183986664,
      "learning_rate": 1.4604166666666666e-05,
      "loss": 0.0024,
      "step": 84950
    },
    {
      "epoch": 5.664,
      "grad_norm": 0.10965146869421005,
      "learning_rate": 1.4599999999999999e-05,
      "loss": 0.0028,
      "step": 84960
    },
    {
      "epoch": 5.664666666666666,
      "grad_norm": 0.40791136026382446,
      "learning_rate": 1.4595833333333333e-05,
      "loss": 0.0013,
      "step": 84970
    },
    {
      "epoch": 5.665333333333333,
      "grad_norm": 0.29309985041618347,
      "learning_rate": 1.4591666666666668e-05,
      "loss": 0.0016,
      "step": 84980
    },
    {
      "epoch": 5.666,
      "grad_norm": 0.17083701491355896,
      "learning_rate": 1.45875e-05,
      "loss": 0.0017,
      "step": 84990
    },
    {
      "epoch": 5.666666666666667,
      "grad_norm": 0.13882790505886078,
      "learning_rate": 1.4583333333333335e-05,
      "loss": 0.0017,
      "step": 85000
    },
    {
      "epoch": 5.667333333333334,
      "grad_norm": 0.2153911590576172,
      "learning_rate": 1.457916666666667e-05,
      "loss": 0.0014,
      "step": 85010
    },
    {
      "epoch": 5.668,
      "grad_norm": 0.08689246326684952,
      "learning_rate": 1.4575e-05,
      "loss": 0.0021,
      "step": 85020
    },
    {
      "epoch": 5.668666666666667,
      "grad_norm": 0.2755683660507202,
      "learning_rate": 1.4570833333333333e-05,
      "loss": 0.002,
      "step": 85030
    },
    {
      "epoch": 5.669333333333333,
      "grad_norm": 0.07438047975301743,
      "learning_rate": 1.4566666666666667e-05,
      "loss": 0.0021,
      "step": 85040
    },
    {
      "epoch": 5.67,
      "grad_norm": 0.30945727229118347,
      "learning_rate": 1.4562500000000002e-05,
      "loss": 0.0016,
      "step": 85050
    },
    {
      "epoch": 5.6706666666666665,
      "grad_norm": 0.5642204880714417,
      "learning_rate": 1.4558333333333334e-05,
      "loss": 0.0019,
      "step": 85060
    },
    {
      "epoch": 5.671333333333333,
      "grad_norm": 0.1077655777335167,
      "learning_rate": 1.4554166666666669e-05,
      "loss": 0.0021,
      "step": 85070
    },
    {
      "epoch": 5.672,
      "grad_norm": 0.6807517409324646,
      "learning_rate": 1.455e-05,
      "loss": 0.0023,
      "step": 85080
    },
    {
      "epoch": 5.672666666666666,
      "grad_norm": 0.5556495785713196,
      "learning_rate": 1.4545833333333334e-05,
      "loss": 0.0026,
      "step": 85090
    },
    {
      "epoch": 5.673333333333334,
      "grad_norm": 0.039893556386232376,
      "learning_rate": 1.4541666666666667e-05,
      "loss": 0.0014,
      "step": 85100
    },
    {
      "epoch": 5.674,
      "grad_norm": 0.05501887947320938,
      "learning_rate": 1.4537500000000001e-05,
      "loss": 0.0022,
      "step": 85110
    },
    {
      "epoch": 5.674666666666667,
      "grad_norm": 0.2701171934604645,
      "learning_rate": 1.4533333333333335e-05,
      "loss": 0.0028,
      "step": 85120
    },
    {
      "epoch": 5.675333333333334,
      "grad_norm": 0.1079888865351677,
      "learning_rate": 1.4529166666666668e-05,
      "loss": 0.0024,
      "step": 85130
    },
    {
      "epoch": 5.676,
      "grad_norm": 0.3520878553390503,
      "learning_rate": 1.4524999999999999e-05,
      "loss": 0.002,
      "step": 85140
    },
    {
      "epoch": 5.676666666666667,
      "grad_norm": 0.210014209151268,
      "learning_rate": 1.4520833333333333e-05,
      "loss": 0.0015,
      "step": 85150
    },
    {
      "epoch": 5.677333333333333,
      "grad_norm": 0.10510099679231644,
      "learning_rate": 1.4516666666666668e-05,
      "loss": 0.0037,
      "step": 85160
    },
    {
      "epoch": 5.678,
      "grad_norm": 0.30835604667663574,
      "learning_rate": 1.45125e-05,
      "loss": 0.0024,
      "step": 85170
    },
    {
      "epoch": 5.6786666666666665,
      "grad_norm": 0.07668288797140121,
      "learning_rate": 1.4508333333333335e-05,
      "loss": 0.0019,
      "step": 85180
    },
    {
      "epoch": 5.679333333333333,
      "grad_norm": 0.0422184132039547,
      "learning_rate": 1.4504166666666669e-05,
      "loss": 0.0021,
      "step": 85190
    },
    {
      "epoch": 5.68,
      "grad_norm": 0.28313148021698,
      "learning_rate": 1.45e-05,
      "loss": 0.0021,
      "step": 85200
    },
    {
      "epoch": 5.680666666666666,
      "grad_norm": 0.15591049194335938,
      "learning_rate": 1.4495833333333333e-05,
      "loss": 0.0018,
      "step": 85210
    },
    {
      "epoch": 5.681333333333333,
      "grad_norm": 0.04693407192826271,
      "learning_rate": 1.4491666666666667e-05,
      "loss": 0.0023,
      "step": 85220
    },
    {
      "epoch": 5.682,
      "grad_norm": 0.20935994386672974,
      "learning_rate": 1.4487500000000001e-05,
      "loss": 0.0019,
      "step": 85230
    },
    {
      "epoch": 5.682666666666667,
      "grad_norm": 0.5399898290634155,
      "learning_rate": 1.4483333333333334e-05,
      "loss": 0.0017,
      "step": 85240
    },
    {
      "epoch": 5.683333333333334,
      "grad_norm": 0.0461135059595108,
      "learning_rate": 1.4479166666666669e-05,
      "loss": 0.002,
      "step": 85250
    },
    {
      "epoch": 5.684,
      "grad_norm": 0.3520601987838745,
      "learning_rate": 1.4475e-05,
      "loss": 0.0023,
      "step": 85260
    },
    {
      "epoch": 5.684666666666667,
      "grad_norm": 0.04351331293582916,
      "learning_rate": 1.4470833333333334e-05,
      "loss": 0.0016,
      "step": 85270
    },
    {
      "epoch": 5.685333333333333,
      "grad_norm": 0.24052561819553375,
      "learning_rate": 1.4466666666666667e-05,
      "loss": 0.0018,
      "step": 85280
    },
    {
      "epoch": 5.686,
      "grad_norm": 0.06967418640851974,
      "learning_rate": 1.4462500000000001e-05,
      "loss": 0.002,
      "step": 85290
    },
    {
      "epoch": 5.6866666666666665,
      "grad_norm": 0.2598416209220886,
      "learning_rate": 1.4458333333333335e-05,
      "loss": 0.0012,
      "step": 85300
    },
    {
      "epoch": 5.687333333333333,
      "grad_norm": 0.38589993119239807,
      "learning_rate": 1.4454166666666668e-05,
      "loss": 0.0026,
      "step": 85310
    },
    {
      "epoch": 5.688,
      "grad_norm": 0.11042419821023941,
      "learning_rate": 1.4449999999999999e-05,
      "loss": 0.0015,
      "step": 85320
    },
    {
      "epoch": 5.688666666666666,
      "grad_norm": 0.08618335425853729,
      "learning_rate": 1.4445833333333333e-05,
      "loss": 0.002,
      "step": 85330
    },
    {
      "epoch": 5.689333333333334,
      "grad_norm": 0.23778103291988373,
      "learning_rate": 1.4441666666666668e-05,
      "loss": 0.0023,
      "step": 85340
    },
    {
      "epoch": 5.6899999999999995,
      "grad_norm": 0.05856788903474808,
      "learning_rate": 1.44375e-05,
      "loss": 0.002,
      "step": 85350
    },
    {
      "epoch": 5.690666666666667,
      "grad_norm": 0.21515297889709473,
      "learning_rate": 1.4433333333333335e-05,
      "loss": 0.0026,
      "step": 85360
    },
    {
      "epoch": 5.691333333333334,
      "grad_norm": 0.4118381440639496,
      "learning_rate": 1.4429166666666669e-05,
      "loss": 0.0029,
      "step": 85370
    },
    {
      "epoch": 5.692,
      "grad_norm": 0.3759279251098633,
      "learning_rate": 1.4425e-05,
      "loss": 0.002,
      "step": 85380
    },
    {
      "epoch": 5.692666666666667,
      "grad_norm": 0.2801523506641388,
      "learning_rate": 1.4420833333333333e-05,
      "loss": 0.0027,
      "step": 85390
    },
    {
      "epoch": 5.693333333333333,
      "grad_norm": 0.200674369931221,
      "learning_rate": 1.4416666666666667e-05,
      "loss": 0.0019,
      "step": 85400
    },
    {
      "epoch": 5.694,
      "grad_norm": 0.07277281582355499,
      "learning_rate": 1.4412500000000001e-05,
      "loss": 0.0017,
      "step": 85410
    },
    {
      "epoch": 5.6946666666666665,
      "grad_norm": 0.13829520344734192,
      "learning_rate": 1.4408333333333334e-05,
      "loss": 0.0014,
      "step": 85420
    },
    {
      "epoch": 5.695333333333333,
      "grad_norm": 0.33909448981285095,
      "learning_rate": 1.4404166666666668e-05,
      "loss": 0.0015,
      "step": 85430
    },
    {
      "epoch": 5.696,
      "grad_norm": 0.06580189615488052,
      "learning_rate": 1.44e-05,
      "loss": 0.0022,
      "step": 85440
    },
    {
      "epoch": 5.696666666666666,
      "grad_norm": 0.4127056300640106,
      "learning_rate": 1.4395833333333334e-05,
      "loss": 0.0017,
      "step": 85450
    },
    {
      "epoch": 5.697333333333333,
      "grad_norm": 0.35606831312179565,
      "learning_rate": 1.4391666666666666e-05,
      "loss": 0.0016,
      "step": 85460
    },
    {
      "epoch": 5.698,
      "grad_norm": 0.2666618824005127,
      "learning_rate": 1.43875e-05,
      "loss": 0.0018,
      "step": 85470
    },
    {
      "epoch": 5.698666666666667,
      "grad_norm": 0.4511454999446869,
      "learning_rate": 1.4383333333333335e-05,
      "loss": 0.0019,
      "step": 85480
    },
    {
      "epoch": 5.699333333333334,
      "grad_norm": 0.048848215490579605,
      "learning_rate": 1.4379166666666668e-05,
      "loss": 0.0016,
      "step": 85490
    },
    {
      "epoch": 5.7,
      "grad_norm": 0.1732434630393982,
      "learning_rate": 1.4374999999999999e-05,
      "loss": 0.0022,
      "step": 85500
    },
    {
      "epoch": 5.700666666666667,
      "grad_norm": 0.09685385972261429,
      "learning_rate": 1.4370833333333333e-05,
      "loss": 0.0025,
      "step": 85510
    },
    {
      "epoch": 5.701333333333333,
      "grad_norm": 0.4820929765701294,
      "learning_rate": 1.4366666666666667e-05,
      "loss": 0.0022,
      "step": 85520
    },
    {
      "epoch": 5.702,
      "grad_norm": 0.23395122587680817,
      "learning_rate": 1.43625e-05,
      "loss": 0.0016,
      "step": 85530
    },
    {
      "epoch": 5.7026666666666666,
      "grad_norm": 0.06144257262349129,
      "learning_rate": 1.4358333333333334e-05,
      "loss": 0.002,
      "step": 85540
    },
    {
      "epoch": 5.703333333333333,
      "grad_norm": 0.3045981228351593,
      "learning_rate": 1.4354166666666669e-05,
      "loss": 0.002,
      "step": 85550
    },
    {
      "epoch": 5.704,
      "grad_norm": 0.15072812139987946,
      "learning_rate": 1.435e-05,
      "loss": 0.0027,
      "step": 85560
    },
    {
      "epoch": 5.704666666666666,
      "grad_norm": 0.23693442344665527,
      "learning_rate": 1.4345833333333334e-05,
      "loss": 0.002,
      "step": 85570
    },
    {
      "epoch": 5.705333333333334,
      "grad_norm": 0.24057315289974213,
      "learning_rate": 1.4341666666666667e-05,
      "loss": 0.0021,
      "step": 85580
    },
    {
      "epoch": 5.7059999999999995,
      "grad_norm": 0.3545372784137726,
      "learning_rate": 1.4337500000000001e-05,
      "loss": 0.0023,
      "step": 85590
    },
    {
      "epoch": 5.706666666666667,
      "grad_norm": 0.7718107104301453,
      "learning_rate": 1.4333333333333334e-05,
      "loss": 0.0013,
      "step": 85600
    },
    {
      "epoch": 5.707333333333334,
      "grad_norm": 0.4196312129497528,
      "learning_rate": 1.4329166666666668e-05,
      "loss": 0.0016,
      "step": 85610
    },
    {
      "epoch": 5.708,
      "grad_norm": 0.17056956887245178,
      "learning_rate": 1.4325e-05,
      "loss": 0.0022,
      "step": 85620
    },
    {
      "epoch": 5.708666666666667,
      "grad_norm": 0.5391713380813599,
      "learning_rate": 1.4320833333333334e-05,
      "loss": 0.0021,
      "step": 85630
    },
    {
      "epoch": 5.709333333333333,
      "grad_norm": 0.5775952339172363,
      "learning_rate": 1.4316666666666668e-05,
      "loss": 0.0024,
      "step": 85640
    },
    {
      "epoch": 5.71,
      "grad_norm": 0.27346909046173096,
      "learning_rate": 1.43125e-05,
      "loss": 0.0032,
      "step": 85650
    },
    {
      "epoch": 5.710666666666667,
      "grad_norm": 0.4129181206226349,
      "learning_rate": 1.4308333333333335e-05,
      "loss": 0.0015,
      "step": 85660
    },
    {
      "epoch": 5.711333333333333,
      "grad_norm": 0.3116540312767029,
      "learning_rate": 1.430416666666667e-05,
      "loss": 0.0015,
      "step": 85670
    },
    {
      "epoch": 5.712,
      "grad_norm": 0.14566317200660706,
      "learning_rate": 1.43e-05,
      "loss": 0.0024,
      "step": 85680
    },
    {
      "epoch": 5.712666666666666,
      "grad_norm": 0.04483149200677872,
      "learning_rate": 1.4295833333333333e-05,
      "loss": 0.0018,
      "step": 85690
    },
    {
      "epoch": 5.713333333333333,
      "grad_norm": 0.09835365414619446,
      "learning_rate": 1.4291666666666667e-05,
      "loss": 0.0028,
      "step": 85700
    },
    {
      "epoch": 5.714,
      "grad_norm": 0.3832516074180603,
      "learning_rate": 1.4287500000000002e-05,
      "loss": 0.0016,
      "step": 85710
    },
    {
      "epoch": 5.714666666666667,
      "grad_norm": 0.21645672619342804,
      "learning_rate": 1.4283333333333334e-05,
      "loss": 0.0021,
      "step": 85720
    },
    {
      "epoch": 5.715333333333334,
      "grad_norm": 0.0803891271352768,
      "learning_rate": 1.4279166666666669e-05,
      "loss": 0.0014,
      "step": 85730
    },
    {
      "epoch": 5.716,
      "grad_norm": 0.1390622854232788,
      "learning_rate": 1.4275e-05,
      "loss": 0.0013,
      "step": 85740
    },
    {
      "epoch": 5.716666666666667,
      "grad_norm": 0.06519732624292374,
      "learning_rate": 1.4270833333333334e-05,
      "loss": 0.0026,
      "step": 85750
    },
    {
      "epoch": 5.717333333333333,
      "grad_norm": 0.21188968420028687,
      "learning_rate": 1.4266666666666667e-05,
      "loss": 0.003,
      "step": 85760
    },
    {
      "epoch": 5.718,
      "grad_norm": 0.32405373454093933,
      "learning_rate": 1.4262500000000001e-05,
      "loss": 0.0017,
      "step": 85770
    },
    {
      "epoch": 5.718666666666667,
      "grad_norm": 0.5463004112243652,
      "learning_rate": 1.4258333333333335e-05,
      "loss": 0.0025,
      "step": 85780
    },
    {
      "epoch": 5.719333333333333,
      "grad_norm": 0.4769326150417328,
      "learning_rate": 1.4254166666666668e-05,
      "loss": 0.0014,
      "step": 85790
    },
    {
      "epoch": 5.72,
      "grad_norm": 0.167159304022789,
      "learning_rate": 1.4249999999999999e-05,
      "loss": 0.0018,
      "step": 85800
    },
    {
      "epoch": 5.720666666666666,
      "grad_norm": 0.06864116340875626,
      "learning_rate": 1.4245833333333333e-05,
      "loss": 0.0017,
      "step": 85810
    },
    {
      "epoch": 5.721333333333334,
      "grad_norm": 0.6251717805862427,
      "learning_rate": 1.4241666666666668e-05,
      "loss": 0.0022,
      "step": 85820
    },
    {
      "epoch": 5.7219999999999995,
      "grad_norm": 0.0730554386973381,
      "learning_rate": 1.42375e-05,
      "loss": 0.0018,
      "step": 85830
    },
    {
      "epoch": 5.722666666666667,
      "grad_norm": 0.07639788091182709,
      "learning_rate": 1.4233333333333335e-05,
      "loss": 0.002,
      "step": 85840
    },
    {
      "epoch": 5.723333333333334,
      "grad_norm": 0.03949345648288727,
      "learning_rate": 1.422916666666667e-05,
      "loss": 0.0013,
      "step": 85850
    },
    {
      "epoch": 5.724,
      "grad_norm": 0.3413238525390625,
      "learning_rate": 1.4225e-05,
      "loss": 0.0022,
      "step": 85860
    },
    {
      "epoch": 5.724666666666667,
      "grad_norm": 0.7344934940338135,
      "learning_rate": 1.4220833333333333e-05,
      "loss": 0.0026,
      "step": 85870
    },
    {
      "epoch": 5.725333333333333,
      "grad_norm": 0.3253081142902374,
      "learning_rate": 1.4216666666666667e-05,
      "loss": 0.0018,
      "step": 85880
    },
    {
      "epoch": 5.726,
      "grad_norm": 0.1430124044418335,
      "learning_rate": 1.4212500000000002e-05,
      "loss": 0.0027,
      "step": 85890
    },
    {
      "epoch": 5.726666666666667,
      "grad_norm": 0.20855750143527985,
      "learning_rate": 1.4208333333333334e-05,
      "loss": 0.0027,
      "step": 85900
    },
    {
      "epoch": 5.727333333333333,
      "grad_norm": 0.0763583704829216,
      "learning_rate": 1.4204166666666669e-05,
      "loss": 0.0016,
      "step": 85910
    },
    {
      "epoch": 5.728,
      "grad_norm": 0.34104517102241516,
      "learning_rate": 1.42e-05,
      "loss": 0.0015,
      "step": 85920
    },
    {
      "epoch": 5.728666666666666,
      "grad_norm": 0.21093341708183289,
      "learning_rate": 1.4195833333333334e-05,
      "loss": 0.0025,
      "step": 85930
    },
    {
      "epoch": 5.729333333333333,
      "grad_norm": 0.1364482194185257,
      "learning_rate": 1.4191666666666667e-05,
      "loss": 0.002,
      "step": 85940
    },
    {
      "epoch": 5.73,
      "grad_norm": 0.13676278293132782,
      "learning_rate": 1.4187500000000001e-05,
      "loss": 0.0029,
      "step": 85950
    },
    {
      "epoch": 5.730666666666667,
      "grad_norm": 0.2646426856517792,
      "learning_rate": 1.4183333333333335e-05,
      "loss": 0.0019,
      "step": 85960
    },
    {
      "epoch": 5.731333333333334,
      "grad_norm": 0.07616075873374939,
      "learning_rate": 1.4179166666666668e-05,
      "loss": 0.0021,
      "step": 85970
    },
    {
      "epoch": 5.732,
      "grad_norm": 0.02826487272977829,
      "learning_rate": 1.4174999999999999e-05,
      "loss": 0.0013,
      "step": 85980
    },
    {
      "epoch": 5.732666666666667,
      "grad_norm": 0.20682303607463837,
      "learning_rate": 1.4170833333333333e-05,
      "loss": 0.0016,
      "step": 85990
    },
    {
      "epoch": 5.733333333333333,
      "grad_norm": 0.10083578526973724,
      "learning_rate": 1.4166666666666668e-05,
      "loss": 0.0016,
      "step": 86000
    },
    {
      "epoch": 5.734,
      "grad_norm": 0.15567931532859802,
      "learning_rate": 1.41625e-05,
      "loss": 0.0016,
      "step": 86010
    },
    {
      "epoch": 5.734666666666667,
      "grad_norm": 0.11772417277097702,
      "learning_rate": 1.4158333333333335e-05,
      "loss": 0.0013,
      "step": 86020
    },
    {
      "epoch": 5.735333333333333,
      "grad_norm": 0.05156628414988518,
      "learning_rate": 1.4154166666666669e-05,
      "loss": 0.0021,
      "step": 86030
    },
    {
      "epoch": 5.736,
      "grad_norm": 0.33567020297050476,
      "learning_rate": 1.415e-05,
      "loss": 0.0022,
      "step": 86040
    },
    {
      "epoch": 5.736666666666666,
      "grad_norm": 0.6962388157844543,
      "learning_rate": 1.4145833333333333e-05,
      "loss": 0.0019,
      "step": 86050
    },
    {
      "epoch": 5.737333333333333,
      "grad_norm": 0.21472008526325226,
      "learning_rate": 1.4141666666666667e-05,
      "loss": 0.0024,
      "step": 86060
    },
    {
      "epoch": 5.7379999999999995,
      "grad_norm": 0.11386308819055557,
      "learning_rate": 1.4137500000000001e-05,
      "loss": 0.0012,
      "step": 86070
    },
    {
      "epoch": 5.738666666666667,
      "grad_norm": 0.08420353382825851,
      "learning_rate": 1.4133333333333334e-05,
      "loss": 0.0013,
      "step": 86080
    },
    {
      "epoch": 5.739333333333334,
      "grad_norm": 0.1531827449798584,
      "learning_rate": 1.4129166666666668e-05,
      "loss": 0.002,
      "step": 86090
    },
    {
      "epoch": 5.74,
      "grad_norm": 0.4097609519958496,
      "learning_rate": 1.4125e-05,
      "loss": 0.002,
      "step": 86100
    },
    {
      "epoch": 5.740666666666667,
      "grad_norm": 0.40389955043792725,
      "learning_rate": 1.4120833333333334e-05,
      "loss": 0.0022,
      "step": 86110
    },
    {
      "epoch": 5.741333333333333,
      "grad_norm": 0.30474114418029785,
      "learning_rate": 1.4116666666666666e-05,
      "loss": 0.002,
      "step": 86120
    },
    {
      "epoch": 5.742,
      "grad_norm": 0.0637466236948967,
      "learning_rate": 1.41125e-05,
      "loss": 0.0016,
      "step": 86130
    },
    {
      "epoch": 5.742666666666667,
      "grad_norm": 0.05559307709336281,
      "learning_rate": 1.4108333333333335e-05,
      "loss": 0.0016,
      "step": 86140
    },
    {
      "epoch": 5.743333333333333,
      "grad_norm": 0.525663435459137,
      "learning_rate": 1.4104166666666668e-05,
      "loss": 0.0025,
      "step": 86150
    },
    {
      "epoch": 5.744,
      "grad_norm": 0.056327275931835175,
      "learning_rate": 1.4099999999999999e-05,
      "loss": 0.0021,
      "step": 86160
    },
    {
      "epoch": 5.744666666666666,
      "grad_norm": 0.24337975680828094,
      "learning_rate": 1.4095833333333333e-05,
      "loss": 0.002,
      "step": 86170
    },
    {
      "epoch": 5.745333333333333,
      "grad_norm": 0.03228546306490898,
      "learning_rate": 1.4091666666666668e-05,
      "loss": 0.0014,
      "step": 86180
    },
    {
      "epoch": 5.746,
      "grad_norm": 0.27686554193496704,
      "learning_rate": 1.40875e-05,
      "loss": 0.0014,
      "step": 86190
    },
    {
      "epoch": 5.746666666666667,
      "grad_norm": 0.08567416667938232,
      "learning_rate": 1.4083333333333335e-05,
      "loss": 0.0031,
      "step": 86200
    },
    {
      "epoch": 5.747333333333334,
      "grad_norm": 0.3805684745311737,
      "learning_rate": 1.4079166666666669e-05,
      "loss": 0.0013,
      "step": 86210
    },
    {
      "epoch": 5.748,
      "grad_norm": 0.4835978150367737,
      "learning_rate": 1.4075e-05,
      "loss": 0.0018,
      "step": 86220
    },
    {
      "epoch": 5.748666666666667,
      "grad_norm": 0.5078070759773254,
      "learning_rate": 1.4070833333333333e-05,
      "loss": 0.0021,
      "step": 86230
    },
    {
      "epoch": 5.749333333333333,
      "grad_norm": 0.2377399057149887,
      "learning_rate": 1.4066666666666667e-05,
      "loss": 0.0022,
      "step": 86240
    },
    {
      "epoch": 5.75,
      "grad_norm": 0.2361159771680832,
      "learning_rate": 1.4062500000000001e-05,
      "loss": 0.0011,
      "step": 86250
    },
    {
      "epoch": 5.750666666666667,
      "grad_norm": 0.24187573790550232,
      "learning_rate": 1.4058333333333334e-05,
      "loss": 0.0014,
      "step": 86260
    },
    {
      "epoch": 5.751333333333333,
      "grad_norm": 0.11024639010429382,
      "learning_rate": 1.4054166666666668e-05,
      "loss": 0.002,
      "step": 86270
    },
    {
      "epoch": 5.752,
      "grad_norm": 0.5963655114173889,
      "learning_rate": 1.4050000000000003e-05,
      "loss": 0.0018,
      "step": 86280
    },
    {
      "epoch": 5.752666666666666,
      "grad_norm": 0.09171538800001144,
      "learning_rate": 1.4045833333333334e-05,
      "loss": 0.0019,
      "step": 86290
    },
    {
      "epoch": 5.753333333333333,
      "grad_norm": 0.13855166733264923,
      "learning_rate": 1.4041666666666666e-05,
      "loss": 0.002,
      "step": 86300
    },
    {
      "epoch": 5.754,
      "grad_norm": 0.05874788016080856,
      "learning_rate": 1.40375e-05,
      "loss": 0.002,
      "step": 86310
    },
    {
      "epoch": 5.754666666666667,
      "grad_norm": 0.37661176919937134,
      "learning_rate": 1.4033333333333335e-05,
      "loss": 0.0022,
      "step": 86320
    },
    {
      "epoch": 5.755333333333334,
      "grad_norm": 0.43993258476257324,
      "learning_rate": 1.4029166666666668e-05,
      "loss": 0.0014,
      "step": 86330
    },
    {
      "epoch": 5.756,
      "grad_norm": 0.13764502108097076,
      "learning_rate": 1.4025000000000002e-05,
      "loss": 0.0021,
      "step": 86340
    },
    {
      "epoch": 5.756666666666667,
      "grad_norm": 0.2792370617389679,
      "learning_rate": 1.4020833333333333e-05,
      "loss": 0.003,
      "step": 86350
    },
    {
      "epoch": 5.757333333333333,
      "grad_norm": 0.1801435351371765,
      "learning_rate": 1.4016666666666667e-05,
      "loss": 0.0016,
      "step": 86360
    },
    {
      "epoch": 5.758,
      "grad_norm": 0.6262296438217163,
      "learning_rate": 1.40125e-05,
      "loss": 0.0016,
      "step": 86370
    },
    {
      "epoch": 5.758666666666667,
      "grad_norm": 0.2042541801929474,
      "learning_rate": 1.4008333333333334e-05,
      "loss": 0.0016,
      "step": 86380
    },
    {
      "epoch": 5.759333333333333,
      "grad_norm": 0.33914849162101746,
      "learning_rate": 1.4004166666666669e-05,
      "loss": 0.0016,
      "step": 86390
    },
    {
      "epoch": 5.76,
      "grad_norm": 0.3474774956703186,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.0024,
      "step": 86400
    },
    {
      "epoch": 5.760666666666666,
      "grad_norm": 0.5895142555236816,
      "learning_rate": 1.3995833333333332e-05,
      "loss": 0.0025,
      "step": 86410
    },
    {
      "epoch": 5.761333333333333,
      "grad_norm": 0.2759329378604889,
      "learning_rate": 1.3991666666666667e-05,
      "loss": 0.0016,
      "step": 86420
    },
    {
      "epoch": 5.7620000000000005,
      "grad_norm": 0.09364665299654007,
      "learning_rate": 1.3987500000000001e-05,
      "loss": 0.0025,
      "step": 86430
    },
    {
      "epoch": 5.762666666666667,
      "grad_norm": 0.17235125601291656,
      "learning_rate": 1.3983333333333334e-05,
      "loss": 0.0019,
      "step": 86440
    },
    {
      "epoch": 5.763333333333334,
      "grad_norm": 0.339530348777771,
      "learning_rate": 1.3979166666666668e-05,
      "loss": 0.0018,
      "step": 86450
    },
    {
      "epoch": 5.764,
      "grad_norm": 0.06701228022575378,
      "learning_rate": 1.3975000000000003e-05,
      "loss": 0.0014,
      "step": 86460
    },
    {
      "epoch": 5.764666666666667,
      "grad_norm": 0.6805251240730286,
      "learning_rate": 1.3970833333333334e-05,
      "loss": 0.0018,
      "step": 86470
    },
    {
      "epoch": 5.765333333333333,
      "grad_norm": 0.3113292157649994,
      "learning_rate": 1.3966666666666666e-05,
      "loss": 0.0012,
      "step": 86480
    },
    {
      "epoch": 5.766,
      "grad_norm": 0.02636132761836052,
      "learning_rate": 1.39625e-05,
      "loss": 0.002,
      "step": 86490
    },
    {
      "epoch": 5.766666666666667,
      "grad_norm": 0.515796959400177,
      "learning_rate": 1.3958333333333335e-05,
      "loss": 0.002,
      "step": 86500
    },
    {
      "epoch": 5.767333333333333,
      "grad_norm": 0.5079098343849182,
      "learning_rate": 1.3954166666666668e-05,
      "loss": 0.002,
      "step": 86510
    },
    {
      "epoch": 5.768,
      "grad_norm": 0.08305907994508743,
      "learning_rate": 1.3950000000000002e-05,
      "loss": 0.0021,
      "step": 86520
    },
    {
      "epoch": 5.768666666666666,
      "grad_norm": 0.8618624210357666,
      "learning_rate": 1.3945833333333333e-05,
      "loss": 0.0017,
      "step": 86530
    },
    {
      "epoch": 5.769333333333333,
      "grad_norm": 0.07408846914768219,
      "learning_rate": 1.3941666666666667e-05,
      "loss": 0.0027,
      "step": 86540
    },
    {
      "epoch": 5.77,
      "grad_norm": 0.07153158634901047,
      "learning_rate": 1.39375e-05,
      "loss": 0.0017,
      "step": 86550
    },
    {
      "epoch": 5.770666666666667,
      "grad_norm": 0.0702708512544632,
      "learning_rate": 1.3933333333333334e-05,
      "loss": 0.0017,
      "step": 86560
    },
    {
      "epoch": 5.771333333333334,
      "grad_norm": 0.5789734721183777,
      "learning_rate": 1.3929166666666669e-05,
      "loss": 0.0015,
      "step": 86570
    },
    {
      "epoch": 5.772,
      "grad_norm": 0.4416840672492981,
      "learning_rate": 1.3925000000000001e-05,
      "loss": 0.002,
      "step": 86580
    },
    {
      "epoch": 5.772666666666667,
      "grad_norm": 0.11158106476068497,
      "learning_rate": 1.3920833333333332e-05,
      "loss": 0.0022,
      "step": 86590
    },
    {
      "epoch": 5.773333333333333,
      "grad_norm": 0.2050747126340866,
      "learning_rate": 1.3916666666666667e-05,
      "loss": 0.0022,
      "step": 86600
    },
    {
      "epoch": 5.774,
      "grad_norm": 0.055681902915239334,
      "learning_rate": 1.3912500000000001e-05,
      "loss": 0.0024,
      "step": 86610
    },
    {
      "epoch": 5.774666666666667,
      "grad_norm": 0.4119530916213989,
      "learning_rate": 1.3908333333333334e-05,
      "loss": 0.0019,
      "step": 86620
    },
    {
      "epoch": 5.775333333333333,
      "grad_norm": 0.5440561771392822,
      "learning_rate": 1.3904166666666668e-05,
      "loss": 0.0018,
      "step": 86630
    },
    {
      "epoch": 5.776,
      "grad_norm": 0.20619121193885803,
      "learning_rate": 1.3900000000000002e-05,
      "loss": 0.0021,
      "step": 86640
    },
    {
      "epoch": 5.776666666666666,
      "grad_norm": 0.2699473798274994,
      "learning_rate": 1.3895833333333333e-05,
      "loss": 0.0017,
      "step": 86650
    },
    {
      "epoch": 5.777333333333333,
      "grad_norm": 0.2525327801704407,
      "learning_rate": 1.3891666666666666e-05,
      "loss": 0.0016,
      "step": 86660
    },
    {
      "epoch": 5.7780000000000005,
      "grad_norm": 0.3117557466030121,
      "learning_rate": 1.38875e-05,
      "loss": 0.0016,
      "step": 86670
    },
    {
      "epoch": 5.778666666666666,
      "grad_norm": 0.11986597627401352,
      "learning_rate": 1.3883333333333335e-05,
      "loss": 0.0016,
      "step": 86680
    },
    {
      "epoch": 5.779333333333334,
      "grad_norm": 0.08482879400253296,
      "learning_rate": 1.3879166666666667e-05,
      "loss": 0.0019,
      "step": 86690
    },
    {
      "epoch": 5.78,
      "grad_norm": 0.5138558745384216,
      "learning_rate": 1.3875000000000002e-05,
      "loss": 0.0019,
      "step": 86700
    },
    {
      "epoch": 5.780666666666667,
      "grad_norm": 0.2729167342185974,
      "learning_rate": 1.3870833333333333e-05,
      "loss": 0.0021,
      "step": 86710
    },
    {
      "epoch": 5.781333333333333,
      "grad_norm": 0.23875468969345093,
      "learning_rate": 1.3866666666666667e-05,
      "loss": 0.0013,
      "step": 86720
    },
    {
      "epoch": 5.782,
      "grad_norm": 0.03699207305908203,
      "learning_rate": 1.38625e-05,
      "loss": 0.0028,
      "step": 86730
    },
    {
      "epoch": 5.782666666666667,
      "grad_norm": 0.05648528411984444,
      "learning_rate": 1.3858333333333334e-05,
      "loss": 0.0012,
      "step": 86740
    },
    {
      "epoch": 5.783333333333333,
      "grad_norm": 0.4745546281337738,
      "learning_rate": 1.3854166666666669e-05,
      "loss": 0.0015,
      "step": 86750
    },
    {
      "epoch": 5.784,
      "grad_norm": 0.5392516255378723,
      "learning_rate": 1.3850000000000001e-05,
      "loss": 0.0017,
      "step": 86760
    },
    {
      "epoch": 5.784666666666666,
      "grad_norm": 0.531782329082489,
      "learning_rate": 1.3845833333333332e-05,
      "loss": 0.0017,
      "step": 86770
    },
    {
      "epoch": 5.785333333333333,
      "grad_norm": 0.5505728125572205,
      "learning_rate": 1.3841666666666667e-05,
      "loss": 0.0017,
      "step": 86780
    },
    {
      "epoch": 5.786,
      "grad_norm": 0.17039871215820312,
      "learning_rate": 1.3837500000000001e-05,
      "loss": 0.0021,
      "step": 86790
    },
    {
      "epoch": 5.786666666666667,
      "grad_norm": 0.41808125376701355,
      "learning_rate": 1.3833333333333334e-05,
      "loss": 0.0022,
      "step": 86800
    },
    {
      "epoch": 5.787333333333334,
      "grad_norm": 0.13559232652187347,
      "learning_rate": 1.3829166666666668e-05,
      "loss": 0.0019,
      "step": 86810
    },
    {
      "epoch": 5.788,
      "grad_norm": 0.050666745752096176,
      "learning_rate": 1.3825000000000002e-05,
      "loss": 0.0019,
      "step": 86820
    },
    {
      "epoch": 5.788666666666667,
      "grad_norm": 0.07875154912471771,
      "learning_rate": 1.3820833333333333e-05,
      "loss": 0.0012,
      "step": 86830
    },
    {
      "epoch": 5.789333333333333,
      "grad_norm": 0.05329585820436478,
      "learning_rate": 1.3816666666666666e-05,
      "loss": 0.0019,
      "step": 86840
    },
    {
      "epoch": 5.79,
      "grad_norm": 0.583394467830658,
      "learning_rate": 1.38125e-05,
      "loss": 0.0017,
      "step": 86850
    },
    {
      "epoch": 5.790666666666667,
      "grad_norm": 0.02764073945581913,
      "learning_rate": 1.3808333333333335e-05,
      "loss": 0.002,
      "step": 86860
    },
    {
      "epoch": 5.791333333333333,
      "grad_norm": 0.11104726791381836,
      "learning_rate": 1.3804166666666667e-05,
      "loss": 0.0018,
      "step": 86870
    },
    {
      "epoch": 5.792,
      "grad_norm": 0.17755362391471863,
      "learning_rate": 1.3800000000000002e-05,
      "loss": 0.0028,
      "step": 86880
    },
    {
      "epoch": 5.792666666666666,
      "grad_norm": 0.025718335062265396,
      "learning_rate": 1.3795833333333333e-05,
      "loss": 0.0018,
      "step": 86890
    },
    {
      "epoch": 5.793333333333333,
      "grad_norm": 0.20747947692871094,
      "learning_rate": 1.3791666666666667e-05,
      "loss": 0.0016,
      "step": 86900
    },
    {
      "epoch": 5.7940000000000005,
      "grad_norm": 0.29213643074035645,
      "learning_rate": 1.37875e-05,
      "loss": 0.0018,
      "step": 86910
    },
    {
      "epoch": 5.794666666666666,
      "grad_norm": 0.14792901277542114,
      "learning_rate": 1.3783333333333334e-05,
      "loss": 0.0024,
      "step": 86920
    },
    {
      "epoch": 5.795333333333334,
      "grad_norm": 0.3455359637737274,
      "learning_rate": 1.3779166666666668e-05,
      "loss": 0.0019,
      "step": 86930
    },
    {
      "epoch": 5.796,
      "grad_norm": 0.2754732072353363,
      "learning_rate": 1.3775000000000001e-05,
      "loss": 0.0013,
      "step": 86940
    },
    {
      "epoch": 5.796666666666667,
      "grad_norm": 0.8009305000305176,
      "learning_rate": 1.3770833333333332e-05,
      "loss": 0.0017,
      "step": 86950
    },
    {
      "epoch": 5.7973333333333334,
      "grad_norm": 0.03526858240365982,
      "learning_rate": 1.3766666666666666e-05,
      "loss": 0.0017,
      "step": 86960
    },
    {
      "epoch": 5.798,
      "grad_norm": 0.2388768047094345,
      "learning_rate": 1.37625e-05,
      "loss": 0.0016,
      "step": 86970
    },
    {
      "epoch": 5.798666666666667,
      "grad_norm": 0.046434663236141205,
      "learning_rate": 1.3758333333333333e-05,
      "loss": 0.0028,
      "step": 86980
    },
    {
      "epoch": 5.799333333333333,
      "grad_norm": 0.17944727838039398,
      "learning_rate": 1.3754166666666668e-05,
      "loss": 0.0021,
      "step": 86990
    },
    {
      "epoch": 5.8,
      "grad_norm": 0.04267750307917595,
      "learning_rate": 1.3750000000000002e-05,
      "loss": 0.0027,
      "step": 87000
    },
    {
      "epoch": 5.800666666666666,
      "grad_norm": 0.07201620936393738,
      "learning_rate": 1.3745833333333333e-05,
      "loss": 0.0024,
      "step": 87010
    },
    {
      "epoch": 5.801333333333333,
      "grad_norm": 0.336734414100647,
      "learning_rate": 1.3741666666666666e-05,
      "loss": 0.0029,
      "step": 87020
    },
    {
      "epoch": 5.802,
      "grad_norm": 0.8154028654098511,
      "learning_rate": 1.37375e-05,
      "loss": 0.0015,
      "step": 87030
    },
    {
      "epoch": 5.802666666666667,
      "grad_norm": 0.3463931083679199,
      "learning_rate": 1.3733333333333335e-05,
      "loss": 0.0028,
      "step": 87040
    },
    {
      "epoch": 5.803333333333334,
      "grad_norm": 0.5420591235160828,
      "learning_rate": 1.3729166666666667e-05,
      "loss": 0.0015,
      "step": 87050
    },
    {
      "epoch": 5.804,
      "grad_norm": 0.40718573331832886,
      "learning_rate": 1.3725000000000002e-05,
      "loss": 0.0021,
      "step": 87060
    },
    {
      "epoch": 5.804666666666667,
      "grad_norm": 0.1802665740251541,
      "learning_rate": 1.3720833333333333e-05,
      "loss": 0.0014,
      "step": 87070
    },
    {
      "epoch": 5.8053333333333335,
      "grad_norm": 0.20614342391490936,
      "learning_rate": 1.3716666666666667e-05,
      "loss": 0.0014,
      "step": 87080
    },
    {
      "epoch": 5.806,
      "grad_norm": 0.10287762433290482,
      "learning_rate": 1.37125e-05,
      "loss": 0.0022,
      "step": 87090
    },
    {
      "epoch": 5.806666666666667,
      "grad_norm": 0.6041214466094971,
      "learning_rate": 1.3708333333333334e-05,
      "loss": 0.0014,
      "step": 87100
    },
    {
      "epoch": 5.807333333333333,
      "grad_norm": 0.3100699186325073,
      "learning_rate": 1.3704166666666668e-05,
      "loss": 0.0017,
      "step": 87110
    },
    {
      "epoch": 5.808,
      "grad_norm": 0.4259633421897888,
      "learning_rate": 1.3700000000000001e-05,
      "loss": 0.0021,
      "step": 87120
    },
    {
      "epoch": 5.808666666666666,
      "grad_norm": 0.045406173914670944,
      "learning_rate": 1.3695833333333332e-05,
      "loss": 0.0026,
      "step": 87130
    },
    {
      "epoch": 5.809333333333333,
      "grad_norm": 0.10857506841421127,
      "learning_rate": 1.3691666666666666e-05,
      "loss": 0.0014,
      "step": 87140
    },
    {
      "epoch": 5.8100000000000005,
      "grad_norm": 0.14059586822986603,
      "learning_rate": 1.36875e-05,
      "loss": 0.0012,
      "step": 87150
    },
    {
      "epoch": 5.810666666666666,
      "grad_norm": 0.05099082738161087,
      "learning_rate": 1.3683333333333333e-05,
      "loss": 0.002,
      "step": 87160
    },
    {
      "epoch": 5.811333333333334,
      "grad_norm": 0.3500629961490631,
      "learning_rate": 1.3679166666666668e-05,
      "loss": 0.0022,
      "step": 87170
    },
    {
      "epoch": 5.812,
      "grad_norm": 0.30727967619895935,
      "learning_rate": 1.3675000000000002e-05,
      "loss": 0.0021,
      "step": 87180
    },
    {
      "epoch": 5.812666666666667,
      "grad_norm": 0.2494693547487259,
      "learning_rate": 1.3670833333333333e-05,
      "loss": 0.0015,
      "step": 87190
    },
    {
      "epoch": 5.8133333333333335,
      "grad_norm": 0.27651268243789673,
      "learning_rate": 1.3666666666666666e-05,
      "loss": 0.0015,
      "step": 87200
    },
    {
      "epoch": 5.814,
      "grad_norm": 0.2708377242088318,
      "learning_rate": 1.36625e-05,
      "loss": 0.0017,
      "step": 87210
    },
    {
      "epoch": 5.814666666666667,
      "grad_norm": 0.04752626642584801,
      "learning_rate": 1.3658333333333334e-05,
      "loss": 0.002,
      "step": 87220
    },
    {
      "epoch": 5.815333333333333,
      "grad_norm": 0.30670738220214844,
      "learning_rate": 1.3654166666666667e-05,
      "loss": 0.0024,
      "step": 87230
    },
    {
      "epoch": 5.816,
      "grad_norm": 0.41324323415756226,
      "learning_rate": 1.3650000000000001e-05,
      "loss": 0.0015,
      "step": 87240
    },
    {
      "epoch": 5.816666666666666,
      "grad_norm": 0.2661921977996826,
      "learning_rate": 1.3645833333333332e-05,
      "loss": 0.0013,
      "step": 87250
    },
    {
      "epoch": 5.817333333333333,
      "grad_norm": 0.2216711789369583,
      "learning_rate": 1.3641666666666667e-05,
      "loss": 0.0027,
      "step": 87260
    },
    {
      "epoch": 5.818,
      "grad_norm": 0.7874154448509216,
      "learning_rate": 1.36375e-05,
      "loss": 0.0023,
      "step": 87270
    },
    {
      "epoch": 5.818666666666667,
      "grad_norm": 0.4641976058483124,
      "learning_rate": 1.3633333333333334e-05,
      "loss": 0.0018,
      "step": 87280
    },
    {
      "epoch": 5.819333333333334,
      "grad_norm": 0.17767870426177979,
      "learning_rate": 1.3629166666666668e-05,
      "loss": 0.0021,
      "step": 87290
    },
    {
      "epoch": 5.82,
      "grad_norm": 0.40690723061561584,
      "learning_rate": 1.3625e-05,
      "loss": 0.0026,
      "step": 87300
    },
    {
      "epoch": 5.820666666666667,
      "grad_norm": 0.40840744972229004,
      "learning_rate": 1.3620833333333334e-05,
      "loss": 0.0021,
      "step": 87310
    },
    {
      "epoch": 5.8213333333333335,
      "grad_norm": 0.37815216183662415,
      "learning_rate": 1.3616666666666666e-05,
      "loss": 0.0021,
      "step": 87320
    },
    {
      "epoch": 5.822,
      "grad_norm": 0.17508795857429504,
      "learning_rate": 1.36125e-05,
      "loss": 0.0012,
      "step": 87330
    },
    {
      "epoch": 5.822666666666667,
      "grad_norm": 0.3407392203807831,
      "learning_rate": 1.3608333333333333e-05,
      "loss": 0.0019,
      "step": 87340
    },
    {
      "epoch": 5.823333333333333,
      "grad_norm": 0.20283599197864532,
      "learning_rate": 1.3604166666666668e-05,
      "loss": 0.0017,
      "step": 87350
    },
    {
      "epoch": 5.824,
      "grad_norm": 0.21889740228652954,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.0014,
      "step": 87360
    },
    {
      "epoch": 5.824666666666666,
      "grad_norm": 0.38162532448768616,
      "learning_rate": 1.3595833333333333e-05,
      "loss": 0.0012,
      "step": 87370
    },
    {
      "epoch": 5.825333333333333,
      "grad_norm": 0.16949710249900818,
      "learning_rate": 1.3591666666666667e-05,
      "loss": 0.0019,
      "step": 87380
    },
    {
      "epoch": 5.826,
      "grad_norm": 0.5419236421585083,
      "learning_rate": 1.35875e-05,
      "loss": 0.0023,
      "step": 87390
    },
    {
      "epoch": 5.826666666666666,
      "grad_norm": 0.34488382935523987,
      "learning_rate": 1.3583333333333334e-05,
      "loss": 0.0016,
      "step": 87400
    },
    {
      "epoch": 5.827333333333334,
      "grad_norm": 0.5171624422073364,
      "learning_rate": 1.3579166666666669e-05,
      "loss": 0.0014,
      "step": 87410
    },
    {
      "epoch": 5.828,
      "grad_norm": 0.26947420835494995,
      "learning_rate": 1.3575000000000001e-05,
      "loss": 0.0019,
      "step": 87420
    },
    {
      "epoch": 5.828666666666667,
      "grad_norm": 0.28701257705688477,
      "learning_rate": 1.3570833333333332e-05,
      "loss": 0.0014,
      "step": 87430
    },
    {
      "epoch": 5.8293333333333335,
      "grad_norm": 0.2409641146659851,
      "learning_rate": 1.3566666666666667e-05,
      "loss": 0.0017,
      "step": 87440
    },
    {
      "epoch": 5.83,
      "grad_norm": 0.07442477345466614,
      "learning_rate": 1.3562500000000001e-05,
      "loss": 0.0019,
      "step": 87450
    },
    {
      "epoch": 5.830666666666667,
      "grad_norm": 0.38186776638031006,
      "learning_rate": 1.3558333333333334e-05,
      "loss": 0.0022,
      "step": 87460
    },
    {
      "epoch": 5.831333333333333,
      "grad_norm": 0.13810671865940094,
      "learning_rate": 1.3554166666666668e-05,
      "loss": 0.002,
      "step": 87470
    },
    {
      "epoch": 5.832,
      "grad_norm": 0.14638927578926086,
      "learning_rate": 1.3550000000000002e-05,
      "loss": 0.0013,
      "step": 87480
    },
    {
      "epoch": 5.832666666666666,
      "grad_norm": 0.05376851558685303,
      "learning_rate": 1.3545833333333333e-05,
      "loss": 0.0015,
      "step": 87490
    },
    {
      "epoch": 5.833333333333333,
      "grad_norm": 0.3007074296474457,
      "learning_rate": 1.3541666666666666e-05,
      "loss": 0.0017,
      "step": 87500
    },
    {
      "epoch": 5.834,
      "grad_norm": 0.06614309549331665,
      "learning_rate": 1.35375e-05,
      "loss": 0.0021,
      "step": 87510
    },
    {
      "epoch": 5.834666666666667,
      "grad_norm": 0.24428154528141022,
      "learning_rate": 1.3533333333333335e-05,
      "loss": 0.0012,
      "step": 87520
    },
    {
      "epoch": 5.835333333333334,
      "grad_norm": 0.4080696105957031,
      "learning_rate": 1.3529166666666667e-05,
      "loss": 0.0027,
      "step": 87530
    },
    {
      "epoch": 5.836,
      "grad_norm": 0.788853108882904,
      "learning_rate": 1.3525000000000002e-05,
      "loss": 0.0017,
      "step": 87540
    },
    {
      "epoch": 5.836666666666667,
      "grad_norm": 0.18304598331451416,
      "learning_rate": 1.3520833333333336e-05,
      "loss": 0.0025,
      "step": 87550
    },
    {
      "epoch": 5.8373333333333335,
      "grad_norm": 0.5468465685844421,
      "learning_rate": 1.3516666666666667e-05,
      "loss": 0.0027,
      "step": 87560
    },
    {
      "epoch": 5.838,
      "grad_norm": 0.5664116144180298,
      "learning_rate": 1.35125e-05,
      "loss": 0.0014,
      "step": 87570
    },
    {
      "epoch": 5.838666666666667,
      "grad_norm": 0.34478652477264404,
      "learning_rate": 1.3508333333333334e-05,
      "loss": 0.0021,
      "step": 87580
    },
    {
      "epoch": 5.839333333333333,
      "grad_norm": 0.046067699790000916,
      "learning_rate": 1.3504166666666669e-05,
      "loss": 0.0026,
      "step": 87590
    },
    {
      "epoch": 5.84,
      "grad_norm": 0.13543550670146942,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 0.0037,
      "step": 87600
    },
    {
      "epoch": 5.8406666666666665,
      "grad_norm": 0.1228596419095993,
      "learning_rate": 1.3495833333333336e-05,
      "loss": 0.0015,
      "step": 87610
    },
    {
      "epoch": 5.841333333333333,
      "grad_norm": 0.14056849479675293,
      "learning_rate": 1.3491666666666667e-05,
      "loss": 0.0019,
      "step": 87620
    },
    {
      "epoch": 5.842,
      "grad_norm": 0.1441616266965866,
      "learning_rate": 1.3487500000000001e-05,
      "loss": 0.0031,
      "step": 87630
    },
    {
      "epoch": 5.842666666666666,
      "grad_norm": 0.24446973204612732,
      "learning_rate": 1.3483333333333334e-05,
      "loss": 0.0028,
      "step": 87640
    },
    {
      "epoch": 5.843333333333334,
      "grad_norm": 0.24618534743785858,
      "learning_rate": 1.3479166666666668e-05,
      "loss": 0.0018,
      "step": 87650
    },
    {
      "epoch": 5.844,
      "grad_norm": 0.19856558740139008,
      "learning_rate": 1.3475000000000002e-05,
      "loss": 0.0015,
      "step": 87660
    },
    {
      "epoch": 5.844666666666667,
      "grad_norm": 0.5779620409011841,
      "learning_rate": 1.3470833333333335e-05,
      "loss": 0.0015,
      "step": 87670
    },
    {
      "epoch": 5.8453333333333335,
      "grad_norm": 0.05501260235905647,
      "learning_rate": 1.3466666666666666e-05,
      "loss": 0.0017,
      "step": 87680
    },
    {
      "epoch": 5.846,
      "grad_norm": 0.47581589221954346,
      "learning_rate": 1.34625e-05,
      "loss": 0.0031,
      "step": 87690
    },
    {
      "epoch": 5.846666666666667,
      "grad_norm": 0.3471772074699402,
      "learning_rate": 1.3458333333333335e-05,
      "loss": 0.0025,
      "step": 87700
    },
    {
      "epoch": 5.847333333333333,
      "grad_norm": 0.07620461285114288,
      "learning_rate": 1.3454166666666667e-05,
      "loss": 0.0023,
      "step": 87710
    },
    {
      "epoch": 5.848,
      "grad_norm": 0.05783947557210922,
      "learning_rate": 1.3450000000000002e-05,
      "loss": 0.002,
      "step": 87720
    },
    {
      "epoch": 5.8486666666666665,
      "grad_norm": 0.07164183259010315,
      "learning_rate": 1.3445833333333336e-05,
      "loss": 0.0019,
      "step": 87730
    },
    {
      "epoch": 5.849333333333333,
      "grad_norm": 0.06186346337199211,
      "learning_rate": 1.3441666666666667e-05,
      "loss": 0.0023,
      "step": 87740
    },
    {
      "epoch": 5.85,
      "grad_norm": 0.08120568096637726,
      "learning_rate": 1.34375e-05,
      "loss": 0.0012,
      "step": 87750
    },
    {
      "epoch": 5.850666666666667,
      "grad_norm": 0.4061180353164673,
      "learning_rate": 1.3433333333333334e-05,
      "loss": 0.0014,
      "step": 87760
    },
    {
      "epoch": 5.851333333333334,
      "grad_norm": 0.5878506898880005,
      "learning_rate": 1.3429166666666668e-05,
      "loss": 0.0018,
      "step": 87770
    },
    {
      "epoch": 5.852,
      "grad_norm": 0.07840210944414139,
      "learning_rate": 1.3425000000000001e-05,
      "loss": 0.0022,
      "step": 87780
    },
    {
      "epoch": 5.852666666666667,
      "grad_norm": 0.07114896923303604,
      "learning_rate": 1.3420833333333335e-05,
      "loss": 0.0016,
      "step": 87790
    },
    {
      "epoch": 5.8533333333333335,
      "grad_norm": 0.24143536388874054,
      "learning_rate": 1.3416666666666666e-05,
      "loss": 0.0022,
      "step": 87800
    },
    {
      "epoch": 5.854,
      "grad_norm": 0.43074020743370056,
      "learning_rate": 1.34125e-05,
      "loss": 0.0019,
      "step": 87810
    },
    {
      "epoch": 5.854666666666667,
      "grad_norm": 0.603272020816803,
      "learning_rate": 1.3408333333333333e-05,
      "loss": 0.0019,
      "step": 87820
    },
    {
      "epoch": 5.855333333333333,
      "grad_norm": 0.5756837129592896,
      "learning_rate": 1.3404166666666668e-05,
      "loss": 0.0032,
      "step": 87830
    },
    {
      "epoch": 5.856,
      "grad_norm": 0.08050771802663803,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 0.0019,
      "step": 87840
    },
    {
      "epoch": 5.8566666666666665,
      "grad_norm": 0.18344146013259888,
      "learning_rate": 1.3395833333333335e-05,
      "loss": 0.0029,
      "step": 87850
    },
    {
      "epoch": 5.857333333333333,
      "grad_norm": 0.13825008273124695,
      "learning_rate": 1.3391666666666666e-05,
      "loss": 0.0034,
      "step": 87860
    },
    {
      "epoch": 5.858,
      "grad_norm": 0.4419514834880829,
      "learning_rate": 1.33875e-05,
      "loss": 0.0023,
      "step": 87870
    },
    {
      "epoch": 5.858666666666666,
      "grad_norm": 0.2367301881313324,
      "learning_rate": 1.3383333333333335e-05,
      "loss": 0.0013,
      "step": 87880
    },
    {
      "epoch": 5.859333333333334,
      "grad_norm": 0.33591359853744507,
      "learning_rate": 1.3379166666666667e-05,
      "loss": 0.0018,
      "step": 87890
    },
    {
      "epoch": 5.86,
      "grad_norm": 0.03754984587430954,
      "learning_rate": 1.3375000000000002e-05,
      "loss": 0.0023,
      "step": 87900
    },
    {
      "epoch": 5.860666666666667,
      "grad_norm": 0.107254758477211,
      "learning_rate": 1.3370833333333336e-05,
      "loss": 0.0012,
      "step": 87910
    },
    {
      "epoch": 5.8613333333333335,
      "grad_norm": 0.17531929910182953,
      "learning_rate": 1.3366666666666667e-05,
      "loss": 0.0019,
      "step": 87920
    },
    {
      "epoch": 5.862,
      "grad_norm": 0.47630980610847473,
      "learning_rate": 1.33625e-05,
      "loss": 0.0014,
      "step": 87930
    },
    {
      "epoch": 5.862666666666667,
      "grad_norm": 0.6501576900482178,
      "learning_rate": 1.3358333333333334e-05,
      "loss": 0.0014,
      "step": 87940
    },
    {
      "epoch": 5.863333333333333,
      "grad_norm": 0.09801953285932541,
      "learning_rate": 1.3354166666666668e-05,
      "loss": 0.0012,
      "step": 87950
    },
    {
      "epoch": 5.864,
      "grad_norm": 0.10397311300039291,
      "learning_rate": 1.3350000000000001e-05,
      "loss": 0.0025,
      "step": 87960
    },
    {
      "epoch": 5.8646666666666665,
      "grad_norm": 0.497200071811676,
      "learning_rate": 1.3345833333333335e-05,
      "loss": 0.0016,
      "step": 87970
    },
    {
      "epoch": 5.865333333333333,
      "grad_norm": 0.05270243063569069,
      "learning_rate": 1.3341666666666666e-05,
      "loss": 0.0021,
      "step": 87980
    },
    {
      "epoch": 5.866,
      "grad_norm": 0.36821359395980835,
      "learning_rate": 1.33375e-05,
      "loss": 0.002,
      "step": 87990
    },
    {
      "epoch": 5.866666666666667,
      "grad_norm": 0.4765644967556,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.0016,
      "step": 88000
    },
    {
      "epoch": 5.867333333333333,
      "grad_norm": 0.17127662897109985,
      "learning_rate": 1.3329166666666668e-05,
      "loss": 0.002,
      "step": 88010
    },
    {
      "epoch": 5.868,
      "grad_norm": 0.18599069118499756,
      "learning_rate": 1.3325000000000002e-05,
      "loss": 0.0019,
      "step": 88020
    },
    {
      "epoch": 5.868666666666667,
      "grad_norm": 0.30742567777633667,
      "learning_rate": 1.3320833333333335e-05,
      "loss": 0.0024,
      "step": 88030
    },
    {
      "epoch": 5.8693333333333335,
      "grad_norm": 0.47975289821624756,
      "learning_rate": 1.3316666666666666e-05,
      "loss": 0.0012,
      "step": 88040
    },
    {
      "epoch": 5.87,
      "grad_norm": 0.11461430788040161,
      "learning_rate": 1.33125e-05,
      "loss": 0.0027,
      "step": 88050
    },
    {
      "epoch": 5.870666666666667,
      "grad_norm": 0.407916396856308,
      "learning_rate": 1.3308333333333334e-05,
      "loss": 0.0021,
      "step": 88060
    },
    {
      "epoch": 5.871333333333333,
      "grad_norm": 0.27451664209365845,
      "learning_rate": 1.3304166666666667e-05,
      "loss": 0.0022,
      "step": 88070
    },
    {
      "epoch": 5.872,
      "grad_norm": 0.28148457407951355,
      "learning_rate": 1.3300000000000001e-05,
      "loss": 0.0013,
      "step": 88080
    },
    {
      "epoch": 5.8726666666666665,
      "grad_norm": 0.03910357505083084,
      "learning_rate": 1.3295833333333336e-05,
      "loss": 0.0018,
      "step": 88090
    },
    {
      "epoch": 5.873333333333333,
      "grad_norm": 0.3824383318424225,
      "learning_rate": 1.3291666666666667e-05,
      "loss": 0.0021,
      "step": 88100
    },
    {
      "epoch": 5.874,
      "grad_norm": 0.17343749105930328,
      "learning_rate": 1.32875e-05,
      "loss": 0.0019,
      "step": 88110
    },
    {
      "epoch": 5.874666666666666,
      "grad_norm": 0.016637204214930534,
      "learning_rate": 1.3283333333333334e-05,
      "loss": 0.0014,
      "step": 88120
    },
    {
      "epoch": 5.875333333333334,
      "grad_norm": 0.4471360146999359,
      "learning_rate": 1.3279166666666668e-05,
      "loss": 0.0014,
      "step": 88130
    },
    {
      "epoch": 5.876,
      "grad_norm": 0.10474888980388641,
      "learning_rate": 1.3275e-05,
      "loss": 0.0012,
      "step": 88140
    },
    {
      "epoch": 5.876666666666667,
      "grad_norm": 0.4501206576824188,
      "learning_rate": 1.3270833333333335e-05,
      "loss": 0.0017,
      "step": 88150
    },
    {
      "epoch": 5.8773333333333335,
      "grad_norm": 0.34224238991737366,
      "learning_rate": 1.3266666666666666e-05,
      "loss": 0.0025,
      "step": 88160
    },
    {
      "epoch": 5.878,
      "grad_norm": 0.8182545304298401,
      "learning_rate": 1.32625e-05,
      "loss": 0.0024,
      "step": 88170
    },
    {
      "epoch": 5.878666666666667,
      "grad_norm": 0.2421254813671112,
      "learning_rate": 1.3258333333333333e-05,
      "loss": 0.0019,
      "step": 88180
    },
    {
      "epoch": 5.879333333333333,
      "grad_norm": 0.306927353143692,
      "learning_rate": 1.3254166666666668e-05,
      "loss": 0.0019,
      "step": 88190
    },
    {
      "epoch": 5.88,
      "grad_norm": 0.2408813089132309,
      "learning_rate": 1.3250000000000002e-05,
      "loss": 0.0016,
      "step": 88200
    },
    {
      "epoch": 5.8806666666666665,
      "grad_norm": 0.13416922092437744,
      "learning_rate": 1.3245833333333335e-05,
      "loss": 0.0022,
      "step": 88210
    },
    {
      "epoch": 5.881333333333333,
      "grad_norm": 0.40499886870384216,
      "learning_rate": 1.3241666666666666e-05,
      "loss": 0.0016,
      "step": 88220
    },
    {
      "epoch": 5.882,
      "grad_norm": 0.3626551926136017,
      "learning_rate": 1.32375e-05,
      "loss": 0.0013,
      "step": 88230
    },
    {
      "epoch": 5.882666666666667,
      "grad_norm": 0.08012217283248901,
      "learning_rate": 1.3233333333333334e-05,
      "loss": 0.0017,
      "step": 88240
    },
    {
      "epoch": 5.883333333333333,
      "grad_norm": 0.3264181613922119,
      "learning_rate": 1.3229166666666667e-05,
      "loss": 0.0018,
      "step": 88250
    },
    {
      "epoch": 5.884,
      "grad_norm": 0.17597581446170807,
      "learning_rate": 1.3225000000000001e-05,
      "loss": 0.002,
      "step": 88260
    },
    {
      "epoch": 5.884666666666667,
      "grad_norm": 0.13687168061733246,
      "learning_rate": 1.3220833333333336e-05,
      "loss": 0.0026,
      "step": 88270
    },
    {
      "epoch": 5.8853333333333335,
      "grad_norm": 0.5753908157348633,
      "learning_rate": 1.3216666666666667e-05,
      "loss": 0.0022,
      "step": 88280
    },
    {
      "epoch": 5.886,
      "grad_norm": 0.18237780034542084,
      "learning_rate": 1.32125e-05,
      "loss": 0.0018,
      "step": 88290
    },
    {
      "epoch": 5.886666666666667,
      "grad_norm": 0.24213527143001556,
      "learning_rate": 1.3208333333333334e-05,
      "loss": 0.0021,
      "step": 88300
    },
    {
      "epoch": 5.887333333333333,
      "grad_norm": 0.551745593547821,
      "learning_rate": 1.3204166666666668e-05,
      "loss": 0.0016,
      "step": 88310
    },
    {
      "epoch": 5.888,
      "grad_norm": 0.37685683369636536,
      "learning_rate": 1.32e-05,
      "loss": 0.0016,
      "step": 88320
    },
    {
      "epoch": 5.8886666666666665,
      "grad_norm": 0.24072717130184174,
      "learning_rate": 1.3195833333333335e-05,
      "loss": 0.0022,
      "step": 88330
    },
    {
      "epoch": 5.889333333333333,
      "grad_norm": 0.08406071364879608,
      "learning_rate": 1.3191666666666666e-05,
      "loss": 0.0018,
      "step": 88340
    },
    {
      "epoch": 5.89,
      "grad_norm": 0.11481133848428726,
      "learning_rate": 1.31875e-05,
      "loss": 0.0014,
      "step": 88350
    },
    {
      "epoch": 5.890666666666666,
      "grad_norm": 0.15400144457817078,
      "learning_rate": 1.3183333333333333e-05,
      "loss": 0.0021,
      "step": 88360
    },
    {
      "epoch": 5.891333333333334,
      "grad_norm": 0.07466277480125427,
      "learning_rate": 1.3179166666666667e-05,
      "loss": 0.0019,
      "step": 88370
    },
    {
      "epoch": 5.892,
      "grad_norm": 0.17476791143417358,
      "learning_rate": 1.3175000000000002e-05,
      "loss": 0.0017,
      "step": 88380
    },
    {
      "epoch": 5.892666666666667,
      "grad_norm": 0.0808204635977745,
      "learning_rate": 1.3170833333333334e-05,
      "loss": 0.0018,
      "step": 88390
    },
    {
      "epoch": 5.8933333333333335,
      "grad_norm": 0.13272757828235626,
      "learning_rate": 1.3166666666666665e-05,
      "loss": 0.002,
      "step": 88400
    },
    {
      "epoch": 5.894,
      "grad_norm": 0.30466943979263306,
      "learning_rate": 1.31625e-05,
      "loss": 0.0019,
      "step": 88410
    },
    {
      "epoch": 5.894666666666667,
      "grad_norm": 0.26292645931243896,
      "learning_rate": 1.3158333333333334e-05,
      "loss": 0.0016,
      "step": 88420
    },
    {
      "epoch": 5.895333333333333,
      "grad_norm": 0.6795094609260559,
      "learning_rate": 1.3154166666666667e-05,
      "loss": 0.0012,
      "step": 88430
    },
    {
      "epoch": 5.896,
      "grad_norm": 0.2145393043756485,
      "learning_rate": 1.3150000000000001e-05,
      "loss": 0.0027,
      "step": 88440
    },
    {
      "epoch": 5.8966666666666665,
      "grad_norm": 0.1367470622062683,
      "learning_rate": 1.3145833333333336e-05,
      "loss": 0.0013,
      "step": 88450
    },
    {
      "epoch": 5.897333333333333,
      "grad_norm": 0.1365678906440735,
      "learning_rate": 1.3141666666666666e-05,
      "loss": 0.002,
      "step": 88460
    },
    {
      "epoch": 5.898,
      "grad_norm": 0.3699076175689697,
      "learning_rate": 1.31375e-05,
      "loss": 0.0014,
      "step": 88470
    },
    {
      "epoch": 5.898666666666666,
      "grad_norm": 0.4074389338493347,
      "learning_rate": 1.3133333333333334e-05,
      "loss": 0.0022,
      "step": 88480
    },
    {
      "epoch": 5.899333333333333,
      "grad_norm": 0.44839516282081604,
      "learning_rate": 1.3129166666666668e-05,
      "loss": 0.0037,
      "step": 88490
    },
    {
      "epoch": 5.9,
      "grad_norm": 0.4906194508075714,
      "learning_rate": 1.3125e-05,
      "loss": 0.0014,
      "step": 88500
    },
    {
      "epoch": 5.900666666666667,
      "grad_norm": 0.27412936091423035,
      "learning_rate": 1.3120833333333335e-05,
      "loss": 0.0025,
      "step": 88510
    },
    {
      "epoch": 5.9013333333333335,
      "grad_norm": 0.14431072771549225,
      "learning_rate": 1.3116666666666666e-05,
      "loss": 0.0021,
      "step": 88520
    },
    {
      "epoch": 5.902,
      "grad_norm": 0.08949823677539825,
      "learning_rate": 1.31125e-05,
      "loss": 0.0017,
      "step": 88530
    },
    {
      "epoch": 5.902666666666667,
      "grad_norm": 0.3364769220352173,
      "learning_rate": 1.3108333333333333e-05,
      "loss": 0.002,
      "step": 88540
    },
    {
      "epoch": 5.903333333333333,
      "grad_norm": 0.07779286801815033,
      "learning_rate": 1.3104166666666667e-05,
      "loss": 0.0017,
      "step": 88550
    },
    {
      "epoch": 5.904,
      "grad_norm": 0.05159654840826988,
      "learning_rate": 1.3100000000000002e-05,
      "loss": 0.0023,
      "step": 88560
    },
    {
      "epoch": 5.9046666666666665,
      "grad_norm": 0.1777314841747284,
      "learning_rate": 1.3095833333333334e-05,
      "loss": 0.0021,
      "step": 88570
    },
    {
      "epoch": 5.905333333333333,
      "grad_norm": 0.41580137610435486,
      "learning_rate": 1.3091666666666665e-05,
      "loss": 0.0023,
      "step": 88580
    },
    {
      "epoch": 5.906,
      "grad_norm": 0.4455386996269226,
      "learning_rate": 1.30875e-05,
      "loss": 0.0017,
      "step": 88590
    },
    {
      "epoch": 5.906666666666666,
      "grad_norm": 0.1372230499982834,
      "learning_rate": 1.3083333333333334e-05,
      "loss": 0.0022,
      "step": 88600
    },
    {
      "epoch": 5.907333333333334,
      "grad_norm": 0.04619639739394188,
      "learning_rate": 1.3079166666666667e-05,
      "loss": 0.0013,
      "step": 88610
    },
    {
      "epoch": 5.908,
      "grad_norm": 0.2998020052909851,
      "learning_rate": 1.3075000000000001e-05,
      "loss": 0.002,
      "step": 88620
    },
    {
      "epoch": 5.908666666666667,
      "grad_norm": 0.025867849588394165,
      "learning_rate": 1.3070833333333335e-05,
      "loss": 0.0026,
      "step": 88630
    },
    {
      "epoch": 5.9093333333333335,
      "grad_norm": 0.3039647042751312,
      "learning_rate": 1.3066666666666666e-05,
      "loss": 0.002,
      "step": 88640
    },
    {
      "epoch": 5.91,
      "grad_norm": 0.23690247535705566,
      "learning_rate": 1.3062499999999999e-05,
      "loss": 0.0012,
      "step": 88650
    },
    {
      "epoch": 5.910666666666667,
      "grad_norm": 0.31006669998168945,
      "learning_rate": 1.3058333333333333e-05,
      "loss": 0.0019,
      "step": 88660
    },
    {
      "epoch": 5.911333333333333,
      "grad_norm": 0.04833430424332619,
      "learning_rate": 1.3054166666666668e-05,
      "loss": 0.0018,
      "step": 88670
    },
    {
      "epoch": 5.912,
      "grad_norm": 0.1357579529285431,
      "learning_rate": 1.305e-05,
      "loss": 0.002,
      "step": 88680
    },
    {
      "epoch": 5.9126666666666665,
      "grad_norm": 0.12206196039915085,
      "learning_rate": 1.3045833333333335e-05,
      "loss": 0.0017,
      "step": 88690
    },
    {
      "epoch": 5.913333333333333,
      "grad_norm": 0.2760271430015564,
      "learning_rate": 1.3041666666666666e-05,
      "loss": 0.0016,
      "step": 88700
    },
    {
      "epoch": 5.914,
      "grad_norm": 0.03721507266163826,
      "learning_rate": 1.30375e-05,
      "loss": 0.0019,
      "step": 88710
    },
    {
      "epoch": 5.914666666666666,
      "grad_norm": 0.10795131325721741,
      "learning_rate": 1.3033333333333333e-05,
      "loss": 0.001,
      "step": 88720
    },
    {
      "epoch": 5.915333333333333,
      "grad_norm": 0.7558916807174683,
      "learning_rate": 1.3029166666666667e-05,
      "loss": 0.002,
      "step": 88730
    },
    {
      "epoch": 5.916,
      "grad_norm": 0.05615661293268204,
      "learning_rate": 1.3025000000000002e-05,
      "loss": 0.0023,
      "step": 88740
    },
    {
      "epoch": 5.916666666666667,
      "grad_norm": 0.4845162034034729,
      "learning_rate": 1.3020833333333334e-05,
      "loss": 0.0018,
      "step": 88750
    },
    {
      "epoch": 5.917333333333334,
      "grad_norm": 0.14044490456581116,
      "learning_rate": 1.3016666666666669e-05,
      "loss": 0.0019,
      "step": 88760
    },
    {
      "epoch": 5.918,
      "grad_norm": 0.3601422905921936,
      "learning_rate": 1.30125e-05,
      "loss": 0.0016,
      "step": 88770
    },
    {
      "epoch": 5.918666666666667,
      "grad_norm": 0.5047484040260315,
      "learning_rate": 1.3008333333333334e-05,
      "loss": 0.002,
      "step": 88780
    },
    {
      "epoch": 5.919333333333333,
      "grad_norm": 0.17989373207092285,
      "learning_rate": 1.3004166666666667e-05,
      "loss": 0.0022,
      "step": 88790
    },
    {
      "epoch": 5.92,
      "grad_norm": 0.07387751340866089,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.0021,
      "step": 88800
    },
    {
      "epoch": 5.9206666666666665,
      "grad_norm": 0.11180852353572845,
      "learning_rate": 1.2995833333333335e-05,
      "loss": 0.0023,
      "step": 88810
    },
    {
      "epoch": 5.921333333333333,
      "grad_norm": 0.4497017562389374,
      "learning_rate": 1.2991666666666668e-05,
      "loss": 0.0024,
      "step": 88820
    },
    {
      "epoch": 5.922,
      "grad_norm": 0.16848944127559662,
      "learning_rate": 1.2987499999999999e-05,
      "loss": 0.0017,
      "step": 88830
    },
    {
      "epoch": 5.922666666666666,
      "grad_norm": 0.07437944412231445,
      "learning_rate": 1.2983333333333333e-05,
      "loss": 0.0019,
      "step": 88840
    },
    {
      "epoch": 5.923333333333334,
      "grad_norm": 0.07100287079811096,
      "learning_rate": 1.2979166666666668e-05,
      "loss": 0.0022,
      "step": 88850
    },
    {
      "epoch": 5.924,
      "grad_norm": 0.5500189065933228,
      "learning_rate": 1.2975e-05,
      "loss": 0.0016,
      "step": 88860
    },
    {
      "epoch": 5.924666666666667,
      "grad_norm": 0.06954076886177063,
      "learning_rate": 1.2970833333333335e-05,
      "loss": 0.0017,
      "step": 88870
    },
    {
      "epoch": 5.925333333333334,
      "grad_norm": 0.14521576464176178,
      "learning_rate": 1.2966666666666669e-05,
      "loss": 0.0012,
      "step": 88880
    },
    {
      "epoch": 5.926,
      "grad_norm": 0.2505151927471161,
      "learning_rate": 1.29625e-05,
      "loss": 0.0014,
      "step": 88890
    },
    {
      "epoch": 5.926666666666667,
      "grad_norm": 0.2060171663761139,
      "learning_rate": 1.2958333333333333e-05,
      "loss": 0.0014,
      "step": 88900
    },
    {
      "epoch": 5.927333333333333,
      "grad_norm": 0.04026978835463524,
      "learning_rate": 1.2954166666666667e-05,
      "loss": 0.0018,
      "step": 88910
    },
    {
      "epoch": 5.928,
      "grad_norm": 0.2020096629858017,
      "learning_rate": 1.2950000000000001e-05,
      "loss": 0.0019,
      "step": 88920
    },
    {
      "epoch": 5.9286666666666665,
      "grad_norm": 0.6485221982002258,
      "learning_rate": 1.2945833333333334e-05,
      "loss": 0.0025,
      "step": 88930
    },
    {
      "epoch": 5.929333333333333,
      "grad_norm": 0.45596253871917725,
      "learning_rate": 1.2941666666666668e-05,
      "loss": 0.0013,
      "step": 88940
    },
    {
      "epoch": 5.93,
      "grad_norm": 0.19094295799732208,
      "learning_rate": 1.29375e-05,
      "loss": 0.0021,
      "step": 88950
    },
    {
      "epoch": 5.930666666666666,
      "grad_norm": 0.1676761507987976,
      "learning_rate": 1.2933333333333334e-05,
      "loss": 0.0014,
      "step": 88960
    },
    {
      "epoch": 5.931333333333333,
      "grad_norm": 0.055579811334609985,
      "learning_rate": 1.2929166666666666e-05,
      "loss": 0.002,
      "step": 88970
    },
    {
      "epoch": 5.932,
      "grad_norm": 0.5872192978858948,
      "learning_rate": 1.2925e-05,
      "loss": 0.0023,
      "step": 88980
    },
    {
      "epoch": 5.932666666666667,
      "grad_norm": 0.5118924975395203,
      "learning_rate": 1.2920833333333335e-05,
      "loss": 0.0016,
      "step": 88990
    },
    {
      "epoch": 5.933333333333334,
      "grad_norm": 0.13778644800186157,
      "learning_rate": 1.2916666666666668e-05,
      "loss": 0.0018,
      "step": 89000
    },
    {
      "epoch": 5.934,
      "grad_norm": 0.17890594899654388,
      "learning_rate": 1.2912499999999999e-05,
      "loss": 0.0012,
      "step": 89010
    },
    {
      "epoch": 5.934666666666667,
      "grad_norm": 0.1362873911857605,
      "learning_rate": 1.2908333333333333e-05,
      "loss": 0.0013,
      "step": 89020
    },
    {
      "epoch": 5.935333333333333,
      "grad_norm": 0.07957791537046432,
      "learning_rate": 1.2904166666666667e-05,
      "loss": 0.0018,
      "step": 89030
    },
    {
      "epoch": 5.936,
      "grad_norm": 0.43390917778015137,
      "learning_rate": 1.29e-05,
      "loss": 0.0016,
      "step": 89040
    },
    {
      "epoch": 5.9366666666666665,
      "grad_norm": 0.10917169600725174,
      "learning_rate": 1.2895833333333335e-05,
      "loss": 0.0018,
      "step": 89050
    },
    {
      "epoch": 5.937333333333333,
      "grad_norm": 0.7412017583847046,
      "learning_rate": 1.2891666666666669e-05,
      "loss": 0.0021,
      "step": 89060
    },
    {
      "epoch": 5.938,
      "grad_norm": 0.24773387610912323,
      "learning_rate": 1.28875e-05,
      "loss": 0.0026,
      "step": 89070
    },
    {
      "epoch": 5.938666666666666,
      "grad_norm": 0.05818285420536995,
      "learning_rate": 1.2883333333333333e-05,
      "loss": 0.002,
      "step": 89080
    },
    {
      "epoch": 5.939333333333334,
      "grad_norm": 0.24009811878204346,
      "learning_rate": 1.2879166666666667e-05,
      "loss": 0.0014,
      "step": 89090
    },
    {
      "epoch": 5.9399999999999995,
      "grad_norm": 0.3488537669181824,
      "learning_rate": 1.2875000000000001e-05,
      "loss": 0.0016,
      "step": 89100
    },
    {
      "epoch": 5.940666666666667,
      "grad_norm": 0.6674925088882446,
      "learning_rate": 1.2870833333333334e-05,
      "loss": 0.0017,
      "step": 89110
    },
    {
      "epoch": 5.941333333333334,
      "grad_norm": 0.21262304484844208,
      "learning_rate": 1.2866666666666668e-05,
      "loss": 0.0024,
      "step": 89120
    },
    {
      "epoch": 5.942,
      "grad_norm": 0.3483193516731262,
      "learning_rate": 1.28625e-05,
      "loss": 0.0017,
      "step": 89130
    },
    {
      "epoch": 5.942666666666667,
      "grad_norm": 0.20950181782245636,
      "learning_rate": 1.2858333333333334e-05,
      "loss": 0.0018,
      "step": 89140
    },
    {
      "epoch": 5.943333333333333,
      "grad_norm": 0.4194795787334442,
      "learning_rate": 1.2854166666666668e-05,
      "loss": 0.0019,
      "step": 89150
    },
    {
      "epoch": 5.944,
      "grad_norm": 0.5109190940856934,
      "learning_rate": 1.285e-05,
      "loss": 0.0024,
      "step": 89160
    },
    {
      "epoch": 5.9446666666666665,
      "grad_norm": 0.11916673928499222,
      "learning_rate": 1.2845833333333335e-05,
      "loss": 0.0019,
      "step": 89170
    },
    {
      "epoch": 5.945333333333333,
      "grad_norm": 0.6167510151863098,
      "learning_rate": 1.2841666666666668e-05,
      "loss": 0.0021,
      "step": 89180
    },
    {
      "epoch": 5.946,
      "grad_norm": 0.03640670329332352,
      "learning_rate": 1.28375e-05,
      "loss": 0.0021,
      "step": 89190
    },
    {
      "epoch": 5.946666666666666,
      "grad_norm": 0.10064958035945892,
      "learning_rate": 1.2833333333333333e-05,
      "loss": 0.002,
      "step": 89200
    },
    {
      "epoch": 5.947333333333333,
      "grad_norm": 0.5105802416801453,
      "learning_rate": 1.2829166666666667e-05,
      "loss": 0.0015,
      "step": 89210
    },
    {
      "epoch": 5.948,
      "grad_norm": 0.050043895840644836,
      "learning_rate": 1.2825000000000002e-05,
      "loss": 0.0013,
      "step": 89220
    },
    {
      "epoch": 5.948666666666667,
      "grad_norm": 0.30307328701019287,
      "learning_rate": 1.2820833333333334e-05,
      "loss": 0.0017,
      "step": 89230
    },
    {
      "epoch": 5.949333333333334,
      "grad_norm": 0.22378167510032654,
      "learning_rate": 1.2816666666666669e-05,
      "loss": 0.0019,
      "step": 89240
    },
    {
      "epoch": 5.95,
      "grad_norm": 0.07926422357559204,
      "learning_rate": 1.28125e-05,
      "loss": 0.0016,
      "step": 89250
    },
    {
      "epoch": 5.950666666666667,
      "grad_norm": 0.08035825937986374,
      "learning_rate": 1.2808333333333334e-05,
      "loss": 0.0019,
      "step": 89260
    },
    {
      "epoch": 5.951333333333333,
      "grad_norm": 0.19121567904949188,
      "learning_rate": 1.2804166666666667e-05,
      "loss": 0.0015,
      "step": 89270
    },
    {
      "epoch": 5.952,
      "grad_norm": 0.23805545270442963,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.0017,
      "step": 89280
    },
    {
      "epoch": 5.9526666666666666,
      "grad_norm": 0.17928877472877502,
      "learning_rate": 1.2795833333333335e-05,
      "loss": 0.0017,
      "step": 89290
    },
    {
      "epoch": 5.953333333333333,
      "grad_norm": 0.2794908285140991,
      "learning_rate": 1.2791666666666668e-05,
      "loss": 0.0019,
      "step": 89300
    },
    {
      "epoch": 5.954,
      "grad_norm": 0.07688695937395096,
      "learning_rate": 1.2787499999999999e-05,
      "loss": 0.0014,
      "step": 89310
    },
    {
      "epoch": 5.954666666666666,
      "grad_norm": 0.037851013243198395,
      "learning_rate": 1.2783333333333333e-05,
      "loss": 0.0015,
      "step": 89320
    },
    {
      "epoch": 5.955333333333334,
      "grad_norm": 0.2058122307062149,
      "learning_rate": 1.2779166666666668e-05,
      "loss": 0.0019,
      "step": 89330
    },
    {
      "epoch": 5.9559999999999995,
      "grad_norm": 0.23671354353427887,
      "learning_rate": 1.2775e-05,
      "loss": 0.0034,
      "step": 89340
    },
    {
      "epoch": 5.956666666666667,
      "grad_norm": 0.3422950804233551,
      "learning_rate": 1.2770833333333335e-05,
      "loss": 0.0015,
      "step": 89350
    },
    {
      "epoch": 5.957333333333334,
      "grad_norm": 0.34113895893096924,
      "learning_rate": 1.276666666666667e-05,
      "loss": 0.0015,
      "step": 89360
    },
    {
      "epoch": 5.958,
      "grad_norm": 0.04342585802078247,
      "learning_rate": 1.27625e-05,
      "loss": 0.0024,
      "step": 89370
    },
    {
      "epoch": 5.958666666666667,
      "grad_norm": 0.4764924645423889,
      "learning_rate": 1.2758333333333333e-05,
      "loss": 0.0016,
      "step": 89380
    },
    {
      "epoch": 5.959333333333333,
      "grad_norm": 0.18005169928073883,
      "learning_rate": 1.2754166666666667e-05,
      "loss": 0.0016,
      "step": 89390
    },
    {
      "epoch": 5.96,
      "grad_norm": 0.08103826642036438,
      "learning_rate": 1.2750000000000002e-05,
      "loss": 0.0023,
      "step": 89400
    },
    {
      "epoch": 5.960666666666667,
      "grad_norm": 0.09102829545736313,
      "learning_rate": 1.2745833333333334e-05,
      "loss": 0.0022,
      "step": 89410
    },
    {
      "epoch": 5.961333333333333,
      "grad_norm": 0.20885814726352692,
      "learning_rate": 1.2741666666666669e-05,
      "loss": 0.0016,
      "step": 89420
    },
    {
      "epoch": 5.962,
      "grad_norm": 0.07008995860815048,
      "learning_rate": 1.27375e-05,
      "loss": 0.0028,
      "step": 89430
    },
    {
      "epoch": 5.962666666666666,
      "grad_norm": 0.345149427652359,
      "learning_rate": 1.2733333333333334e-05,
      "loss": 0.0018,
      "step": 89440
    },
    {
      "epoch": 5.963333333333333,
      "grad_norm": 0.08558889478445053,
      "learning_rate": 1.2729166666666667e-05,
      "loss": 0.0014,
      "step": 89450
    },
    {
      "epoch": 5.964,
      "grad_norm": 0.379116952419281,
      "learning_rate": 1.2725000000000001e-05,
      "loss": 0.0021,
      "step": 89460
    },
    {
      "epoch": 5.964666666666667,
      "grad_norm": 0.577810525894165,
      "learning_rate": 1.2720833333333335e-05,
      "loss": 0.0016,
      "step": 89470
    },
    {
      "epoch": 5.965333333333334,
      "grad_norm": 0.03580080345273018,
      "learning_rate": 1.2716666666666668e-05,
      "loss": 0.0031,
      "step": 89480
    },
    {
      "epoch": 5.966,
      "grad_norm": 0.1373196840286255,
      "learning_rate": 1.2712499999999999e-05,
      "loss": 0.0014,
      "step": 89490
    },
    {
      "epoch": 5.966666666666667,
      "grad_norm": 0.5437836050987244,
      "learning_rate": 1.2708333333333333e-05,
      "loss": 0.0016,
      "step": 89500
    },
    {
      "epoch": 5.967333333333333,
      "grad_norm": 0.20581763982772827,
      "learning_rate": 1.2704166666666668e-05,
      "loss": 0.0013,
      "step": 89510
    },
    {
      "epoch": 5.968,
      "grad_norm": 0.19910690188407898,
      "learning_rate": 1.27e-05,
      "loss": 0.0019,
      "step": 89520
    },
    {
      "epoch": 5.968666666666667,
      "grad_norm": 0.23355616629123688,
      "learning_rate": 1.2695833333333335e-05,
      "loss": 0.0015,
      "step": 89530
    },
    {
      "epoch": 5.969333333333333,
      "grad_norm": 0.30367663502693176,
      "learning_rate": 1.2691666666666669e-05,
      "loss": 0.0015,
      "step": 89540
    },
    {
      "epoch": 5.97,
      "grad_norm": 0.7406103610992432,
      "learning_rate": 1.26875e-05,
      "loss": 0.0016,
      "step": 89550
    },
    {
      "epoch": 5.970666666666666,
      "grad_norm": 0.7345926761627197,
      "learning_rate": 1.2683333333333333e-05,
      "loss": 0.0018,
      "step": 89560
    },
    {
      "epoch": 5.971333333333334,
      "grad_norm": 0.34269264340400696,
      "learning_rate": 1.2679166666666667e-05,
      "loss": 0.0018,
      "step": 89570
    },
    {
      "epoch": 5.9719999999999995,
      "grad_norm": 0.4163420796394348,
      "learning_rate": 1.2675000000000001e-05,
      "loss": 0.002,
      "step": 89580
    },
    {
      "epoch": 5.972666666666667,
      "grad_norm": 0.29198867082595825,
      "learning_rate": 1.2670833333333334e-05,
      "loss": 0.0024,
      "step": 89590
    },
    {
      "epoch": 5.973333333333334,
      "grad_norm": 0.2753335237503052,
      "learning_rate": 1.2666666666666668e-05,
      "loss": 0.0022,
      "step": 89600
    },
    {
      "epoch": 5.974,
      "grad_norm": 0.23603148758411407,
      "learning_rate": 1.26625e-05,
      "loss": 0.002,
      "step": 89610
    },
    {
      "epoch": 5.974666666666667,
      "grad_norm": 0.041222479194402695,
      "learning_rate": 1.2658333333333334e-05,
      "loss": 0.0015,
      "step": 89620
    },
    {
      "epoch": 5.975333333333333,
      "grad_norm": 0.04763709381222725,
      "learning_rate": 1.2654166666666666e-05,
      "loss": 0.0026,
      "step": 89630
    },
    {
      "epoch": 5.976,
      "grad_norm": 0.33920371532440186,
      "learning_rate": 1.2650000000000001e-05,
      "loss": 0.0017,
      "step": 89640
    },
    {
      "epoch": 5.976666666666667,
      "grad_norm": 0.13573016226291656,
      "learning_rate": 1.2645833333333335e-05,
      "loss": 0.0015,
      "step": 89650
    },
    {
      "epoch": 5.977333333333333,
      "grad_norm": 0.30481085181236267,
      "learning_rate": 1.2641666666666668e-05,
      "loss": 0.0018,
      "step": 89660
    },
    {
      "epoch": 5.978,
      "grad_norm": 0.20915216207504272,
      "learning_rate": 1.2637499999999999e-05,
      "loss": 0.0024,
      "step": 89670
    },
    {
      "epoch": 5.978666666666666,
      "grad_norm": 0.052031196653842926,
      "learning_rate": 1.2633333333333333e-05,
      "loss": 0.0027,
      "step": 89680
    },
    {
      "epoch": 5.979333333333333,
      "grad_norm": 0.03781202435493469,
      "learning_rate": 1.2629166666666668e-05,
      "loss": 0.0019,
      "step": 89690
    },
    {
      "epoch": 5.98,
      "grad_norm": 0.7498514652252197,
      "learning_rate": 1.2625e-05,
      "loss": 0.0017,
      "step": 89700
    },
    {
      "epoch": 5.980666666666667,
      "grad_norm": 0.5660186409950256,
      "learning_rate": 1.2620833333333335e-05,
      "loss": 0.0018,
      "step": 89710
    },
    {
      "epoch": 5.981333333333334,
      "grad_norm": 0.141014963388443,
      "learning_rate": 1.2616666666666669e-05,
      "loss": 0.0021,
      "step": 89720
    },
    {
      "epoch": 5.982,
      "grad_norm": 0.4699398875236511,
      "learning_rate": 1.26125e-05,
      "loss": 0.0029,
      "step": 89730
    },
    {
      "epoch": 5.982666666666667,
      "grad_norm": 0.08725818991661072,
      "learning_rate": 1.2608333333333333e-05,
      "loss": 0.0023,
      "step": 89740
    },
    {
      "epoch": 5.983333333333333,
      "grad_norm": 0.13271261751651764,
      "learning_rate": 1.2604166666666667e-05,
      "loss": 0.003,
      "step": 89750
    },
    {
      "epoch": 5.984,
      "grad_norm": 0.3825501501560211,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 0.0021,
      "step": 89760
    },
    {
      "epoch": 5.984666666666667,
      "grad_norm": 0.10613265633583069,
      "learning_rate": 1.2595833333333334e-05,
      "loss": 0.0019,
      "step": 89770
    },
    {
      "epoch": 5.985333333333333,
      "grad_norm": 0.05705060064792633,
      "learning_rate": 1.2591666666666668e-05,
      "loss": 0.0023,
      "step": 89780
    },
    {
      "epoch": 5.986,
      "grad_norm": 0.3144993185997009,
      "learning_rate": 1.25875e-05,
      "loss": 0.0014,
      "step": 89790
    },
    {
      "epoch": 5.986666666666666,
      "grad_norm": 0.4099583923816681,
      "learning_rate": 1.2583333333333334e-05,
      "loss": 0.0023,
      "step": 89800
    },
    {
      "epoch": 5.987333333333333,
      "grad_norm": 0.2028406411409378,
      "learning_rate": 1.2579166666666666e-05,
      "loss": 0.0016,
      "step": 89810
    },
    {
      "epoch": 5.9879999999999995,
      "grad_norm": 0.5204429626464844,
      "learning_rate": 1.2575e-05,
      "loss": 0.0019,
      "step": 89820
    },
    {
      "epoch": 5.988666666666667,
      "grad_norm": 0.07788562774658203,
      "learning_rate": 1.2570833333333335e-05,
      "loss": 0.0026,
      "step": 89830
    },
    {
      "epoch": 5.989333333333334,
      "grad_norm": 0.14121219515800476,
      "learning_rate": 1.2566666666666668e-05,
      "loss": 0.0016,
      "step": 89840
    },
    {
      "epoch": 5.99,
      "grad_norm": 0.14464640617370605,
      "learning_rate": 1.2562499999999999e-05,
      "loss": 0.0021,
      "step": 89850
    },
    {
      "epoch": 5.990666666666667,
      "grad_norm": 0.10406732559204102,
      "learning_rate": 1.2558333333333333e-05,
      "loss": 0.0022,
      "step": 89860
    },
    {
      "epoch": 5.991333333333333,
      "grad_norm": 0.05432987958192825,
      "learning_rate": 1.2554166666666667e-05,
      "loss": 0.003,
      "step": 89870
    },
    {
      "epoch": 5.992,
      "grad_norm": 0.14060111343860626,
      "learning_rate": 1.255e-05,
      "loss": 0.0038,
      "step": 89880
    },
    {
      "epoch": 5.992666666666667,
      "grad_norm": 0.11739998310804367,
      "learning_rate": 1.2545833333333334e-05,
      "loss": 0.0019,
      "step": 89890
    },
    {
      "epoch": 5.993333333333333,
      "grad_norm": 0.16742965579032898,
      "learning_rate": 1.2541666666666669e-05,
      "loss": 0.0016,
      "step": 89900
    },
    {
      "epoch": 5.994,
      "grad_norm": 0.3533101975917816,
      "learning_rate": 1.25375e-05,
      "loss": 0.0027,
      "step": 89910
    },
    {
      "epoch": 5.994666666666666,
      "grad_norm": 0.6123003363609314,
      "learning_rate": 1.2533333333333332e-05,
      "loss": 0.0018,
      "step": 89920
    },
    {
      "epoch": 5.995333333333333,
      "grad_norm": 0.2752491533756256,
      "learning_rate": 1.2529166666666667e-05,
      "loss": 0.0027,
      "step": 89930
    },
    {
      "epoch": 5.996,
      "grad_norm": 0.4423504173755646,
      "learning_rate": 1.2525000000000001e-05,
      "loss": 0.0016,
      "step": 89940
    },
    {
      "epoch": 5.996666666666667,
      "grad_norm": 0.11276379972696304,
      "learning_rate": 1.2520833333333334e-05,
      "loss": 0.0021,
      "step": 89950
    },
    {
      "epoch": 5.997333333333334,
      "grad_norm": 0.37585487961769104,
      "learning_rate": 1.2516666666666668e-05,
      "loss": 0.002,
      "step": 89960
    },
    {
      "epoch": 5.998,
      "grad_norm": 0.30384695529937744,
      "learning_rate": 1.25125e-05,
      "loss": 0.0013,
      "step": 89970
    },
    {
      "epoch": 5.998666666666667,
      "grad_norm": 0.3829430937767029,
      "learning_rate": 1.2508333333333334e-05,
      "loss": 0.0022,
      "step": 89980
    },
    {
      "epoch": 5.999333333333333,
      "grad_norm": 0.30651333928108215,
      "learning_rate": 1.2504166666666666e-05,
      "loss": 0.0028,
      "step": 89990
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.30570459365844727,
      "learning_rate": 1.25e-05,
      "loss": 0.002,
      "step": 90000
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.001887280261144042,
      "eval_runtime": 186.8993,
      "eval_samples_per_second": 1070.095,
      "eval_steps_per_second": 26.752,
      "step": 90000
    },
    {
      "epoch": 6.000666666666667,
      "grad_norm": 0.10652846097946167,
      "learning_rate": 1.2495833333333335e-05,
      "loss": 0.0016,
      "step": 90010
    },
    {
      "epoch": 6.001333333333333,
      "grad_norm": 0.4427286684513092,
      "learning_rate": 1.2491666666666668e-05,
      "loss": 0.0029,
      "step": 90020
    },
    {
      "epoch": 6.002,
      "grad_norm": 0.021704087033867836,
      "learning_rate": 1.24875e-05,
      "loss": 0.0018,
      "step": 90030
    },
    {
      "epoch": 6.002666666666666,
      "grad_norm": 0.046848248690366745,
      "learning_rate": 1.2483333333333335e-05,
      "loss": 0.0026,
      "step": 90040
    },
    {
      "epoch": 6.003333333333333,
      "grad_norm": 0.7478758096694946,
      "learning_rate": 1.2479166666666667e-05,
      "loss": 0.0021,
      "step": 90050
    },
    {
      "epoch": 6.004,
      "grad_norm": 0.14579825103282928,
      "learning_rate": 1.2475e-05,
      "loss": 0.0018,
      "step": 90060
    },
    {
      "epoch": 6.004666666666667,
      "grad_norm": 0.15194548666477203,
      "learning_rate": 1.2470833333333334e-05,
      "loss": 0.0026,
      "step": 90070
    },
    {
      "epoch": 6.005333333333334,
      "grad_norm": 0.20786215364933014,
      "learning_rate": 1.2466666666666667e-05,
      "loss": 0.0018,
      "step": 90080
    },
    {
      "epoch": 6.006,
      "grad_norm": 0.10506227612495422,
      "learning_rate": 1.2462500000000001e-05,
      "loss": 0.0016,
      "step": 90090
    },
    {
      "epoch": 6.006666666666667,
      "grad_norm": 0.047467030584812164,
      "learning_rate": 1.2458333333333334e-05,
      "loss": 0.0013,
      "step": 90100
    },
    {
      "epoch": 6.007333333333333,
      "grad_norm": 0.04773193597793579,
      "learning_rate": 1.2454166666666667e-05,
      "loss": 0.0017,
      "step": 90110
    },
    {
      "epoch": 6.008,
      "grad_norm": 0.06861672550439835,
      "learning_rate": 1.2450000000000001e-05,
      "loss": 0.0018,
      "step": 90120
    },
    {
      "epoch": 6.008666666666667,
      "grad_norm": 0.33791300654411316,
      "learning_rate": 1.2445833333333334e-05,
      "loss": 0.0023,
      "step": 90130
    },
    {
      "epoch": 6.009333333333333,
      "grad_norm": 0.1417514532804489,
      "learning_rate": 1.2441666666666666e-05,
      "loss": 0.0017,
      "step": 90140
    },
    {
      "epoch": 6.01,
      "grad_norm": 0.6472262740135193,
      "learning_rate": 1.24375e-05,
      "loss": 0.0025,
      "step": 90150
    },
    {
      "epoch": 6.010666666666666,
      "grad_norm": 0.26815953850746155,
      "learning_rate": 1.2433333333333335e-05,
      "loss": 0.0013,
      "step": 90160
    },
    {
      "epoch": 6.011333333333333,
      "grad_norm": 0.10962533950805664,
      "learning_rate": 1.2429166666666666e-05,
      "loss": 0.0019,
      "step": 90170
    },
    {
      "epoch": 6.012,
      "grad_norm": 0.44664523005485535,
      "learning_rate": 1.2425e-05,
      "loss": 0.0023,
      "step": 90180
    },
    {
      "epoch": 6.012666666666667,
      "grad_norm": 0.3921826183795929,
      "learning_rate": 1.2420833333333335e-05,
      "loss": 0.0019,
      "step": 90190
    },
    {
      "epoch": 6.013333333333334,
      "grad_norm": 0.07590825110673904,
      "learning_rate": 1.2416666666666667e-05,
      "loss": 0.0014,
      "step": 90200
    },
    {
      "epoch": 6.014,
      "grad_norm": 0.05077622830867767,
      "learning_rate": 1.24125e-05,
      "loss": 0.0024,
      "step": 90210
    },
    {
      "epoch": 6.014666666666667,
      "grad_norm": 0.1742144078016281,
      "learning_rate": 1.2408333333333335e-05,
      "loss": 0.0016,
      "step": 90220
    },
    {
      "epoch": 6.015333333333333,
      "grad_norm": 0.28374266624450684,
      "learning_rate": 1.2404166666666667e-05,
      "loss": 0.0014,
      "step": 90230
    },
    {
      "epoch": 6.016,
      "grad_norm": 0.11852787435054779,
      "learning_rate": 1.24e-05,
      "loss": 0.0018,
      "step": 90240
    },
    {
      "epoch": 6.016666666666667,
      "grad_norm": 0.044610679149627686,
      "learning_rate": 1.2395833333333334e-05,
      "loss": 0.0021,
      "step": 90250
    },
    {
      "epoch": 6.017333333333333,
      "grad_norm": 0.27170830965042114,
      "learning_rate": 1.2391666666666667e-05,
      "loss": 0.0016,
      "step": 90260
    },
    {
      "epoch": 6.018,
      "grad_norm": 0.14892469346523285,
      "learning_rate": 1.2387500000000001e-05,
      "loss": 0.002,
      "step": 90270
    },
    {
      "epoch": 6.018666666666666,
      "grad_norm": 0.10370949655771255,
      "learning_rate": 1.2383333333333334e-05,
      "loss": 0.0017,
      "step": 90280
    },
    {
      "epoch": 6.019333333333333,
      "grad_norm": 0.13479477167129517,
      "learning_rate": 1.2379166666666667e-05,
      "loss": 0.0019,
      "step": 90290
    },
    {
      "epoch": 6.02,
      "grad_norm": 0.11536862701177597,
      "learning_rate": 1.2375000000000001e-05,
      "loss": 0.0019,
      "step": 90300
    },
    {
      "epoch": 6.020666666666667,
      "grad_norm": 0.05001073703169823,
      "learning_rate": 1.2370833333333334e-05,
      "loss": 0.002,
      "step": 90310
    },
    {
      "epoch": 6.021333333333334,
      "grad_norm": 0.48094871640205383,
      "learning_rate": 1.2366666666666666e-05,
      "loss": 0.0016,
      "step": 90320
    },
    {
      "epoch": 6.022,
      "grad_norm": 0.2012423723936081,
      "learning_rate": 1.23625e-05,
      "loss": 0.0017,
      "step": 90330
    },
    {
      "epoch": 6.022666666666667,
      "grad_norm": 0.44540703296661377,
      "learning_rate": 1.2358333333333335e-05,
      "loss": 0.002,
      "step": 90340
    },
    {
      "epoch": 6.023333333333333,
      "grad_norm": 0.5943692326545715,
      "learning_rate": 1.2354166666666666e-05,
      "loss": 0.0027,
      "step": 90350
    },
    {
      "epoch": 6.024,
      "grad_norm": 0.17586958408355713,
      "learning_rate": 1.235e-05,
      "loss": 0.0012,
      "step": 90360
    },
    {
      "epoch": 6.024666666666667,
      "grad_norm": 0.4372241795063019,
      "learning_rate": 1.2345833333333335e-05,
      "loss": 0.0021,
      "step": 90370
    },
    {
      "epoch": 6.025333333333333,
      "grad_norm": 0.6056591868400574,
      "learning_rate": 1.2341666666666667e-05,
      "loss": 0.0019,
      "step": 90380
    },
    {
      "epoch": 6.026,
      "grad_norm": 0.10234503448009491,
      "learning_rate": 1.23375e-05,
      "loss": 0.0022,
      "step": 90390
    },
    {
      "epoch": 6.026666666666666,
      "grad_norm": 0.34821033477783203,
      "learning_rate": 1.2333333333333334e-05,
      "loss": 0.0029,
      "step": 90400
    },
    {
      "epoch": 6.027333333333333,
      "grad_norm": 0.1747833639383316,
      "learning_rate": 1.2329166666666667e-05,
      "loss": 0.0018,
      "step": 90410
    },
    {
      "epoch": 6.028,
      "grad_norm": 0.158706396818161,
      "learning_rate": 1.2325e-05,
      "loss": 0.0012,
      "step": 90420
    },
    {
      "epoch": 6.028666666666667,
      "grad_norm": 0.44755634665489197,
      "learning_rate": 1.2320833333333334e-05,
      "loss": 0.0013,
      "step": 90430
    },
    {
      "epoch": 6.029333333333334,
      "grad_norm": 0.10857954621315002,
      "learning_rate": 1.2316666666666667e-05,
      "loss": 0.002,
      "step": 90440
    },
    {
      "epoch": 6.03,
      "grad_norm": 0.41238683462142944,
      "learning_rate": 1.2312500000000001e-05,
      "loss": 0.0019,
      "step": 90450
    },
    {
      "epoch": 6.030666666666667,
      "grad_norm": 0.048511818051338196,
      "learning_rate": 1.2308333333333334e-05,
      "loss": 0.0015,
      "step": 90460
    },
    {
      "epoch": 6.031333333333333,
      "grad_norm": 0.5384330153465271,
      "learning_rate": 1.2304166666666666e-05,
      "loss": 0.0026,
      "step": 90470
    },
    {
      "epoch": 6.032,
      "grad_norm": 0.2962801158428192,
      "learning_rate": 1.23e-05,
      "loss": 0.0026,
      "step": 90480
    },
    {
      "epoch": 6.032666666666667,
      "grad_norm": 0.13866065442562103,
      "learning_rate": 1.2295833333333333e-05,
      "loss": 0.0016,
      "step": 90490
    },
    {
      "epoch": 6.033333333333333,
      "grad_norm": 0.11603148281574249,
      "learning_rate": 1.2291666666666666e-05,
      "loss": 0.0016,
      "step": 90500
    },
    {
      "epoch": 6.034,
      "grad_norm": 0.02891925722360611,
      "learning_rate": 1.22875e-05,
      "loss": 0.0017,
      "step": 90510
    },
    {
      "epoch": 6.034666666666666,
      "grad_norm": 0.3419701159000397,
      "learning_rate": 1.2283333333333335e-05,
      "loss": 0.0018,
      "step": 90520
    },
    {
      "epoch": 6.035333333333333,
      "grad_norm": 0.5723559856414795,
      "learning_rate": 1.2279166666666666e-05,
      "loss": 0.0017,
      "step": 90530
    },
    {
      "epoch": 6.036,
      "grad_norm": 0.4262729585170746,
      "learning_rate": 1.2275e-05,
      "loss": 0.0018,
      "step": 90540
    },
    {
      "epoch": 6.036666666666667,
      "grad_norm": 0.219941183924675,
      "learning_rate": 1.2270833333333335e-05,
      "loss": 0.0011,
      "step": 90550
    },
    {
      "epoch": 6.037333333333334,
      "grad_norm": 0.3839852511882782,
      "learning_rate": 1.2266666666666667e-05,
      "loss": 0.0017,
      "step": 90560
    },
    {
      "epoch": 6.038,
      "grad_norm": 0.16793212294578552,
      "learning_rate": 1.22625e-05,
      "loss": 0.0013,
      "step": 90570
    },
    {
      "epoch": 6.038666666666667,
      "grad_norm": 0.40742966532707214,
      "learning_rate": 1.2258333333333334e-05,
      "loss": 0.0019,
      "step": 90580
    },
    {
      "epoch": 6.039333333333333,
      "grad_norm": 0.03823338821530342,
      "learning_rate": 1.2254166666666667e-05,
      "loss": 0.0017,
      "step": 90590
    },
    {
      "epoch": 6.04,
      "grad_norm": 0.029233118519186974,
      "learning_rate": 1.225e-05,
      "loss": 0.0018,
      "step": 90600
    },
    {
      "epoch": 6.040666666666667,
      "grad_norm": 0.2096080780029297,
      "learning_rate": 1.2245833333333334e-05,
      "loss": 0.0021,
      "step": 90610
    },
    {
      "epoch": 6.041333333333333,
      "grad_norm": 0.13310183584690094,
      "learning_rate": 1.2241666666666667e-05,
      "loss": 0.0019,
      "step": 90620
    },
    {
      "epoch": 6.042,
      "grad_norm": 0.1454365849494934,
      "learning_rate": 1.2237500000000001e-05,
      "loss": 0.0015,
      "step": 90630
    },
    {
      "epoch": 6.042666666666666,
      "grad_norm": 0.7801297307014465,
      "learning_rate": 1.2233333333333334e-05,
      "loss": 0.0022,
      "step": 90640
    },
    {
      "epoch": 6.043333333333333,
      "grad_norm": 0.05309022590517998,
      "learning_rate": 1.2229166666666668e-05,
      "loss": 0.0021,
      "step": 90650
    },
    {
      "epoch": 6.044,
      "grad_norm": 0.3804580271244049,
      "learning_rate": 1.2225e-05,
      "loss": 0.0015,
      "step": 90660
    },
    {
      "epoch": 6.044666666666667,
      "grad_norm": 0.04793903976678848,
      "learning_rate": 1.2220833333333333e-05,
      "loss": 0.0019,
      "step": 90670
    },
    {
      "epoch": 6.045333333333334,
      "grad_norm": 0.5132133364677429,
      "learning_rate": 1.2216666666666668e-05,
      "loss": 0.002,
      "step": 90680
    },
    {
      "epoch": 6.046,
      "grad_norm": 0.1762068271636963,
      "learning_rate": 1.22125e-05,
      "loss": 0.0019,
      "step": 90690
    },
    {
      "epoch": 6.046666666666667,
      "grad_norm": 0.3203716278076172,
      "learning_rate": 1.2208333333333335e-05,
      "loss": 0.0019,
      "step": 90700
    },
    {
      "epoch": 6.0473333333333334,
      "grad_norm": 0.050310827791690826,
      "learning_rate": 1.2204166666666667e-05,
      "loss": 0.0014,
      "step": 90710
    },
    {
      "epoch": 6.048,
      "grad_norm": 0.16954731941223145,
      "learning_rate": 1.22e-05,
      "loss": 0.0029,
      "step": 90720
    },
    {
      "epoch": 6.048666666666667,
      "grad_norm": 0.5420475006103516,
      "learning_rate": 1.2195833333333334e-05,
      "loss": 0.0022,
      "step": 90730
    },
    {
      "epoch": 6.049333333333333,
      "grad_norm": 0.24456079304218292,
      "learning_rate": 1.2191666666666667e-05,
      "loss": 0.002,
      "step": 90740
    },
    {
      "epoch": 6.05,
      "grad_norm": 0.1291821449995041,
      "learning_rate": 1.21875e-05,
      "loss": 0.0018,
      "step": 90750
    },
    {
      "epoch": 6.050666666666666,
      "grad_norm": 0.17351633310317993,
      "learning_rate": 1.2183333333333334e-05,
      "loss": 0.0017,
      "step": 90760
    },
    {
      "epoch": 6.051333333333333,
      "grad_norm": 0.2754075825214386,
      "learning_rate": 1.2179166666666669e-05,
      "loss": 0.0015,
      "step": 90770
    },
    {
      "epoch": 6.052,
      "grad_norm": 0.4136432111263275,
      "learning_rate": 1.2175e-05,
      "loss": 0.0013,
      "step": 90780
    },
    {
      "epoch": 6.052666666666667,
      "grad_norm": 0.6770349740982056,
      "learning_rate": 1.2170833333333334e-05,
      "loss": 0.0016,
      "step": 90790
    },
    {
      "epoch": 6.053333333333334,
      "grad_norm": 0.23559704422950745,
      "learning_rate": 1.2166666666666668e-05,
      "loss": 0.0019,
      "step": 90800
    },
    {
      "epoch": 6.054,
      "grad_norm": 0.615646243095398,
      "learning_rate": 1.2162500000000001e-05,
      "loss": 0.0022,
      "step": 90810
    },
    {
      "epoch": 6.054666666666667,
      "grad_norm": 0.4743037819862366,
      "learning_rate": 1.2158333333333334e-05,
      "loss": 0.0016,
      "step": 90820
    },
    {
      "epoch": 6.0553333333333335,
      "grad_norm": 0.14269356429576874,
      "learning_rate": 1.2154166666666668e-05,
      "loss": 0.002,
      "step": 90830
    },
    {
      "epoch": 6.056,
      "grad_norm": 0.13870376348495483,
      "learning_rate": 1.215e-05,
      "loss": 0.0017,
      "step": 90840
    },
    {
      "epoch": 6.056666666666667,
      "grad_norm": 0.04034750908613205,
      "learning_rate": 1.2145833333333333e-05,
      "loss": 0.0013,
      "step": 90850
    },
    {
      "epoch": 6.057333333333333,
      "grad_norm": 0.4023095965385437,
      "learning_rate": 1.2141666666666668e-05,
      "loss": 0.002,
      "step": 90860
    },
    {
      "epoch": 6.058,
      "grad_norm": 0.3395232558250427,
      "learning_rate": 1.21375e-05,
      "loss": 0.0019,
      "step": 90870
    },
    {
      "epoch": 6.058666666666666,
      "grad_norm": 0.07881733030080795,
      "learning_rate": 1.2133333333333335e-05,
      "loss": 0.0018,
      "step": 90880
    },
    {
      "epoch": 6.059333333333333,
      "grad_norm": 0.10261230915784836,
      "learning_rate": 1.2129166666666667e-05,
      "loss": 0.0018,
      "step": 90890
    },
    {
      "epoch": 6.06,
      "grad_norm": 0.28064802289009094,
      "learning_rate": 1.2125e-05,
      "loss": 0.002,
      "step": 90900
    },
    {
      "epoch": 6.060666666666667,
      "grad_norm": 0.11503337323665619,
      "learning_rate": 1.2120833333333334e-05,
      "loss": 0.0015,
      "step": 90910
    },
    {
      "epoch": 6.061333333333334,
      "grad_norm": 0.249101921916008,
      "learning_rate": 1.2116666666666669e-05,
      "loss": 0.0015,
      "step": 90920
    },
    {
      "epoch": 6.062,
      "grad_norm": 0.40121424198150635,
      "learning_rate": 1.21125e-05,
      "loss": 0.002,
      "step": 90930
    },
    {
      "epoch": 6.062666666666667,
      "grad_norm": 0.05702288821339607,
      "learning_rate": 1.2108333333333334e-05,
      "loss": 0.0014,
      "step": 90940
    },
    {
      "epoch": 6.0633333333333335,
      "grad_norm": 0.052459511905908585,
      "learning_rate": 1.2104166666666668e-05,
      "loss": 0.0021,
      "step": 90950
    },
    {
      "epoch": 6.064,
      "grad_norm": 0.05429605767130852,
      "learning_rate": 1.2100000000000001e-05,
      "loss": 0.002,
      "step": 90960
    },
    {
      "epoch": 6.064666666666667,
      "grad_norm": 0.3432425856590271,
      "learning_rate": 1.2095833333333334e-05,
      "loss": 0.0018,
      "step": 90970
    },
    {
      "epoch": 6.065333333333333,
      "grad_norm": 0.038645390421152115,
      "learning_rate": 1.2091666666666668e-05,
      "loss": 0.0028,
      "step": 90980
    },
    {
      "epoch": 6.066,
      "grad_norm": 0.37405553460121155,
      "learning_rate": 1.20875e-05,
      "loss": 0.0014,
      "step": 90990
    },
    {
      "epoch": 6.066666666666666,
      "grad_norm": 0.20340129733085632,
      "learning_rate": 1.2083333333333333e-05,
      "loss": 0.0023,
      "step": 91000
    },
    {
      "epoch": 6.067333333333333,
      "grad_norm": 0.3372378945350647,
      "learning_rate": 1.2079166666666668e-05,
      "loss": 0.0015,
      "step": 91010
    },
    {
      "epoch": 6.068,
      "grad_norm": 0.11670227348804474,
      "learning_rate": 1.2075e-05,
      "loss": 0.0032,
      "step": 91020
    },
    {
      "epoch": 6.068666666666667,
      "grad_norm": 0.3696414530277252,
      "learning_rate": 1.2070833333333335e-05,
      "loss": 0.002,
      "step": 91030
    },
    {
      "epoch": 6.069333333333334,
      "grad_norm": 0.3727620244026184,
      "learning_rate": 1.2066666666666667e-05,
      "loss": 0.0017,
      "step": 91040
    },
    {
      "epoch": 6.07,
      "grad_norm": 0.34663498401641846,
      "learning_rate": 1.20625e-05,
      "loss": 0.0015,
      "step": 91050
    },
    {
      "epoch": 6.070666666666667,
      "grad_norm": 0.43785831332206726,
      "learning_rate": 1.2058333333333334e-05,
      "loss": 0.0021,
      "step": 91060
    },
    {
      "epoch": 6.0713333333333335,
      "grad_norm": 0.20689667761325836,
      "learning_rate": 1.2054166666666667e-05,
      "loss": 0.0023,
      "step": 91070
    },
    {
      "epoch": 6.072,
      "grad_norm": 0.050919100642204285,
      "learning_rate": 1.205e-05,
      "loss": 0.0011,
      "step": 91080
    },
    {
      "epoch": 6.072666666666667,
      "grad_norm": 0.23499272763729095,
      "learning_rate": 1.2045833333333334e-05,
      "loss": 0.0018,
      "step": 91090
    },
    {
      "epoch": 6.073333333333333,
      "grad_norm": 0.17323948442935944,
      "learning_rate": 1.2041666666666669e-05,
      "loss": 0.0017,
      "step": 91100
    },
    {
      "epoch": 6.074,
      "grad_norm": 0.2705860137939453,
      "learning_rate": 1.20375e-05,
      "loss": 0.0015,
      "step": 91110
    },
    {
      "epoch": 6.074666666666666,
      "grad_norm": 0.2469858080148697,
      "learning_rate": 1.2033333333333334e-05,
      "loss": 0.0014,
      "step": 91120
    },
    {
      "epoch": 6.075333333333333,
      "grad_norm": 0.33818623423576355,
      "learning_rate": 1.2029166666666668e-05,
      "loss": 0.0019,
      "step": 91130
    },
    {
      "epoch": 6.076,
      "grad_norm": 0.3727071285247803,
      "learning_rate": 1.2025000000000001e-05,
      "loss": 0.0019,
      "step": 91140
    },
    {
      "epoch": 6.076666666666666,
      "grad_norm": 0.16661593317985535,
      "learning_rate": 1.2020833333333334e-05,
      "loss": 0.0025,
      "step": 91150
    },
    {
      "epoch": 6.077333333333334,
      "grad_norm": 0.6384459137916565,
      "learning_rate": 1.2016666666666668e-05,
      "loss": 0.0018,
      "step": 91160
    },
    {
      "epoch": 6.078,
      "grad_norm": 0.14364956319332123,
      "learning_rate": 1.20125e-05,
      "loss": 0.0016,
      "step": 91170
    },
    {
      "epoch": 6.078666666666667,
      "grad_norm": 0.06023791432380676,
      "learning_rate": 1.2008333333333333e-05,
      "loss": 0.0014,
      "step": 91180
    },
    {
      "epoch": 6.0793333333333335,
      "grad_norm": 0.3089735209941864,
      "learning_rate": 1.2004166666666668e-05,
      "loss": 0.0018,
      "step": 91190
    },
    {
      "epoch": 6.08,
      "grad_norm": 0.14421901106834412,
      "learning_rate": 1.2e-05,
      "loss": 0.002,
      "step": 91200
    },
    {
      "epoch": 6.080666666666667,
      "grad_norm": 0.3465583622455597,
      "learning_rate": 1.1995833333333335e-05,
      "loss": 0.002,
      "step": 91210
    },
    {
      "epoch": 6.081333333333333,
      "grad_norm": 0.10013394057750702,
      "learning_rate": 1.1991666666666667e-05,
      "loss": 0.0013,
      "step": 91220
    },
    {
      "epoch": 6.082,
      "grad_norm": 0.05923787131905556,
      "learning_rate": 1.19875e-05,
      "loss": 0.0012,
      "step": 91230
    },
    {
      "epoch": 6.082666666666666,
      "grad_norm": 0.23723702132701874,
      "learning_rate": 1.1983333333333334e-05,
      "loss": 0.0021,
      "step": 91240
    },
    {
      "epoch": 6.083333333333333,
      "grad_norm": 0.4034302532672882,
      "learning_rate": 1.1979166666666667e-05,
      "loss": 0.0015,
      "step": 91250
    },
    {
      "epoch": 6.084,
      "grad_norm": 0.10936193913221359,
      "learning_rate": 1.1975e-05,
      "loss": 0.0015,
      "step": 91260
    },
    {
      "epoch": 6.084666666666667,
      "grad_norm": 0.47155046463012695,
      "learning_rate": 1.1970833333333334e-05,
      "loss": 0.0018,
      "step": 91270
    },
    {
      "epoch": 6.085333333333334,
      "grad_norm": 0.05921001359820366,
      "learning_rate": 1.1966666666666668e-05,
      "loss": 0.0021,
      "step": 91280
    },
    {
      "epoch": 6.086,
      "grad_norm": 0.040914829820394516,
      "learning_rate": 1.19625e-05,
      "loss": 0.0012,
      "step": 91290
    },
    {
      "epoch": 6.086666666666667,
      "grad_norm": 0.43107059597969055,
      "learning_rate": 1.1958333333333334e-05,
      "loss": 0.0022,
      "step": 91300
    },
    {
      "epoch": 6.0873333333333335,
      "grad_norm": 0.3053259551525116,
      "learning_rate": 1.1954166666666668e-05,
      "loss": 0.0016,
      "step": 91310
    },
    {
      "epoch": 6.088,
      "grad_norm": 0.5118589401245117,
      "learning_rate": 1.195e-05,
      "loss": 0.0025,
      "step": 91320
    },
    {
      "epoch": 6.088666666666667,
      "grad_norm": 0.046147607266902924,
      "learning_rate": 1.1945833333333333e-05,
      "loss": 0.0015,
      "step": 91330
    },
    {
      "epoch": 6.089333333333333,
      "grad_norm": 0.044469673186540604,
      "learning_rate": 1.1941666666666668e-05,
      "loss": 0.0016,
      "step": 91340
    },
    {
      "epoch": 6.09,
      "grad_norm": 0.4005296528339386,
      "learning_rate": 1.19375e-05,
      "loss": 0.002,
      "step": 91350
    },
    {
      "epoch": 6.0906666666666665,
      "grad_norm": 0.11472919583320618,
      "learning_rate": 1.1933333333333333e-05,
      "loss": 0.0015,
      "step": 91360
    },
    {
      "epoch": 6.091333333333333,
      "grad_norm": 0.04323473572731018,
      "learning_rate": 1.1929166666666668e-05,
      "loss": 0.0026,
      "step": 91370
    },
    {
      "epoch": 6.092,
      "grad_norm": 0.16967155039310455,
      "learning_rate": 1.1925e-05,
      "loss": 0.002,
      "step": 91380
    },
    {
      "epoch": 6.092666666666666,
      "grad_norm": 0.04543796926736832,
      "learning_rate": 1.1920833333333335e-05,
      "loss": 0.0016,
      "step": 91390
    },
    {
      "epoch": 6.093333333333334,
      "grad_norm": 0.2078382819890976,
      "learning_rate": 1.1916666666666667e-05,
      "loss": 0.0021,
      "step": 91400
    },
    {
      "epoch": 6.094,
      "grad_norm": 0.5678043365478516,
      "learning_rate": 1.19125e-05,
      "loss": 0.0019,
      "step": 91410
    },
    {
      "epoch": 6.094666666666667,
      "grad_norm": 0.2511199414730072,
      "learning_rate": 1.1908333333333334e-05,
      "loss": 0.0025,
      "step": 91420
    },
    {
      "epoch": 6.0953333333333335,
      "grad_norm": 0.2716706693172455,
      "learning_rate": 1.1904166666666667e-05,
      "loss": 0.0016,
      "step": 91430
    },
    {
      "epoch": 6.096,
      "grad_norm": 0.44235390424728394,
      "learning_rate": 1.19e-05,
      "loss": 0.0023,
      "step": 91440
    },
    {
      "epoch": 6.096666666666667,
      "grad_norm": 0.06931226700544357,
      "learning_rate": 1.1895833333333334e-05,
      "loss": 0.0014,
      "step": 91450
    },
    {
      "epoch": 6.097333333333333,
      "grad_norm": 0.07461424171924591,
      "learning_rate": 1.1891666666666668e-05,
      "loss": 0.0011,
      "step": 91460
    },
    {
      "epoch": 6.098,
      "grad_norm": 0.04623283073306084,
      "learning_rate": 1.18875e-05,
      "loss": 0.0019,
      "step": 91470
    },
    {
      "epoch": 6.0986666666666665,
      "grad_norm": 0.29828572273254395,
      "learning_rate": 1.1883333333333334e-05,
      "loss": 0.0019,
      "step": 91480
    },
    {
      "epoch": 6.099333333333333,
      "grad_norm": 0.21965256333351135,
      "learning_rate": 1.1879166666666668e-05,
      "loss": 0.0021,
      "step": 91490
    },
    {
      "epoch": 6.1,
      "grad_norm": 0.3098929822444916,
      "learning_rate": 1.1875e-05,
      "loss": 0.0018,
      "step": 91500
    },
    {
      "epoch": 6.100666666666666,
      "grad_norm": 0.48218241333961487,
      "learning_rate": 1.1870833333333333e-05,
      "loss": 0.002,
      "step": 91510
    },
    {
      "epoch": 6.101333333333334,
      "grad_norm": 0.26665931940078735,
      "learning_rate": 1.1866666666666668e-05,
      "loss": 0.0024,
      "step": 91520
    },
    {
      "epoch": 6.102,
      "grad_norm": 0.06660464406013489,
      "learning_rate": 1.18625e-05,
      "loss": 0.0023,
      "step": 91530
    },
    {
      "epoch": 6.102666666666667,
      "grad_norm": 0.37830719351768494,
      "learning_rate": 1.1858333333333333e-05,
      "loss": 0.0016,
      "step": 91540
    },
    {
      "epoch": 6.1033333333333335,
      "grad_norm": 0.08017214387655258,
      "learning_rate": 1.1854166666666667e-05,
      "loss": 0.0013,
      "step": 91550
    },
    {
      "epoch": 6.104,
      "grad_norm": 0.20975995063781738,
      "learning_rate": 1.185e-05,
      "loss": 0.002,
      "step": 91560
    },
    {
      "epoch": 6.104666666666667,
      "grad_norm": 0.10334485024213791,
      "learning_rate": 1.1845833333333334e-05,
      "loss": 0.0022,
      "step": 91570
    },
    {
      "epoch": 6.105333333333333,
      "grad_norm": 0.10746049880981445,
      "learning_rate": 1.1841666666666667e-05,
      "loss": 0.0013,
      "step": 91580
    },
    {
      "epoch": 6.106,
      "grad_norm": 0.3583942651748657,
      "learning_rate": 1.18375e-05,
      "loss": 0.002,
      "step": 91590
    },
    {
      "epoch": 6.1066666666666665,
      "grad_norm": 0.2693127691745758,
      "learning_rate": 1.1833333333333334e-05,
      "loss": 0.0028,
      "step": 91600
    },
    {
      "epoch": 6.107333333333333,
      "grad_norm": 0.3793103098869324,
      "learning_rate": 1.1829166666666667e-05,
      "loss": 0.0021,
      "step": 91610
    },
    {
      "epoch": 6.108,
      "grad_norm": 0.08203158527612686,
      "learning_rate": 1.1825e-05,
      "loss": 0.0024,
      "step": 91620
    },
    {
      "epoch": 6.108666666666666,
      "grad_norm": 0.40299031138420105,
      "learning_rate": 1.1820833333333334e-05,
      "loss": 0.0027,
      "step": 91630
    },
    {
      "epoch": 6.109333333333334,
      "grad_norm": 0.372196763753891,
      "learning_rate": 1.1816666666666668e-05,
      "loss": 0.0017,
      "step": 91640
    },
    {
      "epoch": 6.11,
      "grad_norm": 0.030693350359797478,
      "learning_rate": 1.1812499999999999e-05,
      "loss": 0.0018,
      "step": 91650
    },
    {
      "epoch": 6.110666666666667,
      "grad_norm": 0.4150174856185913,
      "learning_rate": 1.1808333333333333e-05,
      "loss": 0.0018,
      "step": 91660
    },
    {
      "epoch": 6.1113333333333335,
      "grad_norm": 0.1366262286901474,
      "learning_rate": 1.1804166666666668e-05,
      "loss": 0.002,
      "step": 91670
    },
    {
      "epoch": 6.112,
      "grad_norm": 0.08805207163095474,
      "learning_rate": 1.18e-05,
      "loss": 0.0013,
      "step": 91680
    },
    {
      "epoch": 6.112666666666667,
      "grad_norm": 0.49056944251060486,
      "learning_rate": 1.1795833333333333e-05,
      "loss": 0.0028,
      "step": 91690
    },
    {
      "epoch": 6.113333333333333,
      "grad_norm": 0.7146510481834412,
      "learning_rate": 1.1791666666666668e-05,
      "loss": 0.0017,
      "step": 91700
    },
    {
      "epoch": 6.114,
      "grad_norm": 0.4685767889022827,
      "learning_rate": 1.17875e-05,
      "loss": 0.0023,
      "step": 91710
    },
    {
      "epoch": 6.1146666666666665,
      "grad_norm": 0.04067642614245415,
      "learning_rate": 1.1783333333333333e-05,
      "loss": 0.002,
      "step": 91720
    },
    {
      "epoch": 6.115333333333333,
      "grad_norm": 0.4026896357536316,
      "learning_rate": 1.1779166666666667e-05,
      "loss": 0.0019,
      "step": 91730
    },
    {
      "epoch": 6.116,
      "grad_norm": 0.07682999968528748,
      "learning_rate": 1.1775e-05,
      "loss": 0.0023,
      "step": 91740
    },
    {
      "epoch": 6.116666666666666,
      "grad_norm": 0.1098124161362648,
      "learning_rate": 1.1770833333333334e-05,
      "loss": 0.0014,
      "step": 91750
    },
    {
      "epoch": 6.117333333333334,
      "grad_norm": 0.23799078166484833,
      "learning_rate": 1.1766666666666667e-05,
      "loss": 0.0023,
      "step": 91760
    },
    {
      "epoch": 6.118,
      "grad_norm": 0.42811596393585205,
      "learning_rate": 1.17625e-05,
      "loss": 0.0014,
      "step": 91770
    },
    {
      "epoch": 6.118666666666667,
      "grad_norm": 0.17773273587226868,
      "learning_rate": 1.1758333333333334e-05,
      "loss": 0.0017,
      "step": 91780
    },
    {
      "epoch": 6.1193333333333335,
      "grad_norm": 0.23093833029270172,
      "learning_rate": 1.1754166666666668e-05,
      "loss": 0.0021,
      "step": 91790
    },
    {
      "epoch": 6.12,
      "grad_norm": 0.3301839828491211,
      "learning_rate": 1.175e-05,
      "loss": 0.0017,
      "step": 91800
    },
    {
      "epoch": 6.120666666666667,
      "grad_norm": 0.10658301413059235,
      "learning_rate": 1.1745833333333334e-05,
      "loss": 0.0016,
      "step": 91810
    },
    {
      "epoch": 6.121333333333333,
      "grad_norm": 0.13733884692192078,
      "learning_rate": 1.1741666666666668e-05,
      "loss": 0.0017,
      "step": 91820
    },
    {
      "epoch": 6.122,
      "grad_norm": 0.10951203852891922,
      "learning_rate": 1.17375e-05,
      "loss": 0.002,
      "step": 91830
    },
    {
      "epoch": 6.1226666666666665,
      "grad_norm": 0.232084259390831,
      "learning_rate": 1.1733333333333333e-05,
      "loss": 0.0025,
      "step": 91840
    },
    {
      "epoch": 6.123333333333333,
      "grad_norm": 0.2731034755706787,
      "learning_rate": 1.1729166666666668e-05,
      "loss": 0.0019,
      "step": 91850
    },
    {
      "epoch": 6.124,
      "grad_norm": 0.47518038749694824,
      "learning_rate": 1.1725e-05,
      "loss": 0.0016,
      "step": 91860
    },
    {
      "epoch": 6.124666666666666,
      "grad_norm": 0.4667501151561737,
      "learning_rate": 1.1720833333333333e-05,
      "loss": 0.0015,
      "step": 91870
    },
    {
      "epoch": 6.125333333333334,
      "grad_norm": 0.20381127297878265,
      "learning_rate": 1.1716666666666667e-05,
      "loss": 0.0032,
      "step": 91880
    },
    {
      "epoch": 6.126,
      "grad_norm": 0.07663165032863617,
      "learning_rate": 1.1712500000000002e-05,
      "loss": 0.0017,
      "step": 91890
    },
    {
      "epoch": 6.126666666666667,
      "grad_norm": 0.2773660719394684,
      "learning_rate": 1.1708333333333334e-05,
      "loss": 0.0024,
      "step": 91900
    },
    {
      "epoch": 6.1273333333333335,
      "grad_norm": 0.34147074818611145,
      "learning_rate": 1.1704166666666667e-05,
      "loss": 0.0019,
      "step": 91910
    },
    {
      "epoch": 6.128,
      "grad_norm": 0.30559399724006653,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 0.003,
      "step": 91920
    },
    {
      "epoch": 6.128666666666667,
      "grad_norm": 0.6386788487434387,
      "learning_rate": 1.1695833333333334e-05,
      "loss": 0.0014,
      "step": 91930
    },
    {
      "epoch": 6.129333333333333,
      "grad_norm": 0.40610823035240173,
      "learning_rate": 1.1691666666666667e-05,
      "loss": 0.0016,
      "step": 91940
    },
    {
      "epoch": 6.13,
      "grad_norm": 0.917915403842926,
      "learning_rate": 1.1687500000000001e-05,
      "loss": 0.0017,
      "step": 91950
    },
    {
      "epoch": 6.1306666666666665,
      "grad_norm": 0.35503044724464417,
      "learning_rate": 1.1683333333333334e-05,
      "loss": 0.0019,
      "step": 91960
    },
    {
      "epoch": 6.131333333333333,
      "grad_norm": 0.5735518932342529,
      "learning_rate": 1.1679166666666668e-05,
      "loss": 0.0012,
      "step": 91970
    },
    {
      "epoch": 6.132,
      "grad_norm": 0.23432636260986328,
      "learning_rate": 1.1675000000000001e-05,
      "loss": 0.0015,
      "step": 91980
    },
    {
      "epoch": 6.132666666666666,
      "grad_norm": 0.47444555163383484,
      "learning_rate": 1.1670833333333334e-05,
      "loss": 0.0018,
      "step": 91990
    },
    {
      "epoch": 6.133333333333334,
      "grad_norm": 0.05015083774924278,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 0.0017,
      "step": 92000
    },
    {
      "epoch": 6.134,
      "grad_norm": 0.30541542172431946,
      "learning_rate": 1.16625e-05,
      "loss": 0.0018,
      "step": 92010
    },
    {
      "epoch": 6.134666666666667,
      "grad_norm": 0.5191846489906311,
      "learning_rate": 1.1658333333333333e-05,
      "loss": 0.002,
      "step": 92020
    },
    {
      "epoch": 6.1353333333333335,
      "grad_norm": 0.06936439126729965,
      "learning_rate": 1.1654166666666668e-05,
      "loss": 0.0016,
      "step": 92030
    },
    {
      "epoch": 6.136,
      "grad_norm": 0.40684953331947327,
      "learning_rate": 1.1650000000000002e-05,
      "loss": 0.0016,
      "step": 92040
    },
    {
      "epoch": 6.136666666666667,
      "grad_norm": 0.20613332092761993,
      "learning_rate": 1.1645833333333333e-05,
      "loss": 0.0024,
      "step": 92050
    },
    {
      "epoch": 6.137333333333333,
      "grad_norm": 0.14319856464862823,
      "learning_rate": 1.1641666666666667e-05,
      "loss": 0.002,
      "step": 92060
    },
    {
      "epoch": 6.138,
      "grad_norm": 0.4827685058116913,
      "learning_rate": 1.1637500000000002e-05,
      "loss": 0.0023,
      "step": 92070
    },
    {
      "epoch": 6.1386666666666665,
      "grad_norm": 0.2712402641773224,
      "learning_rate": 1.1633333333333334e-05,
      "loss": 0.0015,
      "step": 92080
    },
    {
      "epoch": 6.139333333333333,
      "grad_norm": 0.37089529633522034,
      "learning_rate": 1.1629166666666667e-05,
      "loss": 0.0014,
      "step": 92090
    },
    {
      "epoch": 6.14,
      "grad_norm": 0.30020061135292053,
      "learning_rate": 1.1625000000000001e-05,
      "loss": 0.0014,
      "step": 92100
    },
    {
      "epoch": 6.140666666666666,
      "grad_norm": 0.5185198783874512,
      "learning_rate": 1.1620833333333334e-05,
      "loss": 0.0017,
      "step": 92110
    },
    {
      "epoch": 6.141333333333334,
      "grad_norm": 0.5932317972183228,
      "learning_rate": 1.1616666666666667e-05,
      "loss": 0.002,
      "step": 92120
    },
    {
      "epoch": 6.142,
      "grad_norm": 0.25936853885650635,
      "learning_rate": 1.1612500000000001e-05,
      "loss": 0.0014,
      "step": 92130
    },
    {
      "epoch": 6.142666666666667,
      "grad_norm": 0.4166986048221588,
      "learning_rate": 1.1608333333333334e-05,
      "loss": 0.0026,
      "step": 92140
    },
    {
      "epoch": 6.1433333333333335,
      "grad_norm": 0.33980369567871094,
      "learning_rate": 1.1604166666666668e-05,
      "loss": 0.0018,
      "step": 92150
    },
    {
      "epoch": 6.144,
      "grad_norm": 0.09721767902374268,
      "learning_rate": 1.16e-05,
      "loss": 0.0028,
      "step": 92160
    },
    {
      "epoch": 6.144666666666667,
      "grad_norm": 0.20660918951034546,
      "learning_rate": 1.1595833333333333e-05,
      "loss": 0.0014,
      "step": 92170
    },
    {
      "epoch": 6.145333333333333,
      "grad_norm": 0.04410902410745621,
      "learning_rate": 1.1591666666666668e-05,
      "loss": 0.0013,
      "step": 92180
    },
    {
      "epoch": 6.146,
      "grad_norm": 0.6295490860939026,
      "learning_rate": 1.15875e-05,
      "loss": 0.0014,
      "step": 92190
    },
    {
      "epoch": 6.1466666666666665,
      "grad_norm": 0.02780214510858059,
      "learning_rate": 1.1583333333333333e-05,
      "loss": 0.0029,
      "step": 92200
    },
    {
      "epoch": 6.147333333333333,
      "grad_norm": 0.3051982522010803,
      "learning_rate": 1.1579166666666667e-05,
      "loss": 0.0018,
      "step": 92210
    },
    {
      "epoch": 6.148,
      "grad_norm": 0.4260464608669281,
      "learning_rate": 1.1575000000000002e-05,
      "loss": 0.0027,
      "step": 92220
    },
    {
      "epoch": 6.148666666666666,
      "grad_norm": 0.16996972262859344,
      "learning_rate": 1.1570833333333333e-05,
      "loss": 0.002,
      "step": 92230
    },
    {
      "epoch": 6.149333333333334,
      "grad_norm": 0.3054708242416382,
      "learning_rate": 1.1566666666666667e-05,
      "loss": 0.0023,
      "step": 92240
    },
    {
      "epoch": 6.15,
      "grad_norm": 0.06811346113681793,
      "learning_rate": 1.1562500000000002e-05,
      "loss": 0.002,
      "step": 92250
    },
    {
      "epoch": 6.150666666666667,
      "grad_norm": 0.3716438114643097,
      "learning_rate": 1.1558333333333334e-05,
      "loss": 0.0013,
      "step": 92260
    },
    {
      "epoch": 6.1513333333333335,
      "grad_norm": 0.10417363047599792,
      "learning_rate": 1.1554166666666667e-05,
      "loss": 0.0013,
      "step": 92270
    },
    {
      "epoch": 6.152,
      "grad_norm": 0.06795176863670349,
      "learning_rate": 1.1550000000000001e-05,
      "loss": 0.0013,
      "step": 92280
    },
    {
      "epoch": 6.152666666666667,
      "grad_norm": 0.103389210999012,
      "learning_rate": 1.1545833333333334e-05,
      "loss": 0.0019,
      "step": 92290
    },
    {
      "epoch": 6.153333333333333,
      "grad_norm": 0.04923626407980919,
      "learning_rate": 1.1541666666666667e-05,
      "loss": 0.0015,
      "step": 92300
    },
    {
      "epoch": 6.154,
      "grad_norm": 0.14428670704364777,
      "learning_rate": 1.1537500000000001e-05,
      "loss": 0.0011,
      "step": 92310
    },
    {
      "epoch": 6.1546666666666665,
      "grad_norm": 0.14641159772872925,
      "learning_rate": 1.1533333333333334e-05,
      "loss": 0.0015,
      "step": 92320
    },
    {
      "epoch": 6.155333333333333,
      "grad_norm": 0.24290581047534943,
      "learning_rate": 1.1529166666666668e-05,
      "loss": 0.0017,
      "step": 92330
    },
    {
      "epoch": 6.156,
      "grad_norm": 0.17237411439418793,
      "learning_rate": 1.1525e-05,
      "loss": 0.0021,
      "step": 92340
    },
    {
      "epoch": 6.156666666666666,
      "grad_norm": 0.40454041957855225,
      "learning_rate": 1.1520833333333333e-05,
      "loss": 0.0027,
      "step": 92350
    },
    {
      "epoch": 6.157333333333334,
      "grad_norm": 0.20637576282024384,
      "learning_rate": 1.1516666666666668e-05,
      "loss": 0.0013,
      "step": 92360
    },
    {
      "epoch": 6.158,
      "grad_norm": 0.5456802248954773,
      "learning_rate": 1.15125e-05,
      "loss": 0.0019,
      "step": 92370
    },
    {
      "epoch": 6.158666666666667,
      "grad_norm": 0.6447410583496094,
      "learning_rate": 1.1508333333333333e-05,
      "loss": 0.0014,
      "step": 92380
    },
    {
      "epoch": 6.1593333333333335,
      "grad_norm": 0.5086157321929932,
      "learning_rate": 1.1504166666666667e-05,
      "loss": 0.0014,
      "step": 92390
    },
    {
      "epoch": 6.16,
      "grad_norm": 0.2742253541946411,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 0.0017,
      "step": 92400
    },
    {
      "epoch": 6.160666666666667,
      "grad_norm": 0.3064371645450592,
      "learning_rate": 1.1495833333333333e-05,
      "loss": 0.0016,
      "step": 92410
    },
    {
      "epoch": 6.161333333333333,
      "grad_norm": 0.08132168650627136,
      "learning_rate": 1.1491666666666667e-05,
      "loss": 0.0015,
      "step": 92420
    },
    {
      "epoch": 6.162,
      "grad_norm": 0.4783237874507904,
      "learning_rate": 1.1487500000000001e-05,
      "loss": 0.0014,
      "step": 92430
    },
    {
      "epoch": 6.1626666666666665,
      "grad_norm": 0.27093103528022766,
      "learning_rate": 1.1483333333333334e-05,
      "loss": 0.0021,
      "step": 92440
    },
    {
      "epoch": 6.163333333333333,
      "grad_norm": 0.33918970823287964,
      "learning_rate": 1.1479166666666667e-05,
      "loss": 0.0014,
      "step": 92450
    },
    {
      "epoch": 6.164,
      "grad_norm": 0.07374332845211029,
      "learning_rate": 1.1475000000000001e-05,
      "loss": 0.0013,
      "step": 92460
    },
    {
      "epoch": 6.164666666666666,
      "grad_norm": 0.14249496161937714,
      "learning_rate": 1.1470833333333334e-05,
      "loss": 0.0012,
      "step": 92470
    },
    {
      "epoch": 6.165333333333333,
      "grad_norm": 0.5338664054870605,
      "learning_rate": 1.1466666666666666e-05,
      "loss": 0.0025,
      "step": 92480
    },
    {
      "epoch": 6.166,
      "grad_norm": 0.6381124258041382,
      "learning_rate": 1.14625e-05,
      "loss": 0.0015,
      "step": 92490
    },
    {
      "epoch": 6.166666666666667,
      "grad_norm": 0.028775781393051147,
      "learning_rate": 1.1458333333333333e-05,
      "loss": 0.0024,
      "step": 92500
    },
    {
      "epoch": 6.167333333333334,
      "grad_norm": 1.114298701286316,
      "learning_rate": 1.1454166666666668e-05,
      "loss": 0.002,
      "step": 92510
    },
    {
      "epoch": 6.168,
      "grad_norm": 0.3063371181488037,
      "learning_rate": 1.145e-05,
      "loss": 0.0015,
      "step": 92520
    },
    {
      "epoch": 6.168666666666667,
      "grad_norm": 0.10988140106201172,
      "learning_rate": 1.1445833333333333e-05,
      "loss": 0.0021,
      "step": 92530
    },
    {
      "epoch": 6.169333333333333,
      "grad_norm": 0.3136674165725708,
      "learning_rate": 1.1441666666666668e-05,
      "loss": 0.0018,
      "step": 92540
    },
    {
      "epoch": 6.17,
      "grad_norm": 0.1373276710510254,
      "learning_rate": 1.14375e-05,
      "loss": 0.0022,
      "step": 92550
    },
    {
      "epoch": 6.1706666666666665,
      "grad_norm": 0.0702129676938057,
      "learning_rate": 1.1433333333333333e-05,
      "loss": 0.0018,
      "step": 92560
    },
    {
      "epoch": 6.171333333333333,
      "grad_norm": 0.07746318727731705,
      "learning_rate": 1.1429166666666667e-05,
      "loss": 0.0012,
      "step": 92570
    },
    {
      "epoch": 6.172,
      "grad_norm": 0.4103679060935974,
      "learning_rate": 1.1425000000000002e-05,
      "loss": 0.0018,
      "step": 92580
    },
    {
      "epoch": 6.172666666666666,
      "grad_norm": 0.13893663883209229,
      "learning_rate": 1.1420833333333333e-05,
      "loss": 0.0016,
      "step": 92590
    },
    {
      "epoch": 6.173333333333334,
      "grad_norm": 0.32398152351379395,
      "learning_rate": 1.1416666666666667e-05,
      "loss": 0.0025,
      "step": 92600
    },
    {
      "epoch": 6.174,
      "grad_norm": 0.6455736756324768,
      "learning_rate": 1.1412500000000001e-05,
      "loss": 0.0022,
      "step": 92610
    },
    {
      "epoch": 6.174666666666667,
      "grad_norm": 0.4874032735824585,
      "learning_rate": 1.1408333333333334e-05,
      "loss": 0.0023,
      "step": 92620
    },
    {
      "epoch": 6.175333333333334,
      "grad_norm": 0.07762860506772995,
      "learning_rate": 1.1404166666666667e-05,
      "loss": 0.0013,
      "step": 92630
    },
    {
      "epoch": 6.176,
      "grad_norm": 0.11071190983057022,
      "learning_rate": 1.1400000000000001e-05,
      "loss": 0.0016,
      "step": 92640
    },
    {
      "epoch": 6.176666666666667,
      "grad_norm": 0.04560956731438637,
      "learning_rate": 1.1395833333333334e-05,
      "loss": 0.002,
      "step": 92650
    },
    {
      "epoch": 6.177333333333333,
      "grad_norm": 0.11022859066724777,
      "learning_rate": 1.1391666666666668e-05,
      "loss": 0.0014,
      "step": 92660
    },
    {
      "epoch": 6.178,
      "grad_norm": 0.8423048853874207,
      "learning_rate": 1.13875e-05,
      "loss": 0.0012,
      "step": 92670
    },
    {
      "epoch": 6.1786666666666665,
      "grad_norm": 0.31231358647346497,
      "learning_rate": 1.1383333333333333e-05,
      "loss": 0.0026,
      "step": 92680
    },
    {
      "epoch": 6.179333333333333,
      "grad_norm": 0.5795530080795288,
      "learning_rate": 1.1379166666666668e-05,
      "loss": 0.002,
      "step": 92690
    },
    {
      "epoch": 6.18,
      "grad_norm": 0.3931659162044525,
      "learning_rate": 1.1375e-05,
      "loss": 0.0016,
      "step": 92700
    },
    {
      "epoch": 6.180666666666666,
      "grad_norm": 0.3466031551361084,
      "learning_rate": 1.1370833333333333e-05,
      "loss": 0.002,
      "step": 92710
    },
    {
      "epoch": 6.181333333333333,
      "grad_norm": 0.17291469871997833,
      "learning_rate": 1.1366666666666667e-05,
      "loss": 0.0022,
      "step": 92720
    },
    {
      "epoch": 6.182,
      "grad_norm": 0.3097125291824341,
      "learning_rate": 1.1362500000000002e-05,
      "loss": 0.0021,
      "step": 92730
    },
    {
      "epoch": 6.182666666666667,
      "grad_norm": 0.23690620064735413,
      "learning_rate": 1.1358333333333333e-05,
      "loss": 0.0013,
      "step": 92740
    },
    {
      "epoch": 6.183333333333334,
      "grad_norm": 0.3800814151763916,
      "learning_rate": 1.1354166666666667e-05,
      "loss": 0.0019,
      "step": 92750
    },
    {
      "epoch": 6.184,
      "grad_norm": 0.03980162739753723,
      "learning_rate": 1.1350000000000001e-05,
      "loss": 0.0017,
      "step": 92760
    },
    {
      "epoch": 6.184666666666667,
      "grad_norm": 0.0825696662068367,
      "learning_rate": 1.1345833333333334e-05,
      "loss": 0.0012,
      "step": 92770
    },
    {
      "epoch": 6.185333333333333,
      "grad_norm": 0.08463291078805923,
      "learning_rate": 1.1341666666666667e-05,
      "loss": 0.0017,
      "step": 92780
    },
    {
      "epoch": 6.186,
      "grad_norm": 0.1713787317276001,
      "learning_rate": 1.1337500000000001e-05,
      "loss": 0.0023,
      "step": 92790
    },
    {
      "epoch": 6.1866666666666665,
      "grad_norm": 0.4797501862049103,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 0.0021,
      "step": 92800
    },
    {
      "epoch": 6.187333333333333,
      "grad_norm": 0.2369142472743988,
      "learning_rate": 1.1329166666666666e-05,
      "loss": 0.0024,
      "step": 92810
    },
    {
      "epoch": 6.188,
      "grad_norm": 0.3434922695159912,
      "learning_rate": 1.1325e-05,
      "loss": 0.0022,
      "step": 92820
    },
    {
      "epoch": 6.188666666666666,
      "grad_norm": 0.24098235368728638,
      "learning_rate": 1.1320833333333334e-05,
      "loss": 0.0015,
      "step": 92830
    },
    {
      "epoch": 6.189333333333333,
      "grad_norm": 0.047922976315021515,
      "learning_rate": 1.1316666666666668e-05,
      "loss": 0.0024,
      "step": 92840
    },
    {
      "epoch": 6.19,
      "grad_norm": 0.17367622256278992,
      "learning_rate": 1.13125e-05,
      "loss": 0.0019,
      "step": 92850
    },
    {
      "epoch": 6.190666666666667,
      "grad_norm": 0.31452134251594543,
      "learning_rate": 1.1308333333333333e-05,
      "loss": 0.0019,
      "step": 92860
    },
    {
      "epoch": 6.191333333333334,
      "grad_norm": 0.46878960728645325,
      "learning_rate": 1.1304166666666668e-05,
      "loss": 0.0017,
      "step": 92870
    },
    {
      "epoch": 6.192,
      "grad_norm": 0.5849286317825317,
      "learning_rate": 1.13e-05,
      "loss": 0.0017,
      "step": 92880
    },
    {
      "epoch": 6.192666666666667,
      "grad_norm": 0.05378517508506775,
      "learning_rate": 1.1295833333333333e-05,
      "loss": 0.0023,
      "step": 92890
    },
    {
      "epoch": 6.193333333333333,
      "grad_norm": 0.7209771275520325,
      "learning_rate": 1.1291666666666667e-05,
      "loss": 0.0017,
      "step": 92900
    },
    {
      "epoch": 6.194,
      "grad_norm": 0.21672257781028748,
      "learning_rate": 1.1287500000000002e-05,
      "loss": 0.0025,
      "step": 92910
    },
    {
      "epoch": 6.1946666666666665,
      "grad_norm": 0.17621538043022156,
      "learning_rate": 1.1283333333333333e-05,
      "loss": 0.002,
      "step": 92920
    },
    {
      "epoch": 6.195333333333333,
      "grad_norm": 0.04213275387883186,
      "learning_rate": 1.1279166666666667e-05,
      "loss": 0.0013,
      "step": 92930
    },
    {
      "epoch": 6.196,
      "grad_norm": 0.14064453542232513,
      "learning_rate": 1.1275000000000001e-05,
      "loss": 0.002,
      "step": 92940
    },
    {
      "epoch": 6.196666666666666,
      "grad_norm": 0.4823607802391052,
      "learning_rate": 1.1270833333333334e-05,
      "loss": 0.0012,
      "step": 92950
    },
    {
      "epoch": 6.197333333333333,
      "grad_norm": 0.11488474905490875,
      "learning_rate": 1.1266666666666667e-05,
      "loss": 0.0013,
      "step": 92960
    },
    {
      "epoch": 6.198,
      "grad_norm": 0.21147863566875458,
      "learning_rate": 1.1262500000000001e-05,
      "loss": 0.0022,
      "step": 92970
    },
    {
      "epoch": 6.198666666666667,
      "grad_norm": 0.11598655581474304,
      "learning_rate": 1.1258333333333334e-05,
      "loss": 0.0017,
      "step": 92980
    },
    {
      "epoch": 6.199333333333334,
      "grad_norm": 0.13903023302555084,
      "learning_rate": 1.1254166666666666e-05,
      "loss": 0.0015,
      "step": 92990
    },
    {
      "epoch": 6.2,
      "grad_norm": 0.20402836799621582,
      "learning_rate": 1.125e-05,
      "loss": 0.0018,
      "step": 93000
    },
    {
      "epoch": 6.200666666666667,
      "grad_norm": 0.1420239508152008,
      "learning_rate": 1.1245833333333333e-05,
      "loss": 0.0015,
      "step": 93010
    },
    {
      "epoch": 6.201333333333333,
      "grad_norm": 0.3374670743942261,
      "learning_rate": 1.1241666666666668e-05,
      "loss": 0.0017,
      "step": 93020
    },
    {
      "epoch": 6.202,
      "grad_norm": 0.09929300844669342,
      "learning_rate": 1.12375e-05,
      "loss": 0.0021,
      "step": 93030
    },
    {
      "epoch": 6.2026666666666666,
      "grad_norm": 0.3738310635089874,
      "learning_rate": 1.1233333333333333e-05,
      "loss": 0.0022,
      "step": 93040
    },
    {
      "epoch": 6.203333333333333,
      "grad_norm": 0.07607805728912354,
      "learning_rate": 1.1229166666666667e-05,
      "loss": 0.002,
      "step": 93050
    },
    {
      "epoch": 6.204,
      "grad_norm": 0.1791410893201828,
      "learning_rate": 1.1225e-05,
      "loss": 0.0017,
      "step": 93060
    },
    {
      "epoch": 6.204666666666666,
      "grad_norm": 0.11431293934583664,
      "learning_rate": 1.1220833333333333e-05,
      "loss": 0.0021,
      "step": 93070
    },
    {
      "epoch": 6.205333333333333,
      "grad_norm": 0.48538193106651306,
      "learning_rate": 1.1216666666666667e-05,
      "loss": 0.0022,
      "step": 93080
    },
    {
      "epoch": 6.206,
      "grad_norm": 0.05968736857175827,
      "learning_rate": 1.1212500000000001e-05,
      "loss": 0.0019,
      "step": 93090
    },
    {
      "epoch": 6.206666666666667,
      "grad_norm": 0.4693728983402252,
      "learning_rate": 1.1208333333333332e-05,
      "loss": 0.0022,
      "step": 93100
    },
    {
      "epoch": 6.207333333333334,
      "grad_norm": 0.2066446840763092,
      "learning_rate": 1.1204166666666667e-05,
      "loss": 0.0018,
      "step": 93110
    },
    {
      "epoch": 6.208,
      "grad_norm": 0.2746908664703369,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.002,
      "step": 93120
    },
    {
      "epoch": 6.208666666666667,
      "grad_norm": 0.2560984194278717,
      "learning_rate": 1.1195833333333334e-05,
      "loss": 0.0028,
      "step": 93130
    },
    {
      "epoch": 6.209333333333333,
      "grad_norm": 0.08480706810951233,
      "learning_rate": 1.1191666666666667e-05,
      "loss": 0.0025,
      "step": 93140
    },
    {
      "epoch": 6.21,
      "grad_norm": 0.5726766586303711,
      "learning_rate": 1.1187500000000001e-05,
      "loss": 0.0019,
      "step": 93150
    },
    {
      "epoch": 6.210666666666667,
      "grad_norm": 0.44068458676338196,
      "learning_rate": 1.1183333333333335e-05,
      "loss": 0.0012,
      "step": 93160
    },
    {
      "epoch": 6.211333333333333,
      "grad_norm": 0.48093241453170776,
      "learning_rate": 1.1179166666666666e-05,
      "loss": 0.0019,
      "step": 93170
    },
    {
      "epoch": 6.212,
      "grad_norm": 0.20212745666503906,
      "learning_rate": 1.1175e-05,
      "loss": 0.0025,
      "step": 93180
    },
    {
      "epoch": 6.212666666666666,
      "grad_norm": 0.1457628607749939,
      "learning_rate": 1.1170833333333335e-05,
      "loss": 0.0013,
      "step": 93190
    },
    {
      "epoch": 6.213333333333333,
      "grad_norm": 0.20376735925674438,
      "learning_rate": 1.1166666666666668e-05,
      "loss": 0.0021,
      "step": 93200
    },
    {
      "epoch": 6.214,
      "grad_norm": 0.5359683632850647,
      "learning_rate": 1.11625e-05,
      "loss": 0.0023,
      "step": 93210
    },
    {
      "epoch": 6.214666666666667,
      "grad_norm": 0.20564420521259308,
      "learning_rate": 1.1158333333333335e-05,
      "loss": 0.0027,
      "step": 93220
    },
    {
      "epoch": 6.215333333333334,
      "grad_norm": 0.20159310102462769,
      "learning_rate": 1.1154166666666667e-05,
      "loss": 0.0022,
      "step": 93230
    },
    {
      "epoch": 6.216,
      "grad_norm": 0.101211778819561,
      "learning_rate": 1.115e-05,
      "loss": 0.0021,
      "step": 93240
    },
    {
      "epoch": 6.216666666666667,
      "grad_norm": 0.21386854350566864,
      "learning_rate": 1.1145833333333334e-05,
      "loss": 0.0022,
      "step": 93250
    },
    {
      "epoch": 6.217333333333333,
      "grad_norm": 0.36638176441192627,
      "learning_rate": 1.1141666666666667e-05,
      "loss": 0.0029,
      "step": 93260
    },
    {
      "epoch": 6.218,
      "grad_norm": 0.5804647207260132,
      "learning_rate": 1.1137500000000001e-05,
      "loss": 0.0022,
      "step": 93270
    },
    {
      "epoch": 6.218666666666667,
      "grad_norm": 0.30942779779434204,
      "learning_rate": 1.1133333333333334e-05,
      "loss": 0.0021,
      "step": 93280
    },
    {
      "epoch": 6.219333333333333,
      "grad_norm": 0.6778637170791626,
      "learning_rate": 1.1129166666666667e-05,
      "loss": 0.002,
      "step": 93290
    },
    {
      "epoch": 6.22,
      "grad_norm": 0.08695909380912781,
      "learning_rate": 1.1125000000000001e-05,
      "loss": 0.0014,
      "step": 93300
    },
    {
      "epoch": 6.220666666666666,
      "grad_norm": 1.0329742431640625,
      "learning_rate": 1.1120833333333334e-05,
      "loss": 0.0021,
      "step": 93310
    },
    {
      "epoch": 6.221333333333333,
      "grad_norm": 0.024746647104620934,
      "learning_rate": 1.1116666666666666e-05,
      "loss": 0.0019,
      "step": 93320
    },
    {
      "epoch": 6.222,
      "grad_norm": 0.17225120961666107,
      "learning_rate": 1.11125e-05,
      "loss": 0.0015,
      "step": 93330
    },
    {
      "epoch": 6.222666666666667,
      "grad_norm": 0.10682971775531769,
      "learning_rate": 1.1108333333333335e-05,
      "loss": 0.0021,
      "step": 93340
    },
    {
      "epoch": 6.223333333333334,
      "grad_norm": 0.3365596532821655,
      "learning_rate": 1.1104166666666666e-05,
      "loss": 0.0017,
      "step": 93350
    },
    {
      "epoch": 6.224,
      "grad_norm": 0.2336752861738205,
      "learning_rate": 1.11e-05,
      "loss": 0.0017,
      "step": 93360
    },
    {
      "epoch": 6.224666666666667,
      "grad_norm": 0.09490744769573212,
      "learning_rate": 1.1095833333333335e-05,
      "loss": 0.0019,
      "step": 93370
    },
    {
      "epoch": 6.225333333333333,
      "grad_norm": 0.07565217465162277,
      "learning_rate": 1.1091666666666667e-05,
      "loss": 0.0015,
      "step": 93380
    },
    {
      "epoch": 6.226,
      "grad_norm": 0.07688216865062714,
      "learning_rate": 1.10875e-05,
      "loss": 0.0017,
      "step": 93390
    },
    {
      "epoch": 6.226666666666667,
      "grad_norm": 0.20969556272029877,
      "learning_rate": 1.1083333333333335e-05,
      "loss": 0.0017,
      "step": 93400
    },
    {
      "epoch": 6.227333333333333,
      "grad_norm": 0.3067580759525299,
      "learning_rate": 1.1079166666666667e-05,
      "loss": 0.0019,
      "step": 93410
    },
    {
      "epoch": 6.228,
      "grad_norm": 0.23981696367263794,
      "learning_rate": 1.1075e-05,
      "loss": 0.0024,
      "step": 93420
    },
    {
      "epoch": 6.228666666666666,
      "grad_norm": 0.07140466570854187,
      "learning_rate": 1.1070833333333334e-05,
      "loss": 0.0023,
      "step": 93430
    },
    {
      "epoch": 6.229333333333333,
      "grad_norm": 0.11170163005590439,
      "learning_rate": 1.1066666666666667e-05,
      "loss": 0.0015,
      "step": 93440
    },
    {
      "epoch": 6.23,
      "grad_norm": 0.3396560251712799,
      "learning_rate": 1.1062500000000001e-05,
      "loss": 0.0022,
      "step": 93450
    },
    {
      "epoch": 6.230666666666667,
      "grad_norm": 0.14697299897670746,
      "learning_rate": 1.1058333333333334e-05,
      "loss": 0.0013,
      "step": 93460
    },
    {
      "epoch": 6.231333333333334,
      "grad_norm": 0.36066222190856934,
      "learning_rate": 1.1054166666666667e-05,
      "loss": 0.0016,
      "step": 93470
    },
    {
      "epoch": 6.232,
      "grad_norm": 0.6302253007888794,
      "learning_rate": 1.1050000000000001e-05,
      "loss": 0.0021,
      "step": 93480
    },
    {
      "epoch": 6.232666666666667,
      "grad_norm": 0.11101356148719788,
      "learning_rate": 1.1045833333333334e-05,
      "loss": 0.0018,
      "step": 93490
    },
    {
      "epoch": 6.233333333333333,
      "grad_norm": 0.24252811074256897,
      "learning_rate": 1.1041666666666666e-05,
      "loss": 0.002,
      "step": 93500
    },
    {
      "epoch": 6.234,
      "grad_norm": 0.33273550868034363,
      "learning_rate": 1.10375e-05,
      "loss": 0.0014,
      "step": 93510
    },
    {
      "epoch": 6.234666666666667,
      "grad_norm": 0.03534664958715439,
      "learning_rate": 1.1033333333333335e-05,
      "loss": 0.0016,
      "step": 93520
    },
    {
      "epoch": 6.235333333333333,
      "grad_norm": 0.1679418832063675,
      "learning_rate": 1.1029166666666668e-05,
      "loss": 0.0013,
      "step": 93530
    },
    {
      "epoch": 6.236,
      "grad_norm": 0.05122235417366028,
      "learning_rate": 1.1025e-05,
      "loss": 0.0013,
      "step": 93540
    },
    {
      "epoch": 6.236666666666666,
      "grad_norm": 0.04123637452721596,
      "learning_rate": 1.1020833333333335e-05,
      "loss": 0.0015,
      "step": 93550
    },
    {
      "epoch": 6.237333333333333,
      "grad_norm": 0.04033152386546135,
      "learning_rate": 1.1016666666666667e-05,
      "loss": 0.0017,
      "step": 93560
    },
    {
      "epoch": 6.2379999999999995,
      "grad_norm": 0.030648652464151382,
      "learning_rate": 1.10125e-05,
      "loss": 0.0012,
      "step": 93570
    },
    {
      "epoch": 6.238666666666667,
      "grad_norm": 0.7054093480110168,
      "learning_rate": 1.1008333333333334e-05,
      "loss": 0.0016,
      "step": 93580
    },
    {
      "epoch": 6.239333333333334,
      "grad_norm": 0.30448925495147705,
      "learning_rate": 1.1004166666666667e-05,
      "loss": 0.0017,
      "step": 93590
    },
    {
      "epoch": 6.24,
      "grad_norm": 0.07440873235464096,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.0022,
      "step": 93600
    },
    {
      "epoch": 6.240666666666667,
      "grad_norm": 0.33068057894706726,
      "learning_rate": 1.0995833333333334e-05,
      "loss": 0.0017,
      "step": 93610
    },
    {
      "epoch": 6.241333333333333,
      "grad_norm": 0.4380018711090088,
      "learning_rate": 1.0991666666666667e-05,
      "loss": 0.0015,
      "step": 93620
    },
    {
      "epoch": 6.242,
      "grad_norm": 0.3916185796260834,
      "learning_rate": 1.0987500000000001e-05,
      "loss": 0.0013,
      "step": 93630
    },
    {
      "epoch": 6.242666666666667,
      "grad_norm": 0.10474114120006561,
      "learning_rate": 1.0983333333333334e-05,
      "loss": 0.0015,
      "step": 93640
    },
    {
      "epoch": 6.243333333333333,
      "grad_norm": 0.17803724110126495,
      "learning_rate": 1.0979166666666666e-05,
      "loss": 0.0018,
      "step": 93650
    },
    {
      "epoch": 6.244,
      "grad_norm": 0.08021032065153122,
      "learning_rate": 1.0975e-05,
      "loss": 0.002,
      "step": 93660
    },
    {
      "epoch": 6.244666666666666,
      "grad_norm": 0.14260925352573395,
      "learning_rate": 1.0970833333333335e-05,
      "loss": 0.0014,
      "step": 93670
    },
    {
      "epoch": 6.245333333333333,
      "grad_norm": 0.08000028133392334,
      "learning_rate": 1.0966666666666666e-05,
      "loss": 0.0026,
      "step": 93680
    },
    {
      "epoch": 6.246,
      "grad_norm": 0.23298363387584686,
      "learning_rate": 1.09625e-05,
      "loss": 0.002,
      "step": 93690
    },
    {
      "epoch": 6.246666666666667,
      "grad_norm": 0.30556347966194153,
      "learning_rate": 1.0958333333333335e-05,
      "loss": 0.0015,
      "step": 93700
    },
    {
      "epoch": 6.247333333333334,
      "grad_norm": 0.06971757858991623,
      "learning_rate": 1.0954166666666668e-05,
      "loss": 0.002,
      "step": 93710
    },
    {
      "epoch": 6.248,
      "grad_norm": 0.37294653058052063,
      "learning_rate": 1.095e-05,
      "loss": 0.0021,
      "step": 93720
    },
    {
      "epoch": 6.248666666666667,
      "grad_norm": 0.44536682963371277,
      "learning_rate": 1.0945833333333335e-05,
      "loss": 0.002,
      "step": 93730
    },
    {
      "epoch": 6.249333333333333,
      "grad_norm": 0.08191337436437607,
      "learning_rate": 1.0941666666666667e-05,
      "loss": 0.0019,
      "step": 93740
    },
    {
      "epoch": 6.25,
      "grad_norm": 0.044811222702264786,
      "learning_rate": 1.09375e-05,
      "loss": 0.0017,
      "step": 93750
    },
    {
      "epoch": 6.250666666666667,
      "grad_norm": 0.1081160232424736,
      "learning_rate": 1.0933333333333334e-05,
      "loss": 0.0022,
      "step": 93760
    },
    {
      "epoch": 6.251333333333333,
      "grad_norm": 0.3796998858451843,
      "learning_rate": 1.0929166666666667e-05,
      "loss": 0.0027,
      "step": 93770
    },
    {
      "epoch": 6.252,
      "grad_norm": 0.20715856552124023,
      "learning_rate": 1.0925000000000001e-05,
      "loss": 0.002,
      "step": 93780
    },
    {
      "epoch": 6.252666666666666,
      "grad_norm": 0.03531313315033913,
      "learning_rate": 1.0920833333333334e-05,
      "loss": 0.0015,
      "step": 93790
    },
    {
      "epoch": 6.253333333333333,
      "grad_norm": 0.23825310170650482,
      "learning_rate": 1.0916666666666667e-05,
      "loss": 0.002,
      "step": 93800
    },
    {
      "epoch": 6.254,
      "grad_norm": 0.07919346541166306,
      "learning_rate": 1.0912500000000001e-05,
      "loss": 0.0025,
      "step": 93810
    },
    {
      "epoch": 6.254666666666667,
      "grad_norm": 0.5625881552696228,
      "learning_rate": 1.0908333333333334e-05,
      "loss": 0.0023,
      "step": 93820
    },
    {
      "epoch": 6.255333333333334,
      "grad_norm": 0.744511604309082,
      "learning_rate": 1.0904166666666666e-05,
      "loss": 0.0018,
      "step": 93830
    },
    {
      "epoch": 6.256,
      "grad_norm": 0.17094986140727997,
      "learning_rate": 1.09e-05,
      "loss": 0.0024,
      "step": 93840
    },
    {
      "epoch": 6.256666666666667,
      "grad_norm": 0.23956604301929474,
      "learning_rate": 1.0895833333333335e-05,
      "loss": 0.0015,
      "step": 93850
    },
    {
      "epoch": 6.257333333333333,
      "grad_norm": 0.407413512468338,
      "learning_rate": 1.0891666666666666e-05,
      "loss": 0.0019,
      "step": 93860
    },
    {
      "epoch": 6.258,
      "grad_norm": 0.3031688332557678,
      "learning_rate": 1.08875e-05,
      "loss": 0.0023,
      "step": 93870
    },
    {
      "epoch": 6.258666666666667,
      "grad_norm": 0.110453300178051,
      "learning_rate": 1.0883333333333335e-05,
      "loss": 0.0025,
      "step": 93880
    },
    {
      "epoch": 6.259333333333333,
      "grad_norm": 0.26909077167510986,
      "learning_rate": 1.0879166666666667e-05,
      "loss": 0.0018,
      "step": 93890
    },
    {
      "epoch": 6.26,
      "grad_norm": 0.20368437469005585,
      "learning_rate": 1.0875e-05,
      "loss": 0.0015,
      "step": 93900
    },
    {
      "epoch": 6.260666666666666,
      "grad_norm": 0.23931480944156647,
      "learning_rate": 1.0870833333333334e-05,
      "loss": 0.0021,
      "step": 93910
    },
    {
      "epoch": 6.261333333333333,
      "grad_norm": 0.10758481174707413,
      "learning_rate": 1.0866666666666667e-05,
      "loss": 0.0014,
      "step": 93920
    },
    {
      "epoch": 6.2620000000000005,
      "grad_norm": 0.26836106181144714,
      "learning_rate": 1.08625e-05,
      "loss": 0.0017,
      "step": 93930
    },
    {
      "epoch": 6.262666666666667,
      "grad_norm": 0.3168289363384247,
      "learning_rate": 1.0858333333333334e-05,
      "loss": 0.0017,
      "step": 93940
    },
    {
      "epoch": 6.263333333333334,
      "grad_norm": 0.11175964772701263,
      "learning_rate": 1.0854166666666667e-05,
      "loss": 0.0022,
      "step": 93950
    },
    {
      "epoch": 6.264,
      "grad_norm": 0.39024028182029724,
      "learning_rate": 1.0850000000000001e-05,
      "loss": 0.0018,
      "step": 93960
    },
    {
      "epoch": 6.264666666666667,
      "grad_norm": 0.09925530105829239,
      "learning_rate": 1.0845833333333334e-05,
      "loss": 0.0018,
      "step": 93970
    },
    {
      "epoch": 6.265333333333333,
      "grad_norm": 0.04647665470838547,
      "learning_rate": 1.0841666666666666e-05,
      "loss": 0.0014,
      "step": 93980
    },
    {
      "epoch": 6.266,
      "grad_norm": 0.3986198306083679,
      "learning_rate": 1.08375e-05,
      "loss": 0.0017,
      "step": 93990
    },
    {
      "epoch": 6.266666666666667,
      "grad_norm": 0.1459864228963852,
      "learning_rate": 1.0833333333333334e-05,
      "loss": 0.0018,
      "step": 94000
    },
    {
      "epoch": 6.267333333333333,
      "grad_norm": 0.1370880901813507,
      "learning_rate": 1.0829166666666666e-05,
      "loss": 0.0019,
      "step": 94010
    },
    {
      "epoch": 6.268,
      "grad_norm": 0.3038448393344879,
      "learning_rate": 1.0825e-05,
      "loss": 0.0015,
      "step": 94020
    },
    {
      "epoch": 6.268666666666666,
      "grad_norm": 0.12382977455854416,
      "learning_rate": 1.0820833333333335e-05,
      "loss": 0.0019,
      "step": 94030
    },
    {
      "epoch": 6.269333333333333,
      "grad_norm": 0.5382028818130493,
      "learning_rate": 1.0816666666666666e-05,
      "loss": 0.0014,
      "step": 94040
    },
    {
      "epoch": 6.27,
      "grad_norm": 0.09184576570987701,
      "learning_rate": 1.08125e-05,
      "loss": 0.0014,
      "step": 94050
    },
    {
      "epoch": 6.270666666666667,
      "grad_norm": 0.07836496084928513,
      "learning_rate": 1.0808333333333335e-05,
      "loss": 0.0019,
      "step": 94060
    },
    {
      "epoch": 6.271333333333334,
      "grad_norm": 0.2739674150943756,
      "learning_rate": 1.0804166666666667e-05,
      "loss": 0.0014,
      "step": 94070
    },
    {
      "epoch": 6.272,
      "grad_norm": 0.04898909479379654,
      "learning_rate": 1.08e-05,
      "loss": 0.0022,
      "step": 94080
    },
    {
      "epoch": 6.272666666666667,
      "grad_norm": 0.5803175568580627,
      "learning_rate": 1.0795833333333334e-05,
      "loss": 0.0013,
      "step": 94090
    },
    {
      "epoch": 6.273333333333333,
      "grad_norm": 0.5099289417266846,
      "learning_rate": 1.0791666666666667e-05,
      "loss": 0.0023,
      "step": 94100
    },
    {
      "epoch": 6.274,
      "grad_norm": 0.786203145980835,
      "learning_rate": 1.07875e-05,
      "loss": 0.0013,
      "step": 94110
    },
    {
      "epoch": 6.274666666666667,
      "grad_norm": 0.12926822900772095,
      "learning_rate": 1.0783333333333334e-05,
      "loss": 0.0019,
      "step": 94120
    },
    {
      "epoch": 6.275333333333333,
      "grad_norm": 0.37144431471824646,
      "learning_rate": 1.0779166666666667e-05,
      "loss": 0.0021,
      "step": 94130
    },
    {
      "epoch": 6.276,
      "grad_norm": 0.2374102771282196,
      "learning_rate": 1.0775000000000001e-05,
      "loss": 0.0017,
      "step": 94140
    },
    {
      "epoch": 6.276666666666666,
      "grad_norm": 0.04000002518296242,
      "learning_rate": 1.0770833333333334e-05,
      "loss": 0.0016,
      "step": 94150
    },
    {
      "epoch": 6.277333333333333,
      "grad_norm": 0.07466073334217072,
      "learning_rate": 1.0766666666666666e-05,
      "loss": 0.0026,
      "step": 94160
    },
    {
      "epoch": 6.2780000000000005,
      "grad_norm": 0.04900311678647995,
      "learning_rate": 1.07625e-05,
      "loss": 0.0021,
      "step": 94170
    },
    {
      "epoch": 6.278666666666667,
      "grad_norm": 0.026900842785835266,
      "learning_rate": 1.0758333333333333e-05,
      "loss": 0.0014,
      "step": 94180
    },
    {
      "epoch": 6.279333333333334,
      "grad_norm": 0.2683320939540863,
      "learning_rate": 1.0754166666666666e-05,
      "loss": 0.0015,
      "step": 94190
    },
    {
      "epoch": 6.28,
      "grad_norm": 0.16189169883728027,
      "learning_rate": 1.075e-05,
      "loss": 0.0029,
      "step": 94200
    },
    {
      "epoch": 6.280666666666667,
      "grad_norm": 0.665628969669342,
      "learning_rate": 1.0745833333333335e-05,
      "loss": 0.0019,
      "step": 94210
    },
    {
      "epoch": 6.281333333333333,
      "grad_norm": 0.21494923532009125,
      "learning_rate": 1.0741666666666666e-05,
      "loss": 0.0013,
      "step": 94220
    },
    {
      "epoch": 6.282,
      "grad_norm": 0.07504962384700775,
      "learning_rate": 1.07375e-05,
      "loss": 0.0018,
      "step": 94230
    },
    {
      "epoch": 6.282666666666667,
      "grad_norm": 0.2717815637588501,
      "learning_rate": 1.0733333333333334e-05,
      "loss": 0.0015,
      "step": 94240
    },
    {
      "epoch": 6.283333333333333,
      "grad_norm": 0.41781291365623474,
      "learning_rate": 1.0729166666666667e-05,
      "loss": 0.0026,
      "step": 94250
    },
    {
      "epoch": 6.284,
      "grad_norm": 0.17345105111598969,
      "learning_rate": 1.0725e-05,
      "loss": 0.0014,
      "step": 94260
    },
    {
      "epoch": 6.284666666666666,
      "grad_norm": 0.11217619478702545,
      "learning_rate": 1.0720833333333334e-05,
      "loss": 0.0021,
      "step": 94270
    },
    {
      "epoch": 6.285333333333333,
      "grad_norm": 0.2045293152332306,
      "learning_rate": 1.0716666666666667e-05,
      "loss": 0.002,
      "step": 94280
    },
    {
      "epoch": 6.286,
      "grad_norm": 0.44168317317962646,
      "learning_rate": 1.07125e-05,
      "loss": 0.0014,
      "step": 94290
    },
    {
      "epoch": 6.286666666666667,
      "grad_norm": 0.5223336815834045,
      "learning_rate": 1.0708333333333334e-05,
      "loss": 0.0022,
      "step": 94300
    },
    {
      "epoch": 6.287333333333334,
      "grad_norm": 0.10610896348953247,
      "learning_rate": 1.0704166666666667e-05,
      "loss": 0.0012,
      "step": 94310
    },
    {
      "epoch": 6.288,
      "grad_norm": 0.051838893443346024,
      "learning_rate": 1.0700000000000001e-05,
      "loss": 0.002,
      "step": 94320
    },
    {
      "epoch": 6.288666666666667,
      "grad_norm": 0.6377713680267334,
      "learning_rate": 1.0695833333333334e-05,
      "loss": 0.0017,
      "step": 94330
    },
    {
      "epoch": 6.289333333333333,
      "grad_norm": 0.03996136784553528,
      "learning_rate": 1.0691666666666666e-05,
      "loss": 0.0016,
      "step": 94340
    },
    {
      "epoch": 6.29,
      "grad_norm": 0.05681199952960014,
      "learning_rate": 1.06875e-05,
      "loss": 0.0024,
      "step": 94350
    },
    {
      "epoch": 6.290666666666667,
      "grad_norm": 0.498331755399704,
      "learning_rate": 1.0683333333333333e-05,
      "loss": 0.0014,
      "step": 94360
    },
    {
      "epoch": 6.291333333333333,
      "grad_norm": 0.2057974636554718,
      "learning_rate": 1.0679166666666666e-05,
      "loss": 0.0015,
      "step": 94370
    },
    {
      "epoch": 6.292,
      "grad_norm": 0.07951715588569641,
      "learning_rate": 1.0675e-05,
      "loss": 0.002,
      "step": 94380
    },
    {
      "epoch": 6.292666666666666,
      "grad_norm": 0.9124161601066589,
      "learning_rate": 1.0670833333333335e-05,
      "loss": 0.0017,
      "step": 94390
    },
    {
      "epoch": 6.293333333333333,
      "grad_norm": 0.3351474702358246,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 0.0012,
      "step": 94400
    },
    {
      "epoch": 6.294,
      "grad_norm": 0.09959471225738525,
      "learning_rate": 1.06625e-05,
      "loss": 0.002,
      "step": 94410
    },
    {
      "epoch": 6.294666666666667,
      "grad_norm": 0.37065598368644714,
      "learning_rate": 1.0658333333333334e-05,
      "loss": 0.0016,
      "step": 94420
    },
    {
      "epoch": 6.295333333333334,
      "grad_norm": 0.032667260617017746,
      "learning_rate": 1.0654166666666667e-05,
      "loss": 0.0017,
      "step": 94430
    },
    {
      "epoch": 6.296,
      "grad_norm": 0.4875280559062958,
      "learning_rate": 1.065e-05,
      "loss": 0.0021,
      "step": 94440
    },
    {
      "epoch": 6.296666666666667,
      "grad_norm": 0.1434011310338974,
      "learning_rate": 1.0645833333333334e-05,
      "loss": 0.0015,
      "step": 94450
    },
    {
      "epoch": 6.2973333333333334,
      "grad_norm": 0.2856518626213074,
      "learning_rate": 1.0641666666666668e-05,
      "loss": 0.0016,
      "step": 94460
    },
    {
      "epoch": 6.298,
      "grad_norm": 0.07562464475631714,
      "learning_rate": 1.0637500000000001e-05,
      "loss": 0.0028,
      "step": 94470
    },
    {
      "epoch": 6.298666666666667,
      "grad_norm": 0.11149322986602783,
      "learning_rate": 1.0633333333333334e-05,
      "loss": 0.0028,
      "step": 94480
    },
    {
      "epoch": 6.299333333333333,
      "grad_norm": 0.24281498789787292,
      "learning_rate": 1.0629166666666668e-05,
      "loss": 0.0017,
      "step": 94490
    },
    {
      "epoch": 6.3,
      "grad_norm": 0.13330702483654022,
      "learning_rate": 1.0625e-05,
      "loss": 0.0015,
      "step": 94500
    },
    {
      "epoch": 6.300666666666666,
      "grad_norm": 0.3361702859401703,
      "learning_rate": 1.0620833333333333e-05,
      "loss": 0.0019,
      "step": 94510
    },
    {
      "epoch": 6.301333333333333,
      "grad_norm": 0.1335434764623642,
      "learning_rate": 1.0616666666666668e-05,
      "loss": 0.0019,
      "step": 94520
    },
    {
      "epoch": 6.302,
      "grad_norm": 0.2041933834552765,
      "learning_rate": 1.06125e-05,
      "loss": 0.002,
      "step": 94530
    },
    {
      "epoch": 6.302666666666667,
      "grad_norm": 0.2306395024061203,
      "learning_rate": 1.0608333333333335e-05,
      "loss": 0.003,
      "step": 94540
    },
    {
      "epoch": 6.303333333333334,
      "grad_norm": 0.1805470734834671,
      "learning_rate": 1.0604166666666667e-05,
      "loss": 0.0017,
      "step": 94550
    },
    {
      "epoch": 6.304,
      "grad_norm": 0.3391825556755066,
      "learning_rate": 1.06e-05,
      "loss": 0.0026,
      "step": 94560
    },
    {
      "epoch": 6.304666666666667,
      "grad_norm": 0.14039672911167145,
      "learning_rate": 1.0595833333333335e-05,
      "loss": 0.0014,
      "step": 94570
    },
    {
      "epoch": 6.3053333333333335,
      "grad_norm": 0.3755146563053131,
      "learning_rate": 1.0591666666666667e-05,
      "loss": 0.0014,
      "step": 94580
    },
    {
      "epoch": 6.306,
      "grad_norm": 0.14018972218036652,
      "learning_rate": 1.05875e-05,
      "loss": 0.0024,
      "step": 94590
    },
    {
      "epoch": 6.306666666666667,
      "grad_norm": 0.24395215511322021,
      "learning_rate": 1.0583333333333334e-05,
      "loss": 0.002,
      "step": 94600
    },
    {
      "epoch": 6.307333333333333,
      "grad_norm": 0.0711795836687088,
      "learning_rate": 1.0579166666666669e-05,
      "loss": 0.0022,
      "step": 94610
    },
    {
      "epoch": 6.308,
      "grad_norm": 0.022967318072915077,
      "learning_rate": 1.0575e-05,
      "loss": 0.0022,
      "step": 94620
    },
    {
      "epoch": 6.308666666666666,
      "grad_norm": 0.37431779503822327,
      "learning_rate": 1.0570833333333334e-05,
      "loss": 0.0015,
      "step": 94630
    },
    {
      "epoch": 6.309333333333333,
      "grad_norm": 0.5430802702903748,
      "learning_rate": 1.0566666666666668e-05,
      "loss": 0.0023,
      "step": 94640
    },
    {
      "epoch": 6.31,
      "grad_norm": 0.07580003887414932,
      "learning_rate": 1.0562500000000001e-05,
      "loss": 0.0023,
      "step": 94650
    },
    {
      "epoch": 6.310666666666666,
      "grad_norm": 0.10375839471817017,
      "learning_rate": 1.0558333333333334e-05,
      "loss": 0.0014,
      "step": 94660
    },
    {
      "epoch": 6.311333333333334,
      "grad_norm": 0.10807148367166519,
      "learning_rate": 1.0554166666666668e-05,
      "loss": 0.002,
      "step": 94670
    },
    {
      "epoch": 6.312,
      "grad_norm": 0.07432930171489716,
      "learning_rate": 1.055e-05,
      "loss": 0.0019,
      "step": 94680
    },
    {
      "epoch": 6.312666666666667,
      "grad_norm": 0.33661505579948425,
      "learning_rate": 1.0545833333333333e-05,
      "loss": 0.0021,
      "step": 94690
    },
    {
      "epoch": 6.3133333333333335,
      "grad_norm": 0.437257319688797,
      "learning_rate": 1.0541666666666668e-05,
      "loss": 0.0022,
      "step": 94700
    },
    {
      "epoch": 6.314,
      "grad_norm": 0.4050543010234833,
      "learning_rate": 1.05375e-05,
      "loss": 0.0017,
      "step": 94710
    },
    {
      "epoch": 6.314666666666667,
      "grad_norm": 0.14039967954158783,
      "learning_rate": 1.0533333333333335e-05,
      "loss": 0.0016,
      "step": 94720
    },
    {
      "epoch": 6.315333333333333,
      "grad_norm": 0.03455346077680588,
      "learning_rate": 1.0529166666666667e-05,
      "loss": 0.0015,
      "step": 94730
    },
    {
      "epoch": 6.316,
      "grad_norm": 0.07332201302051544,
      "learning_rate": 1.0525e-05,
      "loss": 0.0016,
      "step": 94740
    },
    {
      "epoch": 6.316666666666666,
      "grad_norm": 0.04935605451464653,
      "learning_rate": 1.0520833333333334e-05,
      "loss": 0.0015,
      "step": 94750
    },
    {
      "epoch": 6.317333333333333,
      "grad_norm": 0.43392235040664673,
      "learning_rate": 1.0516666666666667e-05,
      "loss": 0.0014,
      "step": 94760
    },
    {
      "epoch": 6.318,
      "grad_norm": 0.13787730038166046,
      "learning_rate": 1.05125e-05,
      "loss": 0.0021,
      "step": 94770
    },
    {
      "epoch": 6.318666666666667,
      "grad_norm": 0.336702823638916,
      "learning_rate": 1.0508333333333334e-05,
      "loss": 0.0024,
      "step": 94780
    },
    {
      "epoch": 6.319333333333334,
      "grad_norm": 0.4145917296409607,
      "learning_rate": 1.0504166666666668e-05,
      "loss": 0.0012,
      "step": 94790
    },
    {
      "epoch": 6.32,
      "grad_norm": 0.23693934082984924,
      "learning_rate": 1.05e-05,
      "loss": 0.0016,
      "step": 94800
    },
    {
      "epoch": 6.320666666666667,
      "grad_norm": 0.204664409160614,
      "learning_rate": 1.0495833333333334e-05,
      "loss": 0.0021,
      "step": 94810
    },
    {
      "epoch": 6.3213333333333335,
      "grad_norm": 0.5066617131233215,
      "learning_rate": 1.0491666666666668e-05,
      "loss": 0.0022,
      "step": 94820
    },
    {
      "epoch": 6.322,
      "grad_norm": 0.3100651204586029,
      "learning_rate": 1.04875e-05,
      "loss": 0.0014,
      "step": 94830
    },
    {
      "epoch": 6.322666666666667,
      "grad_norm": 0.20968745648860931,
      "learning_rate": 1.0483333333333333e-05,
      "loss": 0.0033,
      "step": 94840
    },
    {
      "epoch": 6.323333333333333,
      "grad_norm": 0.08056354522705078,
      "learning_rate": 1.0479166666666668e-05,
      "loss": 0.002,
      "step": 94850
    },
    {
      "epoch": 6.324,
      "grad_norm": 0.38395756483078003,
      "learning_rate": 1.0475e-05,
      "loss": 0.0014,
      "step": 94860
    },
    {
      "epoch": 6.324666666666666,
      "grad_norm": 0.20889340341091156,
      "learning_rate": 1.0470833333333333e-05,
      "loss": 0.0015,
      "step": 94870
    },
    {
      "epoch": 6.325333333333333,
      "grad_norm": 0.4046512246131897,
      "learning_rate": 1.0466666666666668e-05,
      "loss": 0.0019,
      "step": 94880
    },
    {
      "epoch": 6.326,
      "grad_norm": 0.1144062951207161,
      "learning_rate": 1.04625e-05,
      "loss": 0.0022,
      "step": 94890
    },
    {
      "epoch": 6.326666666666666,
      "grad_norm": 0.10665883123874664,
      "learning_rate": 1.0458333333333335e-05,
      "loss": 0.0013,
      "step": 94900
    },
    {
      "epoch": 6.327333333333334,
      "grad_norm": 0.3385312557220459,
      "learning_rate": 1.0454166666666667e-05,
      "loss": 0.0023,
      "step": 94910
    },
    {
      "epoch": 6.328,
      "grad_norm": 0.07526946067810059,
      "learning_rate": 1.045e-05,
      "loss": 0.0015,
      "step": 94920
    },
    {
      "epoch": 6.328666666666667,
      "grad_norm": 0.04298406466841698,
      "learning_rate": 1.0445833333333334e-05,
      "loss": 0.0021,
      "step": 94930
    },
    {
      "epoch": 6.3293333333333335,
      "grad_norm": 0.20457564294338226,
      "learning_rate": 1.0441666666666667e-05,
      "loss": 0.0023,
      "step": 94940
    },
    {
      "epoch": 6.33,
      "grad_norm": 0.14182604849338531,
      "learning_rate": 1.04375e-05,
      "loss": 0.0021,
      "step": 94950
    },
    {
      "epoch": 6.330666666666667,
      "grad_norm": 0.49302467703819275,
      "learning_rate": 1.0433333333333334e-05,
      "loss": 0.002,
      "step": 94960
    },
    {
      "epoch": 6.331333333333333,
      "grad_norm": 0.20793277025222778,
      "learning_rate": 1.0429166666666668e-05,
      "loss": 0.0021,
      "step": 94970
    },
    {
      "epoch": 6.332,
      "grad_norm": 0.20857426524162292,
      "learning_rate": 1.0425e-05,
      "loss": 0.0013,
      "step": 94980
    },
    {
      "epoch": 6.332666666666666,
      "grad_norm": 0.4133198857307434,
      "learning_rate": 1.0420833333333334e-05,
      "loss": 0.0016,
      "step": 94990
    },
    {
      "epoch": 6.333333333333333,
      "grad_norm": 0.06961235404014587,
      "learning_rate": 1.0416666666666668e-05,
      "loss": 0.0015,
      "step": 95000
    },
    {
      "epoch": 6.334,
      "grad_norm": 0.23316264152526855,
      "learning_rate": 1.04125e-05,
      "loss": 0.0017,
      "step": 95010
    },
    {
      "epoch": 6.334666666666667,
      "grad_norm": 0.2089310735464096,
      "learning_rate": 1.0408333333333333e-05,
      "loss": 0.0025,
      "step": 95020
    },
    {
      "epoch": 6.335333333333334,
      "grad_norm": 0.07730940729379654,
      "learning_rate": 1.0404166666666668e-05,
      "loss": 0.0016,
      "step": 95030
    },
    {
      "epoch": 6.336,
      "grad_norm": 0.22966597974300385,
      "learning_rate": 1.04e-05,
      "loss": 0.0015,
      "step": 95040
    },
    {
      "epoch": 6.336666666666667,
      "grad_norm": 0.2333614081144333,
      "learning_rate": 1.0395833333333333e-05,
      "loss": 0.0015,
      "step": 95050
    },
    {
      "epoch": 6.3373333333333335,
      "grad_norm": 0.504892885684967,
      "learning_rate": 1.0391666666666667e-05,
      "loss": 0.0029,
      "step": 95060
    },
    {
      "epoch": 6.338,
      "grad_norm": 0.046328913420438766,
      "learning_rate": 1.03875e-05,
      "loss": 0.002,
      "step": 95070
    },
    {
      "epoch": 6.338666666666667,
      "grad_norm": 0.16974012553691864,
      "learning_rate": 1.0383333333333334e-05,
      "loss": 0.002,
      "step": 95080
    },
    {
      "epoch": 6.339333333333333,
      "grad_norm": 0.04392145574092865,
      "learning_rate": 1.0379166666666667e-05,
      "loss": 0.0014,
      "step": 95090
    },
    {
      "epoch": 6.34,
      "grad_norm": 0.22121258080005646,
      "learning_rate": 1.0375e-05,
      "loss": 0.0024,
      "step": 95100
    },
    {
      "epoch": 6.3406666666666665,
      "grad_norm": 0.3351004719734192,
      "learning_rate": 1.0370833333333334e-05,
      "loss": 0.0015,
      "step": 95110
    },
    {
      "epoch": 6.341333333333333,
      "grad_norm": 0.211079940199852,
      "learning_rate": 1.0366666666666667e-05,
      "loss": 0.0013,
      "step": 95120
    },
    {
      "epoch": 6.342,
      "grad_norm": 0.212967187166214,
      "learning_rate": 1.03625e-05,
      "loss": 0.0015,
      "step": 95130
    },
    {
      "epoch": 6.342666666666666,
      "grad_norm": 0.2058296650648117,
      "learning_rate": 1.0358333333333334e-05,
      "loss": 0.0011,
      "step": 95140
    },
    {
      "epoch": 6.343333333333334,
      "grad_norm": 0.17903119325637817,
      "learning_rate": 1.0354166666666668e-05,
      "loss": 0.0025,
      "step": 95150
    },
    {
      "epoch": 6.344,
      "grad_norm": 0.04402602091431618,
      "learning_rate": 1.035e-05,
      "loss": 0.002,
      "step": 95160
    },
    {
      "epoch": 6.344666666666667,
      "grad_norm": 0.13338778913021088,
      "learning_rate": 1.0345833333333334e-05,
      "loss": 0.0024,
      "step": 95170
    },
    {
      "epoch": 6.3453333333333335,
      "grad_norm": 0.4402953088283539,
      "learning_rate": 1.0341666666666668e-05,
      "loss": 0.0024,
      "step": 95180
    },
    {
      "epoch": 6.346,
      "grad_norm": 0.6684728264808655,
      "learning_rate": 1.03375e-05,
      "loss": 0.0027,
      "step": 95190
    },
    {
      "epoch": 6.346666666666667,
      "grad_norm": 0.04257075488567352,
      "learning_rate": 1.0333333333333333e-05,
      "loss": 0.0017,
      "step": 95200
    },
    {
      "epoch": 6.347333333333333,
      "grad_norm": 0.2993619740009308,
      "learning_rate": 1.0329166666666668e-05,
      "loss": 0.0021,
      "step": 95210
    },
    {
      "epoch": 6.348,
      "grad_norm": 0.06388287246227264,
      "learning_rate": 1.0325e-05,
      "loss": 0.003,
      "step": 95220
    },
    {
      "epoch": 6.3486666666666665,
      "grad_norm": 0.4715317189693451,
      "learning_rate": 1.0320833333333333e-05,
      "loss": 0.0017,
      "step": 95230
    },
    {
      "epoch": 6.349333333333333,
      "grad_norm": 0.08644991368055344,
      "learning_rate": 1.0316666666666667e-05,
      "loss": 0.0021,
      "step": 95240
    },
    {
      "epoch": 6.35,
      "grad_norm": 0.24812278151512146,
      "learning_rate": 1.03125e-05,
      "loss": 0.0036,
      "step": 95250
    },
    {
      "epoch": 6.350666666666667,
      "grad_norm": 0.6298291087150574,
      "learning_rate": 1.0308333333333334e-05,
      "loss": 0.0017,
      "step": 95260
    },
    {
      "epoch": 6.351333333333334,
      "grad_norm": 0.3697875142097473,
      "learning_rate": 1.0304166666666667e-05,
      "loss": 0.0014,
      "step": 95270
    },
    {
      "epoch": 6.352,
      "grad_norm": 0.30370211601257324,
      "learning_rate": 1.03e-05,
      "loss": 0.0019,
      "step": 95280
    },
    {
      "epoch": 6.352666666666667,
      "grad_norm": 0.09520681947469711,
      "learning_rate": 1.0295833333333334e-05,
      "loss": 0.0017,
      "step": 95290
    },
    {
      "epoch": 6.3533333333333335,
      "grad_norm": 0.052458398044109344,
      "learning_rate": 1.0291666666666667e-05,
      "loss": 0.0023,
      "step": 95300
    },
    {
      "epoch": 6.354,
      "grad_norm": 0.10115287452936172,
      "learning_rate": 1.02875e-05,
      "loss": 0.0017,
      "step": 95310
    },
    {
      "epoch": 6.354666666666667,
      "grad_norm": 0.2027032971382141,
      "learning_rate": 1.0283333333333334e-05,
      "loss": 0.0021,
      "step": 95320
    },
    {
      "epoch": 6.355333333333333,
      "grad_norm": 0.3673754334449768,
      "learning_rate": 1.0279166666666668e-05,
      "loss": 0.0015,
      "step": 95330
    },
    {
      "epoch": 6.356,
      "grad_norm": 0.2042761743068695,
      "learning_rate": 1.0275e-05,
      "loss": 0.0014,
      "step": 95340
    },
    {
      "epoch": 6.3566666666666665,
      "grad_norm": 0.4074733257293701,
      "learning_rate": 1.0270833333333333e-05,
      "loss": 0.002,
      "step": 95350
    },
    {
      "epoch": 6.357333333333333,
      "grad_norm": 0.4462197422981262,
      "learning_rate": 1.0266666666666668e-05,
      "loss": 0.0021,
      "step": 95360
    },
    {
      "epoch": 6.358,
      "grad_norm": 0.10324697196483612,
      "learning_rate": 1.02625e-05,
      "loss": 0.0027,
      "step": 95370
    },
    {
      "epoch": 6.358666666666666,
      "grad_norm": 0.5058543086051941,
      "learning_rate": 1.0258333333333333e-05,
      "loss": 0.0023,
      "step": 95380
    },
    {
      "epoch": 6.359333333333334,
      "grad_norm": 0.413566529750824,
      "learning_rate": 1.0254166666666667e-05,
      "loss": 0.0014,
      "step": 95390
    },
    {
      "epoch": 6.36,
      "grad_norm": 0.18119123578071594,
      "learning_rate": 1.025e-05,
      "loss": 0.0017,
      "step": 95400
    },
    {
      "epoch": 6.360666666666667,
      "grad_norm": 0.16827309131622314,
      "learning_rate": 1.0245833333333334e-05,
      "loss": 0.002,
      "step": 95410
    },
    {
      "epoch": 6.3613333333333335,
      "grad_norm": 0.34993723034858704,
      "learning_rate": 1.0241666666666667e-05,
      "loss": 0.0019,
      "step": 95420
    },
    {
      "epoch": 6.362,
      "grad_norm": 0.07244185358285904,
      "learning_rate": 1.02375e-05,
      "loss": 0.0014,
      "step": 95430
    },
    {
      "epoch": 6.362666666666667,
      "grad_norm": 0.5318865776062012,
      "learning_rate": 1.0233333333333334e-05,
      "loss": 0.002,
      "step": 95440
    },
    {
      "epoch": 6.363333333333333,
      "grad_norm": 0.17055507004261017,
      "learning_rate": 1.0229166666666667e-05,
      "loss": 0.0019,
      "step": 95450
    },
    {
      "epoch": 6.364,
      "grad_norm": 0.0730060338973999,
      "learning_rate": 1.0225e-05,
      "loss": 0.0012,
      "step": 95460
    },
    {
      "epoch": 6.3646666666666665,
      "grad_norm": 0.22044967114925385,
      "learning_rate": 1.0220833333333334e-05,
      "loss": 0.0016,
      "step": 95470
    },
    {
      "epoch": 6.365333333333333,
      "grad_norm": 0.30714476108551025,
      "learning_rate": 1.0216666666666668e-05,
      "loss": 0.0015,
      "step": 95480
    },
    {
      "epoch": 6.366,
      "grad_norm": 0.21125061810016632,
      "learning_rate": 1.02125e-05,
      "loss": 0.0024,
      "step": 95490
    },
    {
      "epoch": 6.366666666666666,
      "grad_norm": 0.2645139992237091,
      "learning_rate": 1.0208333333333334e-05,
      "loss": 0.0022,
      "step": 95500
    },
    {
      "epoch": 6.367333333333334,
      "grad_norm": 0.5276440382003784,
      "learning_rate": 1.0204166666666668e-05,
      "loss": 0.0017,
      "step": 95510
    },
    {
      "epoch": 6.368,
      "grad_norm": 0.3037806451320648,
      "learning_rate": 1.02e-05,
      "loss": 0.0014,
      "step": 95520
    },
    {
      "epoch": 6.368666666666667,
      "grad_norm": 0.27822476625442505,
      "learning_rate": 1.0195833333333333e-05,
      "loss": 0.0022,
      "step": 95530
    },
    {
      "epoch": 6.3693333333333335,
      "grad_norm": 0.6762251257896423,
      "learning_rate": 1.0191666666666668e-05,
      "loss": 0.0029,
      "step": 95540
    },
    {
      "epoch": 6.37,
      "grad_norm": 0.4647286832332611,
      "learning_rate": 1.01875e-05,
      "loss": 0.0022,
      "step": 95550
    },
    {
      "epoch": 6.370666666666667,
      "grad_norm": 0.07102757692337036,
      "learning_rate": 1.0183333333333333e-05,
      "loss": 0.0021,
      "step": 95560
    },
    {
      "epoch": 6.371333333333333,
      "grad_norm": 0.5525611639022827,
      "learning_rate": 1.0179166666666667e-05,
      "loss": 0.0018,
      "step": 95570
    },
    {
      "epoch": 6.372,
      "grad_norm": 0.2082444280385971,
      "learning_rate": 1.0175e-05,
      "loss": 0.0027,
      "step": 95580
    },
    {
      "epoch": 6.3726666666666665,
      "grad_norm": 0.27141544222831726,
      "learning_rate": 1.0170833333333334e-05,
      "loss": 0.0015,
      "step": 95590
    },
    {
      "epoch": 6.373333333333333,
      "grad_norm": 0.21107934415340424,
      "learning_rate": 1.0166666666666667e-05,
      "loss": 0.0015,
      "step": 95600
    },
    {
      "epoch": 6.374,
      "grad_norm": 0.146575927734375,
      "learning_rate": 1.01625e-05,
      "loss": 0.002,
      "step": 95610
    },
    {
      "epoch": 6.374666666666666,
      "grad_norm": 0.4701143801212311,
      "learning_rate": 1.0158333333333334e-05,
      "loss": 0.0032,
      "step": 95620
    },
    {
      "epoch": 6.375333333333334,
      "grad_norm": 0.07397104799747467,
      "learning_rate": 1.0154166666666667e-05,
      "loss": 0.0019,
      "step": 95630
    },
    {
      "epoch": 6.376,
      "grad_norm": 0.46871569752693176,
      "learning_rate": 1.0150000000000001e-05,
      "loss": 0.0017,
      "step": 95640
    },
    {
      "epoch": 6.376666666666667,
      "grad_norm": 0.04138641804456711,
      "learning_rate": 1.0145833333333334e-05,
      "loss": 0.0017,
      "step": 95650
    },
    {
      "epoch": 6.3773333333333335,
      "grad_norm": 0.10718735307455063,
      "learning_rate": 1.0141666666666668e-05,
      "loss": 0.0025,
      "step": 95660
    },
    {
      "epoch": 6.378,
      "grad_norm": 0.2322729229927063,
      "learning_rate": 1.01375e-05,
      "loss": 0.0011,
      "step": 95670
    },
    {
      "epoch": 6.378666666666667,
      "grad_norm": 0.5386195182800293,
      "learning_rate": 1.0133333333333333e-05,
      "loss": 0.0023,
      "step": 95680
    },
    {
      "epoch": 6.379333333333333,
      "grad_norm": 0.6276968717575073,
      "learning_rate": 1.0129166666666668e-05,
      "loss": 0.0025,
      "step": 95690
    },
    {
      "epoch": 6.38,
      "grad_norm": 0.6403106451034546,
      "learning_rate": 1.0125e-05,
      "loss": 0.0023,
      "step": 95700
    },
    {
      "epoch": 6.3806666666666665,
      "grad_norm": 0.30492109060287476,
      "learning_rate": 1.0120833333333333e-05,
      "loss": 0.0015,
      "step": 95710
    },
    {
      "epoch": 6.381333333333333,
      "grad_norm": 0.34023380279541016,
      "learning_rate": 1.0116666666666667e-05,
      "loss": 0.0017,
      "step": 95720
    },
    {
      "epoch": 6.382,
      "grad_norm": 0.466107040643692,
      "learning_rate": 1.0112500000000002e-05,
      "loss": 0.0023,
      "step": 95730
    },
    {
      "epoch": 6.382666666666666,
      "grad_norm": 0.4125055968761444,
      "learning_rate": 1.0108333333333333e-05,
      "loss": 0.0013,
      "step": 95740
    },
    {
      "epoch": 6.383333333333334,
      "grad_norm": 0.42715415358543396,
      "learning_rate": 1.0104166666666667e-05,
      "loss": 0.0016,
      "step": 95750
    },
    {
      "epoch": 6.384,
      "grad_norm": 0.1376536637544632,
      "learning_rate": 1.0100000000000002e-05,
      "loss": 0.0023,
      "step": 95760
    },
    {
      "epoch": 6.384666666666667,
      "grad_norm": 0.0349699966609478,
      "learning_rate": 1.0095833333333334e-05,
      "loss": 0.0023,
      "step": 95770
    },
    {
      "epoch": 6.3853333333333335,
      "grad_norm": 0.38211044669151306,
      "learning_rate": 1.0091666666666667e-05,
      "loss": 0.0017,
      "step": 95780
    },
    {
      "epoch": 6.386,
      "grad_norm": 0.4637686312198639,
      "learning_rate": 1.0087500000000001e-05,
      "loss": 0.0014,
      "step": 95790
    },
    {
      "epoch": 6.386666666666667,
      "grad_norm": 0.23819167912006378,
      "learning_rate": 1.0083333333333334e-05,
      "loss": 0.0019,
      "step": 95800
    },
    {
      "epoch": 6.387333333333333,
      "grad_norm": 0.2414369136095047,
      "learning_rate": 1.0079166666666667e-05,
      "loss": 0.0016,
      "step": 95810
    },
    {
      "epoch": 6.388,
      "grad_norm": 0.17563457787036896,
      "learning_rate": 1.0075000000000001e-05,
      "loss": 0.0029,
      "step": 95820
    },
    {
      "epoch": 6.3886666666666665,
      "grad_norm": 0.23505014181137085,
      "learning_rate": 1.0070833333333334e-05,
      "loss": 0.0012,
      "step": 95830
    },
    {
      "epoch": 6.389333333333333,
      "grad_norm": 0.20499330759048462,
      "learning_rate": 1.0066666666666668e-05,
      "loss": 0.0016,
      "step": 95840
    },
    {
      "epoch": 6.39,
      "grad_norm": 0.23738549649715424,
      "learning_rate": 1.00625e-05,
      "loss": 0.0024,
      "step": 95850
    },
    {
      "epoch": 6.390666666666666,
      "grad_norm": 0.34963953495025635,
      "learning_rate": 1.0058333333333333e-05,
      "loss": 0.0023,
      "step": 95860
    },
    {
      "epoch": 6.391333333333334,
      "grad_norm": 0.20489023625850677,
      "learning_rate": 1.0054166666666668e-05,
      "loss": 0.0015,
      "step": 95870
    },
    {
      "epoch": 6.392,
      "grad_norm": 0.3720802962779999,
      "learning_rate": 1.005e-05,
      "loss": 0.0024,
      "step": 95880
    },
    {
      "epoch": 6.392666666666667,
      "grad_norm": 0.27300554513931274,
      "learning_rate": 1.0045833333333333e-05,
      "loss": 0.0015,
      "step": 95890
    },
    {
      "epoch": 6.3933333333333335,
      "grad_norm": 0.07364318519830704,
      "learning_rate": 1.0041666666666667e-05,
      "loss": 0.0021,
      "step": 95900
    },
    {
      "epoch": 6.394,
      "grad_norm": 0.3746304214000702,
      "learning_rate": 1.0037500000000002e-05,
      "loss": 0.0015,
      "step": 95910
    },
    {
      "epoch": 6.394666666666667,
      "grad_norm": 0.2765079736709595,
      "learning_rate": 1.0033333333333333e-05,
      "loss": 0.0017,
      "step": 95920
    },
    {
      "epoch": 6.395333333333333,
      "grad_norm": 0.03430163487792015,
      "learning_rate": 1.0029166666666667e-05,
      "loss": 0.0021,
      "step": 95930
    },
    {
      "epoch": 6.396,
      "grad_norm": 0.33675453066825867,
      "learning_rate": 1.0025000000000001e-05,
      "loss": 0.0018,
      "step": 95940
    },
    {
      "epoch": 6.3966666666666665,
      "grad_norm": 0.5894301533699036,
      "learning_rate": 1.0020833333333334e-05,
      "loss": 0.0013,
      "step": 95950
    },
    {
      "epoch": 6.397333333333333,
      "grad_norm": 0.14059242606163025,
      "learning_rate": 1.0016666666666667e-05,
      "loss": 0.0024,
      "step": 95960
    },
    {
      "epoch": 6.398,
      "grad_norm": 0.5198137164115906,
      "learning_rate": 1.0012500000000001e-05,
      "loss": 0.0013,
      "step": 95970
    },
    {
      "epoch": 6.398666666666666,
      "grad_norm": 0.30797332525253296,
      "learning_rate": 1.0008333333333334e-05,
      "loss": 0.0028,
      "step": 95980
    },
    {
      "epoch": 6.399333333333333,
      "grad_norm": 0.05335160344839096,
      "learning_rate": 1.0004166666666666e-05,
      "loss": 0.002,
      "step": 95990
    },
    {
      "epoch": 6.4,
      "grad_norm": 0.26732027530670166,
      "learning_rate": 1e-05,
      "loss": 0.0022,
      "step": 96000
    },
    {
      "epoch": 6.400666666666667,
      "grad_norm": 0.030964920297265053,
      "learning_rate": 9.995833333333333e-06,
      "loss": 0.0012,
      "step": 96010
    },
    {
      "epoch": 6.4013333333333335,
      "grad_norm": 0.3555360734462738,
      "learning_rate": 9.991666666666668e-06,
      "loss": 0.0019,
      "step": 96020
    },
    {
      "epoch": 6.402,
      "grad_norm": 0.0460876040160656,
      "learning_rate": 9.9875e-06,
      "loss": 0.0019,
      "step": 96030
    },
    {
      "epoch": 6.402666666666667,
      "grad_norm": 0.021414898335933685,
      "learning_rate": 9.983333333333333e-06,
      "loss": 0.0019,
      "step": 96040
    },
    {
      "epoch": 6.403333333333333,
      "grad_norm": 0.2027837038040161,
      "learning_rate": 9.979166666666668e-06,
      "loss": 0.0012,
      "step": 96050
    },
    {
      "epoch": 6.404,
      "grad_norm": 0.10639108717441559,
      "learning_rate": 9.975e-06,
      "loss": 0.002,
      "step": 96060
    },
    {
      "epoch": 6.4046666666666665,
      "grad_norm": 0.30644315481185913,
      "learning_rate": 9.970833333333333e-06,
      "loss": 0.0022,
      "step": 96070
    },
    {
      "epoch": 6.405333333333333,
      "grad_norm": 0.3049388527870178,
      "learning_rate": 9.966666666666667e-06,
      "loss": 0.0013,
      "step": 96080
    },
    {
      "epoch": 6.406,
      "grad_norm": 0.044059865176677704,
      "learning_rate": 9.962500000000002e-06,
      "loss": 0.0019,
      "step": 96090
    },
    {
      "epoch": 6.406666666666666,
      "grad_norm": 0.48628687858581543,
      "learning_rate": 9.958333333333333e-06,
      "loss": 0.0023,
      "step": 96100
    },
    {
      "epoch": 6.407333333333334,
      "grad_norm": 0.05559490993618965,
      "learning_rate": 9.954166666666667e-06,
      "loss": 0.0023,
      "step": 96110
    },
    {
      "epoch": 6.408,
      "grad_norm": 0.06925256550312042,
      "learning_rate": 9.950000000000001e-06,
      "loss": 0.0016,
      "step": 96120
    },
    {
      "epoch": 6.408666666666667,
      "grad_norm": 0.6429784893989563,
      "learning_rate": 9.945833333333334e-06,
      "loss": 0.0024,
      "step": 96130
    },
    {
      "epoch": 6.4093333333333335,
      "grad_norm": 0.14336737990379333,
      "learning_rate": 9.941666666666667e-06,
      "loss": 0.0016,
      "step": 96140
    },
    {
      "epoch": 6.41,
      "grad_norm": 0.2343999445438385,
      "learning_rate": 9.937500000000001e-06,
      "loss": 0.0014,
      "step": 96150
    },
    {
      "epoch": 6.410666666666667,
      "grad_norm": 0.40825849771499634,
      "learning_rate": 9.933333333333334e-06,
      "loss": 0.0019,
      "step": 96160
    },
    {
      "epoch": 6.411333333333333,
      "grad_norm": 0.4093710780143738,
      "learning_rate": 9.929166666666666e-06,
      "loss": 0.0021,
      "step": 96170
    },
    {
      "epoch": 6.412,
      "grad_norm": 0.07669539749622345,
      "learning_rate": 9.925e-06,
      "loss": 0.0013,
      "step": 96180
    },
    {
      "epoch": 6.4126666666666665,
      "grad_norm": 0.6400415301322937,
      "learning_rate": 9.920833333333333e-06,
      "loss": 0.0021,
      "step": 96190
    },
    {
      "epoch": 6.413333333333333,
      "grad_norm": 0.09875397384166718,
      "learning_rate": 9.916666666666668e-06,
      "loss": 0.0019,
      "step": 96200
    },
    {
      "epoch": 6.414,
      "grad_norm": 0.031226761639118195,
      "learning_rate": 9.9125e-06,
      "loss": 0.0023,
      "step": 96210
    },
    {
      "epoch": 6.414666666666666,
      "grad_norm": 0.13938367366790771,
      "learning_rate": 9.908333333333333e-06,
      "loss": 0.0019,
      "step": 96220
    },
    {
      "epoch": 6.415333333333333,
      "grad_norm": 0.3548296093940735,
      "learning_rate": 9.904166666666667e-06,
      "loss": 0.0022,
      "step": 96230
    },
    {
      "epoch": 6.416,
      "grad_norm": 0.15080231428146362,
      "learning_rate": 9.900000000000002e-06,
      "loss": 0.0015,
      "step": 96240
    },
    {
      "epoch": 6.416666666666667,
      "grad_norm": 0.3863183557987213,
      "learning_rate": 9.895833333333333e-06,
      "loss": 0.0019,
      "step": 96250
    },
    {
      "epoch": 6.417333333333334,
      "grad_norm": 0.5349041223526001,
      "learning_rate": 9.891666666666667e-06,
      "loss": 0.0013,
      "step": 96260
    },
    {
      "epoch": 6.418,
      "grad_norm": 0.11638271808624268,
      "learning_rate": 9.887500000000001e-06,
      "loss": 0.0017,
      "step": 96270
    },
    {
      "epoch": 6.418666666666667,
      "grad_norm": 0.13669750094413757,
      "learning_rate": 9.883333333333334e-06,
      "loss": 0.0028,
      "step": 96280
    },
    {
      "epoch": 6.419333333333333,
      "grad_norm": 0.05569177865982056,
      "learning_rate": 9.879166666666667e-06,
      "loss": 0.0013,
      "step": 96290
    },
    {
      "epoch": 6.42,
      "grad_norm": 0.23514950275421143,
      "learning_rate": 9.875000000000001e-06,
      "loss": 0.0014,
      "step": 96300
    },
    {
      "epoch": 6.4206666666666665,
      "grad_norm": 0.3361092805862427,
      "learning_rate": 9.870833333333334e-06,
      "loss": 0.0012,
      "step": 96310
    },
    {
      "epoch": 6.421333333333333,
      "grad_norm": 0.09440802782773972,
      "learning_rate": 9.866666666666667e-06,
      "loss": 0.0021,
      "step": 96320
    },
    {
      "epoch": 6.422,
      "grad_norm": 0.2335023134946823,
      "learning_rate": 9.862500000000001e-06,
      "loss": 0.0016,
      "step": 96330
    },
    {
      "epoch": 6.422666666666666,
      "grad_norm": 0.14989598095417023,
      "learning_rate": 9.858333333333334e-06,
      "loss": 0.0018,
      "step": 96340
    },
    {
      "epoch": 6.423333333333334,
      "grad_norm": 0.3068634867668152,
      "learning_rate": 9.854166666666668e-06,
      "loss": 0.003,
      "step": 96350
    },
    {
      "epoch": 6.424,
      "grad_norm": 0.08035542815923691,
      "learning_rate": 9.85e-06,
      "loss": 0.0019,
      "step": 96360
    },
    {
      "epoch": 6.424666666666667,
      "grad_norm": 0.20116615295410156,
      "learning_rate": 9.845833333333333e-06,
      "loss": 0.0015,
      "step": 96370
    },
    {
      "epoch": 6.425333333333334,
      "grad_norm": 0.17261573672294617,
      "learning_rate": 9.841666666666668e-06,
      "loss": 0.0017,
      "step": 96380
    },
    {
      "epoch": 6.426,
      "grad_norm": 0.11252620071172714,
      "learning_rate": 9.8375e-06,
      "loss": 0.0012,
      "step": 96390
    },
    {
      "epoch": 6.426666666666667,
      "grad_norm": 0.051114555448293686,
      "learning_rate": 9.833333333333333e-06,
      "loss": 0.0021,
      "step": 96400
    },
    {
      "epoch": 6.427333333333333,
      "grad_norm": 0.7732593417167664,
      "learning_rate": 9.829166666666667e-06,
      "loss": 0.0018,
      "step": 96410
    },
    {
      "epoch": 6.428,
      "grad_norm": 0.06804286688566208,
      "learning_rate": 9.825000000000002e-06,
      "loss": 0.002,
      "step": 96420
    },
    {
      "epoch": 6.4286666666666665,
      "grad_norm": 0.31012922525405884,
      "learning_rate": 9.820833333333333e-06,
      "loss": 0.0016,
      "step": 96430
    },
    {
      "epoch": 6.429333333333333,
      "grad_norm": 0.04414494335651398,
      "learning_rate": 9.816666666666667e-06,
      "loss": 0.0015,
      "step": 96440
    },
    {
      "epoch": 6.43,
      "grad_norm": 0.10529350489377975,
      "learning_rate": 9.812500000000001e-06,
      "loss": 0.0025,
      "step": 96450
    },
    {
      "epoch": 6.430666666666666,
      "grad_norm": 0.24727070331573486,
      "learning_rate": 9.808333333333334e-06,
      "loss": 0.002,
      "step": 96460
    },
    {
      "epoch": 6.431333333333333,
      "grad_norm": 0.5163122415542603,
      "learning_rate": 9.804166666666667e-06,
      "loss": 0.0014,
      "step": 96470
    },
    {
      "epoch": 6.432,
      "grad_norm": 0.16978485882282257,
      "learning_rate": 9.800000000000001e-06,
      "loss": 0.0017,
      "step": 96480
    },
    {
      "epoch": 6.432666666666667,
      "grad_norm": 0.262774258852005,
      "learning_rate": 9.795833333333334e-06,
      "loss": 0.0015,
      "step": 96490
    },
    {
      "epoch": 6.433333333333334,
      "grad_norm": 0.047793805599212646,
      "learning_rate": 9.791666666666666e-06,
      "loss": 0.0017,
      "step": 96500
    },
    {
      "epoch": 6.434,
      "grad_norm": 0.3159102201461792,
      "learning_rate": 9.7875e-06,
      "loss": 0.0014,
      "step": 96510
    },
    {
      "epoch": 6.434666666666667,
      "grad_norm": 0.20611977577209473,
      "learning_rate": 9.783333333333333e-06,
      "loss": 0.0012,
      "step": 96520
    },
    {
      "epoch": 6.435333333333333,
      "grad_norm": 0.6964221596717834,
      "learning_rate": 9.779166666666668e-06,
      "loss": 0.0018,
      "step": 96530
    },
    {
      "epoch": 6.436,
      "grad_norm": 0.5817003846168518,
      "learning_rate": 9.775e-06,
      "loss": 0.0025,
      "step": 96540
    },
    {
      "epoch": 6.4366666666666665,
      "grad_norm": 0.3044510781764984,
      "learning_rate": 9.770833333333333e-06,
      "loss": 0.0018,
      "step": 96550
    },
    {
      "epoch": 6.437333333333333,
      "grad_norm": 0.20015843212604523,
      "learning_rate": 9.766666666666667e-06,
      "loss": 0.0015,
      "step": 96560
    },
    {
      "epoch": 6.438,
      "grad_norm": 0.0707601010799408,
      "learning_rate": 9.7625e-06,
      "loss": 0.0013,
      "step": 96570
    },
    {
      "epoch": 6.438666666666666,
      "grad_norm": 0.053585346788167953,
      "learning_rate": 9.758333333333333e-06,
      "loss": 0.0035,
      "step": 96580
    },
    {
      "epoch": 6.439333333333334,
      "grad_norm": 0.4489959478378296,
      "learning_rate": 9.754166666666667e-06,
      "loss": 0.0028,
      "step": 96590
    },
    {
      "epoch": 6.44,
      "grad_norm": 0.33967500925064087,
      "learning_rate": 9.750000000000002e-06,
      "loss": 0.0024,
      "step": 96600
    },
    {
      "epoch": 6.440666666666667,
      "grad_norm": 0.3769431710243225,
      "learning_rate": 9.745833333333332e-06,
      "loss": 0.002,
      "step": 96610
    },
    {
      "epoch": 6.441333333333334,
      "grad_norm": 0.8135969638824463,
      "learning_rate": 9.741666666666667e-06,
      "loss": 0.0014,
      "step": 96620
    },
    {
      "epoch": 6.442,
      "grad_norm": 0.20447678864002228,
      "learning_rate": 9.737500000000001e-06,
      "loss": 0.002,
      "step": 96630
    },
    {
      "epoch": 6.442666666666667,
      "grad_norm": 0.041836928576231,
      "learning_rate": 9.733333333333334e-06,
      "loss": 0.0012,
      "step": 96640
    },
    {
      "epoch": 6.443333333333333,
      "grad_norm": 0.10551692545413971,
      "learning_rate": 9.729166666666667e-06,
      "loss": 0.002,
      "step": 96650
    },
    {
      "epoch": 6.444,
      "grad_norm": 0.1726185381412506,
      "learning_rate": 9.725000000000001e-06,
      "loss": 0.0015,
      "step": 96660
    },
    {
      "epoch": 6.4446666666666665,
      "grad_norm": 0.4351511299610138,
      "learning_rate": 9.720833333333334e-06,
      "loss": 0.0019,
      "step": 96670
    },
    {
      "epoch": 6.445333333333333,
      "grad_norm": 0.30951640009880066,
      "learning_rate": 9.716666666666666e-06,
      "loss": 0.0023,
      "step": 96680
    },
    {
      "epoch": 6.446,
      "grad_norm": 0.2062540203332901,
      "learning_rate": 9.7125e-06,
      "loss": 0.0014,
      "step": 96690
    },
    {
      "epoch": 6.446666666666666,
      "grad_norm": 0.05104611814022064,
      "learning_rate": 9.708333333333333e-06,
      "loss": 0.0018,
      "step": 96700
    },
    {
      "epoch": 6.447333333333333,
      "grad_norm": 0.29577264189720154,
      "learning_rate": 9.704166666666668e-06,
      "loss": 0.0027,
      "step": 96710
    },
    {
      "epoch": 6.448,
      "grad_norm": 0.42927950620651245,
      "learning_rate": 9.7e-06,
      "loss": 0.0014,
      "step": 96720
    },
    {
      "epoch": 6.448666666666667,
      "grad_norm": 0.13551925122737885,
      "learning_rate": 9.695833333333333e-06,
      "loss": 0.0017,
      "step": 96730
    },
    {
      "epoch": 6.449333333333334,
      "grad_norm": 0.2725948691368103,
      "learning_rate": 9.691666666666667e-06,
      "loss": 0.0021,
      "step": 96740
    },
    {
      "epoch": 6.45,
      "grad_norm": 0.5038609504699707,
      "learning_rate": 9.6875e-06,
      "loss": 0.004,
      "step": 96750
    },
    {
      "epoch": 6.450666666666667,
      "grad_norm": 0.3331204354763031,
      "learning_rate": 9.683333333333333e-06,
      "loss": 0.0017,
      "step": 96760
    },
    {
      "epoch": 6.451333333333333,
      "grad_norm": 0.5421403646469116,
      "learning_rate": 9.679166666666667e-06,
      "loss": 0.0017,
      "step": 96770
    },
    {
      "epoch": 6.452,
      "grad_norm": 0.4788476526737213,
      "learning_rate": 9.675000000000001e-06,
      "loss": 0.0022,
      "step": 96780
    },
    {
      "epoch": 6.4526666666666666,
      "grad_norm": 0.10574748367071152,
      "learning_rate": 9.670833333333332e-06,
      "loss": 0.0018,
      "step": 96790
    },
    {
      "epoch": 6.453333333333333,
      "grad_norm": 0.33998221158981323,
      "learning_rate": 9.666666666666667e-06,
      "loss": 0.0013,
      "step": 96800
    },
    {
      "epoch": 6.454,
      "grad_norm": 0.16903437674045563,
      "learning_rate": 9.662500000000001e-06,
      "loss": 0.0022,
      "step": 96810
    },
    {
      "epoch": 6.454666666666666,
      "grad_norm": 0.410255491733551,
      "learning_rate": 9.658333333333334e-06,
      "loss": 0.0015,
      "step": 96820
    },
    {
      "epoch": 6.455333333333333,
      "grad_norm": 0.8781683444976807,
      "learning_rate": 9.654166666666666e-06,
      "loss": 0.0021,
      "step": 96830
    },
    {
      "epoch": 6.456,
      "grad_norm": 0.05830090492963791,
      "learning_rate": 9.65e-06,
      "loss": 0.0026,
      "step": 96840
    },
    {
      "epoch": 6.456666666666667,
      "grad_norm": 0.10646771639585495,
      "learning_rate": 9.645833333333333e-06,
      "loss": 0.0022,
      "step": 96850
    },
    {
      "epoch": 6.457333333333334,
      "grad_norm": 0.2828404903411865,
      "learning_rate": 9.641666666666666e-06,
      "loss": 0.0032,
      "step": 96860
    },
    {
      "epoch": 6.458,
      "grad_norm": 0.3589235544204712,
      "learning_rate": 9.6375e-06,
      "loss": 0.0027,
      "step": 96870
    },
    {
      "epoch": 6.458666666666667,
      "grad_norm": 0.6434462070465088,
      "learning_rate": 9.633333333333335e-06,
      "loss": 0.0014,
      "step": 96880
    },
    {
      "epoch": 6.459333333333333,
      "grad_norm": 0.44547155499458313,
      "learning_rate": 9.629166666666668e-06,
      "loss": 0.0019,
      "step": 96890
    },
    {
      "epoch": 6.46,
      "grad_norm": 0.13191311061382294,
      "learning_rate": 9.625e-06,
      "loss": 0.0014,
      "step": 96900
    },
    {
      "epoch": 6.460666666666667,
      "grad_norm": 0.17802943289279938,
      "learning_rate": 9.620833333333335e-06,
      "loss": 0.0024,
      "step": 96910
    },
    {
      "epoch": 6.461333333333333,
      "grad_norm": 0.23394173383712769,
      "learning_rate": 9.616666666666667e-06,
      "loss": 0.0015,
      "step": 96920
    },
    {
      "epoch": 6.462,
      "grad_norm": 0.10981284081935883,
      "learning_rate": 9.6125e-06,
      "loss": 0.0024,
      "step": 96930
    },
    {
      "epoch": 6.462666666666666,
      "grad_norm": 0.3412620425224304,
      "learning_rate": 9.608333333333334e-06,
      "loss": 0.0014,
      "step": 96940
    },
    {
      "epoch": 6.463333333333333,
      "grad_norm": 0.1077876165509224,
      "learning_rate": 9.604166666666667e-06,
      "loss": 0.0028,
      "step": 96950
    },
    {
      "epoch": 6.464,
      "grad_norm": 0.233159139752388,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.0017,
      "step": 96960
    },
    {
      "epoch": 6.464666666666667,
      "grad_norm": 0.19397631287574768,
      "learning_rate": 9.595833333333334e-06,
      "loss": 0.002,
      "step": 96970
    },
    {
      "epoch": 6.465333333333334,
      "grad_norm": 0.37003669142723083,
      "learning_rate": 9.591666666666667e-06,
      "loss": 0.0028,
      "step": 96980
    },
    {
      "epoch": 6.466,
      "grad_norm": 0.05079284682869911,
      "learning_rate": 9.587500000000001e-06,
      "loss": 0.0014,
      "step": 96990
    },
    {
      "epoch": 6.466666666666667,
      "grad_norm": 0.1401011198759079,
      "learning_rate": 9.583333333333334e-06,
      "loss": 0.0017,
      "step": 97000
    },
    {
      "epoch": 6.467333333333333,
      "grad_norm": 0.7497854828834534,
      "learning_rate": 9.579166666666666e-06,
      "loss": 0.0019,
      "step": 97010
    },
    {
      "epoch": 6.468,
      "grad_norm": 0.04755616933107376,
      "learning_rate": 9.575e-06,
      "loss": 0.0017,
      "step": 97020
    },
    {
      "epoch": 6.468666666666667,
      "grad_norm": 0.10744272917509079,
      "learning_rate": 9.570833333333335e-06,
      "loss": 0.002,
      "step": 97030
    },
    {
      "epoch": 6.469333333333333,
      "grad_norm": 0.10717137157917023,
      "learning_rate": 9.566666666666666e-06,
      "loss": 0.0019,
      "step": 97040
    },
    {
      "epoch": 6.47,
      "grad_norm": 0.2743339538574219,
      "learning_rate": 9.5625e-06,
      "loss": 0.0015,
      "step": 97050
    },
    {
      "epoch": 6.470666666666666,
      "grad_norm": 0.04693782329559326,
      "learning_rate": 9.558333333333335e-06,
      "loss": 0.0012,
      "step": 97060
    },
    {
      "epoch": 6.471333333333333,
      "grad_norm": 0.3372781574726105,
      "learning_rate": 9.554166666666667e-06,
      "loss": 0.0012,
      "step": 97070
    },
    {
      "epoch": 6.4719999999999995,
      "grad_norm": 0.16911762952804565,
      "learning_rate": 9.55e-06,
      "loss": 0.0018,
      "step": 97080
    },
    {
      "epoch": 6.472666666666667,
      "grad_norm": 0.15117990970611572,
      "learning_rate": 9.545833333333334e-06,
      "loss": 0.0025,
      "step": 97090
    },
    {
      "epoch": 6.473333333333334,
      "grad_norm": 0.4356831908226013,
      "learning_rate": 9.541666666666667e-06,
      "loss": 0.0016,
      "step": 97100
    },
    {
      "epoch": 6.474,
      "grad_norm": 0.2501830458641052,
      "learning_rate": 9.537500000000001e-06,
      "loss": 0.0014,
      "step": 97110
    },
    {
      "epoch": 6.474666666666667,
      "grad_norm": 0.6279461979866028,
      "learning_rate": 9.533333333333334e-06,
      "loss": 0.0025,
      "step": 97120
    },
    {
      "epoch": 6.475333333333333,
      "grad_norm": 0.6063685417175293,
      "learning_rate": 9.529166666666667e-06,
      "loss": 0.0016,
      "step": 97130
    },
    {
      "epoch": 6.476,
      "grad_norm": 0.17305207252502441,
      "learning_rate": 9.525000000000001e-06,
      "loss": 0.0012,
      "step": 97140
    },
    {
      "epoch": 6.476666666666667,
      "grad_norm": 0.478436678647995,
      "learning_rate": 9.520833333333334e-06,
      "loss": 0.0031,
      "step": 97150
    },
    {
      "epoch": 6.477333333333333,
      "grad_norm": 0.5210986137390137,
      "learning_rate": 9.516666666666666e-06,
      "loss": 0.0025,
      "step": 97160
    },
    {
      "epoch": 6.478,
      "grad_norm": 0.10858278721570969,
      "learning_rate": 9.5125e-06,
      "loss": 0.0021,
      "step": 97170
    },
    {
      "epoch": 6.478666666666666,
      "grad_norm": 0.3161008358001709,
      "learning_rate": 9.508333333333335e-06,
      "loss": 0.0017,
      "step": 97180
    },
    {
      "epoch": 6.479333333333333,
      "grad_norm": 0.568610668182373,
      "learning_rate": 9.504166666666666e-06,
      "loss": 0.0015,
      "step": 97190
    },
    {
      "epoch": 6.48,
      "grad_norm": 0.5296119451522827,
      "learning_rate": 9.5e-06,
      "loss": 0.0022,
      "step": 97200
    },
    {
      "epoch": 6.480666666666667,
      "grad_norm": 0.07367829233407974,
      "learning_rate": 9.495833333333335e-06,
      "loss": 0.0015,
      "step": 97210
    },
    {
      "epoch": 6.481333333333334,
      "grad_norm": 0.2336719036102295,
      "learning_rate": 9.491666666666668e-06,
      "loss": 0.0024,
      "step": 97220
    },
    {
      "epoch": 6.482,
      "grad_norm": 0.4943397641181946,
      "learning_rate": 9.4875e-06,
      "loss": 0.002,
      "step": 97230
    },
    {
      "epoch": 6.482666666666667,
      "grad_norm": 0.5749635100364685,
      "learning_rate": 9.483333333333335e-06,
      "loss": 0.0028,
      "step": 97240
    },
    {
      "epoch": 6.483333333333333,
      "grad_norm": 0.2632945477962494,
      "learning_rate": 9.479166666666667e-06,
      "loss": 0.0018,
      "step": 97250
    },
    {
      "epoch": 6.484,
      "grad_norm": 0.2014520913362503,
      "learning_rate": 9.475e-06,
      "loss": 0.0013,
      "step": 97260
    },
    {
      "epoch": 6.484666666666667,
      "grad_norm": 0.07248903810977936,
      "learning_rate": 9.470833333333334e-06,
      "loss": 0.0016,
      "step": 97270
    },
    {
      "epoch": 6.485333333333333,
      "grad_norm": 0.056592170149087906,
      "learning_rate": 9.466666666666667e-06,
      "loss": 0.0013,
      "step": 97280
    },
    {
      "epoch": 6.486,
      "grad_norm": 0.10526679456233978,
      "learning_rate": 9.462500000000001e-06,
      "loss": 0.0017,
      "step": 97290
    },
    {
      "epoch": 6.486666666666666,
      "grad_norm": 0.08531451970338821,
      "learning_rate": 9.458333333333334e-06,
      "loss": 0.0019,
      "step": 97300
    },
    {
      "epoch": 6.487333333333333,
      "grad_norm": 0.1043214276432991,
      "learning_rate": 9.454166666666667e-06,
      "loss": 0.0018,
      "step": 97310
    },
    {
      "epoch": 6.4879999999999995,
      "grad_norm": 0.40308189392089844,
      "learning_rate": 9.450000000000001e-06,
      "loss": 0.0026,
      "step": 97320
    },
    {
      "epoch": 6.488666666666667,
      "grad_norm": 0.5416100025177002,
      "learning_rate": 9.445833333333334e-06,
      "loss": 0.0022,
      "step": 97330
    },
    {
      "epoch": 6.489333333333334,
      "grad_norm": 0.4751944839954376,
      "learning_rate": 9.441666666666666e-06,
      "loss": 0.0021,
      "step": 97340
    },
    {
      "epoch": 6.49,
      "grad_norm": 0.6816864609718323,
      "learning_rate": 9.4375e-06,
      "loss": 0.002,
      "step": 97350
    },
    {
      "epoch": 6.490666666666667,
      "grad_norm": 0.3398694097995758,
      "learning_rate": 9.433333333333335e-06,
      "loss": 0.0016,
      "step": 97360
    },
    {
      "epoch": 6.491333333333333,
      "grad_norm": 0.03753737360239029,
      "learning_rate": 9.429166666666666e-06,
      "loss": 0.0014,
      "step": 97370
    },
    {
      "epoch": 6.492,
      "grad_norm": 0.3711627423763275,
      "learning_rate": 9.425e-06,
      "loss": 0.0021,
      "step": 97380
    },
    {
      "epoch": 6.492666666666667,
      "grad_norm": 0.4063602685928345,
      "learning_rate": 9.420833333333335e-06,
      "loss": 0.0026,
      "step": 97390
    },
    {
      "epoch": 6.493333333333333,
      "grad_norm": 0.17366230487823486,
      "learning_rate": 9.416666666666667e-06,
      "loss": 0.0016,
      "step": 97400
    },
    {
      "epoch": 6.494,
      "grad_norm": 0.27848273515701294,
      "learning_rate": 9.4125e-06,
      "loss": 0.0016,
      "step": 97410
    },
    {
      "epoch": 6.494666666666666,
      "grad_norm": 0.14183180034160614,
      "learning_rate": 9.408333333333334e-06,
      "loss": 0.0025,
      "step": 97420
    },
    {
      "epoch": 6.495333333333333,
      "grad_norm": 0.33763980865478516,
      "learning_rate": 9.404166666666667e-06,
      "loss": 0.0023,
      "step": 97430
    },
    {
      "epoch": 6.496,
      "grad_norm": 0.04945114254951477,
      "learning_rate": 9.4e-06,
      "loss": 0.0018,
      "step": 97440
    },
    {
      "epoch": 6.496666666666667,
      "grad_norm": 0.023932088166475296,
      "learning_rate": 9.395833333333334e-06,
      "loss": 0.0015,
      "step": 97450
    },
    {
      "epoch": 6.497333333333334,
      "grad_norm": 0.3047334551811218,
      "learning_rate": 9.391666666666667e-06,
      "loss": 0.0022,
      "step": 97460
    },
    {
      "epoch": 6.498,
      "grad_norm": 0.025538291782140732,
      "learning_rate": 9.387500000000001e-06,
      "loss": 0.0023,
      "step": 97470
    },
    {
      "epoch": 6.498666666666667,
      "grad_norm": 0.06976433843374252,
      "learning_rate": 9.383333333333334e-06,
      "loss": 0.0032,
      "step": 97480
    },
    {
      "epoch": 6.499333333333333,
      "grad_norm": 0.2313781976699829,
      "learning_rate": 9.379166666666667e-06,
      "loss": 0.0022,
      "step": 97490
    },
    {
      "epoch": 6.5,
      "grad_norm": 0.32446539402008057,
      "learning_rate": 9.375000000000001e-06,
      "loss": 0.0015,
      "step": 97500
    },
    {
      "epoch": 6.500666666666667,
      "grad_norm": 0.5390145182609558,
      "learning_rate": 9.370833333333334e-06,
      "loss": 0.002,
      "step": 97510
    },
    {
      "epoch": 6.501333333333333,
      "grad_norm": 0.2747989296913147,
      "learning_rate": 9.366666666666666e-06,
      "loss": 0.0014,
      "step": 97520
    },
    {
      "epoch": 6.502,
      "grad_norm": 0.20139625668525696,
      "learning_rate": 9.3625e-06,
      "loss": 0.0021,
      "step": 97530
    },
    {
      "epoch": 6.502666666666666,
      "grad_norm": 0.2668722867965698,
      "learning_rate": 9.358333333333335e-06,
      "loss": 0.0018,
      "step": 97540
    },
    {
      "epoch": 6.503333333333333,
      "grad_norm": 0.4374340772628784,
      "learning_rate": 9.354166666666666e-06,
      "loss": 0.0019,
      "step": 97550
    },
    {
      "epoch": 6.504,
      "grad_norm": 0.053322684019804,
      "learning_rate": 9.35e-06,
      "loss": 0.0014,
      "step": 97560
    },
    {
      "epoch": 6.504666666666667,
      "grad_norm": 0.29559072852134705,
      "learning_rate": 9.345833333333335e-06,
      "loss": 0.0028,
      "step": 97570
    },
    {
      "epoch": 6.505333333333334,
      "grad_norm": 0.036699987947940826,
      "learning_rate": 9.341666666666667e-06,
      "loss": 0.0016,
      "step": 97580
    },
    {
      "epoch": 6.506,
      "grad_norm": 0.4965716600418091,
      "learning_rate": 9.3375e-06,
      "loss": 0.0019,
      "step": 97590
    },
    {
      "epoch": 6.506666666666667,
      "grad_norm": 0.17233604192733765,
      "learning_rate": 9.333333333333334e-06,
      "loss": 0.0014,
      "step": 97600
    },
    {
      "epoch": 6.507333333333333,
      "grad_norm": 0.39121803641319275,
      "learning_rate": 9.329166666666667e-06,
      "loss": 0.0013,
      "step": 97610
    },
    {
      "epoch": 6.508,
      "grad_norm": 0.33922675251960754,
      "learning_rate": 9.325e-06,
      "loss": 0.0025,
      "step": 97620
    },
    {
      "epoch": 6.508666666666667,
      "grad_norm": 0.40616801381111145,
      "learning_rate": 9.320833333333334e-06,
      "loss": 0.0013,
      "step": 97630
    },
    {
      "epoch": 6.509333333333333,
      "grad_norm": 0.09905342012643814,
      "learning_rate": 9.316666666666667e-06,
      "loss": 0.0016,
      "step": 97640
    },
    {
      "epoch": 6.51,
      "grad_norm": 0.2949058413505554,
      "learning_rate": 9.312500000000001e-06,
      "loss": 0.0028,
      "step": 97650
    },
    {
      "epoch": 6.510666666666666,
      "grad_norm": 0.2981410026550293,
      "learning_rate": 9.308333333333334e-06,
      "loss": 0.0028,
      "step": 97660
    },
    {
      "epoch": 6.511333333333333,
      "grad_norm": 0.06979896873235703,
      "learning_rate": 9.304166666666666e-06,
      "loss": 0.0015,
      "step": 97670
    },
    {
      "epoch": 6.5120000000000005,
      "grad_norm": 0.11268676072359085,
      "learning_rate": 9.3e-06,
      "loss": 0.0017,
      "step": 97680
    },
    {
      "epoch": 6.512666666666667,
      "grad_norm": 0.10474983602762222,
      "learning_rate": 9.295833333333333e-06,
      "loss": 0.0014,
      "step": 97690
    },
    {
      "epoch": 6.513333333333334,
      "grad_norm": 0.2057758867740631,
      "learning_rate": 9.291666666666666e-06,
      "loss": 0.0018,
      "step": 97700
    },
    {
      "epoch": 6.514,
      "grad_norm": 0.23752374947071075,
      "learning_rate": 9.2875e-06,
      "loss": 0.0011,
      "step": 97710
    },
    {
      "epoch": 6.514666666666667,
      "grad_norm": 0.47977757453918457,
      "learning_rate": 9.283333333333335e-06,
      "loss": 0.0021,
      "step": 97720
    },
    {
      "epoch": 6.515333333333333,
      "grad_norm": 0.5197007060050964,
      "learning_rate": 9.279166666666666e-06,
      "loss": 0.0014,
      "step": 97730
    },
    {
      "epoch": 6.516,
      "grad_norm": 0.26714444160461426,
      "learning_rate": 9.275e-06,
      "loss": 0.0014,
      "step": 97740
    },
    {
      "epoch": 6.516666666666667,
      "grad_norm": 0.14369744062423706,
      "learning_rate": 9.270833333333334e-06,
      "loss": 0.0017,
      "step": 97750
    },
    {
      "epoch": 6.517333333333333,
      "grad_norm": 0.10877452790737152,
      "learning_rate": 9.266666666666667e-06,
      "loss": 0.0017,
      "step": 97760
    },
    {
      "epoch": 6.518,
      "grad_norm": 0.03954436630010605,
      "learning_rate": 9.2625e-06,
      "loss": 0.0014,
      "step": 97770
    },
    {
      "epoch": 6.518666666666666,
      "grad_norm": 0.4382961392402649,
      "learning_rate": 9.258333333333334e-06,
      "loss": 0.0015,
      "step": 97780
    },
    {
      "epoch": 6.519333333333333,
      "grad_norm": 0.20748919248580933,
      "learning_rate": 9.254166666666667e-06,
      "loss": 0.0025,
      "step": 97790
    },
    {
      "epoch": 6.52,
      "grad_norm": 0.18121008574962616,
      "learning_rate": 9.25e-06,
      "loss": 0.0017,
      "step": 97800
    },
    {
      "epoch": 6.520666666666667,
      "grad_norm": 0.07385271787643433,
      "learning_rate": 9.245833333333334e-06,
      "loss": 0.0012,
      "step": 97810
    },
    {
      "epoch": 6.521333333333334,
      "grad_norm": 0.1389414519071579,
      "learning_rate": 9.241666666666667e-06,
      "loss": 0.0017,
      "step": 97820
    },
    {
      "epoch": 6.522,
      "grad_norm": 0.4551627039909363,
      "learning_rate": 9.237500000000001e-06,
      "loss": 0.0018,
      "step": 97830
    },
    {
      "epoch": 6.522666666666667,
      "grad_norm": 0.4656912386417389,
      "learning_rate": 9.233333333333334e-06,
      "loss": 0.0016,
      "step": 97840
    },
    {
      "epoch": 6.523333333333333,
      "grad_norm": 0.29688259959220886,
      "learning_rate": 9.229166666666666e-06,
      "loss": 0.0018,
      "step": 97850
    },
    {
      "epoch": 6.524,
      "grad_norm": 0.050593554973602295,
      "learning_rate": 9.225e-06,
      "loss": 0.0012,
      "step": 97860
    },
    {
      "epoch": 6.524666666666667,
      "grad_norm": 0.17508289217948914,
      "learning_rate": 9.220833333333333e-06,
      "loss": 0.0014,
      "step": 97870
    },
    {
      "epoch": 6.525333333333333,
      "grad_norm": 0.1703837811946869,
      "learning_rate": 9.216666666666666e-06,
      "loss": 0.002,
      "step": 97880
    },
    {
      "epoch": 6.526,
      "grad_norm": 0.049728646874427795,
      "learning_rate": 9.2125e-06,
      "loss": 0.0019,
      "step": 97890
    },
    {
      "epoch": 6.526666666666666,
      "grad_norm": 0.08551142364740372,
      "learning_rate": 9.208333333333335e-06,
      "loss": 0.0013,
      "step": 97900
    },
    {
      "epoch": 6.527333333333333,
      "grad_norm": 0.36885395646095276,
      "learning_rate": 9.204166666666667e-06,
      "loss": 0.0017,
      "step": 97910
    },
    {
      "epoch": 6.5280000000000005,
      "grad_norm": 0.20145569741725922,
      "learning_rate": 9.2e-06,
      "loss": 0.0018,
      "step": 97920
    },
    {
      "epoch": 6.528666666666666,
      "grad_norm": 0.37107935547828674,
      "learning_rate": 9.195833333333334e-06,
      "loss": 0.0023,
      "step": 97930
    },
    {
      "epoch": 6.529333333333334,
      "grad_norm": 0.24671392142772675,
      "learning_rate": 9.191666666666667e-06,
      "loss": 0.0029,
      "step": 97940
    },
    {
      "epoch": 6.53,
      "grad_norm": 0.08459027111530304,
      "learning_rate": 9.1875e-06,
      "loss": 0.0024,
      "step": 97950
    },
    {
      "epoch": 6.530666666666667,
      "grad_norm": 0.14022891223430634,
      "learning_rate": 9.183333333333334e-06,
      "loss": 0.0019,
      "step": 97960
    },
    {
      "epoch": 6.531333333333333,
      "grad_norm": 0.05761737748980522,
      "learning_rate": 9.179166666666667e-06,
      "loss": 0.0024,
      "step": 97970
    },
    {
      "epoch": 6.532,
      "grad_norm": 0.05978905409574509,
      "learning_rate": 9.175000000000001e-06,
      "loss": 0.0021,
      "step": 97980
    },
    {
      "epoch": 6.532666666666667,
      "grad_norm": 0.2790316343307495,
      "learning_rate": 9.170833333333334e-06,
      "loss": 0.0021,
      "step": 97990
    },
    {
      "epoch": 6.533333333333333,
      "grad_norm": 0.37482303380966187,
      "learning_rate": 9.166666666666666e-06,
      "loss": 0.0014,
      "step": 98000
    },
    {
      "epoch": 6.534,
      "grad_norm": 0.06398800015449524,
      "learning_rate": 9.1625e-06,
      "loss": 0.0014,
      "step": 98010
    },
    {
      "epoch": 6.534666666666666,
      "grad_norm": 0.20585189759731293,
      "learning_rate": 9.158333333333333e-06,
      "loss": 0.0016,
      "step": 98020
    },
    {
      "epoch": 6.535333333333333,
      "grad_norm": 0.24984361231327057,
      "learning_rate": 9.154166666666666e-06,
      "loss": 0.0014,
      "step": 98030
    },
    {
      "epoch": 6.536,
      "grad_norm": 0.07098390907049179,
      "learning_rate": 9.15e-06,
      "loss": 0.0014,
      "step": 98040
    },
    {
      "epoch": 6.536666666666667,
      "grad_norm": 0.4984542727470398,
      "learning_rate": 9.145833333333335e-06,
      "loss": 0.0017,
      "step": 98050
    },
    {
      "epoch": 6.537333333333334,
      "grad_norm": 0.42449045181274414,
      "learning_rate": 9.141666666666666e-06,
      "loss": 0.0017,
      "step": 98060
    },
    {
      "epoch": 6.538,
      "grad_norm": 0.13431492447853088,
      "learning_rate": 9.1375e-06,
      "loss": 0.0015,
      "step": 98070
    },
    {
      "epoch": 6.538666666666667,
      "grad_norm": 0.10073300451040268,
      "learning_rate": 9.133333333333335e-06,
      "loss": 0.0019,
      "step": 98080
    },
    {
      "epoch": 6.539333333333333,
      "grad_norm": 0.25613731145858765,
      "learning_rate": 9.129166666666667e-06,
      "loss": 0.0024,
      "step": 98090
    },
    {
      "epoch": 6.54,
      "grad_norm": 0.08290576934814453,
      "learning_rate": 9.125e-06,
      "loss": 0.0029,
      "step": 98100
    },
    {
      "epoch": 6.540666666666667,
      "grad_norm": 0.23169563710689545,
      "learning_rate": 9.120833333333334e-06,
      "loss": 0.0024,
      "step": 98110
    },
    {
      "epoch": 6.541333333333333,
      "grad_norm": 0.20424306392669678,
      "learning_rate": 9.116666666666667e-06,
      "loss": 0.0027,
      "step": 98120
    },
    {
      "epoch": 6.542,
      "grad_norm": 0.2053583562374115,
      "learning_rate": 9.1125e-06,
      "loss": 0.0013,
      "step": 98130
    },
    {
      "epoch": 6.542666666666666,
      "grad_norm": 0.23701193928718567,
      "learning_rate": 9.108333333333334e-06,
      "loss": 0.002,
      "step": 98140
    },
    {
      "epoch": 6.543333333333333,
      "grad_norm": 0.10844100266695023,
      "learning_rate": 9.104166666666668e-06,
      "loss": 0.0018,
      "step": 98150
    },
    {
      "epoch": 6.5440000000000005,
      "grad_norm": 0.11344350874423981,
      "learning_rate": 9.100000000000001e-06,
      "loss": 0.0016,
      "step": 98160
    },
    {
      "epoch": 6.544666666666666,
      "grad_norm": 0.0751684382557869,
      "learning_rate": 9.095833333333334e-06,
      "loss": 0.0022,
      "step": 98170
    },
    {
      "epoch": 6.545333333333334,
      "grad_norm": 0.466480553150177,
      "learning_rate": 9.091666666666668e-06,
      "loss": 0.0024,
      "step": 98180
    },
    {
      "epoch": 6.546,
      "grad_norm": 0.23770546913146973,
      "learning_rate": 9.0875e-06,
      "loss": 0.002,
      "step": 98190
    },
    {
      "epoch": 6.546666666666667,
      "grad_norm": 0.07435805350542068,
      "learning_rate": 9.083333333333333e-06,
      "loss": 0.0011,
      "step": 98200
    },
    {
      "epoch": 6.5473333333333334,
      "grad_norm": 0.08297350257635117,
      "learning_rate": 9.079166666666668e-06,
      "loss": 0.0018,
      "step": 98210
    },
    {
      "epoch": 6.548,
      "grad_norm": 0.26318439841270447,
      "learning_rate": 9.075e-06,
      "loss": 0.0021,
      "step": 98220
    },
    {
      "epoch": 6.548666666666667,
      "grad_norm": 0.08173391968011856,
      "learning_rate": 9.070833333333335e-06,
      "loss": 0.002,
      "step": 98230
    },
    {
      "epoch": 6.549333333333333,
      "grad_norm": 0.20318134129047394,
      "learning_rate": 9.066666666666667e-06,
      "loss": 0.0013,
      "step": 98240
    },
    {
      "epoch": 6.55,
      "grad_norm": 0.4548875093460083,
      "learning_rate": 9.0625e-06,
      "loss": 0.0019,
      "step": 98250
    },
    {
      "epoch": 6.550666666666666,
      "grad_norm": 0.1032363697886467,
      "learning_rate": 9.058333333333334e-06,
      "loss": 0.0018,
      "step": 98260
    },
    {
      "epoch": 6.551333333333333,
      "grad_norm": 0.36147022247314453,
      "learning_rate": 9.054166666666667e-06,
      "loss": 0.0017,
      "step": 98270
    },
    {
      "epoch": 6.552,
      "grad_norm": 0.5443000197410583,
      "learning_rate": 9.05e-06,
      "loss": 0.002,
      "step": 98280
    },
    {
      "epoch": 6.552666666666667,
      "grad_norm": 0.05503535643219948,
      "learning_rate": 9.045833333333334e-06,
      "loss": 0.0021,
      "step": 98290
    },
    {
      "epoch": 6.553333333333334,
      "grad_norm": 0.16902363300323486,
      "learning_rate": 9.041666666666668e-06,
      "loss": 0.0017,
      "step": 98300
    },
    {
      "epoch": 6.554,
      "grad_norm": 0.16998331248760223,
      "learning_rate": 9.0375e-06,
      "loss": 0.0034,
      "step": 98310
    },
    {
      "epoch": 6.554666666666667,
      "grad_norm": 0.3743138909339905,
      "learning_rate": 9.033333333333334e-06,
      "loss": 0.0011,
      "step": 98320
    },
    {
      "epoch": 6.5553333333333335,
      "grad_norm": 0.24469700455665588,
      "learning_rate": 9.029166666666668e-06,
      "loss": 0.002,
      "step": 98330
    },
    {
      "epoch": 6.556,
      "grad_norm": 0.07293527573347092,
      "learning_rate": 9.025e-06,
      "loss": 0.0013,
      "step": 98340
    },
    {
      "epoch": 6.556666666666667,
      "grad_norm": 0.10743246972560883,
      "learning_rate": 9.020833333333334e-06,
      "loss": 0.0013,
      "step": 98350
    },
    {
      "epoch": 6.557333333333333,
      "grad_norm": 0.20153823494911194,
      "learning_rate": 9.016666666666668e-06,
      "loss": 0.0015,
      "step": 98360
    },
    {
      "epoch": 6.558,
      "grad_norm": 0.21642065048217773,
      "learning_rate": 9.0125e-06,
      "loss": 0.0015,
      "step": 98370
    },
    {
      "epoch": 6.558666666666666,
      "grad_norm": 0.03909832239151001,
      "learning_rate": 9.008333333333333e-06,
      "loss": 0.0014,
      "step": 98380
    },
    {
      "epoch": 6.559333333333333,
      "grad_norm": 0.039650581777095795,
      "learning_rate": 9.004166666666668e-06,
      "loss": 0.0014,
      "step": 98390
    },
    {
      "epoch": 6.5600000000000005,
      "grad_norm": 0.1330929547548294,
      "learning_rate": 9e-06,
      "loss": 0.0016,
      "step": 98400
    },
    {
      "epoch": 6.560666666666666,
      "grad_norm": 0.028368297964334488,
      "learning_rate": 8.995833333333335e-06,
      "loss": 0.0014,
      "step": 98410
    },
    {
      "epoch": 6.561333333333334,
      "grad_norm": 0.20397213101387024,
      "learning_rate": 8.991666666666667e-06,
      "loss": 0.0018,
      "step": 98420
    },
    {
      "epoch": 6.562,
      "grad_norm": 0.029881933704018593,
      "learning_rate": 8.9875e-06,
      "loss": 0.0017,
      "step": 98430
    },
    {
      "epoch": 6.562666666666667,
      "grad_norm": 0.1090063750743866,
      "learning_rate": 8.983333333333334e-06,
      "loss": 0.002,
      "step": 98440
    },
    {
      "epoch": 6.5633333333333335,
      "grad_norm": 0.12044952064752579,
      "learning_rate": 8.979166666666667e-06,
      "loss": 0.0024,
      "step": 98450
    },
    {
      "epoch": 6.564,
      "grad_norm": 0.06057404726743698,
      "learning_rate": 8.975e-06,
      "loss": 0.0013,
      "step": 98460
    },
    {
      "epoch": 6.564666666666667,
      "grad_norm": 0.3432248830795288,
      "learning_rate": 8.970833333333334e-06,
      "loss": 0.0013,
      "step": 98470
    },
    {
      "epoch": 6.565333333333333,
      "grad_norm": 0.16955481469631195,
      "learning_rate": 8.966666666666668e-06,
      "loss": 0.0011,
      "step": 98480
    },
    {
      "epoch": 6.566,
      "grad_norm": 0.04193820804357529,
      "learning_rate": 8.9625e-06,
      "loss": 0.002,
      "step": 98490
    },
    {
      "epoch": 6.566666666666666,
      "grad_norm": 0.1253276914358139,
      "learning_rate": 8.958333333333334e-06,
      "loss": 0.0022,
      "step": 98500
    },
    {
      "epoch": 6.567333333333333,
      "grad_norm": 0.4034464955329895,
      "learning_rate": 8.954166666666668e-06,
      "loss": 0.0024,
      "step": 98510
    },
    {
      "epoch": 6.568,
      "grad_norm": 0.41062772274017334,
      "learning_rate": 8.95e-06,
      "loss": 0.0014,
      "step": 98520
    },
    {
      "epoch": 6.568666666666667,
      "grad_norm": 0.3170374035835266,
      "learning_rate": 8.945833333333333e-06,
      "loss": 0.0025,
      "step": 98530
    },
    {
      "epoch": 6.569333333333334,
      "grad_norm": 0.13579368591308594,
      "learning_rate": 8.941666666666668e-06,
      "loss": 0.0018,
      "step": 98540
    },
    {
      "epoch": 6.57,
      "grad_norm": 0.14646586775779724,
      "learning_rate": 8.9375e-06,
      "loss": 0.0018,
      "step": 98550
    },
    {
      "epoch": 6.570666666666667,
      "grad_norm": 0.32000675797462463,
      "learning_rate": 8.933333333333333e-06,
      "loss": 0.0017,
      "step": 98560
    },
    {
      "epoch": 6.5713333333333335,
      "grad_norm": 0.07667901366949081,
      "learning_rate": 8.929166666666667e-06,
      "loss": 0.0017,
      "step": 98570
    },
    {
      "epoch": 6.572,
      "grad_norm": 0.08603934943675995,
      "learning_rate": 8.925e-06,
      "loss": 0.002,
      "step": 98580
    },
    {
      "epoch": 6.572666666666667,
      "grad_norm": 0.20811541378498077,
      "learning_rate": 8.920833333333334e-06,
      "loss": 0.0018,
      "step": 98590
    },
    {
      "epoch": 6.573333333333333,
      "grad_norm": 0.17991319298744202,
      "learning_rate": 8.916666666666667e-06,
      "loss": 0.0021,
      "step": 98600
    },
    {
      "epoch": 6.574,
      "grad_norm": 0.20239020884037018,
      "learning_rate": 8.9125e-06,
      "loss": 0.0024,
      "step": 98610
    },
    {
      "epoch": 6.574666666666666,
      "grad_norm": 0.04509090259671211,
      "learning_rate": 8.908333333333334e-06,
      "loss": 0.0023,
      "step": 98620
    },
    {
      "epoch": 6.575333333333333,
      "grad_norm": 0.1454111784696579,
      "learning_rate": 8.904166666666667e-06,
      "loss": 0.0019,
      "step": 98630
    },
    {
      "epoch": 6.576,
      "grad_norm": 0.13700337707996368,
      "learning_rate": 8.9e-06,
      "loss": 0.0017,
      "step": 98640
    },
    {
      "epoch": 6.576666666666666,
      "grad_norm": 0.47250115871429443,
      "learning_rate": 8.895833333333334e-06,
      "loss": 0.0014,
      "step": 98650
    },
    {
      "epoch": 6.577333333333334,
      "grad_norm": 0.04804736748337746,
      "learning_rate": 8.891666666666668e-06,
      "loss": 0.0015,
      "step": 98660
    },
    {
      "epoch": 6.578,
      "grad_norm": 0.16939832270145416,
      "learning_rate": 8.8875e-06,
      "loss": 0.0022,
      "step": 98670
    },
    {
      "epoch": 6.578666666666667,
      "grad_norm": 0.11226492375135422,
      "learning_rate": 8.883333333333334e-06,
      "loss": 0.0015,
      "step": 98680
    },
    {
      "epoch": 6.5793333333333335,
      "grad_norm": 0.05005919933319092,
      "learning_rate": 8.879166666666668e-06,
      "loss": 0.0025,
      "step": 98690
    },
    {
      "epoch": 6.58,
      "grad_norm": 0.13533760607242584,
      "learning_rate": 8.875e-06,
      "loss": 0.0022,
      "step": 98700
    },
    {
      "epoch": 6.580666666666667,
      "grad_norm": 0.7654847502708435,
      "learning_rate": 8.870833333333333e-06,
      "loss": 0.0024,
      "step": 98710
    },
    {
      "epoch": 6.581333333333333,
      "grad_norm": 0.40742287039756775,
      "learning_rate": 8.866666666666668e-06,
      "loss": 0.0017,
      "step": 98720
    },
    {
      "epoch": 6.582,
      "grad_norm": 0.2755749821662903,
      "learning_rate": 8.8625e-06,
      "loss": 0.0012,
      "step": 98730
    },
    {
      "epoch": 6.582666666666666,
      "grad_norm": 0.3517373502254486,
      "learning_rate": 8.858333333333333e-06,
      "loss": 0.0022,
      "step": 98740
    },
    {
      "epoch": 6.583333333333333,
      "grad_norm": 0.10721099376678467,
      "learning_rate": 8.854166666666667e-06,
      "loss": 0.0013,
      "step": 98750
    },
    {
      "epoch": 6.584,
      "grad_norm": 0.27346205711364746,
      "learning_rate": 8.85e-06,
      "loss": 0.0021,
      "step": 98760
    },
    {
      "epoch": 6.584666666666667,
      "grad_norm": 0.04738979414105415,
      "learning_rate": 8.845833333333334e-06,
      "loss": 0.002,
      "step": 98770
    },
    {
      "epoch": 6.585333333333334,
      "grad_norm": 0.05166522040963173,
      "learning_rate": 8.841666666666667e-06,
      "loss": 0.0024,
      "step": 98780
    },
    {
      "epoch": 6.586,
      "grad_norm": 0.2441510111093521,
      "learning_rate": 8.8375e-06,
      "loss": 0.0014,
      "step": 98790
    },
    {
      "epoch": 6.586666666666667,
      "grad_norm": 0.4694165587425232,
      "learning_rate": 8.833333333333334e-06,
      "loss": 0.0025,
      "step": 98800
    },
    {
      "epoch": 6.5873333333333335,
      "grad_norm": 0.14602895081043243,
      "learning_rate": 8.829166666666667e-06,
      "loss": 0.0014,
      "step": 98810
    },
    {
      "epoch": 6.588,
      "grad_norm": 0.23312661051750183,
      "learning_rate": 8.825e-06,
      "loss": 0.0019,
      "step": 98820
    },
    {
      "epoch": 6.588666666666667,
      "grad_norm": 0.11288532614707947,
      "learning_rate": 8.820833333333334e-06,
      "loss": 0.0015,
      "step": 98830
    },
    {
      "epoch": 6.589333333333333,
      "grad_norm": 0.3336182236671448,
      "learning_rate": 8.816666666666668e-06,
      "loss": 0.0021,
      "step": 98840
    },
    {
      "epoch": 6.59,
      "grad_norm": 0.3363247513771057,
      "learning_rate": 8.8125e-06,
      "loss": 0.0017,
      "step": 98850
    },
    {
      "epoch": 6.5906666666666665,
      "grad_norm": 0.11041766405105591,
      "learning_rate": 8.808333333333333e-06,
      "loss": 0.0021,
      "step": 98860
    },
    {
      "epoch": 6.591333333333333,
      "grad_norm": 0.17253071069717407,
      "learning_rate": 8.804166666666668e-06,
      "loss": 0.0012,
      "step": 98870
    },
    {
      "epoch": 6.592,
      "grad_norm": 0.11769180744886398,
      "learning_rate": 8.8e-06,
      "loss": 0.0018,
      "step": 98880
    },
    {
      "epoch": 6.592666666666666,
      "grad_norm": 0.064899742603302,
      "learning_rate": 8.795833333333333e-06,
      "loss": 0.0015,
      "step": 98890
    },
    {
      "epoch": 6.593333333333334,
      "grad_norm": 0.1513212025165558,
      "learning_rate": 8.791666666666667e-06,
      "loss": 0.0033,
      "step": 98900
    },
    {
      "epoch": 6.594,
      "grad_norm": 0.04402845725417137,
      "learning_rate": 8.7875e-06,
      "loss": 0.0022,
      "step": 98910
    },
    {
      "epoch": 6.594666666666667,
      "grad_norm": 0.28078973293304443,
      "learning_rate": 8.783333333333335e-06,
      "loss": 0.0025,
      "step": 98920
    },
    {
      "epoch": 6.5953333333333335,
      "grad_norm": 0.3126242756843567,
      "learning_rate": 8.779166666666667e-06,
      "loss": 0.0021,
      "step": 98930
    },
    {
      "epoch": 6.596,
      "grad_norm": 0.2943195402622223,
      "learning_rate": 8.775e-06,
      "loss": 0.0015,
      "step": 98940
    },
    {
      "epoch": 6.596666666666667,
      "grad_norm": 0.34049534797668457,
      "learning_rate": 8.770833333333334e-06,
      "loss": 0.002,
      "step": 98950
    },
    {
      "epoch": 6.597333333333333,
      "grad_norm": 0.3011194169521332,
      "learning_rate": 8.766666666666667e-06,
      "loss": 0.0021,
      "step": 98960
    },
    {
      "epoch": 6.598,
      "grad_norm": 0.18278868496418,
      "learning_rate": 8.7625e-06,
      "loss": 0.0015,
      "step": 98970
    },
    {
      "epoch": 6.5986666666666665,
      "grad_norm": 0.5153378844261169,
      "learning_rate": 8.758333333333334e-06,
      "loss": 0.0017,
      "step": 98980
    },
    {
      "epoch": 6.599333333333333,
      "grad_norm": 0.1379161775112152,
      "learning_rate": 8.754166666666668e-06,
      "loss": 0.0018,
      "step": 98990
    },
    {
      "epoch": 6.6,
      "grad_norm": 0.20485758781433105,
      "learning_rate": 8.75e-06,
      "loss": 0.0016,
      "step": 99000
    },
    {
      "epoch": 6.600666666666667,
      "grad_norm": 0.375015527009964,
      "learning_rate": 8.745833333333334e-06,
      "loss": 0.0018,
      "step": 99010
    },
    {
      "epoch": 6.601333333333334,
      "grad_norm": 0.6490823030471802,
      "learning_rate": 8.741666666666668e-06,
      "loss": 0.0014,
      "step": 99020
    },
    {
      "epoch": 6.602,
      "grad_norm": 0.05531720072031021,
      "learning_rate": 8.7375e-06,
      "loss": 0.0015,
      "step": 99030
    },
    {
      "epoch": 6.602666666666667,
      "grad_norm": 0.11293289065361023,
      "learning_rate": 8.733333333333333e-06,
      "loss": 0.0019,
      "step": 99040
    },
    {
      "epoch": 6.6033333333333335,
      "grad_norm": 0.23299413919448853,
      "learning_rate": 8.729166666666668e-06,
      "loss": 0.0015,
      "step": 99050
    },
    {
      "epoch": 6.604,
      "grad_norm": 0.43689119815826416,
      "learning_rate": 8.725e-06,
      "loss": 0.0019,
      "step": 99060
    },
    {
      "epoch": 6.604666666666667,
      "grad_norm": 0.1439475566148758,
      "learning_rate": 8.720833333333333e-06,
      "loss": 0.0019,
      "step": 99070
    },
    {
      "epoch": 6.605333333333333,
      "grad_norm": 0.04781592637300491,
      "learning_rate": 8.716666666666667e-06,
      "loss": 0.0026,
      "step": 99080
    },
    {
      "epoch": 6.606,
      "grad_norm": 0.14366097748279572,
      "learning_rate": 8.7125e-06,
      "loss": 0.002,
      "step": 99090
    },
    {
      "epoch": 6.6066666666666665,
      "grad_norm": 0.09985300153493881,
      "learning_rate": 8.708333333333334e-06,
      "loss": 0.0025,
      "step": 99100
    },
    {
      "epoch": 6.607333333333333,
      "grad_norm": 0.37040233612060547,
      "learning_rate": 8.704166666666667e-06,
      "loss": 0.002,
      "step": 99110
    },
    {
      "epoch": 6.608,
      "grad_norm": 0.16829398274421692,
      "learning_rate": 8.7e-06,
      "loss": 0.0014,
      "step": 99120
    },
    {
      "epoch": 6.608666666666666,
      "grad_norm": 0.7199851870536804,
      "learning_rate": 8.695833333333334e-06,
      "loss": 0.0017,
      "step": 99130
    },
    {
      "epoch": 6.609333333333334,
      "grad_norm": 0.947232723236084,
      "learning_rate": 8.691666666666667e-06,
      "loss": 0.0024,
      "step": 99140
    },
    {
      "epoch": 6.61,
      "grad_norm": 0.8073930144309998,
      "learning_rate": 8.6875e-06,
      "loss": 0.0029,
      "step": 99150
    },
    {
      "epoch": 6.610666666666667,
      "grad_norm": 0.0763351172208786,
      "learning_rate": 8.683333333333334e-06,
      "loss": 0.0016,
      "step": 99160
    },
    {
      "epoch": 6.6113333333333335,
      "grad_norm": 0.30377304553985596,
      "learning_rate": 8.679166666666668e-06,
      "loss": 0.0014,
      "step": 99170
    },
    {
      "epoch": 6.612,
      "grad_norm": 0.405681312084198,
      "learning_rate": 8.674999999999999e-06,
      "loss": 0.002,
      "step": 99180
    },
    {
      "epoch": 6.612666666666667,
      "grad_norm": 0.17428122460842133,
      "learning_rate": 8.670833333333333e-06,
      "loss": 0.0013,
      "step": 99190
    },
    {
      "epoch": 6.613333333333333,
      "grad_norm": 0.27963098883628845,
      "learning_rate": 8.666666666666668e-06,
      "loss": 0.0013,
      "step": 99200
    },
    {
      "epoch": 6.614,
      "grad_norm": 0.5484485626220703,
      "learning_rate": 8.6625e-06,
      "loss": 0.0016,
      "step": 99210
    },
    {
      "epoch": 6.6146666666666665,
      "grad_norm": 0.17340628802776337,
      "learning_rate": 8.658333333333333e-06,
      "loss": 0.0031,
      "step": 99220
    },
    {
      "epoch": 6.615333333333333,
      "grad_norm": 0.1501714289188385,
      "learning_rate": 8.654166666666668e-06,
      "loss": 0.0014,
      "step": 99230
    },
    {
      "epoch": 6.616,
      "grad_norm": 0.4726991057395935,
      "learning_rate": 8.65e-06,
      "loss": 0.0019,
      "step": 99240
    },
    {
      "epoch": 6.616666666666667,
      "grad_norm": 0.06876623630523682,
      "learning_rate": 8.645833333333333e-06,
      "loss": 0.0016,
      "step": 99250
    },
    {
      "epoch": 6.617333333333333,
      "grad_norm": 0.7386950850486755,
      "learning_rate": 8.641666666666667e-06,
      "loss": 0.0017,
      "step": 99260
    },
    {
      "epoch": 6.618,
      "grad_norm": 0.34568482637405396,
      "learning_rate": 8.6375e-06,
      "loss": 0.0015,
      "step": 99270
    },
    {
      "epoch": 6.618666666666667,
      "grad_norm": 0.06913043558597565,
      "learning_rate": 8.633333333333334e-06,
      "loss": 0.002,
      "step": 99280
    },
    {
      "epoch": 6.6193333333333335,
      "grad_norm": 0.17149192094802856,
      "learning_rate": 8.629166666666667e-06,
      "loss": 0.0014,
      "step": 99290
    },
    {
      "epoch": 6.62,
      "grad_norm": 0.050842005759477615,
      "learning_rate": 8.625e-06,
      "loss": 0.0025,
      "step": 99300
    },
    {
      "epoch": 6.620666666666667,
      "grad_norm": 0.46804845333099365,
      "learning_rate": 8.620833333333334e-06,
      "loss": 0.002,
      "step": 99310
    },
    {
      "epoch": 6.621333333333333,
      "grad_norm": 0.03451760113239288,
      "learning_rate": 8.616666666666667e-06,
      "loss": 0.0014,
      "step": 99320
    },
    {
      "epoch": 6.622,
      "grad_norm": 0.5068978071212769,
      "learning_rate": 8.6125e-06,
      "loss": 0.0026,
      "step": 99330
    },
    {
      "epoch": 6.6226666666666665,
      "grad_norm": 0.4376015365123749,
      "learning_rate": 8.608333333333334e-06,
      "loss": 0.0016,
      "step": 99340
    },
    {
      "epoch": 6.623333333333333,
      "grad_norm": 0.051838312298059464,
      "learning_rate": 8.604166666666668e-06,
      "loss": 0.0017,
      "step": 99350
    },
    {
      "epoch": 6.624,
      "grad_norm": 0.10855010896921158,
      "learning_rate": 8.599999999999999e-06,
      "loss": 0.0015,
      "step": 99360
    },
    {
      "epoch": 6.624666666666666,
      "grad_norm": 0.4031919538974762,
      "learning_rate": 8.595833333333333e-06,
      "loss": 0.0013,
      "step": 99370
    },
    {
      "epoch": 6.625333333333334,
      "grad_norm": 0.5450859069824219,
      "learning_rate": 8.591666666666668e-06,
      "loss": 0.002,
      "step": 99380
    },
    {
      "epoch": 6.626,
      "grad_norm": 0.1362995207309723,
      "learning_rate": 8.5875e-06,
      "loss": 0.0013,
      "step": 99390
    },
    {
      "epoch": 6.626666666666667,
      "grad_norm": 0.23204182088375092,
      "learning_rate": 8.583333333333333e-06,
      "loss": 0.0017,
      "step": 99400
    },
    {
      "epoch": 6.6273333333333335,
      "grad_norm": 0.1657319962978363,
      "learning_rate": 8.579166666666667e-06,
      "loss": 0.0018,
      "step": 99410
    },
    {
      "epoch": 6.628,
      "grad_norm": 0.30000922083854675,
      "learning_rate": 8.575000000000002e-06,
      "loss": 0.002,
      "step": 99420
    },
    {
      "epoch": 6.628666666666667,
      "grad_norm": 0.14036844670772552,
      "learning_rate": 8.570833333333333e-06,
      "loss": 0.0022,
      "step": 99430
    },
    {
      "epoch": 6.629333333333333,
      "grad_norm": 0.4042503535747528,
      "learning_rate": 8.566666666666667e-06,
      "loss": 0.0022,
      "step": 99440
    },
    {
      "epoch": 6.63,
      "grad_norm": 0.2064805030822754,
      "learning_rate": 8.562500000000001e-06,
      "loss": 0.0019,
      "step": 99450
    },
    {
      "epoch": 6.6306666666666665,
      "grad_norm": 0.04866405203938484,
      "learning_rate": 8.558333333333334e-06,
      "loss": 0.0026,
      "step": 99460
    },
    {
      "epoch": 6.631333333333333,
      "grad_norm": 0.5295330286026001,
      "learning_rate": 8.554166666666667e-06,
      "loss": 0.002,
      "step": 99470
    },
    {
      "epoch": 6.632,
      "grad_norm": 0.10325490683317184,
      "learning_rate": 8.550000000000001e-06,
      "loss": 0.002,
      "step": 99480
    },
    {
      "epoch": 6.632666666666667,
      "grad_norm": 0.4063780903816223,
      "learning_rate": 8.545833333333334e-06,
      "loss": 0.0019,
      "step": 99490
    },
    {
      "epoch": 6.633333333333333,
      "grad_norm": 0.1667938083410263,
      "learning_rate": 8.541666666666666e-06,
      "loss": 0.0026,
      "step": 99500
    },
    {
      "epoch": 6.634,
      "grad_norm": 0.046049803495407104,
      "learning_rate": 8.5375e-06,
      "loss": 0.0015,
      "step": 99510
    },
    {
      "epoch": 6.634666666666667,
      "grad_norm": 0.33424440026283264,
      "learning_rate": 8.533333333333334e-06,
      "loss": 0.0019,
      "step": 99520
    },
    {
      "epoch": 6.6353333333333335,
      "grad_norm": 0.5392672419548035,
      "learning_rate": 8.529166666666668e-06,
      "loss": 0.0014,
      "step": 99530
    },
    {
      "epoch": 6.636,
      "grad_norm": 0.2382880002260208,
      "learning_rate": 8.525e-06,
      "loss": 0.0013,
      "step": 99540
    },
    {
      "epoch": 6.636666666666667,
      "grad_norm": 0.45587393641471863,
      "learning_rate": 8.520833333333333e-06,
      "loss": 0.0021,
      "step": 99550
    },
    {
      "epoch": 6.637333333333333,
      "grad_norm": 0.41519486904144287,
      "learning_rate": 8.516666666666668e-06,
      "loss": 0.0016,
      "step": 99560
    },
    {
      "epoch": 6.638,
      "grad_norm": 0.3016197979450226,
      "learning_rate": 8.5125e-06,
      "loss": 0.0017,
      "step": 99570
    },
    {
      "epoch": 6.6386666666666665,
      "grad_norm": 0.5376854538917542,
      "learning_rate": 8.508333333333333e-06,
      "loss": 0.0016,
      "step": 99580
    },
    {
      "epoch": 6.639333333333333,
      "grad_norm": 0.23662294447422028,
      "learning_rate": 8.504166666666667e-06,
      "loss": 0.0015,
      "step": 99590
    },
    {
      "epoch": 6.64,
      "grad_norm": 0.14310692250728607,
      "learning_rate": 8.500000000000002e-06,
      "loss": 0.0014,
      "step": 99600
    },
    {
      "epoch": 6.640666666666666,
      "grad_norm": 0.3730163276195526,
      "learning_rate": 8.495833333333333e-06,
      "loss": 0.0017,
      "step": 99610
    },
    {
      "epoch": 6.641333333333334,
      "grad_norm": 0.4685869514942169,
      "learning_rate": 8.491666666666667e-06,
      "loss": 0.0012,
      "step": 99620
    },
    {
      "epoch": 6.642,
      "grad_norm": 0.4915429949760437,
      "learning_rate": 8.487500000000001e-06,
      "loss": 0.0013,
      "step": 99630
    },
    {
      "epoch": 6.642666666666667,
      "grad_norm": 0.17109352350234985,
      "learning_rate": 8.483333333333334e-06,
      "loss": 0.0014,
      "step": 99640
    },
    {
      "epoch": 6.6433333333333335,
      "grad_norm": 0.22055642306804657,
      "learning_rate": 8.479166666666667e-06,
      "loss": 0.0028,
      "step": 99650
    },
    {
      "epoch": 6.644,
      "grad_norm": 0.30981165170669556,
      "learning_rate": 8.475000000000001e-06,
      "loss": 0.002,
      "step": 99660
    },
    {
      "epoch": 6.644666666666667,
      "grad_norm": 0.10126076638698578,
      "learning_rate": 8.470833333333334e-06,
      "loss": 0.0022,
      "step": 99670
    },
    {
      "epoch": 6.645333333333333,
      "grad_norm": 0.6624733209609985,
      "learning_rate": 8.466666666666666e-06,
      "loss": 0.0016,
      "step": 99680
    },
    {
      "epoch": 6.646,
      "grad_norm": 0.1667606234550476,
      "learning_rate": 8.4625e-06,
      "loss": 0.0027,
      "step": 99690
    },
    {
      "epoch": 6.6466666666666665,
      "grad_norm": 0.3449839651584625,
      "learning_rate": 8.458333333333333e-06,
      "loss": 0.0012,
      "step": 99700
    },
    {
      "epoch": 6.647333333333333,
      "grad_norm": 0.5443990230560303,
      "learning_rate": 8.454166666666668e-06,
      "loss": 0.003,
      "step": 99710
    },
    {
      "epoch": 6.648,
      "grad_norm": 0.36318284273147583,
      "learning_rate": 8.45e-06,
      "loss": 0.0017,
      "step": 99720
    },
    {
      "epoch": 6.648666666666666,
      "grad_norm": 0.11740360409021378,
      "learning_rate": 8.445833333333333e-06,
      "loss": 0.0023,
      "step": 99730
    },
    {
      "epoch": 6.649333333333333,
      "grad_norm": 0.272059828042984,
      "learning_rate": 8.441666666666667e-06,
      "loss": 0.0021,
      "step": 99740
    },
    {
      "epoch": 6.65,
      "grad_norm": 0.3726532459259033,
      "learning_rate": 8.437500000000002e-06,
      "loss": 0.0022,
      "step": 99750
    },
    {
      "epoch": 6.650666666666667,
      "grad_norm": 0.20759789645671844,
      "learning_rate": 8.433333333333333e-06,
      "loss": 0.0017,
      "step": 99760
    },
    {
      "epoch": 6.6513333333333335,
      "grad_norm": 0.04744775965809822,
      "learning_rate": 8.429166666666667e-06,
      "loss": 0.0028,
      "step": 99770
    },
    {
      "epoch": 6.652,
      "grad_norm": 0.26724502444267273,
      "learning_rate": 8.425000000000001e-06,
      "loss": 0.0017,
      "step": 99780
    },
    {
      "epoch": 6.652666666666667,
      "grad_norm": 0.14123637974262238,
      "learning_rate": 8.420833333333334e-06,
      "loss": 0.002,
      "step": 99790
    },
    {
      "epoch": 6.653333333333333,
      "grad_norm": 0.30354735255241394,
      "learning_rate": 8.416666666666667e-06,
      "loss": 0.0018,
      "step": 99800
    },
    {
      "epoch": 6.654,
      "grad_norm": 0.21156048774719238,
      "learning_rate": 8.412500000000001e-06,
      "loss": 0.0022,
      "step": 99810
    },
    {
      "epoch": 6.6546666666666665,
      "grad_norm": 0.31655099987983704,
      "learning_rate": 8.408333333333334e-06,
      "loss": 0.0021,
      "step": 99820
    },
    {
      "epoch": 6.655333333333333,
      "grad_norm": 0.05136983096599579,
      "learning_rate": 8.404166666666667e-06,
      "loss": 0.002,
      "step": 99830
    },
    {
      "epoch": 6.656,
      "grad_norm": 0.10719072818756104,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.002,
      "step": 99840
    },
    {
      "epoch": 6.656666666666666,
      "grad_norm": 0.44048190116882324,
      "learning_rate": 8.395833333333334e-06,
      "loss": 0.0023,
      "step": 99850
    },
    {
      "epoch": 6.657333333333334,
      "grad_norm": 0.2664192318916321,
      "learning_rate": 8.391666666666668e-06,
      "loss": 0.002,
      "step": 99860
    },
    {
      "epoch": 6.658,
      "grad_norm": 0.6717636585235596,
      "learning_rate": 8.3875e-06,
      "loss": 0.0019,
      "step": 99870
    },
    {
      "epoch": 6.658666666666667,
      "grad_norm": 0.0437343493103981,
      "learning_rate": 8.383333333333333e-06,
      "loss": 0.0014,
      "step": 99880
    },
    {
      "epoch": 6.6593333333333335,
      "grad_norm": 0.045157164335250854,
      "learning_rate": 8.379166666666668e-06,
      "loss": 0.0016,
      "step": 99890
    },
    {
      "epoch": 6.66,
      "grad_norm": 0.07422604411840439,
      "learning_rate": 8.375e-06,
      "loss": 0.0018,
      "step": 99900
    },
    {
      "epoch": 6.660666666666667,
      "grad_norm": 0.3368140757083893,
      "learning_rate": 8.370833333333333e-06,
      "loss": 0.0013,
      "step": 99910
    },
    {
      "epoch": 6.661333333333333,
      "grad_norm": 0.1335776150226593,
      "learning_rate": 8.366666666666667e-06,
      "loss": 0.0015,
      "step": 99920
    },
    {
      "epoch": 6.662,
      "grad_norm": 0.5051377415657043,
      "learning_rate": 8.362500000000002e-06,
      "loss": 0.0017,
      "step": 99930
    },
    {
      "epoch": 6.6626666666666665,
      "grad_norm": 0.248896524310112,
      "learning_rate": 8.358333333333333e-06,
      "loss": 0.0012,
      "step": 99940
    },
    {
      "epoch": 6.663333333333333,
      "grad_norm": 0.546029269695282,
      "learning_rate": 8.354166666666667e-06,
      "loss": 0.0015,
      "step": 99950
    },
    {
      "epoch": 6.664,
      "grad_norm": 0.06064249202609062,
      "learning_rate": 8.350000000000001e-06,
      "loss": 0.0014,
      "step": 99960
    },
    {
      "epoch": 6.664666666666666,
      "grad_norm": 0.37159401178359985,
      "learning_rate": 8.345833333333334e-06,
      "loss": 0.0017,
      "step": 99970
    },
    {
      "epoch": 6.665333333333333,
      "grad_norm": 0.23552477359771729,
      "learning_rate": 8.341666666666667e-06,
      "loss": 0.0013,
      "step": 99980
    },
    {
      "epoch": 6.666,
      "grad_norm": 0.2811901271343231,
      "learning_rate": 8.337500000000001e-06,
      "loss": 0.002,
      "step": 99990
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 0.30581846833229065,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.0022,
      "step": 100000
    },
    {
      "epoch": 6.667333333333334,
      "grad_norm": 0.05301570147275925,
      "learning_rate": 8.329166666666666e-06,
      "loss": 0.0012,
      "step": 100010
    },
    {
      "epoch": 6.668,
      "grad_norm": 0.20985233783721924,
      "learning_rate": 8.325e-06,
      "loss": 0.0025,
      "step": 100020
    },
    {
      "epoch": 6.668666666666667,
      "grad_norm": 0.25545886158943176,
      "learning_rate": 8.320833333333333e-06,
      "loss": 0.0022,
      "step": 100030
    },
    {
      "epoch": 6.669333333333333,
      "grad_norm": 0.0691724345088005,
      "learning_rate": 8.316666666666668e-06,
      "loss": 0.0017,
      "step": 100040
    },
    {
      "epoch": 6.67,
      "grad_norm": 0.13828225433826447,
      "learning_rate": 8.3125e-06,
      "loss": 0.0023,
      "step": 100050
    },
    {
      "epoch": 6.6706666666666665,
      "grad_norm": 0.03909227252006531,
      "learning_rate": 8.308333333333333e-06,
      "loss": 0.0028,
      "step": 100060
    },
    {
      "epoch": 6.671333333333333,
      "grad_norm": 0.6399499177932739,
      "learning_rate": 8.304166666666667e-06,
      "loss": 0.0025,
      "step": 100070
    },
    {
      "epoch": 6.672,
      "grad_norm": 0.3685285449028015,
      "learning_rate": 8.3e-06,
      "loss": 0.0013,
      "step": 100080
    },
    {
      "epoch": 6.672666666666666,
      "grad_norm": 0.26483047008514404,
      "learning_rate": 8.295833333333333e-06,
      "loss": 0.0024,
      "step": 100090
    },
    {
      "epoch": 6.673333333333334,
      "grad_norm": 0.4063151776790619,
      "learning_rate": 8.291666666666667e-06,
      "loss": 0.0013,
      "step": 100100
    },
    {
      "epoch": 6.674,
      "grad_norm": 0.20248214900493622,
      "learning_rate": 8.287500000000002e-06,
      "loss": 0.002,
      "step": 100110
    },
    {
      "epoch": 6.674666666666667,
      "grad_norm": 0.10453327000141144,
      "learning_rate": 8.283333333333333e-06,
      "loss": 0.0013,
      "step": 100120
    },
    {
      "epoch": 6.675333333333334,
      "grad_norm": 0.2367149144411087,
      "learning_rate": 8.279166666666667e-06,
      "loss": 0.0021,
      "step": 100130
    },
    {
      "epoch": 6.676,
      "grad_norm": 0.2429255247116089,
      "learning_rate": 8.275000000000001e-06,
      "loss": 0.0018,
      "step": 100140
    },
    {
      "epoch": 6.676666666666667,
      "grad_norm": 0.4322923719882965,
      "learning_rate": 8.270833333333334e-06,
      "loss": 0.0016,
      "step": 100150
    },
    {
      "epoch": 6.677333333333333,
      "grad_norm": 0.1668146699666977,
      "learning_rate": 8.266666666666667e-06,
      "loss": 0.0018,
      "step": 100160
    },
    {
      "epoch": 6.678,
      "grad_norm": 0.3419226408004761,
      "learning_rate": 8.262500000000001e-06,
      "loss": 0.0021,
      "step": 100170
    },
    {
      "epoch": 6.6786666666666665,
      "grad_norm": 0.2368043065071106,
      "learning_rate": 8.258333333333334e-06,
      "loss": 0.0018,
      "step": 100180
    },
    {
      "epoch": 6.679333333333333,
      "grad_norm": 0.03706657513976097,
      "learning_rate": 8.254166666666666e-06,
      "loss": 0.0022,
      "step": 100190
    },
    {
      "epoch": 6.68,
      "grad_norm": 0.3102284371852875,
      "learning_rate": 8.25e-06,
      "loss": 0.0016,
      "step": 100200
    },
    {
      "epoch": 6.680666666666666,
      "grad_norm": 0.32210204005241394,
      "learning_rate": 8.245833333333333e-06,
      "loss": 0.0023,
      "step": 100210
    },
    {
      "epoch": 6.681333333333333,
      "grad_norm": 0.18056978285312653,
      "learning_rate": 8.241666666666668e-06,
      "loss": 0.0015,
      "step": 100220
    },
    {
      "epoch": 6.682,
      "grad_norm": 0.20489950478076935,
      "learning_rate": 8.2375e-06,
      "loss": 0.0018,
      "step": 100230
    },
    {
      "epoch": 6.682666666666667,
      "grad_norm": 0.523730456829071,
      "learning_rate": 8.233333333333333e-06,
      "loss": 0.002,
      "step": 100240
    },
    {
      "epoch": 6.683333333333334,
      "grad_norm": 0.10462483018636703,
      "learning_rate": 8.229166666666667e-06,
      "loss": 0.0017,
      "step": 100250
    },
    {
      "epoch": 6.684,
      "grad_norm": 0.3410385847091675,
      "learning_rate": 8.225e-06,
      "loss": 0.0016,
      "step": 100260
    },
    {
      "epoch": 6.684666666666667,
      "grad_norm": 0.14170992374420166,
      "learning_rate": 8.220833333333333e-06,
      "loss": 0.0012,
      "step": 100270
    },
    {
      "epoch": 6.685333333333333,
      "grad_norm": 0.051302168518304825,
      "learning_rate": 8.216666666666667e-06,
      "loss": 0.0016,
      "step": 100280
    },
    {
      "epoch": 6.686,
      "grad_norm": 0.16903215646743774,
      "learning_rate": 8.212500000000001e-06,
      "loss": 0.0015,
      "step": 100290
    },
    {
      "epoch": 6.6866666666666665,
      "grad_norm": 0.1519557535648346,
      "learning_rate": 8.208333333333332e-06,
      "loss": 0.0025,
      "step": 100300
    },
    {
      "epoch": 6.687333333333333,
      "grad_norm": 0.17361631989479065,
      "learning_rate": 8.204166666666667e-06,
      "loss": 0.0015,
      "step": 100310
    },
    {
      "epoch": 6.688,
      "grad_norm": 0.07753787189722061,
      "learning_rate": 8.200000000000001e-06,
      "loss": 0.0023,
      "step": 100320
    },
    {
      "epoch": 6.688666666666666,
      "grad_norm": 0.20840023458003998,
      "learning_rate": 8.195833333333334e-06,
      "loss": 0.002,
      "step": 100330
    },
    {
      "epoch": 6.689333333333334,
      "grad_norm": 0.5459094643592834,
      "learning_rate": 8.191666666666666e-06,
      "loss": 0.0024,
      "step": 100340
    },
    {
      "epoch": 6.6899999999999995,
      "grad_norm": 0.1799224615097046,
      "learning_rate": 8.1875e-06,
      "loss": 0.0019,
      "step": 100350
    },
    {
      "epoch": 6.690666666666667,
      "grad_norm": 0.5201182961463928,
      "learning_rate": 8.183333333333333e-06,
      "loss": 0.0021,
      "step": 100360
    },
    {
      "epoch": 6.691333333333334,
      "grad_norm": 0.4366743564605713,
      "learning_rate": 8.179166666666666e-06,
      "loss": 0.0016,
      "step": 100370
    },
    {
      "epoch": 6.692,
      "grad_norm": 0.07695937156677246,
      "learning_rate": 8.175e-06,
      "loss": 0.0018,
      "step": 100380
    },
    {
      "epoch": 6.692666666666667,
      "grad_norm": 0.06540924310684204,
      "learning_rate": 8.170833333333333e-06,
      "loss": 0.0013,
      "step": 100390
    },
    {
      "epoch": 6.693333333333333,
      "grad_norm": 0.1406647264957428,
      "learning_rate": 8.166666666666668e-06,
      "loss": 0.0038,
      "step": 100400
    },
    {
      "epoch": 6.694,
      "grad_norm": 0.17733845114707947,
      "learning_rate": 8.1625e-06,
      "loss": 0.0025,
      "step": 100410
    },
    {
      "epoch": 6.6946666666666665,
      "grad_norm": 0.25896698236465454,
      "learning_rate": 8.158333333333333e-06,
      "loss": 0.0027,
      "step": 100420
    },
    {
      "epoch": 6.695333333333333,
      "grad_norm": 0.203147292137146,
      "learning_rate": 8.154166666666667e-06,
      "loss": 0.0012,
      "step": 100430
    },
    {
      "epoch": 6.696,
      "grad_norm": 0.41183432936668396,
      "learning_rate": 8.15e-06,
      "loss": 0.0022,
      "step": 100440
    },
    {
      "epoch": 6.696666666666666,
      "grad_norm": 0.2655656635761261,
      "learning_rate": 8.145833333333333e-06,
      "loss": 0.0029,
      "step": 100450
    },
    {
      "epoch": 6.697333333333333,
      "grad_norm": 0.10346642136573792,
      "learning_rate": 8.141666666666667e-06,
      "loss": 0.0016,
      "step": 100460
    },
    {
      "epoch": 6.698,
      "grad_norm": 0.14104951918125153,
      "learning_rate": 8.137500000000001e-06,
      "loss": 0.0018,
      "step": 100470
    },
    {
      "epoch": 6.698666666666667,
      "grad_norm": 0.4701184034347534,
      "learning_rate": 8.133333333333332e-06,
      "loss": 0.0013,
      "step": 100480
    },
    {
      "epoch": 6.699333333333334,
      "grad_norm": 0.19929847121238708,
      "learning_rate": 8.129166666666667e-06,
      "loss": 0.0016,
      "step": 100490
    },
    {
      "epoch": 6.7,
      "grad_norm": 0.4285091459751129,
      "learning_rate": 8.125000000000001e-06,
      "loss": 0.0021,
      "step": 100500
    },
    {
      "epoch": 6.700666666666667,
      "grad_norm": 0.4768166244029999,
      "learning_rate": 8.120833333333334e-06,
      "loss": 0.0025,
      "step": 100510
    },
    {
      "epoch": 6.701333333333333,
      "grad_norm": 0.03663022071123123,
      "learning_rate": 8.116666666666666e-06,
      "loss": 0.0015,
      "step": 100520
    },
    {
      "epoch": 6.702,
      "grad_norm": 0.03992860019207001,
      "learning_rate": 8.1125e-06,
      "loss": 0.0019,
      "step": 100530
    },
    {
      "epoch": 6.7026666666666666,
      "grad_norm": 0.11232712864875793,
      "learning_rate": 8.108333333333333e-06,
      "loss": 0.0018,
      "step": 100540
    },
    {
      "epoch": 6.703333333333333,
      "grad_norm": 0.16768619418144226,
      "learning_rate": 8.104166666666666e-06,
      "loss": 0.0017,
      "step": 100550
    },
    {
      "epoch": 6.704,
      "grad_norm": 0.1366322934627533,
      "learning_rate": 8.1e-06,
      "loss": 0.0015,
      "step": 100560
    },
    {
      "epoch": 6.704666666666666,
      "grad_norm": 0.10778257250785828,
      "learning_rate": 8.095833333333333e-06,
      "loss": 0.0015,
      "step": 100570
    },
    {
      "epoch": 6.705333333333334,
      "grad_norm": 0.4156680405139923,
      "learning_rate": 8.091666666666667e-06,
      "loss": 0.0016,
      "step": 100580
    },
    {
      "epoch": 6.7059999999999995,
      "grad_norm": 0.5045901536941528,
      "learning_rate": 8.0875e-06,
      "loss": 0.0022,
      "step": 100590
    },
    {
      "epoch": 6.706666666666667,
      "grad_norm": 0.23707209527492523,
      "learning_rate": 8.083333333333333e-06,
      "loss": 0.0019,
      "step": 100600
    },
    {
      "epoch": 6.707333333333334,
      "grad_norm": 0.11690099537372589,
      "learning_rate": 8.079166666666667e-06,
      "loss": 0.0014,
      "step": 100610
    },
    {
      "epoch": 6.708,
      "grad_norm": 0.24042628705501556,
      "learning_rate": 8.075000000000001e-06,
      "loss": 0.0021,
      "step": 100620
    },
    {
      "epoch": 6.708666666666667,
      "grad_norm": 0.05498647689819336,
      "learning_rate": 8.070833333333334e-06,
      "loss": 0.0016,
      "step": 100630
    },
    {
      "epoch": 6.709333333333333,
      "grad_norm": 0.04228658601641655,
      "learning_rate": 8.066666666666667e-06,
      "loss": 0.0015,
      "step": 100640
    },
    {
      "epoch": 6.71,
      "grad_norm": 0.24206143617630005,
      "learning_rate": 8.062500000000001e-06,
      "loss": 0.0016,
      "step": 100650
    },
    {
      "epoch": 6.710666666666667,
      "grad_norm": 0.07805950194597244,
      "learning_rate": 8.058333333333334e-06,
      "loss": 0.0014,
      "step": 100660
    },
    {
      "epoch": 6.711333333333333,
      "grad_norm": 0.3003748953342438,
      "learning_rate": 8.054166666666666e-06,
      "loss": 0.0017,
      "step": 100670
    },
    {
      "epoch": 6.712,
      "grad_norm": 0.635729193687439,
      "learning_rate": 8.050000000000001e-06,
      "loss": 0.0017,
      "step": 100680
    },
    {
      "epoch": 6.712666666666666,
      "grad_norm": 0.14120177924633026,
      "learning_rate": 8.045833333333335e-06,
      "loss": 0.0026,
      "step": 100690
    },
    {
      "epoch": 6.713333333333333,
      "grad_norm": 0.076516292989254,
      "learning_rate": 8.041666666666666e-06,
      "loss": 0.0021,
      "step": 100700
    },
    {
      "epoch": 6.714,
      "grad_norm": 0.5660749077796936,
      "learning_rate": 8.0375e-06,
      "loss": 0.0022,
      "step": 100710
    },
    {
      "epoch": 6.714666666666667,
      "grad_norm": 0.17639786005020142,
      "learning_rate": 8.033333333333335e-06,
      "loss": 0.0014,
      "step": 100720
    },
    {
      "epoch": 6.715333333333334,
      "grad_norm": 0.04430174082517624,
      "learning_rate": 8.029166666666668e-06,
      "loss": 0.0024,
      "step": 100730
    },
    {
      "epoch": 6.716,
      "grad_norm": 0.13960404694080353,
      "learning_rate": 8.025e-06,
      "loss": 0.0019,
      "step": 100740
    },
    {
      "epoch": 6.716666666666667,
      "grad_norm": 0.10841360688209534,
      "learning_rate": 8.020833333333335e-06,
      "loss": 0.0018,
      "step": 100750
    },
    {
      "epoch": 6.717333333333333,
      "grad_norm": 0.10173115879297256,
      "learning_rate": 8.016666666666667e-06,
      "loss": 0.0041,
      "step": 100760
    },
    {
      "epoch": 6.718,
      "grad_norm": 0.5023254752159119,
      "learning_rate": 8.0125e-06,
      "loss": 0.0015,
      "step": 100770
    },
    {
      "epoch": 6.718666666666667,
      "grad_norm": 0.14168065786361694,
      "learning_rate": 8.008333333333334e-06,
      "loss": 0.0015,
      "step": 100780
    },
    {
      "epoch": 6.719333333333333,
      "grad_norm": 0.2047029733657837,
      "learning_rate": 8.004166666666667e-06,
      "loss": 0.0011,
      "step": 100790
    },
    {
      "epoch": 6.72,
      "grad_norm": 0.17463509738445282,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.0025,
      "step": 100800
    },
    {
      "epoch": 6.720666666666666,
      "grad_norm": 0.03324154391884804,
      "learning_rate": 7.995833333333334e-06,
      "loss": 0.0017,
      "step": 100810
    },
    {
      "epoch": 6.721333333333334,
      "grad_norm": 0.10900958627462387,
      "learning_rate": 7.991666666666667e-06,
      "loss": 0.0015,
      "step": 100820
    },
    {
      "epoch": 6.7219999999999995,
      "grad_norm": 0.6929230093955994,
      "learning_rate": 7.987500000000001e-06,
      "loss": 0.0015,
      "step": 100830
    },
    {
      "epoch": 6.722666666666667,
      "grad_norm": 0.10582084208726883,
      "learning_rate": 7.983333333333334e-06,
      "loss": 0.0017,
      "step": 100840
    },
    {
      "epoch": 6.723333333333334,
      "grad_norm": 0.1381385177373886,
      "learning_rate": 7.979166666666666e-06,
      "loss": 0.0016,
      "step": 100850
    },
    {
      "epoch": 6.724,
      "grad_norm": 0.04949558898806572,
      "learning_rate": 7.975e-06,
      "loss": 0.002,
      "step": 100860
    },
    {
      "epoch": 6.724666666666667,
      "grad_norm": 0.40177932381629944,
      "learning_rate": 7.970833333333335e-06,
      "loss": 0.0024,
      "step": 100870
    },
    {
      "epoch": 6.725333333333333,
      "grad_norm": 0.25583985447883606,
      "learning_rate": 7.966666666666666e-06,
      "loss": 0.0013,
      "step": 100880
    },
    {
      "epoch": 6.726,
      "grad_norm": 0.10817541182041168,
      "learning_rate": 7.9625e-06,
      "loss": 0.0021,
      "step": 100890
    },
    {
      "epoch": 6.726666666666667,
      "grad_norm": 0.04197879135608673,
      "learning_rate": 7.958333333333335e-06,
      "loss": 0.0021,
      "step": 100900
    },
    {
      "epoch": 6.727333333333333,
      "grad_norm": 0.17268598079681396,
      "learning_rate": 7.954166666666667e-06,
      "loss": 0.002,
      "step": 100910
    },
    {
      "epoch": 6.728,
      "grad_norm": 0.23850345611572266,
      "learning_rate": 7.95e-06,
      "loss": 0.0013,
      "step": 100920
    },
    {
      "epoch": 6.728666666666666,
      "grad_norm": 0.04357925057411194,
      "learning_rate": 7.945833333333334e-06,
      "loss": 0.0014,
      "step": 100930
    },
    {
      "epoch": 6.729333333333333,
      "grad_norm": 0.1356087625026703,
      "learning_rate": 7.941666666666667e-06,
      "loss": 0.0023,
      "step": 100940
    },
    {
      "epoch": 6.73,
      "grad_norm": 0.16509827971458435,
      "learning_rate": 7.9375e-06,
      "loss": 0.0016,
      "step": 100950
    },
    {
      "epoch": 6.730666666666667,
      "grad_norm": 0.15573924779891968,
      "learning_rate": 7.933333333333334e-06,
      "loss": 0.002,
      "step": 100960
    },
    {
      "epoch": 6.731333333333334,
      "grad_norm": 0.11133433133363724,
      "learning_rate": 7.929166666666667e-06,
      "loss": 0.002,
      "step": 100970
    },
    {
      "epoch": 6.732,
      "grad_norm": 0.4125751852989197,
      "learning_rate": 7.925000000000001e-06,
      "loss": 0.0027,
      "step": 100980
    },
    {
      "epoch": 6.732666666666667,
      "grad_norm": 0.04372265562415123,
      "learning_rate": 7.920833333333334e-06,
      "loss": 0.0012,
      "step": 100990
    },
    {
      "epoch": 6.733333333333333,
      "grad_norm": 0.23245862126350403,
      "learning_rate": 7.916666666666667e-06,
      "loss": 0.0015,
      "step": 101000
    },
    {
      "epoch": 6.734,
      "grad_norm": 0.20564036071300507,
      "learning_rate": 7.912500000000001e-06,
      "loss": 0.0023,
      "step": 101010
    },
    {
      "epoch": 6.734666666666667,
      "grad_norm": 0.07150811702013016,
      "learning_rate": 7.908333333333334e-06,
      "loss": 0.0017,
      "step": 101020
    },
    {
      "epoch": 6.735333333333333,
      "grad_norm": 0.44069352746009827,
      "learning_rate": 7.904166666666666e-06,
      "loss": 0.002,
      "step": 101030
    },
    {
      "epoch": 6.736,
      "grad_norm": 0.7109498977661133,
      "learning_rate": 7.9e-06,
      "loss": 0.0018,
      "step": 101040
    },
    {
      "epoch": 6.736666666666666,
      "grad_norm": 0.7054495811462402,
      "learning_rate": 7.895833333333335e-06,
      "loss": 0.002,
      "step": 101050
    },
    {
      "epoch": 6.737333333333333,
      "grad_norm": 0.5013184547424316,
      "learning_rate": 7.891666666666666e-06,
      "loss": 0.0013,
      "step": 101060
    },
    {
      "epoch": 6.7379999999999995,
      "grad_norm": 0.17746692895889282,
      "learning_rate": 7.8875e-06,
      "loss": 0.0025,
      "step": 101070
    },
    {
      "epoch": 6.738666666666667,
      "grad_norm": 0.0741095095872879,
      "learning_rate": 7.883333333333335e-06,
      "loss": 0.0015,
      "step": 101080
    },
    {
      "epoch": 6.739333333333334,
      "grad_norm": 0.10905865579843521,
      "learning_rate": 7.879166666666667e-06,
      "loss": 0.0019,
      "step": 101090
    },
    {
      "epoch": 6.74,
      "grad_norm": 0.2748371362686157,
      "learning_rate": 7.875e-06,
      "loss": 0.0016,
      "step": 101100
    },
    {
      "epoch": 6.740666666666667,
      "grad_norm": 0.16341814398765564,
      "learning_rate": 7.870833333333334e-06,
      "loss": 0.0018,
      "step": 101110
    },
    {
      "epoch": 6.741333333333333,
      "grad_norm": 0.4128419756889343,
      "learning_rate": 7.866666666666667e-06,
      "loss": 0.0024,
      "step": 101120
    },
    {
      "epoch": 6.742,
      "grad_norm": 0.273176908493042,
      "learning_rate": 7.8625e-06,
      "loss": 0.002,
      "step": 101130
    },
    {
      "epoch": 6.742666666666667,
      "grad_norm": 0.17705059051513672,
      "learning_rate": 7.858333333333334e-06,
      "loss": 0.0014,
      "step": 101140
    },
    {
      "epoch": 6.743333333333333,
      "grad_norm": 0.473605751991272,
      "learning_rate": 7.854166666666667e-06,
      "loss": 0.002,
      "step": 101150
    },
    {
      "epoch": 6.744,
      "grad_norm": 0.30232492089271545,
      "learning_rate": 7.850000000000001e-06,
      "loss": 0.002,
      "step": 101160
    },
    {
      "epoch": 6.744666666666666,
      "grad_norm": 0.4454430043697357,
      "learning_rate": 7.845833333333334e-06,
      "loss": 0.0022,
      "step": 101170
    },
    {
      "epoch": 6.745333333333333,
      "grad_norm": 0.07503736019134521,
      "learning_rate": 7.841666666666666e-06,
      "loss": 0.0017,
      "step": 101180
    },
    {
      "epoch": 6.746,
      "grad_norm": 0.30664339661598206,
      "learning_rate": 7.8375e-06,
      "loss": 0.0021,
      "step": 101190
    },
    {
      "epoch": 6.746666666666667,
      "grad_norm": 0.047093313187360764,
      "learning_rate": 7.833333333333333e-06,
      "loss": 0.002,
      "step": 101200
    },
    {
      "epoch": 6.747333333333334,
      "grad_norm": 0.20225924253463745,
      "learning_rate": 7.829166666666666e-06,
      "loss": 0.0025,
      "step": 101210
    },
    {
      "epoch": 6.748,
      "grad_norm": 0.3356879651546478,
      "learning_rate": 7.825e-06,
      "loss": 0.0012,
      "step": 101220
    },
    {
      "epoch": 6.748666666666667,
      "grad_norm": 0.30080416798591614,
      "learning_rate": 7.820833333333335e-06,
      "loss": 0.0016,
      "step": 101230
    },
    {
      "epoch": 6.749333333333333,
      "grad_norm": 0.14334486424922943,
      "learning_rate": 7.816666666666666e-06,
      "loss": 0.0015,
      "step": 101240
    },
    {
      "epoch": 6.75,
      "grad_norm": 0.1772717386484146,
      "learning_rate": 7.8125e-06,
      "loss": 0.0019,
      "step": 101250
    },
    {
      "epoch": 6.750666666666667,
      "grad_norm": 0.2015237808227539,
      "learning_rate": 7.808333333333335e-06,
      "loss": 0.0013,
      "step": 101260
    },
    {
      "epoch": 6.751333333333333,
      "grad_norm": 0.03223314508795738,
      "learning_rate": 7.804166666666667e-06,
      "loss": 0.002,
      "step": 101270
    },
    {
      "epoch": 6.752,
      "grad_norm": 0.3003915846347809,
      "learning_rate": 7.8e-06,
      "loss": 0.0016,
      "step": 101280
    },
    {
      "epoch": 6.752666666666666,
      "grad_norm": 0.37659770250320435,
      "learning_rate": 7.795833333333334e-06,
      "loss": 0.0019,
      "step": 101290
    },
    {
      "epoch": 6.753333333333333,
      "grad_norm": 0.13739143311977386,
      "learning_rate": 7.791666666666667e-06,
      "loss": 0.0018,
      "step": 101300
    },
    {
      "epoch": 6.754,
      "grad_norm": 0.07425298541784286,
      "learning_rate": 7.7875e-06,
      "loss": 0.0023,
      "step": 101310
    },
    {
      "epoch": 6.754666666666667,
      "grad_norm": 0.30602702498435974,
      "learning_rate": 7.783333333333334e-06,
      "loss": 0.0014,
      "step": 101320
    },
    {
      "epoch": 6.755333333333334,
      "grad_norm": 0.20786379277706146,
      "learning_rate": 7.779166666666667e-06,
      "loss": 0.0027,
      "step": 101330
    },
    {
      "epoch": 6.756,
      "grad_norm": 0.43480661511421204,
      "learning_rate": 7.775000000000001e-06,
      "loss": 0.0015,
      "step": 101340
    },
    {
      "epoch": 6.756666666666667,
      "grad_norm": 0.13585321605205536,
      "learning_rate": 7.770833333333334e-06,
      "loss": 0.0014,
      "step": 101350
    },
    {
      "epoch": 6.757333333333333,
      "grad_norm": 0.5204389095306396,
      "learning_rate": 7.766666666666666e-06,
      "loss": 0.0019,
      "step": 101360
    },
    {
      "epoch": 6.758,
      "grad_norm": 0.14825913310050964,
      "learning_rate": 7.7625e-06,
      "loss": 0.0018,
      "step": 101370
    },
    {
      "epoch": 6.758666666666667,
      "grad_norm": 0.10828112810850143,
      "learning_rate": 7.758333333333333e-06,
      "loss": 0.0031,
      "step": 101380
    },
    {
      "epoch": 6.759333333333333,
      "grad_norm": 0.18475602567195892,
      "learning_rate": 7.754166666666666e-06,
      "loss": 0.0025,
      "step": 101390
    },
    {
      "epoch": 6.76,
      "grad_norm": 0.2690257728099823,
      "learning_rate": 7.75e-06,
      "loss": 0.0012,
      "step": 101400
    },
    {
      "epoch": 6.760666666666666,
      "grad_norm": 0.05918073281645775,
      "learning_rate": 7.745833333333335e-06,
      "loss": 0.0014,
      "step": 101410
    },
    {
      "epoch": 6.761333333333333,
      "grad_norm": 0.40884751081466675,
      "learning_rate": 7.741666666666666e-06,
      "loss": 0.0014,
      "step": 101420
    },
    {
      "epoch": 6.7620000000000005,
      "grad_norm": 0.20458446443080902,
      "learning_rate": 7.7375e-06,
      "loss": 0.0013,
      "step": 101430
    },
    {
      "epoch": 6.762666666666667,
      "grad_norm": 0.4410633444786072,
      "learning_rate": 7.733333333333334e-06,
      "loss": 0.002,
      "step": 101440
    },
    {
      "epoch": 6.763333333333334,
      "grad_norm": 0.7060837745666504,
      "learning_rate": 7.729166666666667e-06,
      "loss": 0.0017,
      "step": 101450
    },
    {
      "epoch": 6.764,
      "grad_norm": 0.40089330077171326,
      "learning_rate": 7.725e-06,
      "loss": 0.0013,
      "step": 101460
    },
    {
      "epoch": 6.764666666666667,
      "grad_norm": 0.1447092890739441,
      "learning_rate": 7.720833333333334e-06,
      "loss": 0.0018,
      "step": 101470
    },
    {
      "epoch": 6.765333333333333,
      "grad_norm": 0.23841950297355652,
      "learning_rate": 7.716666666666667e-06,
      "loss": 0.0019,
      "step": 101480
    },
    {
      "epoch": 6.766,
      "grad_norm": 0.3241857886314392,
      "learning_rate": 7.712500000000001e-06,
      "loss": 0.0024,
      "step": 101490
    },
    {
      "epoch": 6.766666666666667,
      "grad_norm": 0.10147488862276077,
      "learning_rate": 7.708333333333334e-06,
      "loss": 0.0021,
      "step": 101500
    },
    {
      "epoch": 6.767333333333333,
      "grad_norm": 0.41200000047683716,
      "learning_rate": 7.704166666666666e-06,
      "loss": 0.0015,
      "step": 101510
    },
    {
      "epoch": 6.768,
      "grad_norm": 0.46672526001930237,
      "learning_rate": 7.7e-06,
      "loss": 0.0016,
      "step": 101520
    },
    {
      "epoch": 6.768666666666666,
      "grad_norm": 0.2991691529750824,
      "learning_rate": 7.695833333333333e-06,
      "loss": 0.0023,
      "step": 101530
    },
    {
      "epoch": 6.769333333333333,
      "grad_norm": 0.17120832204818726,
      "learning_rate": 7.691666666666666e-06,
      "loss": 0.002,
      "step": 101540
    },
    {
      "epoch": 6.77,
      "grad_norm": 0.11330054700374603,
      "learning_rate": 7.6875e-06,
      "loss": 0.0028,
      "step": 101550
    },
    {
      "epoch": 6.770666666666667,
      "grad_norm": 0.20219223201274872,
      "learning_rate": 7.683333333333335e-06,
      "loss": 0.0016,
      "step": 101560
    },
    {
      "epoch": 6.771333333333334,
      "grad_norm": 0.42417246103286743,
      "learning_rate": 7.679166666666666e-06,
      "loss": 0.0019,
      "step": 101570
    },
    {
      "epoch": 6.772,
      "grad_norm": 0.2325325906276703,
      "learning_rate": 7.675e-06,
      "loss": 0.0018,
      "step": 101580
    },
    {
      "epoch": 6.772666666666667,
      "grad_norm": 0.08930457383394241,
      "learning_rate": 7.670833333333335e-06,
      "loss": 0.0017,
      "step": 101590
    },
    {
      "epoch": 6.773333333333333,
      "grad_norm": 0.0712905302643776,
      "learning_rate": 7.666666666666667e-06,
      "loss": 0.0025,
      "step": 101600
    },
    {
      "epoch": 6.774,
      "grad_norm": 0.5877043008804321,
      "learning_rate": 7.6625e-06,
      "loss": 0.0023,
      "step": 101610
    },
    {
      "epoch": 6.774666666666667,
      "grad_norm": 0.17044740915298462,
      "learning_rate": 7.658333333333334e-06,
      "loss": 0.0017,
      "step": 101620
    },
    {
      "epoch": 6.775333333333333,
      "grad_norm": 0.15911169350147247,
      "learning_rate": 7.654166666666667e-06,
      "loss": 0.0021,
      "step": 101630
    },
    {
      "epoch": 6.776,
      "grad_norm": 0.7050141096115112,
      "learning_rate": 7.65e-06,
      "loss": 0.0017,
      "step": 101640
    },
    {
      "epoch": 6.776666666666666,
      "grad_norm": 0.5576760172843933,
      "learning_rate": 7.645833333333334e-06,
      "loss": 0.002,
      "step": 101650
    },
    {
      "epoch": 6.777333333333333,
      "grad_norm": 0.2382034808397293,
      "learning_rate": 7.641666666666667e-06,
      "loss": 0.0016,
      "step": 101660
    },
    {
      "epoch": 6.7780000000000005,
      "grad_norm": 0.2682691514492035,
      "learning_rate": 7.637500000000001e-06,
      "loss": 0.0014,
      "step": 101670
    },
    {
      "epoch": 6.778666666666666,
      "grad_norm": 0.07024497538805008,
      "learning_rate": 7.633333333333334e-06,
      "loss": 0.0021,
      "step": 101680
    },
    {
      "epoch": 6.779333333333334,
      "grad_norm": 0.13324443995952606,
      "learning_rate": 7.629166666666666e-06,
      "loss": 0.0026,
      "step": 101690
    },
    {
      "epoch": 6.78,
      "grad_norm": 0.13802742958068848,
      "learning_rate": 7.625e-06,
      "loss": 0.0022,
      "step": 101700
    },
    {
      "epoch": 6.780666666666667,
      "grad_norm": 0.24032363295555115,
      "learning_rate": 7.620833333333334e-06,
      "loss": 0.0015,
      "step": 101710
    },
    {
      "epoch": 6.781333333333333,
      "grad_norm": 0.2775084674358368,
      "learning_rate": 7.616666666666666e-06,
      "loss": 0.0014,
      "step": 101720
    },
    {
      "epoch": 6.782,
      "grad_norm": 0.10542584210634232,
      "learning_rate": 7.6125e-06,
      "loss": 0.0018,
      "step": 101730
    },
    {
      "epoch": 6.782666666666667,
      "grad_norm": 0.045053575187921524,
      "learning_rate": 7.608333333333334e-06,
      "loss": 0.0019,
      "step": 101740
    },
    {
      "epoch": 6.783333333333333,
      "grad_norm": 0.030416512861847878,
      "learning_rate": 7.6041666666666666e-06,
      "loss": 0.0028,
      "step": 101750
    },
    {
      "epoch": 6.784,
      "grad_norm": 0.4109826982021332,
      "learning_rate": 7.6e-06,
      "loss": 0.0014,
      "step": 101760
    },
    {
      "epoch": 6.784666666666666,
      "grad_norm": 0.3043312430381775,
      "learning_rate": 7.595833333333334e-06,
      "loss": 0.0023,
      "step": 101770
    },
    {
      "epoch": 6.785333333333333,
      "grad_norm": 0.16806261241436005,
      "learning_rate": 7.591666666666666e-06,
      "loss": 0.0014,
      "step": 101780
    },
    {
      "epoch": 6.786,
      "grad_norm": 0.14947360754013062,
      "learning_rate": 7.5875e-06,
      "loss": 0.003,
      "step": 101790
    },
    {
      "epoch": 6.786666666666667,
      "grad_norm": 0.056697994470596313,
      "learning_rate": 7.583333333333334e-06,
      "loss": 0.0023,
      "step": 101800
    },
    {
      "epoch": 6.787333333333334,
      "grad_norm": 0.05696343258023262,
      "learning_rate": 7.579166666666666e-06,
      "loss": 0.0023,
      "step": 101810
    },
    {
      "epoch": 6.788,
      "grad_norm": 0.5692012906074524,
      "learning_rate": 7.575e-06,
      "loss": 0.0023,
      "step": 101820
    },
    {
      "epoch": 6.788666666666667,
      "grad_norm": 0.5739250183105469,
      "learning_rate": 7.570833333333334e-06,
      "loss": 0.0014,
      "step": 101830
    },
    {
      "epoch": 6.789333333333333,
      "grad_norm": 0.5038347244262695,
      "learning_rate": 7.5666666666666665e-06,
      "loss": 0.0013,
      "step": 101840
    },
    {
      "epoch": 6.79,
      "grad_norm": 0.040176499634981155,
      "learning_rate": 7.5625e-06,
      "loss": 0.0016,
      "step": 101850
    },
    {
      "epoch": 6.790666666666667,
      "grad_norm": 0.4364786744117737,
      "learning_rate": 7.5583333333333335e-06,
      "loss": 0.0014,
      "step": 101860
    },
    {
      "epoch": 6.791333333333333,
      "grad_norm": 0.3088286817073822,
      "learning_rate": 7.554166666666666e-06,
      "loss": 0.002,
      "step": 101870
    },
    {
      "epoch": 6.792,
      "grad_norm": 0.2399369329214096,
      "learning_rate": 7.55e-06,
      "loss": 0.0018,
      "step": 101880
    },
    {
      "epoch": 6.792666666666666,
      "grad_norm": 0.023963721469044685,
      "learning_rate": 7.545833333333334e-06,
      "loss": 0.0025,
      "step": 101890
    },
    {
      "epoch": 6.793333333333333,
      "grad_norm": 0.10842461884021759,
      "learning_rate": 7.541666666666668e-06,
      "loss": 0.0013,
      "step": 101900
    },
    {
      "epoch": 6.7940000000000005,
      "grad_norm": 0.3689938485622406,
      "learning_rate": 7.5375e-06,
      "loss": 0.0022,
      "step": 101910
    },
    {
      "epoch": 6.794666666666666,
      "grad_norm": 0.07638464123010635,
      "learning_rate": 7.533333333333334e-06,
      "loss": 0.0025,
      "step": 101920
    },
    {
      "epoch": 6.795333333333334,
      "grad_norm": 0.5072193145751953,
      "learning_rate": 7.529166666666667e-06,
      "loss": 0.0021,
      "step": 101930
    },
    {
      "epoch": 6.796,
      "grad_norm": 0.4412907361984253,
      "learning_rate": 7.525e-06,
      "loss": 0.0015,
      "step": 101940
    },
    {
      "epoch": 6.796666666666667,
      "grad_norm": 0.041821300983428955,
      "learning_rate": 7.520833333333334e-06,
      "loss": 0.0022,
      "step": 101950
    },
    {
      "epoch": 6.7973333333333334,
      "grad_norm": 0.02187817171216011,
      "learning_rate": 7.516666666666668e-06,
      "loss": 0.0016,
      "step": 101960
    },
    {
      "epoch": 6.798,
      "grad_norm": 0.21058186888694763,
      "learning_rate": 7.5125000000000005e-06,
      "loss": 0.0013,
      "step": 101970
    },
    {
      "epoch": 6.798666666666667,
      "grad_norm": 0.10807526856660843,
      "learning_rate": 7.508333333333334e-06,
      "loss": 0.0016,
      "step": 101980
    },
    {
      "epoch": 6.799333333333333,
      "grad_norm": 0.14177784323692322,
      "learning_rate": 7.5041666666666675e-06,
      "loss": 0.0014,
      "step": 101990
    },
    {
      "epoch": 6.8,
      "grad_norm": 0.103016197681427,
      "learning_rate": 7.5e-06,
      "loss": 0.0015,
      "step": 102000
    },
    {
      "epoch": 6.800666666666666,
      "grad_norm": 0.05362418666481972,
      "learning_rate": 7.495833333333334e-06,
      "loss": 0.0013,
      "step": 102010
    },
    {
      "epoch": 6.801333333333333,
      "grad_norm": 0.27555978298187256,
      "learning_rate": 7.491666666666668e-06,
      "loss": 0.0018,
      "step": 102020
    },
    {
      "epoch": 6.802,
      "grad_norm": 0.5135852694511414,
      "learning_rate": 7.4875e-06,
      "loss": 0.0018,
      "step": 102030
    },
    {
      "epoch": 6.802666666666667,
      "grad_norm": 0.09726595878601074,
      "learning_rate": 7.483333333333334e-06,
      "loss": 0.0024,
      "step": 102040
    },
    {
      "epoch": 6.803333333333334,
      "grad_norm": 0.2586638033390045,
      "learning_rate": 7.479166666666668e-06,
      "loss": 0.0013,
      "step": 102050
    },
    {
      "epoch": 6.804,
      "grad_norm": 0.07327434420585632,
      "learning_rate": 7.4750000000000004e-06,
      "loss": 0.0016,
      "step": 102060
    },
    {
      "epoch": 6.804666666666667,
      "grad_norm": 0.19892312586307526,
      "learning_rate": 7.470833333333334e-06,
      "loss": 0.0023,
      "step": 102070
    },
    {
      "epoch": 6.8053333333333335,
      "grad_norm": 0.039443809539079666,
      "learning_rate": 7.4666666666666675e-06,
      "loss": 0.0016,
      "step": 102080
    },
    {
      "epoch": 6.806,
      "grad_norm": 0.3008231520652771,
      "learning_rate": 7.4625e-06,
      "loss": 0.0014,
      "step": 102090
    },
    {
      "epoch": 6.806666666666667,
      "grad_norm": 0.133963942527771,
      "learning_rate": 7.458333333333334e-06,
      "loss": 0.0019,
      "step": 102100
    },
    {
      "epoch": 6.807333333333333,
      "grad_norm": 0.17238792777061462,
      "learning_rate": 7.454166666666668e-06,
      "loss": 0.0025,
      "step": 102110
    },
    {
      "epoch": 6.808,
      "grad_norm": 0.41588065028190613,
      "learning_rate": 7.45e-06,
      "loss": 0.0017,
      "step": 102120
    },
    {
      "epoch": 6.808666666666666,
      "grad_norm": 0.4660344421863556,
      "learning_rate": 7.445833333333334e-06,
      "loss": 0.0012,
      "step": 102130
    },
    {
      "epoch": 6.809333333333333,
      "grad_norm": 0.051133234053850174,
      "learning_rate": 7.441666666666668e-06,
      "loss": 0.0016,
      "step": 102140
    },
    {
      "epoch": 6.8100000000000005,
      "grad_norm": 0.20006674528121948,
      "learning_rate": 7.4375e-06,
      "loss": 0.0015,
      "step": 102150
    },
    {
      "epoch": 6.810666666666666,
      "grad_norm": 0.33661460876464844,
      "learning_rate": 7.433333333333334e-06,
      "loss": 0.0014,
      "step": 102160
    },
    {
      "epoch": 6.811333333333334,
      "grad_norm": 0.168280690908432,
      "learning_rate": 7.429166666666667e-06,
      "loss": 0.0019,
      "step": 102170
    },
    {
      "epoch": 6.812,
      "grad_norm": 0.025161176919937134,
      "learning_rate": 7.425e-06,
      "loss": 0.0018,
      "step": 102180
    },
    {
      "epoch": 6.812666666666667,
      "grad_norm": 0.13708536326885223,
      "learning_rate": 7.4208333333333336e-06,
      "loss": 0.0019,
      "step": 102190
    },
    {
      "epoch": 6.8133333333333335,
      "grad_norm": 0.10264395922422409,
      "learning_rate": 7.416666666666668e-06,
      "loss": 0.0019,
      "step": 102200
    },
    {
      "epoch": 6.814,
      "grad_norm": 0.33813801407814026,
      "learning_rate": 7.4125e-06,
      "loss": 0.0017,
      "step": 102210
    },
    {
      "epoch": 6.814666666666667,
      "grad_norm": 0.2101823091506958,
      "learning_rate": 7.408333333333334e-06,
      "loss": 0.0024,
      "step": 102220
    },
    {
      "epoch": 6.815333333333333,
      "grad_norm": 0.2339366376399994,
      "learning_rate": 7.404166666666668e-06,
      "loss": 0.002,
      "step": 102230
    },
    {
      "epoch": 6.816,
      "grad_norm": 0.20392806828022003,
      "learning_rate": 7.4e-06,
      "loss": 0.002,
      "step": 102240
    },
    {
      "epoch": 6.816666666666666,
      "grad_norm": 0.168520987033844,
      "learning_rate": 7.395833333333334e-06,
      "loss": 0.0026,
      "step": 102250
    },
    {
      "epoch": 6.817333333333333,
      "grad_norm": 0.1196352019906044,
      "learning_rate": 7.391666666666667e-06,
      "loss": 0.002,
      "step": 102260
    },
    {
      "epoch": 6.818,
      "grad_norm": 0.07700630277395248,
      "learning_rate": 7.3875e-06,
      "loss": 0.0025,
      "step": 102270
    },
    {
      "epoch": 6.818666666666667,
      "grad_norm": 0.08098573237657547,
      "learning_rate": 7.3833333333333335e-06,
      "loss": 0.0018,
      "step": 102280
    },
    {
      "epoch": 6.819333333333334,
      "grad_norm": 0.3343556821346283,
      "learning_rate": 7.379166666666668e-06,
      "loss": 0.0018,
      "step": 102290
    },
    {
      "epoch": 6.82,
      "grad_norm": 0.3108123242855072,
      "learning_rate": 7.375e-06,
      "loss": 0.0022,
      "step": 102300
    },
    {
      "epoch": 6.820666666666667,
      "grad_norm": 0.08835403621196747,
      "learning_rate": 7.370833333333334e-06,
      "loss": 0.0016,
      "step": 102310
    },
    {
      "epoch": 6.8213333333333335,
      "grad_norm": 0.2672271728515625,
      "learning_rate": 7.3666666666666676e-06,
      "loss": 0.0016,
      "step": 102320
    },
    {
      "epoch": 6.822,
      "grad_norm": 0.041638508439064026,
      "learning_rate": 7.3625e-06,
      "loss": 0.0013,
      "step": 102330
    },
    {
      "epoch": 6.822666666666667,
      "grad_norm": 0.21090476214885712,
      "learning_rate": 7.358333333333334e-06,
      "loss": 0.0025,
      "step": 102340
    },
    {
      "epoch": 6.823333333333333,
      "grad_norm": 0.1037372574210167,
      "learning_rate": 7.354166666666667e-06,
      "loss": 0.0014,
      "step": 102350
    },
    {
      "epoch": 6.824,
      "grad_norm": 0.07510028034448624,
      "learning_rate": 7.35e-06,
      "loss": 0.0024,
      "step": 102360
    },
    {
      "epoch": 6.824666666666666,
      "grad_norm": 0.16992461681365967,
      "learning_rate": 7.3458333333333334e-06,
      "loss": 0.0012,
      "step": 102370
    },
    {
      "epoch": 6.825333333333333,
      "grad_norm": 0.33438706398010254,
      "learning_rate": 7.341666666666668e-06,
      "loss": 0.0015,
      "step": 102380
    },
    {
      "epoch": 6.826,
      "grad_norm": 0.23800519108772278,
      "learning_rate": 7.3375e-06,
      "loss": 0.0016,
      "step": 102390
    },
    {
      "epoch": 6.826666666666666,
      "grad_norm": 0.3255000114440918,
      "learning_rate": 7.333333333333334e-06,
      "loss": 0.0024,
      "step": 102400
    },
    {
      "epoch": 6.827333333333334,
      "grad_norm": 0.2771402597427368,
      "learning_rate": 7.3291666666666675e-06,
      "loss": 0.0014,
      "step": 102410
    },
    {
      "epoch": 6.828,
      "grad_norm": 0.14779117703437805,
      "learning_rate": 7.325e-06,
      "loss": 0.0016,
      "step": 102420
    },
    {
      "epoch": 6.828666666666667,
      "grad_norm": 0.25482749938964844,
      "learning_rate": 7.320833333333334e-06,
      "loss": 0.0022,
      "step": 102430
    },
    {
      "epoch": 6.8293333333333335,
      "grad_norm": 0.13809452950954437,
      "learning_rate": 7.316666666666667e-06,
      "loss": 0.0022,
      "step": 102440
    },
    {
      "epoch": 6.83,
      "grad_norm": 0.34375104308128357,
      "learning_rate": 7.3125e-06,
      "loss": 0.0019,
      "step": 102450
    },
    {
      "epoch": 6.830666666666667,
      "grad_norm": 0.17363879084587097,
      "learning_rate": 7.308333333333333e-06,
      "loss": 0.0018,
      "step": 102460
    },
    {
      "epoch": 6.831333333333333,
      "grad_norm": 0.20029205083847046,
      "learning_rate": 7.304166666666668e-06,
      "loss": 0.0017,
      "step": 102470
    },
    {
      "epoch": 6.832,
      "grad_norm": 0.396305114030838,
      "learning_rate": 7.2999999999999996e-06,
      "loss": 0.0019,
      "step": 102480
    },
    {
      "epoch": 6.832666666666666,
      "grad_norm": 0.2338320016860962,
      "learning_rate": 7.295833333333334e-06,
      "loss": 0.002,
      "step": 102490
    },
    {
      "epoch": 6.833333333333333,
      "grad_norm": 0.2708909213542938,
      "learning_rate": 7.2916666666666674e-06,
      "loss": 0.0022,
      "step": 102500
    },
    {
      "epoch": 6.834,
      "grad_norm": 0.20303796231746674,
      "learning_rate": 7.2875e-06,
      "loss": 0.0017,
      "step": 102510
    },
    {
      "epoch": 6.834666666666667,
      "grad_norm": 0.21287819743156433,
      "learning_rate": 7.283333333333334e-06,
      "loss": 0.0012,
      "step": 102520
    },
    {
      "epoch": 6.835333333333334,
      "grad_norm": 0.1973188817501068,
      "learning_rate": 7.279166666666667e-06,
      "loss": 0.002,
      "step": 102530
    },
    {
      "epoch": 6.836,
      "grad_norm": 0.31300753355026245,
      "learning_rate": 7.275e-06,
      "loss": 0.0014,
      "step": 102540
    },
    {
      "epoch": 6.836666666666667,
      "grad_norm": 0.5381545424461365,
      "learning_rate": 7.270833333333333e-06,
      "loss": 0.0015,
      "step": 102550
    },
    {
      "epoch": 6.8373333333333335,
      "grad_norm": 0.49702462553977966,
      "learning_rate": 7.266666666666668e-06,
      "loss": 0.0019,
      "step": 102560
    },
    {
      "epoch": 6.838,
      "grad_norm": 0.17382463812828064,
      "learning_rate": 7.2624999999999995e-06,
      "loss": 0.0023,
      "step": 102570
    },
    {
      "epoch": 6.838666666666667,
      "grad_norm": 0.17608171701431274,
      "learning_rate": 7.258333333333334e-06,
      "loss": 0.0012,
      "step": 102580
    },
    {
      "epoch": 6.839333333333333,
      "grad_norm": 0.4617267847061157,
      "learning_rate": 7.254166666666667e-06,
      "loss": 0.0016,
      "step": 102590
    },
    {
      "epoch": 6.84,
      "grad_norm": 0.1673315018415451,
      "learning_rate": 7.25e-06,
      "loss": 0.0018,
      "step": 102600
    },
    {
      "epoch": 6.8406666666666665,
      "grad_norm": 0.17110268771648407,
      "learning_rate": 7.2458333333333336e-06,
      "loss": 0.0038,
      "step": 102610
    },
    {
      "epoch": 6.841333333333333,
      "grad_norm": 0.1827036738395691,
      "learning_rate": 7.241666666666667e-06,
      "loss": 0.002,
      "step": 102620
    },
    {
      "epoch": 6.842,
      "grad_norm": 0.05021820217370987,
      "learning_rate": 7.2375e-06,
      "loss": 0.0018,
      "step": 102630
    },
    {
      "epoch": 6.842666666666666,
      "grad_norm": 0.07654059678316116,
      "learning_rate": 7.233333333333333e-06,
      "loss": 0.0023,
      "step": 102640
    },
    {
      "epoch": 6.843333333333334,
      "grad_norm": 0.2043336182832718,
      "learning_rate": 7.229166666666668e-06,
      "loss": 0.0021,
      "step": 102650
    },
    {
      "epoch": 6.844,
      "grad_norm": 0.3034713864326477,
      "learning_rate": 7.2249999999999994e-06,
      "loss": 0.0014,
      "step": 102660
    },
    {
      "epoch": 6.844666666666667,
      "grad_norm": 0.17022830247879028,
      "learning_rate": 7.220833333333334e-06,
      "loss": 0.0017,
      "step": 102670
    },
    {
      "epoch": 6.8453333333333335,
      "grad_norm": 0.33562666177749634,
      "learning_rate": 7.216666666666667e-06,
      "loss": 0.0017,
      "step": 102680
    },
    {
      "epoch": 6.846,
      "grad_norm": 0.26778164505958557,
      "learning_rate": 7.2125e-06,
      "loss": 0.0023,
      "step": 102690
    },
    {
      "epoch": 6.846666666666667,
      "grad_norm": 0.20066668093204498,
      "learning_rate": 7.2083333333333335e-06,
      "loss": 0.002,
      "step": 102700
    },
    {
      "epoch": 6.847333333333333,
      "grad_norm": 0.4786699116230011,
      "learning_rate": 7.204166666666667e-06,
      "loss": 0.0024,
      "step": 102710
    },
    {
      "epoch": 6.848,
      "grad_norm": 0.8040364980697632,
      "learning_rate": 7.2e-06,
      "loss": 0.0018,
      "step": 102720
    },
    {
      "epoch": 6.8486666666666665,
      "grad_norm": 0.11039121448993683,
      "learning_rate": 7.195833333333333e-06,
      "loss": 0.0022,
      "step": 102730
    },
    {
      "epoch": 6.849333333333333,
      "grad_norm": 0.4069956839084625,
      "learning_rate": 7.1916666666666676e-06,
      "loss": 0.0027,
      "step": 102740
    },
    {
      "epoch": 6.85,
      "grad_norm": 0.08054851740598679,
      "learning_rate": 7.187499999999999e-06,
      "loss": 0.0018,
      "step": 102750
    },
    {
      "epoch": 6.850666666666667,
      "grad_norm": 0.14149507880210876,
      "learning_rate": 7.183333333333334e-06,
      "loss": 0.0019,
      "step": 102760
    },
    {
      "epoch": 6.851333333333334,
      "grad_norm": 0.2015414834022522,
      "learning_rate": 7.179166666666667e-06,
      "loss": 0.0018,
      "step": 102770
    },
    {
      "epoch": 6.852,
      "grad_norm": 0.6123123168945312,
      "learning_rate": 7.175e-06,
      "loss": 0.0025,
      "step": 102780
    },
    {
      "epoch": 6.852666666666667,
      "grad_norm": 0.20117735862731934,
      "learning_rate": 7.1708333333333334e-06,
      "loss": 0.0018,
      "step": 102790
    },
    {
      "epoch": 6.8533333333333335,
      "grad_norm": 0.17245909571647644,
      "learning_rate": 7.166666666666667e-06,
      "loss": 0.0025,
      "step": 102800
    },
    {
      "epoch": 6.854,
      "grad_norm": 0.4754856824874878,
      "learning_rate": 7.1625e-06,
      "loss": 0.0019,
      "step": 102810
    },
    {
      "epoch": 6.854666666666667,
      "grad_norm": 0.12028754502534866,
      "learning_rate": 7.158333333333334e-06,
      "loss": 0.002,
      "step": 102820
    },
    {
      "epoch": 6.855333333333333,
      "grad_norm": 0.10693307965993881,
      "learning_rate": 7.1541666666666675e-06,
      "loss": 0.0016,
      "step": 102830
    },
    {
      "epoch": 6.856,
      "grad_norm": 0.08006654679775238,
      "learning_rate": 7.15e-06,
      "loss": 0.0014,
      "step": 102840
    },
    {
      "epoch": 6.8566666666666665,
      "grad_norm": 0.13659536838531494,
      "learning_rate": 7.145833333333334e-06,
      "loss": 0.0015,
      "step": 102850
    },
    {
      "epoch": 6.857333333333333,
      "grad_norm": 0.5066908597946167,
      "learning_rate": 7.141666666666667e-06,
      "loss": 0.0017,
      "step": 102860
    },
    {
      "epoch": 6.858,
      "grad_norm": 0.12952175736427307,
      "learning_rate": 7.1375e-06,
      "loss": 0.002,
      "step": 102870
    },
    {
      "epoch": 6.858666666666666,
      "grad_norm": 0.13730843365192413,
      "learning_rate": 7.133333333333333e-06,
      "loss": 0.0014,
      "step": 102880
    },
    {
      "epoch": 6.859333333333334,
      "grad_norm": 0.4578551650047302,
      "learning_rate": 7.129166666666668e-06,
      "loss": 0.0025,
      "step": 102890
    },
    {
      "epoch": 6.86,
      "grad_norm": 0.1102694422006607,
      "learning_rate": 7.1249999999999995e-06,
      "loss": 0.0025,
      "step": 102900
    },
    {
      "epoch": 6.860666666666667,
      "grad_norm": 0.5503068566322327,
      "learning_rate": 7.120833333333334e-06,
      "loss": 0.0018,
      "step": 102910
    },
    {
      "epoch": 6.8613333333333335,
      "grad_norm": 0.24013875424861908,
      "learning_rate": 7.116666666666667e-06,
      "loss": 0.0015,
      "step": 102920
    },
    {
      "epoch": 6.862,
      "grad_norm": 0.6123924255371094,
      "learning_rate": 7.1125e-06,
      "loss": 0.0016,
      "step": 102930
    },
    {
      "epoch": 6.862666666666667,
      "grad_norm": 0.5375584959983826,
      "learning_rate": 7.108333333333334e-06,
      "loss": 0.0023,
      "step": 102940
    },
    {
      "epoch": 6.863333333333333,
      "grad_norm": 0.6412089467048645,
      "learning_rate": 7.104166666666667e-06,
      "loss": 0.0013,
      "step": 102950
    },
    {
      "epoch": 6.864,
      "grad_norm": 0.38766324520111084,
      "learning_rate": 7.1e-06,
      "loss": 0.0027,
      "step": 102960
    },
    {
      "epoch": 6.8646666666666665,
      "grad_norm": 0.038699012249708176,
      "learning_rate": 7.095833333333333e-06,
      "loss": 0.0016,
      "step": 102970
    },
    {
      "epoch": 6.865333333333333,
      "grad_norm": 0.5247655510902405,
      "learning_rate": 7.091666666666668e-06,
      "loss": 0.0016,
      "step": 102980
    },
    {
      "epoch": 6.866,
      "grad_norm": 0.1691484898328781,
      "learning_rate": 7.0874999999999995e-06,
      "loss": 0.0026,
      "step": 102990
    },
    {
      "epoch": 6.866666666666667,
      "grad_norm": 0.1380230337381363,
      "learning_rate": 7.083333333333334e-06,
      "loss": 0.0018,
      "step": 103000
    },
    {
      "epoch": 6.867333333333333,
      "grad_norm": 0.33581456542015076,
      "learning_rate": 7.079166666666667e-06,
      "loss": 0.0015,
      "step": 103010
    },
    {
      "epoch": 6.868,
      "grad_norm": 0.3732275068759918,
      "learning_rate": 7.075e-06,
      "loss": 0.0013,
      "step": 103020
    },
    {
      "epoch": 6.868666666666667,
      "grad_norm": 0.3812801241874695,
      "learning_rate": 7.0708333333333335e-06,
      "loss": 0.0018,
      "step": 103030
    },
    {
      "epoch": 6.8693333333333335,
      "grad_norm": 0.30259767174720764,
      "learning_rate": 7.066666666666667e-06,
      "loss": 0.0016,
      "step": 103040
    },
    {
      "epoch": 6.87,
      "grad_norm": 0.23088622093200684,
      "learning_rate": 7.0625e-06,
      "loss": 0.0019,
      "step": 103050
    },
    {
      "epoch": 6.870666666666667,
      "grad_norm": 0.29228293895721436,
      "learning_rate": 7.058333333333333e-06,
      "loss": 0.0013,
      "step": 103060
    },
    {
      "epoch": 6.871333333333333,
      "grad_norm": 0.1735548973083496,
      "learning_rate": 7.054166666666668e-06,
      "loss": 0.0014,
      "step": 103070
    },
    {
      "epoch": 6.872,
      "grad_norm": 0.26756051182746887,
      "learning_rate": 7.049999999999999e-06,
      "loss": 0.002,
      "step": 103080
    },
    {
      "epoch": 6.8726666666666665,
      "grad_norm": 0.07604972273111343,
      "learning_rate": 7.045833333333334e-06,
      "loss": 0.002,
      "step": 103090
    },
    {
      "epoch": 6.873333333333333,
      "grad_norm": 0.16905847191810608,
      "learning_rate": 7.041666666666667e-06,
      "loss": 0.0013,
      "step": 103100
    },
    {
      "epoch": 6.874,
      "grad_norm": 0.43558812141418457,
      "learning_rate": 7.0375e-06,
      "loss": 0.0019,
      "step": 103110
    },
    {
      "epoch": 6.874666666666666,
      "grad_norm": 0.3343692123889923,
      "learning_rate": 7.0333333333333335e-06,
      "loss": 0.002,
      "step": 103120
    },
    {
      "epoch": 6.875333333333334,
      "grad_norm": 0.07198772579431534,
      "learning_rate": 7.029166666666667e-06,
      "loss": 0.0022,
      "step": 103130
    },
    {
      "epoch": 6.876,
      "grad_norm": 0.3477475047111511,
      "learning_rate": 7.025000000000001e-06,
      "loss": 0.0016,
      "step": 103140
    },
    {
      "epoch": 6.876666666666667,
      "grad_norm": 0.046151053160429,
      "learning_rate": 7.020833333333333e-06,
      "loss": 0.0022,
      "step": 103150
    },
    {
      "epoch": 6.8773333333333335,
      "grad_norm": 0.16708946228027344,
      "learning_rate": 7.0166666666666675e-06,
      "loss": 0.0013,
      "step": 103160
    },
    {
      "epoch": 6.878,
      "grad_norm": 0.0536254420876503,
      "learning_rate": 7.012500000000001e-06,
      "loss": 0.0026,
      "step": 103170
    },
    {
      "epoch": 6.878666666666667,
      "grad_norm": 0.549863874912262,
      "learning_rate": 7.008333333333334e-06,
      "loss": 0.0026,
      "step": 103180
    },
    {
      "epoch": 6.879333333333333,
      "grad_norm": 0.14692530035972595,
      "learning_rate": 7.004166666666667e-06,
      "loss": 0.0015,
      "step": 103190
    },
    {
      "epoch": 6.88,
      "grad_norm": 0.3019002377986908,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.002,
      "step": 103200
    },
    {
      "epoch": 6.8806666666666665,
      "grad_norm": 0.050219517201185226,
      "learning_rate": 6.995833333333333e-06,
      "loss": 0.0027,
      "step": 103210
    },
    {
      "epoch": 6.881333333333333,
      "grad_norm": 0.10485038161277771,
      "learning_rate": 6.991666666666667e-06,
      "loss": 0.0026,
      "step": 103220
    },
    {
      "epoch": 6.882,
      "grad_norm": 0.5788440108299255,
      "learning_rate": 6.987500000000001e-06,
      "loss": 0.0018,
      "step": 103230
    },
    {
      "epoch": 6.882666666666667,
      "grad_norm": 0.18493537604808807,
      "learning_rate": 6.983333333333333e-06,
      "loss": 0.0021,
      "step": 103240
    },
    {
      "epoch": 6.883333333333333,
      "grad_norm": 0.4851594865322113,
      "learning_rate": 6.9791666666666675e-06,
      "loss": 0.0013,
      "step": 103250
    },
    {
      "epoch": 6.884,
      "grad_norm": 0.20674757659435272,
      "learning_rate": 6.975000000000001e-06,
      "loss": 0.0013,
      "step": 103260
    },
    {
      "epoch": 6.884666666666667,
      "grad_norm": 0.039241258054971695,
      "learning_rate": 6.970833333333334e-06,
      "loss": 0.0033,
      "step": 103270
    },
    {
      "epoch": 6.8853333333333335,
      "grad_norm": 0.4218349754810333,
      "learning_rate": 6.966666666666667e-06,
      "loss": 0.0015,
      "step": 103280
    },
    {
      "epoch": 6.886,
      "grad_norm": 0.5713860392570496,
      "learning_rate": 6.962500000000001e-06,
      "loss": 0.0014,
      "step": 103290
    },
    {
      "epoch": 6.886666666666667,
      "grad_norm": 0.16467705368995667,
      "learning_rate": 6.958333333333333e-06,
      "loss": 0.0015,
      "step": 103300
    },
    {
      "epoch": 6.887333333333333,
      "grad_norm": 0.3631945550441742,
      "learning_rate": 6.954166666666667e-06,
      "loss": 0.002,
      "step": 103310
    },
    {
      "epoch": 6.888,
      "grad_norm": 0.2001810222864151,
      "learning_rate": 6.950000000000001e-06,
      "loss": 0.0016,
      "step": 103320
    },
    {
      "epoch": 6.8886666666666665,
      "grad_norm": 0.16339737176895142,
      "learning_rate": 6.945833333333333e-06,
      "loss": 0.0019,
      "step": 103330
    },
    {
      "epoch": 6.889333333333333,
      "grad_norm": 0.07196101546287537,
      "learning_rate": 6.941666666666667e-06,
      "loss": 0.0018,
      "step": 103340
    },
    {
      "epoch": 6.89,
      "grad_norm": 0.20136606693267822,
      "learning_rate": 6.937500000000001e-06,
      "loss": 0.0022,
      "step": 103350
    },
    {
      "epoch": 6.890666666666666,
      "grad_norm": 0.16770173609256744,
      "learning_rate": 6.933333333333334e-06,
      "loss": 0.0025,
      "step": 103360
    },
    {
      "epoch": 6.891333333333334,
      "grad_norm": 0.3597579300403595,
      "learning_rate": 6.929166666666667e-06,
      "loss": 0.0019,
      "step": 103370
    },
    {
      "epoch": 6.892,
      "grad_norm": 0.04578647017478943,
      "learning_rate": 6.925000000000001e-06,
      "loss": 0.0016,
      "step": 103380
    },
    {
      "epoch": 6.892666666666667,
      "grad_norm": 0.14138107001781464,
      "learning_rate": 6.920833333333333e-06,
      "loss": 0.0015,
      "step": 103390
    },
    {
      "epoch": 6.8933333333333335,
      "grad_norm": 0.0609092153608799,
      "learning_rate": 6.916666666666667e-06,
      "loss": 0.0013,
      "step": 103400
    },
    {
      "epoch": 6.894,
      "grad_norm": 0.2691364288330078,
      "learning_rate": 6.912500000000001e-06,
      "loss": 0.0015,
      "step": 103410
    },
    {
      "epoch": 6.894666666666667,
      "grad_norm": 0.19849243760108948,
      "learning_rate": 6.908333333333333e-06,
      "loss": 0.0021,
      "step": 103420
    },
    {
      "epoch": 6.895333333333333,
      "grad_norm": 0.13278919458389282,
      "learning_rate": 6.904166666666667e-06,
      "loss": 0.0014,
      "step": 103430
    },
    {
      "epoch": 6.896,
      "grad_norm": 0.1775614470243454,
      "learning_rate": 6.900000000000001e-06,
      "loss": 0.0017,
      "step": 103440
    },
    {
      "epoch": 6.8966666666666665,
      "grad_norm": 0.03626086562871933,
      "learning_rate": 6.8958333333333335e-06,
      "loss": 0.0024,
      "step": 103450
    },
    {
      "epoch": 6.897333333333333,
      "grad_norm": 0.4370065927505493,
      "learning_rate": 6.891666666666667e-06,
      "loss": 0.0024,
      "step": 103460
    },
    {
      "epoch": 6.898,
      "grad_norm": 0.02507377229630947,
      "learning_rate": 6.8875000000000005e-06,
      "loss": 0.0013,
      "step": 103470
    },
    {
      "epoch": 6.898666666666666,
      "grad_norm": 0.5412182211875916,
      "learning_rate": 6.883333333333333e-06,
      "loss": 0.0016,
      "step": 103480
    },
    {
      "epoch": 6.899333333333333,
      "grad_norm": 0.07046810537576675,
      "learning_rate": 6.879166666666667e-06,
      "loss": 0.0017,
      "step": 103490
    },
    {
      "epoch": 6.9,
      "grad_norm": 0.20389805734157562,
      "learning_rate": 6.875000000000001e-06,
      "loss": 0.0025,
      "step": 103500
    },
    {
      "epoch": 6.900666666666667,
      "grad_norm": 0.415951132774353,
      "learning_rate": 6.870833333333333e-06,
      "loss": 0.0026,
      "step": 103510
    },
    {
      "epoch": 6.9013333333333335,
      "grad_norm": 0.3736647367477417,
      "learning_rate": 6.866666666666667e-06,
      "loss": 0.002,
      "step": 103520
    },
    {
      "epoch": 6.902,
      "grad_norm": 0.07692515850067139,
      "learning_rate": 6.862500000000001e-06,
      "loss": 0.0019,
      "step": 103530
    },
    {
      "epoch": 6.902666666666667,
      "grad_norm": 0.07591003179550171,
      "learning_rate": 6.8583333333333335e-06,
      "loss": 0.0016,
      "step": 103540
    },
    {
      "epoch": 6.903333333333333,
      "grad_norm": 0.20495182275772095,
      "learning_rate": 6.854166666666667e-06,
      "loss": 0.002,
      "step": 103550
    },
    {
      "epoch": 6.904,
      "grad_norm": 0.11208708584308624,
      "learning_rate": 6.8500000000000005e-06,
      "loss": 0.0029,
      "step": 103560
    },
    {
      "epoch": 6.9046666666666665,
      "grad_norm": 0.8006142973899841,
      "learning_rate": 6.845833333333333e-06,
      "loss": 0.0021,
      "step": 103570
    },
    {
      "epoch": 6.905333333333333,
      "grad_norm": 0.11264205724000931,
      "learning_rate": 6.841666666666667e-06,
      "loss": 0.0023,
      "step": 103580
    },
    {
      "epoch": 6.906,
      "grad_norm": 0.17842917144298553,
      "learning_rate": 6.837500000000001e-06,
      "loss": 0.0014,
      "step": 103590
    },
    {
      "epoch": 6.906666666666666,
      "grad_norm": 0.20528925955295563,
      "learning_rate": 6.833333333333333e-06,
      "loss": 0.0018,
      "step": 103600
    },
    {
      "epoch": 6.907333333333334,
      "grad_norm": 0.1732187122106552,
      "learning_rate": 6.829166666666667e-06,
      "loss": 0.002,
      "step": 103610
    },
    {
      "epoch": 6.908,
      "grad_norm": 0.36113831400871277,
      "learning_rate": 6.825000000000001e-06,
      "loss": 0.0015,
      "step": 103620
    },
    {
      "epoch": 6.908666666666667,
      "grad_norm": 0.13980373740196228,
      "learning_rate": 6.820833333333333e-06,
      "loss": 0.0028,
      "step": 103630
    },
    {
      "epoch": 6.9093333333333335,
      "grad_norm": 0.46726369857788086,
      "learning_rate": 6.816666666666667e-06,
      "loss": 0.0022,
      "step": 103640
    },
    {
      "epoch": 6.91,
      "grad_norm": 0.11576186120510101,
      "learning_rate": 6.8125e-06,
      "loss": 0.0018,
      "step": 103650
    },
    {
      "epoch": 6.910666666666667,
      "grad_norm": 0.1671484410762787,
      "learning_rate": 6.808333333333333e-06,
      "loss": 0.0016,
      "step": 103660
    },
    {
      "epoch": 6.911333333333333,
      "grad_norm": 0.07919450849294662,
      "learning_rate": 6.804166666666667e-06,
      "loss": 0.0016,
      "step": 103670
    },
    {
      "epoch": 6.912,
      "grad_norm": 0.08545532822608948,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.0019,
      "step": 103680
    },
    {
      "epoch": 6.9126666666666665,
      "grad_norm": 0.2706550657749176,
      "learning_rate": 6.795833333333334e-06,
      "loss": 0.0023,
      "step": 103690
    },
    {
      "epoch": 6.913333333333333,
      "grad_norm": 0.3706633746623993,
      "learning_rate": 6.791666666666667e-06,
      "loss": 0.0014,
      "step": 103700
    },
    {
      "epoch": 6.914,
      "grad_norm": 0.10596604645252228,
      "learning_rate": 6.787500000000001e-06,
      "loss": 0.0014,
      "step": 103710
    },
    {
      "epoch": 6.914666666666666,
      "grad_norm": 0.2979891300201416,
      "learning_rate": 6.783333333333333e-06,
      "loss": 0.0025,
      "step": 103720
    },
    {
      "epoch": 6.915333333333333,
      "grad_norm": 0.14665032923221588,
      "learning_rate": 6.779166666666667e-06,
      "loss": 0.0013,
      "step": 103730
    },
    {
      "epoch": 6.916,
      "grad_norm": 0.16855180263519287,
      "learning_rate": 6.775000000000001e-06,
      "loss": 0.0015,
      "step": 103740
    },
    {
      "epoch": 6.916666666666667,
      "grad_norm": 0.20255093276500702,
      "learning_rate": 6.770833333333333e-06,
      "loss": 0.0018,
      "step": 103750
    },
    {
      "epoch": 6.917333333333334,
      "grad_norm": 0.3376627564430237,
      "learning_rate": 6.766666666666667e-06,
      "loss": 0.0015,
      "step": 103760
    },
    {
      "epoch": 6.918,
      "grad_norm": 0.26476025581359863,
      "learning_rate": 6.762500000000001e-06,
      "loss": 0.002,
      "step": 103770
    },
    {
      "epoch": 6.918666666666667,
      "grad_norm": 0.06906899064779282,
      "learning_rate": 6.7583333333333336e-06,
      "loss": 0.0019,
      "step": 103780
    },
    {
      "epoch": 6.919333333333333,
      "grad_norm": 0.0355626605451107,
      "learning_rate": 6.754166666666667e-06,
      "loss": 0.0016,
      "step": 103790
    },
    {
      "epoch": 6.92,
      "grad_norm": 0.3000345528125763,
      "learning_rate": 6.750000000000001e-06,
      "loss": 0.0019,
      "step": 103800
    },
    {
      "epoch": 6.9206666666666665,
      "grad_norm": 0.17743419110774994,
      "learning_rate": 6.745833333333333e-06,
      "loss": 0.002,
      "step": 103810
    },
    {
      "epoch": 6.921333333333333,
      "grad_norm": 0.24424801766872406,
      "learning_rate": 6.741666666666667e-06,
      "loss": 0.0015,
      "step": 103820
    },
    {
      "epoch": 6.922,
      "grad_norm": 0.2062825858592987,
      "learning_rate": 6.737500000000001e-06,
      "loss": 0.0021,
      "step": 103830
    },
    {
      "epoch": 6.922666666666666,
      "grad_norm": 0.3342665135860443,
      "learning_rate": 6.733333333333333e-06,
      "loss": 0.0021,
      "step": 103840
    },
    {
      "epoch": 6.923333333333334,
      "grad_norm": 0.07335547357797623,
      "learning_rate": 6.729166666666667e-06,
      "loss": 0.0013,
      "step": 103850
    },
    {
      "epoch": 6.924,
      "grad_norm": 0.3318924605846405,
      "learning_rate": 6.725000000000001e-06,
      "loss": 0.0012,
      "step": 103860
    },
    {
      "epoch": 6.924666666666667,
      "grad_norm": 0.10282919555902481,
      "learning_rate": 6.7208333333333335e-06,
      "loss": 0.0011,
      "step": 103870
    },
    {
      "epoch": 6.925333333333334,
      "grad_norm": 0.3083479702472687,
      "learning_rate": 6.716666666666667e-06,
      "loss": 0.0019,
      "step": 103880
    },
    {
      "epoch": 6.926,
      "grad_norm": 0.09122060239315033,
      "learning_rate": 6.7125000000000005e-06,
      "loss": 0.0014,
      "step": 103890
    },
    {
      "epoch": 6.926666666666667,
      "grad_norm": 0.10444643348455429,
      "learning_rate": 6.708333333333333e-06,
      "loss": 0.0017,
      "step": 103900
    },
    {
      "epoch": 6.927333333333333,
      "grad_norm": 0.5419433116912842,
      "learning_rate": 6.704166666666667e-06,
      "loss": 0.0015,
      "step": 103910
    },
    {
      "epoch": 6.928,
      "grad_norm": 0.4097403883934021,
      "learning_rate": 6.700000000000001e-06,
      "loss": 0.0013,
      "step": 103920
    },
    {
      "epoch": 6.9286666666666665,
      "grad_norm": 0.07225640118122101,
      "learning_rate": 6.695833333333333e-06,
      "loss": 0.002,
      "step": 103930
    },
    {
      "epoch": 6.929333333333333,
      "grad_norm": 0.04759974405169487,
      "learning_rate": 6.691666666666667e-06,
      "loss": 0.0022,
      "step": 103940
    },
    {
      "epoch": 6.93,
      "grad_norm": 0.4038443863391876,
      "learning_rate": 6.687500000000001e-06,
      "loss": 0.0017,
      "step": 103950
    },
    {
      "epoch": 6.930666666666666,
      "grad_norm": 0.43598073720932007,
      "learning_rate": 6.6833333333333334e-06,
      "loss": 0.0014,
      "step": 103960
    },
    {
      "epoch": 6.931333333333333,
      "grad_norm": 0.5409671664237976,
      "learning_rate": 6.679166666666667e-06,
      "loss": 0.0014,
      "step": 103970
    },
    {
      "epoch": 6.932,
      "grad_norm": 0.19597919285297394,
      "learning_rate": 6.6750000000000005e-06,
      "loss": 0.0014,
      "step": 103980
    },
    {
      "epoch": 6.932666666666667,
      "grad_norm": 0.20654983818531036,
      "learning_rate": 6.670833333333333e-06,
      "loss": 0.0015,
      "step": 103990
    },
    {
      "epoch": 6.933333333333334,
      "grad_norm": 0.1745745986700058,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.0024,
      "step": 104000
    },
    {
      "epoch": 6.934,
      "grad_norm": 0.20641665160655975,
      "learning_rate": 6.662500000000001e-06,
      "loss": 0.0024,
      "step": 104010
    },
    {
      "epoch": 6.934666666666667,
      "grad_norm": 0.4046405255794525,
      "learning_rate": 6.658333333333333e-06,
      "loss": 0.0016,
      "step": 104020
    },
    {
      "epoch": 6.935333333333333,
      "grad_norm": 0.3661882281303406,
      "learning_rate": 6.654166666666667e-06,
      "loss": 0.0019,
      "step": 104030
    },
    {
      "epoch": 6.936,
      "grad_norm": 0.4316262900829315,
      "learning_rate": 6.650000000000001e-06,
      "loss": 0.0018,
      "step": 104040
    },
    {
      "epoch": 6.9366666666666665,
      "grad_norm": 0.2645406126976013,
      "learning_rate": 6.645833333333333e-06,
      "loss": 0.0015,
      "step": 104050
    },
    {
      "epoch": 6.937333333333333,
      "grad_norm": 0.2555660009384155,
      "learning_rate": 6.641666666666667e-06,
      "loss": 0.003,
      "step": 104060
    },
    {
      "epoch": 6.938,
      "grad_norm": 0.029972868040204048,
      "learning_rate": 6.6375e-06,
      "loss": 0.0016,
      "step": 104070
    },
    {
      "epoch": 6.938666666666666,
      "grad_norm": 0.13766531646251678,
      "learning_rate": 6.633333333333333e-06,
      "loss": 0.003,
      "step": 104080
    },
    {
      "epoch": 6.939333333333334,
      "grad_norm": 0.20633138716220856,
      "learning_rate": 6.629166666666667e-06,
      "loss": 0.0013,
      "step": 104090
    },
    {
      "epoch": 6.9399999999999995,
      "grad_norm": 0.4401475489139557,
      "learning_rate": 6.625000000000001e-06,
      "loss": 0.0013,
      "step": 104100
    },
    {
      "epoch": 6.940666666666667,
      "grad_norm": 0.1980949491262436,
      "learning_rate": 6.620833333333333e-06,
      "loss": 0.0021,
      "step": 104110
    },
    {
      "epoch": 6.941333333333334,
      "grad_norm": 0.4669436812400818,
      "learning_rate": 6.616666666666667e-06,
      "loss": 0.0023,
      "step": 104120
    },
    {
      "epoch": 6.942,
      "grad_norm": 0.1673283725976944,
      "learning_rate": 6.612500000000001e-06,
      "loss": 0.0014,
      "step": 104130
    },
    {
      "epoch": 6.942666666666667,
      "grad_norm": 0.3458249270915985,
      "learning_rate": 6.608333333333333e-06,
      "loss": 0.002,
      "step": 104140
    },
    {
      "epoch": 6.943333333333333,
      "grad_norm": 0.28405308723449707,
      "learning_rate": 6.604166666666667e-06,
      "loss": 0.0018,
      "step": 104150
    },
    {
      "epoch": 6.944,
      "grad_norm": 0.2685283422470093,
      "learning_rate": 6.6e-06,
      "loss": 0.0019,
      "step": 104160
    },
    {
      "epoch": 6.9446666666666665,
      "grad_norm": 0.4413926601409912,
      "learning_rate": 6.595833333333333e-06,
      "loss": 0.0026,
      "step": 104170
    },
    {
      "epoch": 6.945333333333333,
      "grad_norm": 0.1702776998281479,
      "learning_rate": 6.5916666666666665e-06,
      "loss": 0.0016,
      "step": 104180
    },
    {
      "epoch": 6.946,
      "grad_norm": 0.33210933208465576,
      "learning_rate": 6.587500000000001e-06,
      "loss": 0.0018,
      "step": 104190
    },
    {
      "epoch": 6.946666666666666,
      "grad_norm": 0.11189275979995728,
      "learning_rate": 6.583333333333333e-06,
      "loss": 0.0017,
      "step": 104200
    },
    {
      "epoch": 6.947333333333333,
      "grad_norm": 0.040809791535139084,
      "learning_rate": 6.579166666666667e-06,
      "loss": 0.0012,
      "step": 104210
    },
    {
      "epoch": 6.948,
      "grad_norm": 0.2683628499507904,
      "learning_rate": 6.5750000000000006e-06,
      "loss": 0.0013,
      "step": 104220
    },
    {
      "epoch": 6.948666666666667,
      "grad_norm": 0.20210771262645721,
      "learning_rate": 6.570833333333333e-06,
      "loss": 0.0017,
      "step": 104230
    },
    {
      "epoch": 6.949333333333334,
      "grad_norm": 0.20576044917106628,
      "learning_rate": 6.566666666666667e-06,
      "loss": 0.0026,
      "step": 104240
    },
    {
      "epoch": 6.95,
      "grad_norm": 0.0746937021613121,
      "learning_rate": 6.5625e-06,
      "loss": 0.0016,
      "step": 104250
    },
    {
      "epoch": 6.950666666666667,
      "grad_norm": 0.31215301156044006,
      "learning_rate": 6.558333333333333e-06,
      "loss": 0.0019,
      "step": 104260
    },
    {
      "epoch": 6.951333333333333,
      "grad_norm": 0.46498918533325195,
      "learning_rate": 6.5541666666666665e-06,
      "loss": 0.0015,
      "step": 104270
    },
    {
      "epoch": 6.952,
      "grad_norm": 0.20137014985084534,
      "learning_rate": 6.550000000000001e-06,
      "loss": 0.0016,
      "step": 104280
    },
    {
      "epoch": 6.9526666666666666,
      "grad_norm": 0.5563356876373291,
      "learning_rate": 6.545833333333333e-06,
      "loss": 0.0015,
      "step": 104290
    },
    {
      "epoch": 6.953333333333333,
      "grad_norm": 0.2392684370279312,
      "learning_rate": 6.541666666666667e-06,
      "loss": 0.0024,
      "step": 104300
    },
    {
      "epoch": 6.954,
      "grad_norm": 0.07722486555576324,
      "learning_rate": 6.5375000000000005e-06,
      "loss": 0.0029,
      "step": 104310
    },
    {
      "epoch": 6.954666666666666,
      "grad_norm": 0.23462724685668945,
      "learning_rate": 6.533333333333333e-06,
      "loss": 0.0016,
      "step": 104320
    },
    {
      "epoch": 6.955333333333334,
      "grad_norm": 0.4032101333141327,
      "learning_rate": 6.529166666666667e-06,
      "loss": 0.0016,
      "step": 104330
    },
    {
      "epoch": 6.9559999999999995,
      "grad_norm": 0.2065916508436203,
      "learning_rate": 6.525e-06,
      "loss": 0.0014,
      "step": 104340
    },
    {
      "epoch": 6.956666666666667,
      "grad_norm": 0.2980455160140991,
      "learning_rate": 6.520833333333333e-06,
      "loss": 0.0019,
      "step": 104350
    },
    {
      "epoch": 6.957333333333334,
      "grad_norm": 0.3321675658226013,
      "learning_rate": 6.516666666666666e-06,
      "loss": 0.0022,
      "step": 104360
    },
    {
      "epoch": 6.958,
      "grad_norm": 0.2004295289516449,
      "learning_rate": 6.512500000000001e-06,
      "loss": 0.0015,
      "step": 104370
    },
    {
      "epoch": 6.958666666666667,
      "grad_norm": 0.4411647319793701,
      "learning_rate": 6.508333333333334e-06,
      "loss": 0.0014,
      "step": 104380
    },
    {
      "epoch": 6.959333333333333,
      "grad_norm": 0.038092438131570816,
      "learning_rate": 6.504166666666667e-06,
      "loss": 0.0022,
      "step": 104390
    },
    {
      "epoch": 6.96,
      "grad_norm": 0.23723170161247253,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 0.0019,
      "step": 104400
    },
    {
      "epoch": 6.960666666666667,
      "grad_norm": 0.07715463638305664,
      "learning_rate": 6.495833333333334e-06,
      "loss": 0.0016,
      "step": 104410
    },
    {
      "epoch": 6.961333333333333,
      "grad_norm": 0.07246050983667374,
      "learning_rate": 6.491666666666667e-06,
      "loss": 0.0012,
      "step": 104420
    },
    {
      "epoch": 6.962,
      "grad_norm": 0.146244078874588,
      "learning_rate": 6.4875e-06,
      "loss": 0.0015,
      "step": 104430
    },
    {
      "epoch": 6.962666666666666,
      "grad_norm": 0.5070898532867432,
      "learning_rate": 6.4833333333333345e-06,
      "loss": 0.0014,
      "step": 104440
    },
    {
      "epoch": 6.963333333333333,
      "grad_norm": 0.5038846731185913,
      "learning_rate": 6.479166666666666e-06,
      "loss": 0.0015,
      "step": 104450
    },
    {
      "epoch": 6.964,
      "grad_norm": 0.17866365611553192,
      "learning_rate": 6.475000000000001e-06,
      "loss": 0.0017,
      "step": 104460
    },
    {
      "epoch": 6.964666666666667,
      "grad_norm": 0.04807764291763306,
      "learning_rate": 6.470833333333334e-06,
      "loss": 0.0025,
      "step": 104470
    },
    {
      "epoch": 6.965333333333334,
      "grad_norm": 0.5033109784126282,
      "learning_rate": 6.466666666666667e-06,
      "loss": 0.0019,
      "step": 104480
    },
    {
      "epoch": 6.966,
      "grad_norm": 0.0405358262360096,
      "learning_rate": 6.4625e-06,
      "loss": 0.0027,
      "step": 104490
    },
    {
      "epoch": 6.966666666666667,
      "grad_norm": 0.46938997507095337,
      "learning_rate": 6.458333333333334e-06,
      "loss": 0.0018,
      "step": 104500
    },
    {
      "epoch": 6.967333333333333,
      "grad_norm": 0.059136971831321716,
      "learning_rate": 6.4541666666666666e-06,
      "loss": 0.0014,
      "step": 104510
    },
    {
      "epoch": 6.968,
      "grad_norm": 0.1198292225599289,
      "learning_rate": 6.45e-06,
      "loss": 0.0014,
      "step": 104520
    },
    {
      "epoch": 6.968666666666667,
      "grad_norm": 0.1641455888748169,
      "learning_rate": 6.4458333333333344e-06,
      "loss": 0.0016,
      "step": 104530
    },
    {
      "epoch": 6.969333333333333,
      "grad_norm": 0.2319967895746231,
      "learning_rate": 6.441666666666666e-06,
      "loss": 0.0019,
      "step": 104540
    },
    {
      "epoch": 6.97,
      "grad_norm": 0.045301225036382675,
      "learning_rate": 6.437500000000001e-06,
      "loss": 0.0014,
      "step": 104550
    },
    {
      "epoch": 6.970666666666666,
      "grad_norm": 0.12605954706668854,
      "learning_rate": 6.433333333333334e-06,
      "loss": 0.0025,
      "step": 104560
    },
    {
      "epoch": 6.971333333333334,
      "grad_norm": 0.13970309495925903,
      "learning_rate": 6.429166666666667e-06,
      "loss": 0.0014,
      "step": 104570
    },
    {
      "epoch": 6.9719999999999995,
      "grad_norm": 0.49987027049064636,
      "learning_rate": 6.425e-06,
      "loss": 0.0013,
      "step": 104580
    },
    {
      "epoch": 6.972666666666667,
      "grad_norm": 0.13537780940532684,
      "learning_rate": 6.420833333333334e-06,
      "loss": 0.0021,
      "step": 104590
    },
    {
      "epoch": 6.973333333333334,
      "grad_norm": 0.23873792588710785,
      "learning_rate": 6.4166666666666665e-06,
      "loss": 0.0028,
      "step": 104600
    },
    {
      "epoch": 6.974,
      "grad_norm": 0.504897952079773,
      "learning_rate": 6.412500000000001e-06,
      "loss": 0.002,
      "step": 104610
    },
    {
      "epoch": 6.974666666666667,
      "grad_norm": 0.4023924767971039,
      "learning_rate": 6.408333333333334e-06,
      "loss": 0.0027,
      "step": 104620
    },
    {
      "epoch": 6.975333333333333,
      "grad_norm": 0.26433423161506653,
      "learning_rate": 6.404166666666667e-06,
      "loss": 0.0019,
      "step": 104630
    },
    {
      "epoch": 6.976,
      "grad_norm": 0.21525900065898895,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.0023,
      "step": 104640
    },
    {
      "epoch": 6.976666666666667,
      "grad_norm": 0.3725391924381256,
      "learning_rate": 6.395833333333334e-06,
      "loss": 0.0018,
      "step": 104650
    },
    {
      "epoch": 6.977333333333333,
      "grad_norm": 0.07428392022848129,
      "learning_rate": 6.391666666666667e-06,
      "loss": 0.0022,
      "step": 104660
    },
    {
      "epoch": 6.978,
      "grad_norm": 0.3662906289100647,
      "learning_rate": 6.3875e-06,
      "loss": 0.0014,
      "step": 104670
    },
    {
      "epoch": 6.978666666666666,
      "grad_norm": 0.20562724769115448,
      "learning_rate": 6.383333333333335e-06,
      "loss": 0.0014,
      "step": 104680
    },
    {
      "epoch": 6.979333333333333,
      "grad_norm": 0.26496174931526184,
      "learning_rate": 6.3791666666666664e-06,
      "loss": 0.0019,
      "step": 104690
    },
    {
      "epoch": 6.98,
      "grad_norm": 0.4329695403575897,
      "learning_rate": 6.375000000000001e-06,
      "loss": 0.0019,
      "step": 104700
    },
    {
      "epoch": 6.980666666666667,
      "grad_norm": 0.04332411289215088,
      "learning_rate": 6.370833333333334e-06,
      "loss": 0.0029,
      "step": 104710
    },
    {
      "epoch": 6.981333333333334,
      "grad_norm": 0.47105082869529724,
      "learning_rate": 6.366666666666667e-06,
      "loss": 0.0018,
      "step": 104720
    },
    {
      "epoch": 6.982,
      "grad_norm": 0.13768482208251953,
      "learning_rate": 6.3625000000000005e-06,
      "loss": 0.0013,
      "step": 104730
    },
    {
      "epoch": 6.982666666666667,
      "grad_norm": 0.10071523487567902,
      "learning_rate": 6.358333333333334e-06,
      "loss": 0.0025,
      "step": 104740
    },
    {
      "epoch": 6.983333333333333,
      "grad_norm": 0.04361137002706528,
      "learning_rate": 6.354166666666667e-06,
      "loss": 0.0018,
      "step": 104750
    },
    {
      "epoch": 6.984,
      "grad_norm": 0.23417098820209503,
      "learning_rate": 6.35e-06,
      "loss": 0.0015,
      "step": 104760
    },
    {
      "epoch": 6.984666666666667,
      "grad_norm": 0.2015865296125412,
      "learning_rate": 6.3458333333333346e-06,
      "loss": 0.0016,
      "step": 104770
    },
    {
      "epoch": 6.985333333333333,
      "grad_norm": 0.4333954155445099,
      "learning_rate": 6.341666666666666e-06,
      "loss": 0.0022,
      "step": 104780
    },
    {
      "epoch": 6.986,
      "grad_norm": 0.13587483763694763,
      "learning_rate": 6.337500000000001e-06,
      "loss": 0.0021,
      "step": 104790
    },
    {
      "epoch": 6.986666666666666,
      "grad_norm": 0.10401106625795364,
      "learning_rate": 6.333333333333334e-06,
      "loss": 0.0011,
      "step": 104800
    },
    {
      "epoch": 6.987333333333333,
      "grad_norm": 0.4881758391857147,
      "learning_rate": 6.329166666666667e-06,
      "loss": 0.0031,
      "step": 104810
    },
    {
      "epoch": 6.9879999999999995,
      "grad_norm": 0.034701548516750336,
      "learning_rate": 6.3250000000000004e-06,
      "loss": 0.0021,
      "step": 104820
    },
    {
      "epoch": 6.988666666666667,
      "grad_norm": 0.13680268824100494,
      "learning_rate": 6.320833333333334e-06,
      "loss": 0.0014,
      "step": 104830
    },
    {
      "epoch": 6.989333333333334,
      "grad_norm": 0.049972180277109146,
      "learning_rate": 6.316666666666667e-06,
      "loss": 0.002,
      "step": 104840
    },
    {
      "epoch": 6.99,
      "grad_norm": 0.4373629093170166,
      "learning_rate": 6.3125e-06,
      "loss": 0.0016,
      "step": 104850
    },
    {
      "epoch": 6.990666666666667,
      "grad_norm": 0.2739615738391876,
      "learning_rate": 6.3083333333333345e-06,
      "loss": 0.0021,
      "step": 104860
    },
    {
      "epoch": 6.991333333333333,
      "grad_norm": 0.2012382298707962,
      "learning_rate": 6.304166666666666e-06,
      "loss": 0.002,
      "step": 104870
    },
    {
      "epoch": 6.992,
      "grad_norm": 0.20543356239795685,
      "learning_rate": 6.300000000000001e-06,
      "loss": 0.0015,
      "step": 104880
    },
    {
      "epoch": 6.992666666666667,
      "grad_norm": 0.16960635781288147,
      "learning_rate": 6.295833333333334e-06,
      "loss": 0.0034,
      "step": 104890
    },
    {
      "epoch": 6.993333333333333,
      "grad_norm": 0.27051836252212524,
      "learning_rate": 6.291666666666667e-06,
      "loss": 0.0022,
      "step": 104900
    },
    {
      "epoch": 6.994,
      "grad_norm": 0.23410019278526306,
      "learning_rate": 6.2875e-06,
      "loss": 0.0018,
      "step": 104910
    },
    {
      "epoch": 6.994666666666666,
      "grad_norm": 0.6103013157844543,
      "learning_rate": 6.283333333333334e-06,
      "loss": 0.0018,
      "step": 104920
    },
    {
      "epoch": 6.995333333333333,
      "grad_norm": 0.07028354704380035,
      "learning_rate": 6.2791666666666665e-06,
      "loss": 0.0014,
      "step": 104930
    },
    {
      "epoch": 6.996,
      "grad_norm": 0.07749457657337189,
      "learning_rate": 6.275e-06,
      "loss": 0.002,
      "step": 104940
    },
    {
      "epoch": 6.996666666666667,
      "grad_norm": 0.16795526444911957,
      "learning_rate": 6.270833333333334e-06,
      "loss": 0.0014,
      "step": 104950
    },
    {
      "epoch": 6.997333333333334,
      "grad_norm": 0.12166649103164673,
      "learning_rate": 6.266666666666666e-06,
      "loss": 0.0017,
      "step": 104960
    },
    {
      "epoch": 6.998,
      "grad_norm": 0.04629795253276825,
      "learning_rate": 6.262500000000001e-06,
      "loss": 0.0012,
      "step": 104970
    },
    {
      "epoch": 6.998666666666667,
      "grad_norm": 0.4017888009548187,
      "learning_rate": 6.258333333333334e-06,
      "loss": 0.0016,
      "step": 104980
    },
    {
      "epoch": 6.999333333333333,
      "grad_norm": 0.40311241149902344,
      "learning_rate": 6.254166666666667e-06,
      "loss": 0.0018,
      "step": 104990
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.33347287774086,
      "learning_rate": 6.25e-06,
      "loss": 0.0033,
      "step": 105000
    },
    {
      "epoch": 7.0,
      "eval_loss": 0.0018075663829222322,
      "eval_runtime": 155.4724,
      "eval_samples_per_second": 1286.402,
      "eval_steps_per_second": 32.16,
      "step": 105000
    },
    {
      "epoch": 7.000666666666667,
      "grad_norm": 0.030681638047099113,
      "learning_rate": 6.245833333333334e-06,
      "loss": 0.0023,
      "step": 105010
    },
    {
      "epoch": 7.001333333333333,
      "grad_norm": 0.3774433732032776,
      "learning_rate": 6.241666666666667e-06,
      "loss": 0.0015,
      "step": 105020
    },
    {
      "epoch": 7.002,
      "grad_norm": 0.205609530210495,
      "learning_rate": 6.2375e-06,
      "loss": 0.0018,
      "step": 105030
    },
    {
      "epoch": 7.002666666666666,
      "grad_norm": 0.03601944074034691,
      "learning_rate": 6.2333333333333335e-06,
      "loss": 0.0018,
      "step": 105040
    },
    {
      "epoch": 7.003333333333333,
      "grad_norm": 0.27060166001319885,
      "learning_rate": 6.229166666666667e-06,
      "loss": 0.0028,
      "step": 105050
    },
    {
      "epoch": 7.004,
      "grad_norm": 0.10506592690944672,
      "learning_rate": 6.2250000000000005e-06,
      "loss": 0.0022,
      "step": 105060
    },
    {
      "epoch": 7.004666666666667,
      "grad_norm": 0.3234265148639679,
      "learning_rate": 6.220833333333333e-06,
      "loss": 0.0022,
      "step": 105070
    },
    {
      "epoch": 7.005333333333334,
      "grad_norm": 0.16920217871665955,
      "learning_rate": 6.2166666666666676e-06,
      "loss": 0.0014,
      "step": 105080
    },
    {
      "epoch": 7.006,
      "grad_norm": 0.04060649499297142,
      "learning_rate": 6.2125e-06,
      "loss": 0.0016,
      "step": 105090
    },
    {
      "epoch": 7.006666666666667,
      "grad_norm": 0.2347363978624344,
      "learning_rate": 6.208333333333334e-06,
      "loss": 0.0021,
      "step": 105100
    },
    {
      "epoch": 7.007333333333333,
      "grad_norm": 0.18068191409111023,
      "learning_rate": 6.204166666666667e-06,
      "loss": 0.0028,
      "step": 105110
    },
    {
      "epoch": 7.008,
      "grad_norm": 0.13572540879249573,
      "learning_rate": 6.2e-06,
      "loss": 0.003,
      "step": 105120
    },
    {
      "epoch": 7.008666666666667,
      "grad_norm": 0.13603229820728302,
      "learning_rate": 6.1958333333333334e-06,
      "loss": 0.0016,
      "step": 105130
    },
    {
      "epoch": 7.009333333333333,
      "grad_norm": 0.0712982788681984,
      "learning_rate": 6.191666666666667e-06,
      "loss": 0.0012,
      "step": 105140
    },
    {
      "epoch": 7.01,
      "grad_norm": 0.2386840134859085,
      "learning_rate": 6.1875000000000005e-06,
      "loss": 0.0024,
      "step": 105150
    },
    {
      "epoch": 7.010666666666666,
      "grad_norm": 0.27251243591308594,
      "learning_rate": 6.183333333333333e-06,
      "loss": 0.0018,
      "step": 105160
    },
    {
      "epoch": 7.011333333333333,
      "grad_norm": 0.0721830353140831,
      "learning_rate": 6.1791666666666675e-06,
      "loss": 0.0019,
      "step": 105170
    },
    {
      "epoch": 7.012,
      "grad_norm": 0.3056184947490692,
      "learning_rate": 6.175e-06,
      "loss": 0.0018,
      "step": 105180
    },
    {
      "epoch": 7.012666666666667,
      "grad_norm": 0.27467507123947144,
      "learning_rate": 6.170833333333334e-06,
      "loss": 0.0016,
      "step": 105190
    },
    {
      "epoch": 7.013333333333334,
      "grad_norm": 0.24460089206695557,
      "learning_rate": 6.166666666666667e-06,
      "loss": 0.0015,
      "step": 105200
    },
    {
      "epoch": 7.014,
      "grad_norm": 0.11278989166021347,
      "learning_rate": 6.1625e-06,
      "loss": 0.0018,
      "step": 105210
    },
    {
      "epoch": 7.014666666666667,
      "grad_norm": 0.04327196627855301,
      "learning_rate": 6.158333333333333e-06,
      "loss": 0.0018,
      "step": 105220
    },
    {
      "epoch": 7.015333333333333,
      "grad_norm": 0.11841899901628494,
      "learning_rate": 6.154166666666667e-06,
      "loss": 0.0014,
      "step": 105230
    },
    {
      "epoch": 7.016,
      "grad_norm": 0.30725350975990295,
      "learning_rate": 6.15e-06,
      "loss": 0.0016,
      "step": 105240
    },
    {
      "epoch": 7.016666666666667,
      "grad_norm": 0.3689420521259308,
      "learning_rate": 6.145833333333333e-06,
      "loss": 0.0015,
      "step": 105250
    },
    {
      "epoch": 7.017333333333333,
      "grad_norm": 0.46992841362953186,
      "learning_rate": 6.1416666666666674e-06,
      "loss": 0.0019,
      "step": 105260
    },
    {
      "epoch": 7.018,
      "grad_norm": 0.1371040940284729,
      "learning_rate": 6.1375e-06,
      "loss": 0.0016,
      "step": 105270
    },
    {
      "epoch": 7.018666666666666,
      "grad_norm": 0.053969502449035645,
      "learning_rate": 6.133333333333334e-06,
      "loss": 0.0023,
      "step": 105280
    },
    {
      "epoch": 7.019333333333333,
      "grad_norm": 0.14164157211780548,
      "learning_rate": 6.129166666666667e-06,
      "loss": 0.0019,
      "step": 105290
    },
    {
      "epoch": 7.02,
      "grad_norm": 0.5025960803031921,
      "learning_rate": 6.125e-06,
      "loss": 0.0017,
      "step": 105300
    },
    {
      "epoch": 7.020666666666667,
      "grad_norm": 0.4432952404022217,
      "learning_rate": 6.120833333333333e-06,
      "loss": 0.0024,
      "step": 105310
    },
    {
      "epoch": 7.021333333333334,
      "grad_norm": 0.1716562807559967,
      "learning_rate": 6.116666666666667e-06,
      "loss": 0.0014,
      "step": 105320
    },
    {
      "epoch": 7.022,
      "grad_norm": 0.3018938899040222,
      "learning_rate": 6.1125e-06,
      "loss": 0.0016,
      "step": 105330
    },
    {
      "epoch": 7.022666666666667,
      "grad_norm": 0.1103806421160698,
      "learning_rate": 6.108333333333334e-06,
      "loss": 0.0016,
      "step": 105340
    },
    {
      "epoch": 7.023333333333333,
      "grad_norm": 0.10240617394447327,
      "learning_rate": 6.104166666666667e-06,
      "loss": 0.0014,
      "step": 105350
    },
    {
      "epoch": 7.024,
      "grad_norm": 0.4082185924053192,
      "learning_rate": 6.1e-06,
      "loss": 0.0019,
      "step": 105360
    },
    {
      "epoch": 7.024666666666667,
      "grad_norm": 0.27692341804504395,
      "learning_rate": 6.0958333333333336e-06,
      "loss": 0.0021,
      "step": 105370
    },
    {
      "epoch": 7.025333333333333,
      "grad_norm": 0.24640606343746185,
      "learning_rate": 6.091666666666667e-06,
      "loss": 0.0018,
      "step": 105380
    },
    {
      "epoch": 7.026,
      "grad_norm": 0.43532994389533997,
      "learning_rate": 6.0875e-06,
      "loss": 0.0026,
      "step": 105390
    },
    {
      "epoch": 7.026666666666666,
      "grad_norm": 0.552128255367279,
      "learning_rate": 6.083333333333334e-06,
      "loss": 0.0024,
      "step": 105400
    },
    {
      "epoch": 7.027333333333333,
      "grad_norm": 0.6105623841285706,
      "learning_rate": 6.079166666666667e-06,
      "loss": 0.0013,
      "step": 105410
    },
    {
      "epoch": 7.028,
      "grad_norm": 0.10848042368888855,
      "learning_rate": 6.075e-06,
      "loss": 0.0013,
      "step": 105420
    },
    {
      "epoch": 7.028666666666667,
      "grad_norm": 0.26940467953681946,
      "learning_rate": 6.070833333333334e-06,
      "loss": 0.0013,
      "step": 105430
    },
    {
      "epoch": 7.029333333333334,
      "grad_norm": 0.36010146141052246,
      "learning_rate": 6.066666666666667e-06,
      "loss": 0.0013,
      "step": 105440
    },
    {
      "epoch": 7.03,
      "grad_norm": 0.4669696092605591,
      "learning_rate": 6.0625e-06,
      "loss": 0.0025,
      "step": 105450
    },
    {
      "epoch": 7.030666666666667,
      "grad_norm": 0.04292949289083481,
      "learning_rate": 6.058333333333334e-06,
      "loss": 0.0026,
      "step": 105460
    },
    {
      "epoch": 7.031333333333333,
      "grad_norm": 0.14250518381595612,
      "learning_rate": 6.054166666666667e-06,
      "loss": 0.0031,
      "step": 105470
    },
    {
      "epoch": 7.032,
      "grad_norm": 0.031145961955189705,
      "learning_rate": 6.0500000000000005e-06,
      "loss": 0.0013,
      "step": 105480
    },
    {
      "epoch": 7.032666666666667,
      "grad_norm": 0.21104243397712708,
      "learning_rate": 6.045833333333334e-06,
      "loss": 0.0023,
      "step": 105490
    },
    {
      "epoch": 7.033333333333333,
      "grad_norm": 0.11658080667257309,
      "learning_rate": 6.041666666666667e-06,
      "loss": 0.002,
      "step": 105500
    },
    {
      "epoch": 7.034,
      "grad_norm": 0.30939555168151855,
      "learning_rate": 6.0375e-06,
      "loss": 0.002,
      "step": 105510
    },
    {
      "epoch": 7.034666666666666,
      "grad_norm": 0.0703946128487587,
      "learning_rate": 6.033333333333334e-06,
      "loss": 0.0013,
      "step": 105520
    },
    {
      "epoch": 7.035333333333333,
      "grad_norm": 0.40318912267684937,
      "learning_rate": 6.029166666666667e-06,
      "loss": 0.0013,
      "step": 105530
    },
    {
      "epoch": 7.036,
      "grad_norm": 0.2722732722759247,
      "learning_rate": 6.025e-06,
      "loss": 0.0023,
      "step": 105540
    },
    {
      "epoch": 7.036666666666667,
      "grad_norm": 0.11294757574796677,
      "learning_rate": 6.020833333333334e-06,
      "loss": 0.0015,
      "step": 105550
    },
    {
      "epoch": 7.037333333333334,
      "grad_norm": 0.5231403708457947,
      "learning_rate": 6.016666666666667e-06,
      "loss": 0.0022,
      "step": 105560
    },
    {
      "epoch": 7.038,
      "grad_norm": 0.2218484729528427,
      "learning_rate": 6.0125000000000005e-06,
      "loss": 0.0017,
      "step": 105570
    },
    {
      "epoch": 7.038666666666667,
      "grad_norm": 0.09921920299530029,
      "learning_rate": 6.008333333333334e-06,
      "loss": 0.0013,
      "step": 105580
    },
    {
      "epoch": 7.039333333333333,
      "grad_norm": 0.03679858520627022,
      "learning_rate": 6.004166666666667e-06,
      "loss": 0.0019,
      "step": 105590
    },
    {
      "epoch": 7.04,
      "grad_norm": 0.393924355506897,
      "learning_rate": 6e-06,
      "loss": 0.0015,
      "step": 105600
    },
    {
      "epoch": 7.040666666666667,
      "grad_norm": 0.07369887828826904,
      "learning_rate": 5.995833333333334e-06,
      "loss": 0.002,
      "step": 105610
    },
    {
      "epoch": 7.041333333333333,
      "grad_norm": 0.20070460438728333,
      "learning_rate": 5.991666666666667e-06,
      "loss": 0.0021,
      "step": 105620
    },
    {
      "epoch": 7.042,
      "grad_norm": 0.30659061670303345,
      "learning_rate": 5.9875e-06,
      "loss": 0.0011,
      "step": 105630
    },
    {
      "epoch": 7.042666666666666,
      "grad_norm": 0.04300771281123161,
      "learning_rate": 5.983333333333334e-06,
      "loss": 0.002,
      "step": 105640
    },
    {
      "epoch": 7.043333333333333,
      "grad_norm": 0.13410134613513947,
      "learning_rate": 5.979166666666667e-06,
      "loss": 0.0017,
      "step": 105650
    },
    {
      "epoch": 7.044,
      "grad_norm": 0.05049794539809227,
      "learning_rate": 5.975e-06,
      "loss": 0.0013,
      "step": 105660
    },
    {
      "epoch": 7.044666666666667,
      "grad_norm": 0.7873217463493347,
      "learning_rate": 5.970833333333334e-06,
      "loss": 0.0015,
      "step": 105670
    },
    {
      "epoch": 7.045333333333334,
      "grad_norm": 0.1976419985294342,
      "learning_rate": 5.9666666666666666e-06,
      "loss": 0.0019,
      "step": 105680
    },
    {
      "epoch": 7.046,
      "grad_norm": 0.25866398215293884,
      "learning_rate": 5.9625e-06,
      "loss": 0.002,
      "step": 105690
    },
    {
      "epoch": 7.046666666666667,
      "grad_norm": 0.26790285110473633,
      "learning_rate": 5.958333333333334e-06,
      "loss": 0.002,
      "step": 105700
    },
    {
      "epoch": 7.0473333333333334,
      "grad_norm": 0.5903753042221069,
      "learning_rate": 5.954166666666667e-06,
      "loss": 0.0017,
      "step": 105710
    },
    {
      "epoch": 7.048,
      "grad_norm": 0.4359132647514343,
      "learning_rate": 5.95e-06,
      "loss": 0.0016,
      "step": 105720
    },
    {
      "epoch": 7.048666666666667,
      "grad_norm": 0.30502527952194214,
      "learning_rate": 5.945833333333334e-06,
      "loss": 0.0014,
      "step": 105730
    },
    {
      "epoch": 7.049333333333333,
      "grad_norm": 0.04652443900704384,
      "learning_rate": 5.941666666666667e-06,
      "loss": 0.0021,
      "step": 105740
    },
    {
      "epoch": 7.05,
      "grad_norm": 0.47561976313591003,
      "learning_rate": 5.9375e-06,
      "loss": 0.0022,
      "step": 105750
    },
    {
      "epoch": 7.050666666666666,
      "grad_norm": 0.10224021971225739,
      "learning_rate": 5.933333333333334e-06,
      "loss": 0.0012,
      "step": 105760
    },
    {
      "epoch": 7.051333333333333,
      "grad_norm": 0.1769145429134369,
      "learning_rate": 5.9291666666666665e-06,
      "loss": 0.0017,
      "step": 105770
    },
    {
      "epoch": 7.052,
      "grad_norm": 0.17259365320205688,
      "learning_rate": 5.925e-06,
      "loss": 0.0022,
      "step": 105780
    },
    {
      "epoch": 7.052666666666667,
      "grad_norm": 0.1392700970172882,
      "learning_rate": 5.9208333333333335e-06,
      "loss": 0.0018,
      "step": 105790
    },
    {
      "epoch": 7.053333333333334,
      "grad_norm": 0.023243296891450882,
      "learning_rate": 5.916666666666667e-06,
      "loss": 0.0014,
      "step": 105800
    },
    {
      "epoch": 7.054,
      "grad_norm": 0.27370744943618774,
      "learning_rate": 5.9125e-06,
      "loss": 0.0033,
      "step": 105810
    },
    {
      "epoch": 7.054666666666667,
      "grad_norm": 0.2315468192100525,
      "learning_rate": 5.908333333333334e-06,
      "loss": 0.0014,
      "step": 105820
    },
    {
      "epoch": 7.0553333333333335,
      "grad_norm": 0.23549288511276245,
      "learning_rate": 5.904166666666667e-06,
      "loss": 0.0024,
      "step": 105830
    },
    {
      "epoch": 7.056,
      "grad_norm": 0.33754825592041016,
      "learning_rate": 5.9e-06,
      "loss": 0.0016,
      "step": 105840
    },
    {
      "epoch": 7.056666666666667,
      "grad_norm": 0.1406429558992386,
      "learning_rate": 5.895833333333334e-06,
      "loss": 0.0022,
      "step": 105850
    },
    {
      "epoch": 7.057333333333333,
      "grad_norm": 0.3715093731880188,
      "learning_rate": 5.8916666666666664e-06,
      "loss": 0.0024,
      "step": 105860
    },
    {
      "epoch": 7.058,
      "grad_norm": 0.17447258532047272,
      "learning_rate": 5.8875e-06,
      "loss": 0.0015,
      "step": 105870
    },
    {
      "epoch": 7.058666666666666,
      "grad_norm": 0.340431272983551,
      "learning_rate": 5.8833333333333335e-06,
      "loss": 0.0019,
      "step": 105880
    },
    {
      "epoch": 7.059333333333333,
      "grad_norm": 0.042622026056051254,
      "learning_rate": 5.879166666666667e-06,
      "loss": 0.0016,
      "step": 105890
    },
    {
      "epoch": 7.06,
      "grad_norm": 0.10369336605072021,
      "learning_rate": 5.875e-06,
      "loss": 0.0014,
      "step": 105900
    },
    {
      "epoch": 7.060666666666667,
      "grad_norm": 0.07614859938621521,
      "learning_rate": 5.870833333333334e-06,
      "loss": 0.0018,
      "step": 105910
    },
    {
      "epoch": 7.061333333333334,
      "grad_norm": 0.07485374808311462,
      "learning_rate": 5.866666666666667e-06,
      "loss": 0.0012,
      "step": 105920
    },
    {
      "epoch": 7.062,
      "grad_norm": 0.5929796099662781,
      "learning_rate": 5.8625e-06,
      "loss": 0.0019,
      "step": 105930
    },
    {
      "epoch": 7.062666666666667,
      "grad_norm": 0.10207074135541916,
      "learning_rate": 5.858333333333334e-06,
      "loss": 0.0014,
      "step": 105940
    },
    {
      "epoch": 7.0633333333333335,
      "grad_norm": 0.20192520320415497,
      "learning_rate": 5.854166666666667e-06,
      "loss": 0.0027,
      "step": 105950
    },
    {
      "epoch": 7.064,
      "grad_norm": 0.1639249324798584,
      "learning_rate": 5.850000000000001e-06,
      "loss": 0.003,
      "step": 105960
    },
    {
      "epoch": 7.064666666666667,
      "grad_norm": 0.16925908625125885,
      "learning_rate": 5.845833333333333e-06,
      "loss": 0.0018,
      "step": 105970
    },
    {
      "epoch": 7.065333333333333,
      "grad_norm": 0.3355342447757721,
      "learning_rate": 5.841666666666667e-06,
      "loss": 0.0018,
      "step": 105980
    },
    {
      "epoch": 7.066,
      "grad_norm": 0.14183907210826874,
      "learning_rate": 5.8375000000000004e-06,
      "loss": 0.0019,
      "step": 105990
    },
    {
      "epoch": 7.066666666666666,
      "grad_norm": 0.43635207414627075,
      "learning_rate": 5.833333333333334e-06,
      "loss": 0.0016,
      "step": 106000
    },
    {
      "epoch": 7.067333333333333,
      "grad_norm": 0.30014291405677795,
      "learning_rate": 5.829166666666667e-06,
      "loss": 0.0015,
      "step": 106010
    },
    {
      "epoch": 7.068,
      "grad_norm": 0.05071795731782913,
      "learning_rate": 5.825000000000001e-06,
      "loss": 0.0025,
      "step": 106020
    },
    {
      "epoch": 7.068666666666667,
      "grad_norm": 0.44099122285842896,
      "learning_rate": 5.820833333333334e-06,
      "loss": 0.0025,
      "step": 106030
    },
    {
      "epoch": 7.069333333333334,
      "grad_norm": 0.47181791067123413,
      "learning_rate": 5.816666666666667e-06,
      "loss": 0.0019,
      "step": 106040
    },
    {
      "epoch": 7.07,
      "grad_norm": 0.03189695253968239,
      "learning_rate": 5.812500000000001e-06,
      "loss": 0.0014,
      "step": 106050
    },
    {
      "epoch": 7.070666666666667,
      "grad_norm": 0.07144429534673691,
      "learning_rate": 5.808333333333333e-06,
      "loss": 0.0013,
      "step": 106060
    },
    {
      "epoch": 7.0713333333333335,
      "grad_norm": 0.2995948791503906,
      "learning_rate": 5.804166666666667e-06,
      "loss": 0.0022,
      "step": 106070
    },
    {
      "epoch": 7.072,
      "grad_norm": 0.39748138189315796,
      "learning_rate": 5.8e-06,
      "loss": 0.0016,
      "step": 106080
    },
    {
      "epoch": 7.072666666666667,
      "grad_norm": 0.17097683250904083,
      "learning_rate": 5.795833333333334e-06,
      "loss": 0.0022,
      "step": 106090
    },
    {
      "epoch": 7.073333333333333,
      "grad_norm": 0.2711975574493408,
      "learning_rate": 5.7916666666666666e-06,
      "loss": 0.0013,
      "step": 106100
    },
    {
      "epoch": 7.074,
      "grad_norm": 0.8297511339187622,
      "learning_rate": 5.787500000000001e-06,
      "loss": 0.0017,
      "step": 106110
    },
    {
      "epoch": 7.074666666666666,
      "grad_norm": 0.3336294889450073,
      "learning_rate": 5.783333333333334e-06,
      "loss": 0.0016,
      "step": 106120
    },
    {
      "epoch": 7.075333333333333,
      "grad_norm": 0.2333829700946808,
      "learning_rate": 5.779166666666667e-06,
      "loss": 0.002,
      "step": 106130
    },
    {
      "epoch": 7.076,
      "grad_norm": 0.2931811511516571,
      "learning_rate": 5.775000000000001e-06,
      "loss": 0.0027,
      "step": 106140
    },
    {
      "epoch": 7.076666666666666,
      "grad_norm": 0.20285749435424805,
      "learning_rate": 5.770833333333333e-06,
      "loss": 0.0022,
      "step": 106150
    },
    {
      "epoch": 7.077333333333334,
      "grad_norm": 0.2637433111667633,
      "learning_rate": 5.766666666666667e-06,
      "loss": 0.0012,
      "step": 106160
    },
    {
      "epoch": 7.078,
      "grad_norm": 0.10948778688907623,
      "learning_rate": 5.7625e-06,
      "loss": 0.0022,
      "step": 106170
    },
    {
      "epoch": 7.078666666666667,
      "grad_norm": 0.16602861881256104,
      "learning_rate": 5.758333333333334e-06,
      "loss": 0.0012,
      "step": 106180
    },
    {
      "epoch": 7.0793333333333335,
      "grad_norm": 0.08326666057109833,
      "learning_rate": 5.7541666666666665e-06,
      "loss": 0.0017,
      "step": 106190
    },
    {
      "epoch": 7.08,
      "grad_norm": 0.3041892647743225,
      "learning_rate": 5.750000000000001e-06,
      "loss": 0.0016,
      "step": 106200
    },
    {
      "epoch": 7.080666666666667,
      "grad_norm": 0.2323620319366455,
      "learning_rate": 5.7458333333333335e-06,
      "loss": 0.0016,
      "step": 106210
    },
    {
      "epoch": 7.081333333333333,
      "grad_norm": 0.28056856989860535,
      "learning_rate": 5.741666666666667e-06,
      "loss": 0.002,
      "step": 106220
    },
    {
      "epoch": 7.082,
      "grad_norm": 0.1091352254152298,
      "learning_rate": 5.7375000000000005e-06,
      "loss": 0.0014,
      "step": 106230
    },
    {
      "epoch": 7.082666666666666,
      "grad_norm": 0.11351672559976578,
      "learning_rate": 5.733333333333333e-06,
      "loss": 0.0019,
      "step": 106240
    },
    {
      "epoch": 7.083333333333333,
      "grad_norm": 0.04464323818683624,
      "learning_rate": 5.729166666666667e-06,
      "loss": 0.0019,
      "step": 106250
    },
    {
      "epoch": 7.084,
      "grad_norm": 0.4187299907207489,
      "learning_rate": 5.725e-06,
      "loss": 0.002,
      "step": 106260
    },
    {
      "epoch": 7.084666666666667,
      "grad_norm": 0.6656414866447449,
      "learning_rate": 5.720833333333334e-06,
      "loss": 0.0017,
      "step": 106270
    },
    {
      "epoch": 7.085333333333334,
      "grad_norm": 0.090351901948452,
      "learning_rate": 5.7166666666666664e-06,
      "loss": 0.0013,
      "step": 106280
    },
    {
      "epoch": 7.086,
      "grad_norm": 0.2326313853263855,
      "learning_rate": 5.712500000000001e-06,
      "loss": 0.0025,
      "step": 106290
    },
    {
      "epoch": 7.086666666666667,
      "grad_norm": 0.472014844417572,
      "learning_rate": 5.7083333333333335e-06,
      "loss": 0.0025,
      "step": 106300
    },
    {
      "epoch": 7.0873333333333335,
      "grad_norm": 0.40469229221343994,
      "learning_rate": 5.704166666666667e-06,
      "loss": 0.0011,
      "step": 106310
    },
    {
      "epoch": 7.088,
      "grad_norm": 0.23862163722515106,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 0.0019,
      "step": 106320
    },
    {
      "epoch": 7.088666666666667,
      "grad_norm": 0.17542611062526703,
      "learning_rate": 5.695833333333334e-06,
      "loss": 0.0014,
      "step": 106330
    },
    {
      "epoch": 7.089333333333333,
      "grad_norm": 0.49976667761802673,
      "learning_rate": 5.691666666666667e-06,
      "loss": 0.0014,
      "step": 106340
    },
    {
      "epoch": 7.09,
      "grad_norm": 0.08093247562646866,
      "learning_rate": 5.6875e-06,
      "loss": 0.0013,
      "step": 106350
    },
    {
      "epoch": 7.0906666666666665,
      "grad_norm": 0.055942948907613754,
      "learning_rate": 5.683333333333334e-06,
      "loss": 0.0018,
      "step": 106360
    },
    {
      "epoch": 7.091333333333333,
      "grad_norm": 0.4739784598350525,
      "learning_rate": 5.679166666666666e-06,
      "loss": 0.0021,
      "step": 106370
    },
    {
      "epoch": 7.092,
      "grad_norm": 0.10861118882894516,
      "learning_rate": 5.675000000000001e-06,
      "loss": 0.0029,
      "step": 106380
    },
    {
      "epoch": 7.092666666666666,
      "grad_norm": 0.3634966015815735,
      "learning_rate": 5.670833333333333e-06,
      "loss": 0.003,
      "step": 106390
    },
    {
      "epoch": 7.093333333333334,
      "grad_norm": 0.11023453623056412,
      "learning_rate": 5.666666666666667e-06,
      "loss": 0.0018,
      "step": 106400
    },
    {
      "epoch": 7.094,
      "grad_norm": 0.3343812823295593,
      "learning_rate": 5.6625e-06,
      "loss": 0.0013,
      "step": 106410
    },
    {
      "epoch": 7.094666666666667,
      "grad_norm": 0.13856536149978638,
      "learning_rate": 5.658333333333334e-06,
      "loss": 0.0023,
      "step": 106420
    },
    {
      "epoch": 7.0953333333333335,
      "grad_norm": 0.02686282806098461,
      "learning_rate": 5.654166666666667e-06,
      "loss": 0.0017,
      "step": 106430
    },
    {
      "epoch": 7.096,
      "grad_norm": 0.10543088614940643,
      "learning_rate": 5.65e-06,
      "loss": 0.002,
      "step": 106440
    },
    {
      "epoch": 7.096666666666667,
      "grad_norm": 0.23079387843608856,
      "learning_rate": 5.645833333333334e-06,
      "loss": 0.0022,
      "step": 106450
    },
    {
      "epoch": 7.097333333333333,
      "grad_norm": 0.3336772322654724,
      "learning_rate": 5.641666666666666e-06,
      "loss": 0.0026,
      "step": 106460
    },
    {
      "epoch": 7.098,
      "grad_norm": 0.17546655237674713,
      "learning_rate": 5.637500000000001e-06,
      "loss": 0.0024,
      "step": 106470
    },
    {
      "epoch": 7.0986666666666665,
      "grad_norm": 0.16630353033542633,
      "learning_rate": 5.633333333333333e-06,
      "loss": 0.0012,
      "step": 106480
    },
    {
      "epoch": 7.099333333333333,
      "grad_norm": 0.07276522368192673,
      "learning_rate": 5.629166666666667e-06,
      "loss": 0.002,
      "step": 106490
    },
    {
      "epoch": 7.1,
      "grad_norm": 0.4337872266769409,
      "learning_rate": 5.625e-06,
      "loss": 0.0013,
      "step": 106500
    },
    {
      "epoch": 7.100666666666666,
      "grad_norm": 0.13681264221668243,
      "learning_rate": 5.620833333333334e-06,
      "loss": 0.0013,
      "step": 106510
    },
    {
      "epoch": 7.101333333333334,
      "grad_norm": 0.4205077290534973,
      "learning_rate": 5.6166666666666665e-06,
      "loss": 0.0019,
      "step": 106520
    },
    {
      "epoch": 7.102,
      "grad_norm": 0.05451706424355507,
      "learning_rate": 5.6125e-06,
      "loss": 0.0019,
      "step": 106530
    },
    {
      "epoch": 7.102666666666667,
      "grad_norm": 0.2076522409915924,
      "learning_rate": 5.6083333333333336e-06,
      "loss": 0.0014,
      "step": 106540
    },
    {
      "epoch": 7.1033333333333335,
      "grad_norm": 0.5817418098449707,
      "learning_rate": 5.604166666666666e-06,
      "loss": 0.0027,
      "step": 106550
    },
    {
      "epoch": 7.104,
      "grad_norm": 0.23724623024463654,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.0012,
      "step": 106560
    },
    {
      "epoch": 7.104666666666667,
      "grad_norm": 0.3399486243724823,
      "learning_rate": 5.595833333333333e-06,
      "loss": 0.0015,
      "step": 106570
    },
    {
      "epoch": 7.105333333333333,
      "grad_norm": 0.7032221555709839,
      "learning_rate": 5.591666666666668e-06,
      "loss": 0.002,
      "step": 106580
    },
    {
      "epoch": 7.106,
      "grad_norm": 0.171055406332016,
      "learning_rate": 5.5875e-06,
      "loss": 0.0014,
      "step": 106590
    },
    {
      "epoch": 7.1066666666666665,
      "grad_norm": 0.3653852045536041,
      "learning_rate": 5.583333333333334e-06,
      "loss": 0.0022,
      "step": 106600
    },
    {
      "epoch": 7.107333333333333,
      "grad_norm": 0.07030155509710312,
      "learning_rate": 5.579166666666667e-06,
      "loss": 0.0017,
      "step": 106610
    },
    {
      "epoch": 7.108,
      "grad_norm": 0.30042245984077454,
      "learning_rate": 5.575e-06,
      "loss": 0.0021,
      "step": 106620
    },
    {
      "epoch": 7.108666666666666,
      "grad_norm": 0.23381353914737701,
      "learning_rate": 5.5708333333333335e-06,
      "loss": 0.0017,
      "step": 106630
    },
    {
      "epoch": 7.109333333333334,
      "grad_norm": 0.03998361527919769,
      "learning_rate": 5.566666666666667e-06,
      "loss": 0.0015,
      "step": 106640
    },
    {
      "epoch": 7.11,
      "grad_norm": 0.5041199922561646,
      "learning_rate": 5.5625000000000005e-06,
      "loss": 0.0022,
      "step": 106650
    },
    {
      "epoch": 7.110666666666667,
      "grad_norm": 0.07149788737297058,
      "learning_rate": 5.558333333333333e-06,
      "loss": 0.0013,
      "step": 106660
    },
    {
      "epoch": 7.1113333333333335,
      "grad_norm": 0.04503453150391579,
      "learning_rate": 5.5541666666666676e-06,
      "loss": 0.0019,
      "step": 106670
    },
    {
      "epoch": 7.112,
      "grad_norm": 0.13736772537231445,
      "learning_rate": 5.55e-06,
      "loss": 0.0016,
      "step": 106680
    },
    {
      "epoch": 7.112666666666667,
      "grad_norm": 0.3742677867412567,
      "learning_rate": 5.545833333333334e-06,
      "loss": 0.0012,
      "step": 106690
    },
    {
      "epoch": 7.113333333333333,
      "grad_norm": 0.07032658159732819,
      "learning_rate": 5.541666666666667e-06,
      "loss": 0.0014,
      "step": 106700
    },
    {
      "epoch": 7.114,
      "grad_norm": 0.2014426738023758,
      "learning_rate": 5.5375e-06,
      "loss": 0.0016,
      "step": 106710
    },
    {
      "epoch": 7.1146666666666665,
      "grad_norm": 0.17984400689601898,
      "learning_rate": 5.5333333333333334e-06,
      "loss": 0.0015,
      "step": 106720
    },
    {
      "epoch": 7.115333333333333,
      "grad_norm": 0.3051471710205078,
      "learning_rate": 5.529166666666667e-06,
      "loss": 0.0015,
      "step": 106730
    },
    {
      "epoch": 7.116,
      "grad_norm": 0.052373044192790985,
      "learning_rate": 5.5250000000000005e-06,
      "loss": 0.0012,
      "step": 106740
    },
    {
      "epoch": 7.116666666666666,
      "grad_norm": 0.5370961427688599,
      "learning_rate": 5.520833333333333e-06,
      "loss": 0.0014,
      "step": 106750
    },
    {
      "epoch": 7.117333333333334,
      "grad_norm": 0.13711701333522797,
      "learning_rate": 5.5166666666666675e-06,
      "loss": 0.0014,
      "step": 106760
    },
    {
      "epoch": 7.118,
      "grad_norm": 0.46743181347846985,
      "learning_rate": 5.5125e-06,
      "loss": 0.002,
      "step": 106770
    },
    {
      "epoch": 7.118666666666667,
      "grad_norm": 0.23328489065170288,
      "learning_rate": 5.508333333333334e-06,
      "loss": 0.0023,
      "step": 106780
    },
    {
      "epoch": 7.1193333333333335,
      "grad_norm": 0.23892252147197723,
      "learning_rate": 5.504166666666667e-06,
      "loss": 0.0019,
      "step": 106790
    },
    {
      "epoch": 7.12,
      "grad_norm": 0.6358580589294434,
      "learning_rate": 5.500000000000001e-06,
      "loss": 0.0021,
      "step": 106800
    },
    {
      "epoch": 7.120666666666667,
      "grad_norm": 0.39977511763572693,
      "learning_rate": 5.495833333333333e-06,
      "loss": 0.0022,
      "step": 106810
    },
    {
      "epoch": 7.121333333333333,
      "grad_norm": 0.08074149489402771,
      "learning_rate": 5.491666666666667e-06,
      "loss": 0.003,
      "step": 106820
    },
    {
      "epoch": 7.122,
      "grad_norm": 0.2339262068271637,
      "learning_rate": 5.4875e-06,
      "loss": 0.0019,
      "step": 106830
    },
    {
      "epoch": 7.1226666666666665,
      "grad_norm": 0.07521316409111023,
      "learning_rate": 5.483333333333333e-06,
      "loss": 0.0013,
      "step": 106840
    },
    {
      "epoch": 7.123333333333333,
      "grad_norm": 0.23143452405929565,
      "learning_rate": 5.4791666666666674e-06,
      "loss": 0.0023,
      "step": 106850
    },
    {
      "epoch": 7.124,
      "grad_norm": 0.0615917332470417,
      "learning_rate": 5.475e-06,
      "loss": 0.0021,
      "step": 106860
    },
    {
      "epoch": 7.124666666666666,
      "grad_norm": 0.13679426908493042,
      "learning_rate": 5.470833333333334e-06,
      "loss": 0.0023,
      "step": 106870
    },
    {
      "epoch": 7.125333333333334,
      "grad_norm": 0.5064468383789062,
      "learning_rate": 5.466666666666667e-06,
      "loss": 0.0022,
      "step": 106880
    },
    {
      "epoch": 7.126,
      "grad_norm": 0.20333562791347504,
      "learning_rate": 5.462500000000001e-06,
      "loss": 0.0025,
      "step": 106890
    },
    {
      "epoch": 7.126666666666667,
      "grad_norm": 0.33958518505096436,
      "learning_rate": 5.458333333333333e-06,
      "loss": 0.0029,
      "step": 106900
    },
    {
      "epoch": 7.1273333333333335,
      "grad_norm": 0.27558761835098267,
      "learning_rate": 5.454166666666667e-06,
      "loss": 0.0018,
      "step": 106910
    },
    {
      "epoch": 7.128,
      "grad_norm": 0.2699359953403473,
      "learning_rate": 5.45e-06,
      "loss": 0.0014,
      "step": 106920
    },
    {
      "epoch": 7.128666666666667,
      "grad_norm": 0.13607992231845856,
      "learning_rate": 5.445833333333333e-06,
      "loss": 0.0023,
      "step": 106930
    },
    {
      "epoch": 7.129333333333333,
      "grad_norm": 0.1651877909898758,
      "learning_rate": 5.441666666666667e-06,
      "loss": 0.0013,
      "step": 106940
    },
    {
      "epoch": 7.13,
      "grad_norm": 0.11191897094249725,
      "learning_rate": 5.4375e-06,
      "loss": 0.0019,
      "step": 106950
    },
    {
      "epoch": 7.1306666666666665,
      "grad_norm": 0.030421169474720955,
      "learning_rate": 5.4333333333333335e-06,
      "loss": 0.0013,
      "step": 106960
    },
    {
      "epoch": 7.131333333333333,
      "grad_norm": 0.03551141172647476,
      "learning_rate": 5.429166666666667e-06,
      "loss": 0.002,
      "step": 106970
    },
    {
      "epoch": 7.132,
      "grad_norm": 0.3315049111843109,
      "learning_rate": 5.4250000000000006e-06,
      "loss": 0.0025,
      "step": 106980
    },
    {
      "epoch": 7.132666666666666,
      "grad_norm": 0.027014032006263733,
      "learning_rate": 5.420833333333333e-06,
      "loss": 0.0017,
      "step": 106990
    },
    {
      "epoch": 7.133333333333334,
      "grad_norm": 0.18632638454437256,
      "learning_rate": 5.416666666666667e-06,
      "loss": 0.0023,
      "step": 107000
    },
    {
      "epoch": 7.134,
      "grad_norm": 0.04095013812184334,
      "learning_rate": 5.4125e-06,
      "loss": 0.0017,
      "step": 107010
    },
    {
      "epoch": 7.134666666666667,
      "grad_norm": 0.072197325527668,
      "learning_rate": 5.408333333333333e-06,
      "loss": 0.0028,
      "step": 107020
    },
    {
      "epoch": 7.1353333333333335,
      "grad_norm": 0.18664884567260742,
      "learning_rate": 5.404166666666667e-06,
      "loss": 0.0018,
      "step": 107030
    },
    {
      "epoch": 7.136,
      "grad_norm": 0.0449795201420784,
      "learning_rate": 5.4e-06,
      "loss": 0.0015,
      "step": 107040
    },
    {
      "epoch": 7.136666666666667,
      "grad_norm": 0.20236800611019135,
      "learning_rate": 5.3958333333333335e-06,
      "loss": 0.0013,
      "step": 107050
    },
    {
      "epoch": 7.137333333333333,
      "grad_norm": 0.1737762987613678,
      "learning_rate": 5.391666666666667e-06,
      "loss": 0.0013,
      "step": 107060
    },
    {
      "epoch": 7.138,
      "grad_norm": 0.23892438411712646,
      "learning_rate": 5.3875000000000005e-06,
      "loss": 0.0019,
      "step": 107070
    },
    {
      "epoch": 7.1386666666666665,
      "grad_norm": 0.2284374237060547,
      "learning_rate": 5.383333333333333e-06,
      "loss": 0.0019,
      "step": 107080
    },
    {
      "epoch": 7.139333333333333,
      "grad_norm": 0.0730840414762497,
      "learning_rate": 5.379166666666667e-06,
      "loss": 0.0021,
      "step": 107090
    },
    {
      "epoch": 7.14,
      "grad_norm": 0.23373253643512726,
      "learning_rate": 5.375e-06,
      "loss": 0.0016,
      "step": 107100
    },
    {
      "epoch": 7.140666666666666,
      "grad_norm": 0.07524284720420837,
      "learning_rate": 5.370833333333333e-06,
      "loss": 0.0026,
      "step": 107110
    },
    {
      "epoch": 7.141333333333334,
      "grad_norm": 0.07476472109556198,
      "learning_rate": 5.366666666666667e-06,
      "loss": 0.0018,
      "step": 107120
    },
    {
      "epoch": 7.142,
      "grad_norm": 0.07532359659671783,
      "learning_rate": 5.3625e-06,
      "loss": 0.0015,
      "step": 107130
    },
    {
      "epoch": 7.142666666666667,
      "grad_norm": 0.605786144733429,
      "learning_rate": 5.358333333333333e-06,
      "loss": 0.0018,
      "step": 107140
    },
    {
      "epoch": 7.1433333333333335,
      "grad_norm": 0.13701675832271576,
      "learning_rate": 5.354166666666667e-06,
      "loss": 0.0023,
      "step": 107150
    },
    {
      "epoch": 7.144,
      "grad_norm": 0.10893892496824265,
      "learning_rate": 5.3500000000000004e-06,
      "loss": 0.0019,
      "step": 107160
    },
    {
      "epoch": 7.144666666666667,
      "grad_norm": 0.1719483882188797,
      "learning_rate": 5.345833333333333e-06,
      "loss": 0.0023,
      "step": 107170
    },
    {
      "epoch": 7.145333333333333,
      "grad_norm": 0.30323222279548645,
      "learning_rate": 5.341666666666667e-06,
      "loss": 0.0013,
      "step": 107180
    },
    {
      "epoch": 7.146,
      "grad_norm": 0.07274965941905975,
      "learning_rate": 5.3375e-06,
      "loss": 0.0019,
      "step": 107190
    },
    {
      "epoch": 7.1466666666666665,
      "grad_norm": 0.05991154909133911,
      "learning_rate": 5.333333333333334e-06,
      "loss": 0.0011,
      "step": 107200
    },
    {
      "epoch": 7.147333333333333,
      "grad_norm": 0.24054676294326782,
      "learning_rate": 5.329166666666667e-06,
      "loss": 0.0015,
      "step": 107210
    },
    {
      "epoch": 7.148,
      "grad_norm": 0.5682563185691833,
      "learning_rate": 5.325e-06,
      "loss": 0.0023,
      "step": 107220
    },
    {
      "epoch": 7.148666666666666,
      "grad_norm": 0.24495242536067963,
      "learning_rate": 5.320833333333334e-06,
      "loss": 0.0017,
      "step": 107230
    },
    {
      "epoch": 7.149333333333334,
      "grad_norm": 0.23998171091079712,
      "learning_rate": 5.316666666666667e-06,
      "loss": 0.0019,
      "step": 107240
    },
    {
      "epoch": 7.15,
      "grad_norm": 0.02396753802895546,
      "learning_rate": 5.3125e-06,
      "loss": 0.0013,
      "step": 107250
    },
    {
      "epoch": 7.150666666666667,
      "grad_norm": 0.11200625449419022,
      "learning_rate": 5.308333333333334e-06,
      "loss": 0.0013,
      "step": 107260
    },
    {
      "epoch": 7.1513333333333335,
      "grad_norm": 0.26625630259513855,
      "learning_rate": 5.304166666666667e-06,
      "loss": 0.0026,
      "step": 107270
    },
    {
      "epoch": 7.152,
      "grad_norm": 0.26882001757621765,
      "learning_rate": 5.3e-06,
      "loss": 0.0014,
      "step": 107280
    },
    {
      "epoch": 7.152666666666667,
      "grad_norm": 0.38768160343170166,
      "learning_rate": 5.295833333333334e-06,
      "loss": 0.0021,
      "step": 107290
    },
    {
      "epoch": 7.153333333333333,
      "grad_norm": 0.23611776530742645,
      "learning_rate": 5.291666666666667e-06,
      "loss": 0.0016,
      "step": 107300
    },
    {
      "epoch": 7.154,
      "grad_norm": 0.47157028317451477,
      "learning_rate": 5.2875e-06,
      "loss": 0.0016,
      "step": 107310
    },
    {
      "epoch": 7.1546666666666665,
      "grad_norm": 0.4070426821708679,
      "learning_rate": 5.283333333333334e-06,
      "loss": 0.0013,
      "step": 107320
    },
    {
      "epoch": 7.155333333333333,
      "grad_norm": 0.10361123830080032,
      "learning_rate": 5.279166666666667e-06,
      "loss": 0.0011,
      "step": 107330
    },
    {
      "epoch": 7.156,
      "grad_norm": 0.23610225319862366,
      "learning_rate": 5.275e-06,
      "loss": 0.0013,
      "step": 107340
    },
    {
      "epoch": 7.156666666666666,
      "grad_norm": 0.06445585936307907,
      "learning_rate": 5.270833333333334e-06,
      "loss": 0.0019,
      "step": 107350
    },
    {
      "epoch": 7.157333333333334,
      "grad_norm": 0.36453065276145935,
      "learning_rate": 5.266666666666667e-06,
      "loss": 0.0025,
      "step": 107360
    },
    {
      "epoch": 7.158,
      "grad_norm": 0.2876631021499634,
      "learning_rate": 5.2625e-06,
      "loss": 0.0024,
      "step": 107370
    },
    {
      "epoch": 7.158666666666667,
      "grad_norm": 0.2355627566576004,
      "learning_rate": 5.2583333333333335e-06,
      "loss": 0.002,
      "step": 107380
    },
    {
      "epoch": 7.1593333333333335,
      "grad_norm": 0.2044813632965088,
      "learning_rate": 5.254166666666667e-06,
      "loss": 0.0024,
      "step": 107390
    },
    {
      "epoch": 7.16,
      "grad_norm": 0.17736610770225525,
      "learning_rate": 5.25e-06,
      "loss": 0.0016,
      "step": 107400
    },
    {
      "epoch": 7.160666666666667,
      "grad_norm": 0.2108144909143448,
      "learning_rate": 5.245833333333334e-06,
      "loss": 0.0022,
      "step": 107410
    },
    {
      "epoch": 7.161333333333333,
      "grad_norm": 0.38273751735687256,
      "learning_rate": 5.241666666666667e-06,
      "loss": 0.0018,
      "step": 107420
    },
    {
      "epoch": 7.162,
      "grad_norm": 0.03813936188817024,
      "learning_rate": 5.2375e-06,
      "loss": 0.0015,
      "step": 107430
    },
    {
      "epoch": 7.1626666666666665,
      "grad_norm": 0.04537229239940643,
      "learning_rate": 5.233333333333334e-06,
      "loss": 0.0019,
      "step": 107440
    },
    {
      "epoch": 7.163333333333333,
      "grad_norm": 0.0700073093175888,
      "learning_rate": 5.229166666666667e-06,
      "loss": 0.0016,
      "step": 107450
    },
    {
      "epoch": 7.164,
      "grad_norm": 0.04357760027050972,
      "learning_rate": 5.225e-06,
      "loss": 0.0019,
      "step": 107460
    },
    {
      "epoch": 7.164666666666666,
      "grad_norm": 0.14092791080474854,
      "learning_rate": 5.2208333333333335e-06,
      "loss": 0.0028,
      "step": 107470
    },
    {
      "epoch": 7.165333333333333,
      "grad_norm": 0.14011409878730774,
      "learning_rate": 5.216666666666667e-06,
      "loss": 0.0014,
      "step": 107480
    },
    {
      "epoch": 7.166,
      "grad_norm": 0.049124233424663544,
      "learning_rate": 5.2125e-06,
      "loss": 0.0014,
      "step": 107490
    },
    {
      "epoch": 7.166666666666667,
      "grad_norm": 0.30062004923820496,
      "learning_rate": 5.208333333333334e-06,
      "loss": 0.0017,
      "step": 107500
    },
    {
      "epoch": 7.167333333333334,
      "grad_norm": 0.07962213456630707,
      "learning_rate": 5.204166666666667e-06,
      "loss": 0.0013,
      "step": 107510
    },
    {
      "epoch": 7.168,
      "grad_norm": 0.273007333278656,
      "learning_rate": 5.2e-06,
      "loss": 0.0017,
      "step": 107520
    },
    {
      "epoch": 7.168666666666667,
      "grad_norm": 0.30935388803482056,
      "learning_rate": 5.195833333333334e-06,
      "loss": 0.0021,
      "step": 107530
    },
    {
      "epoch": 7.169333333333333,
      "grad_norm": 0.06554765999317169,
      "learning_rate": 5.191666666666667e-06,
      "loss": 0.0018,
      "step": 107540
    },
    {
      "epoch": 7.17,
      "grad_norm": 0.055911704897880554,
      "learning_rate": 5.1875e-06,
      "loss": 0.0016,
      "step": 107550
    },
    {
      "epoch": 7.1706666666666665,
      "grad_norm": 0.10185951739549637,
      "learning_rate": 5.183333333333333e-06,
      "loss": 0.0017,
      "step": 107560
    },
    {
      "epoch": 7.171333333333333,
      "grad_norm": 0.27091342210769653,
      "learning_rate": 5.179166666666667e-06,
      "loss": 0.0015,
      "step": 107570
    },
    {
      "epoch": 7.172,
      "grad_norm": 0.13739025592803955,
      "learning_rate": 5.175e-06,
      "loss": 0.0016,
      "step": 107580
    },
    {
      "epoch": 7.172666666666666,
      "grad_norm": 0.13558770716190338,
      "learning_rate": 5.170833333333334e-06,
      "loss": 0.0021,
      "step": 107590
    },
    {
      "epoch": 7.173333333333334,
      "grad_norm": 0.41188305616378784,
      "learning_rate": 5.166666666666667e-06,
      "loss": 0.0018,
      "step": 107600
    },
    {
      "epoch": 7.174,
      "grad_norm": 0.13534745573997498,
      "learning_rate": 5.1625e-06,
      "loss": 0.0014,
      "step": 107610
    },
    {
      "epoch": 7.174666666666667,
      "grad_norm": 0.0873321145772934,
      "learning_rate": 5.158333333333334e-06,
      "loss": 0.0013,
      "step": 107620
    },
    {
      "epoch": 7.175333333333334,
      "grad_norm": 0.435820072889328,
      "learning_rate": 5.154166666666667e-06,
      "loss": 0.0022,
      "step": 107630
    },
    {
      "epoch": 7.176,
      "grad_norm": 0.5579177141189575,
      "learning_rate": 5.15e-06,
      "loss": 0.0019,
      "step": 107640
    },
    {
      "epoch": 7.176666666666667,
      "grad_norm": 0.07421698421239853,
      "learning_rate": 5.145833333333333e-06,
      "loss": 0.0014,
      "step": 107650
    },
    {
      "epoch": 7.177333333333333,
      "grad_norm": 0.11063290387392044,
      "learning_rate": 5.141666666666667e-06,
      "loss": 0.0022,
      "step": 107660
    },
    {
      "epoch": 7.178,
      "grad_norm": 0.30866265296936035,
      "learning_rate": 5.1375e-06,
      "loss": 0.002,
      "step": 107670
    },
    {
      "epoch": 7.1786666666666665,
      "grad_norm": 0.700533926486969,
      "learning_rate": 5.133333333333334e-06,
      "loss": 0.0038,
      "step": 107680
    },
    {
      "epoch": 7.179333333333333,
      "grad_norm": 0.3310413062572479,
      "learning_rate": 5.1291666666666665e-06,
      "loss": 0.0016,
      "step": 107690
    },
    {
      "epoch": 7.18,
      "grad_norm": 0.20295269787311554,
      "learning_rate": 5.125e-06,
      "loss": 0.0013,
      "step": 107700
    },
    {
      "epoch": 7.180666666666666,
      "grad_norm": 0.13948702812194824,
      "learning_rate": 5.1208333333333336e-06,
      "loss": 0.0015,
      "step": 107710
    },
    {
      "epoch": 7.181333333333333,
      "grad_norm": 0.07238975167274475,
      "learning_rate": 5.116666666666667e-06,
      "loss": 0.0019,
      "step": 107720
    },
    {
      "epoch": 7.182,
      "grad_norm": 0.07829143851995468,
      "learning_rate": 5.1125e-06,
      "loss": 0.0013,
      "step": 107730
    },
    {
      "epoch": 7.182666666666667,
      "grad_norm": 0.30208608508110046,
      "learning_rate": 5.108333333333334e-06,
      "loss": 0.0014,
      "step": 107740
    },
    {
      "epoch": 7.183333333333334,
      "grad_norm": 0.16916678845882416,
      "learning_rate": 5.104166666666667e-06,
      "loss": 0.0021,
      "step": 107750
    },
    {
      "epoch": 7.184,
      "grad_norm": 0.04624062404036522,
      "learning_rate": 5.1e-06,
      "loss": 0.002,
      "step": 107760
    },
    {
      "epoch": 7.184666666666667,
      "grad_norm": 0.17251670360565186,
      "learning_rate": 5.095833333333334e-06,
      "loss": 0.0019,
      "step": 107770
    },
    {
      "epoch": 7.185333333333333,
      "grad_norm": 0.08340679854154587,
      "learning_rate": 5.0916666666666665e-06,
      "loss": 0.0015,
      "step": 107780
    },
    {
      "epoch": 7.186,
      "grad_norm": 0.1797124743461609,
      "learning_rate": 5.0875e-06,
      "loss": 0.0013,
      "step": 107790
    },
    {
      "epoch": 7.1866666666666665,
      "grad_norm": 0.5125336050987244,
      "learning_rate": 5.0833333333333335e-06,
      "loss": 0.0013,
      "step": 107800
    },
    {
      "epoch": 7.187333333333333,
      "grad_norm": 0.2981875240802765,
      "learning_rate": 5.079166666666667e-06,
      "loss": 0.0016,
      "step": 107810
    },
    {
      "epoch": 7.188,
      "grad_norm": 0.24617120623588562,
      "learning_rate": 5.0750000000000005e-06,
      "loss": 0.0016,
      "step": 107820
    },
    {
      "epoch": 7.188666666666666,
      "grad_norm": 0.051332127302885056,
      "learning_rate": 5.070833333333334e-06,
      "loss": 0.0017,
      "step": 107830
    },
    {
      "epoch": 7.189333333333333,
      "grad_norm": 0.20165222883224487,
      "learning_rate": 5.066666666666667e-06,
      "loss": 0.0014,
      "step": 107840
    },
    {
      "epoch": 7.19,
      "grad_norm": 0.26771634817123413,
      "learning_rate": 5.0625e-06,
      "loss": 0.0011,
      "step": 107850
    },
    {
      "epoch": 7.190666666666667,
      "grad_norm": 0.33428308367729187,
      "learning_rate": 5.058333333333334e-06,
      "loss": 0.0014,
      "step": 107860
    },
    {
      "epoch": 7.191333333333334,
      "grad_norm": 0.03940732032060623,
      "learning_rate": 5.054166666666666e-06,
      "loss": 0.0015,
      "step": 107870
    },
    {
      "epoch": 7.192,
      "grad_norm": 0.2360406517982483,
      "learning_rate": 5.050000000000001e-06,
      "loss": 0.0022,
      "step": 107880
    },
    {
      "epoch": 7.192666666666667,
      "grad_norm": 0.10797515511512756,
      "learning_rate": 5.0458333333333334e-06,
      "loss": 0.0017,
      "step": 107890
    },
    {
      "epoch": 7.193333333333333,
      "grad_norm": 0.11575578153133392,
      "learning_rate": 5.041666666666667e-06,
      "loss": 0.002,
      "step": 107900
    },
    {
      "epoch": 7.194,
      "grad_norm": 0.2074805647134781,
      "learning_rate": 5.0375000000000005e-06,
      "loss": 0.0017,
      "step": 107910
    },
    {
      "epoch": 7.1946666666666665,
      "grad_norm": 0.06960032135248184,
      "learning_rate": 5.033333333333334e-06,
      "loss": 0.0014,
      "step": 107920
    },
    {
      "epoch": 7.195333333333333,
      "grad_norm": 0.17516444623470306,
      "learning_rate": 5.029166666666667e-06,
      "loss": 0.0021,
      "step": 107930
    },
    {
      "epoch": 7.196,
      "grad_norm": 0.6953250765800476,
      "learning_rate": 5.025e-06,
      "loss": 0.0025,
      "step": 107940
    },
    {
      "epoch": 7.196666666666666,
      "grad_norm": 0.10695432871580124,
      "learning_rate": 5.020833333333334e-06,
      "loss": 0.0022,
      "step": 107950
    },
    {
      "epoch": 7.197333333333333,
      "grad_norm": 0.07117436826229095,
      "learning_rate": 5.016666666666666e-06,
      "loss": 0.0029,
      "step": 107960
    },
    {
      "epoch": 7.198,
      "grad_norm": 0.36516326665878296,
      "learning_rate": 5.012500000000001e-06,
      "loss": 0.0014,
      "step": 107970
    },
    {
      "epoch": 7.198666666666667,
      "grad_norm": 0.02814730443060398,
      "learning_rate": 5.008333333333333e-06,
      "loss": 0.0028,
      "step": 107980
    },
    {
      "epoch": 7.199333333333334,
      "grad_norm": 0.20164595544338226,
      "learning_rate": 5.004166666666667e-06,
      "loss": 0.0014,
      "step": 107990
    },
    {
      "epoch": 7.2,
      "grad_norm": 0.1688295602798462,
      "learning_rate": 5e-06,
      "loss": 0.0024,
      "step": 108000
    },
    {
      "epoch": 7.200666666666667,
      "grad_norm": 0.3755750358104706,
      "learning_rate": 4.995833333333334e-06,
      "loss": 0.0015,
      "step": 108010
    },
    {
      "epoch": 7.201333333333333,
      "grad_norm": 0.027247071266174316,
      "learning_rate": 4.991666666666667e-06,
      "loss": 0.0016,
      "step": 108020
    },
    {
      "epoch": 7.202,
      "grad_norm": 0.036770593374967575,
      "learning_rate": 4.9875e-06,
      "loss": 0.0015,
      "step": 108030
    },
    {
      "epoch": 7.2026666666666666,
      "grad_norm": 0.14651325345039368,
      "learning_rate": 4.983333333333334e-06,
      "loss": 0.0014,
      "step": 108040
    },
    {
      "epoch": 7.203333333333333,
      "grad_norm": 0.1032542809844017,
      "learning_rate": 4.979166666666666e-06,
      "loss": 0.0021,
      "step": 108050
    },
    {
      "epoch": 7.204,
      "grad_norm": 0.2405022382736206,
      "learning_rate": 4.975000000000001e-06,
      "loss": 0.0014,
      "step": 108060
    },
    {
      "epoch": 7.204666666666666,
      "grad_norm": 0.33582067489624023,
      "learning_rate": 4.970833333333333e-06,
      "loss": 0.0014,
      "step": 108070
    },
    {
      "epoch": 7.205333333333333,
      "grad_norm": 0.1702955663204193,
      "learning_rate": 4.966666666666667e-06,
      "loss": 0.0035,
      "step": 108080
    },
    {
      "epoch": 7.206,
      "grad_norm": 0.2004319429397583,
      "learning_rate": 4.9625e-06,
      "loss": 0.0023,
      "step": 108090
    },
    {
      "epoch": 7.206666666666667,
      "grad_norm": 0.1401389092206955,
      "learning_rate": 4.958333333333334e-06,
      "loss": 0.0023,
      "step": 108100
    },
    {
      "epoch": 7.207333333333334,
      "grad_norm": 0.44832566380500793,
      "learning_rate": 4.9541666666666665e-06,
      "loss": 0.0017,
      "step": 108110
    },
    {
      "epoch": 7.208,
      "grad_norm": 0.041407253593206406,
      "learning_rate": 4.950000000000001e-06,
      "loss": 0.0011,
      "step": 108120
    },
    {
      "epoch": 7.208666666666667,
      "grad_norm": 0.142356276512146,
      "learning_rate": 4.9458333333333336e-06,
      "loss": 0.0015,
      "step": 108130
    },
    {
      "epoch": 7.209333333333333,
      "grad_norm": 0.11663123220205307,
      "learning_rate": 4.941666666666667e-06,
      "loss": 0.0021,
      "step": 108140
    },
    {
      "epoch": 7.21,
      "grad_norm": 0.3033604621887207,
      "learning_rate": 4.937500000000001e-06,
      "loss": 0.0017,
      "step": 108150
    },
    {
      "epoch": 7.210666666666667,
      "grad_norm": 0.02932308241724968,
      "learning_rate": 4.933333333333333e-06,
      "loss": 0.0015,
      "step": 108160
    },
    {
      "epoch": 7.211333333333333,
      "grad_norm": 0.041434455662965775,
      "learning_rate": 4.929166666666667e-06,
      "loss": 0.0025,
      "step": 108170
    },
    {
      "epoch": 7.212,
      "grad_norm": 0.13776236772537231,
      "learning_rate": 4.925e-06,
      "loss": 0.002,
      "step": 108180
    },
    {
      "epoch": 7.212666666666666,
      "grad_norm": 0.4411245286464691,
      "learning_rate": 4.920833333333334e-06,
      "loss": 0.0013,
      "step": 108190
    },
    {
      "epoch": 7.213333333333333,
      "grad_norm": 0.4301232695579529,
      "learning_rate": 4.9166666666666665e-06,
      "loss": 0.0026,
      "step": 108200
    },
    {
      "epoch": 7.214,
      "grad_norm": 0.23675304651260376,
      "learning_rate": 4.912500000000001e-06,
      "loss": 0.0021,
      "step": 108210
    },
    {
      "epoch": 7.214666666666667,
      "grad_norm": 0.345022976398468,
      "learning_rate": 4.9083333333333335e-06,
      "loss": 0.0018,
      "step": 108220
    },
    {
      "epoch": 7.215333333333334,
      "grad_norm": 0.5711812376976013,
      "learning_rate": 4.904166666666667e-06,
      "loss": 0.0016,
      "step": 108230
    },
    {
      "epoch": 7.216,
      "grad_norm": 0.04205114766955376,
      "learning_rate": 4.9000000000000005e-06,
      "loss": 0.0016,
      "step": 108240
    },
    {
      "epoch": 7.216666666666667,
      "grad_norm": 0.30550873279571533,
      "learning_rate": 4.895833333333333e-06,
      "loss": 0.0025,
      "step": 108250
    },
    {
      "epoch": 7.217333333333333,
      "grad_norm": 0.3312057852745056,
      "learning_rate": 4.891666666666667e-06,
      "loss": 0.0018,
      "step": 108260
    },
    {
      "epoch": 7.218,
      "grad_norm": 0.10110197961330414,
      "learning_rate": 4.8875e-06,
      "loss": 0.0013,
      "step": 108270
    },
    {
      "epoch": 7.218666666666667,
      "grad_norm": 0.47068706154823303,
      "learning_rate": 4.883333333333334e-06,
      "loss": 0.0021,
      "step": 108280
    },
    {
      "epoch": 7.219333333333333,
      "grad_norm": 0.041655879467725754,
      "learning_rate": 4.879166666666666e-06,
      "loss": 0.002,
      "step": 108290
    },
    {
      "epoch": 7.22,
      "grad_norm": 0.4367610812187195,
      "learning_rate": 4.875000000000001e-06,
      "loss": 0.0015,
      "step": 108300
    },
    {
      "epoch": 7.220666666666666,
      "grad_norm": 0.048476364463567734,
      "learning_rate": 4.8708333333333334e-06,
      "loss": 0.0019,
      "step": 108310
    },
    {
      "epoch": 7.221333333333333,
      "grad_norm": 0.3015812337398529,
      "learning_rate": 4.866666666666667e-06,
      "loss": 0.0029,
      "step": 108320
    },
    {
      "epoch": 7.222,
      "grad_norm": 0.233334481716156,
      "learning_rate": 4.8625000000000005e-06,
      "loss": 0.002,
      "step": 108330
    },
    {
      "epoch": 7.222666666666667,
      "grad_norm": 0.13594143092632294,
      "learning_rate": 4.858333333333333e-06,
      "loss": 0.0014,
      "step": 108340
    },
    {
      "epoch": 7.223333333333334,
      "grad_norm": 0.13539522886276245,
      "learning_rate": 4.854166666666667e-06,
      "loss": 0.0017,
      "step": 108350
    },
    {
      "epoch": 7.224,
      "grad_norm": 0.3682399094104767,
      "learning_rate": 4.85e-06,
      "loss": 0.0025,
      "step": 108360
    },
    {
      "epoch": 7.224666666666667,
      "grad_norm": 0.4655698239803314,
      "learning_rate": 4.845833333333334e-06,
      "loss": 0.0014,
      "step": 108370
    },
    {
      "epoch": 7.225333333333333,
      "grad_norm": 0.08612209558486938,
      "learning_rate": 4.841666666666666e-06,
      "loss": 0.0014,
      "step": 108380
    },
    {
      "epoch": 7.226,
      "grad_norm": 0.05316994711756706,
      "learning_rate": 4.837500000000001e-06,
      "loss": 0.0019,
      "step": 108390
    },
    {
      "epoch": 7.226666666666667,
      "grad_norm": 0.40332016348838806,
      "learning_rate": 4.833333333333333e-06,
      "loss": 0.0025,
      "step": 108400
    },
    {
      "epoch": 7.227333333333333,
      "grad_norm": 0.16625379025936127,
      "learning_rate": 4.829166666666667e-06,
      "loss": 0.0014,
      "step": 108410
    },
    {
      "epoch": 7.228,
      "grad_norm": 0.24323530495166779,
      "learning_rate": 4.825e-06,
      "loss": 0.0016,
      "step": 108420
    },
    {
      "epoch": 7.228666666666666,
      "grad_norm": 0.40404099225997925,
      "learning_rate": 4.820833333333333e-06,
      "loss": 0.0017,
      "step": 108430
    },
    {
      "epoch": 7.229333333333333,
      "grad_norm": 0.3533993661403656,
      "learning_rate": 4.816666666666667e-06,
      "loss": 0.0019,
      "step": 108440
    },
    {
      "epoch": 7.23,
      "grad_norm": 0.10251984745264053,
      "learning_rate": 4.8125e-06,
      "loss": 0.0015,
      "step": 108450
    },
    {
      "epoch": 7.230666666666667,
      "grad_norm": 0.07961295545101166,
      "learning_rate": 4.808333333333334e-06,
      "loss": 0.0013,
      "step": 108460
    },
    {
      "epoch": 7.231333333333334,
      "grad_norm": 0.07766970247030258,
      "learning_rate": 4.804166666666667e-06,
      "loss": 0.0029,
      "step": 108470
    },
    {
      "epoch": 7.232,
      "grad_norm": 0.29116860032081604,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.0015,
      "step": 108480
    },
    {
      "epoch": 7.232666666666667,
      "grad_norm": 0.19785213470458984,
      "learning_rate": 4.795833333333333e-06,
      "loss": 0.0019,
      "step": 108490
    },
    {
      "epoch": 7.233333333333333,
      "grad_norm": 0.3406176269054413,
      "learning_rate": 4.791666666666667e-06,
      "loss": 0.002,
      "step": 108500
    },
    {
      "epoch": 7.234,
      "grad_norm": 0.13919135928153992,
      "learning_rate": 4.7875e-06,
      "loss": 0.0034,
      "step": 108510
    },
    {
      "epoch": 7.234666666666667,
      "grad_norm": 0.053951021283864975,
      "learning_rate": 4.783333333333333e-06,
      "loss": 0.0014,
      "step": 108520
    },
    {
      "epoch": 7.235333333333333,
      "grad_norm": 0.4019787311553955,
      "learning_rate": 4.779166666666667e-06,
      "loss": 0.0018,
      "step": 108530
    },
    {
      "epoch": 7.236,
      "grad_norm": 0.13683657348155975,
      "learning_rate": 4.775e-06,
      "loss": 0.0013,
      "step": 108540
    },
    {
      "epoch": 7.236666666666666,
      "grad_norm": 0.366268128156662,
      "learning_rate": 4.7708333333333335e-06,
      "loss": 0.0022,
      "step": 108550
    },
    {
      "epoch": 7.237333333333333,
      "grad_norm": 0.21222802996635437,
      "learning_rate": 4.766666666666667e-06,
      "loss": 0.0019,
      "step": 108560
    },
    {
      "epoch": 7.2379999999999995,
      "grad_norm": 0.10622277855873108,
      "learning_rate": 4.7625000000000006e-06,
      "loss": 0.002,
      "step": 108570
    },
    {
      "epoch": 7.238666666666667,
      "grad_norm": 0.4385617971420288,
      "learning_rate": 4.758333333333333e-06,
      "loss": 0.0015,
      "step": 108580
    },
    {
      "epoch": 7.239333333333334,
      "grad_norm": 0.13358041644096375,
      "learning_rate": 4.754166666666668e-06,
      "loss": 0.0019,
      "step": 108590
    },
    {
      "epoch": 7.24,
      "grad_norm": 0.04206554964184761,
      "learning_rate": 4.75e-06,
      "loss": 0.0014,
      "step": 108600
    },
    {
      "epoch": 7.240666666666667,
      "grad_norm": 0.044626329094171524,
      "learning_rate": 4.745833333333334e-06,
      "loss": 0.002,
      "step": 108610
    },
    {
      "epoch": 7.241333333333333,
      "grad_norm": 0.974649965763092,
      "learning_rate": 4.741666666666667e-06,
      "loss": 0.0023,
      "step": 108620
    },
    {
      "epoch": 7.242,
      "grad_norm": 0.11411549150943756,
      "learning_rate": 4.7375e-06,
      "loss": 0.0016,
      "step": 108630
    },
    {
      "epoch": 7.242666666666667,
      "grad_norm": 0.5576915144920349,
      "learning_rate": 4.7333333333333335e-06,
      "loss": 0.0022,
      "step": 108640
    },
    {
      "epoch": 7.243333333333333,
      "grad_norm": 0.06954445689916611,
      "learning_rate": 4.729166666666667e-06,
      "loss": 0.0015,
      "step": 108650
    },
    {
      "epoch": 7.244,
      "grad_norm": 0.4342949092388153,
      "learning_rate": 4.7250000000000005e-06,
      "loss": 0.0021,
      "step": 108660
    },
    {
      "epoch": 7.244666666666666,
      "grad_norm": 0.3716031312942505,
      "learning_rate": 4.720833333333333e-06,
      "loss": 0.0012,
      "step": 108670
    },
    {
      "epoch": 7.245333333333333,
      "grad_norm": 0.3374326527118683,
      "learning_rate": 4.7166666666666675e-06,
      "loss": 0.0012,
      "step": 108680
    },
    {
      "epoch": 7.246,
      "grad_norm": 0.05438870191574097,
      "learning_rate": 4.7125e-06,
      "loss": 0.002,
      "step": 108690
    },
    {
      "epoch": 7.246666666666667,
      "grad_norm": 0.04794210195541382,
      "learning_rate": 4.708333333333334e-06,
      "loss": 0.0029,
      "step": 108700
    },
    {
      "epoch": 7.247333333333334,
      "grad_norm": 0.5380311012268066,
      "learning_rate": 4.704166666666667e-06,
      "loss": 0.0021,
      "step": 108710
    },
    {
      "epoch": 7.248,
      "grad_norm": 0.4321715831756592,
      "learning_rate": 4.7e-06,
      "loss": 0.0027,
      "step": 108720
    },
    {
      "epoch": 7.248666666666667,
      "grad_norm": 0.14322759211063385,
      "learning_rate": 4.695833333333333e-06,
      "loss": 0.0015,
      "step": 108730
    },
    {
      "epoch": 7.249333333333333,
      "grad_norm": 0.04007115960121155,
      "learning_rate": 4.691666666666667e-06,
      "loss": 0.0014,
      "step": 108740
    },
    {
      "epoch": 7.25,
      "grad_norm": 0.07472489029169083,
      "learning_rate": 4.6875000000000004e-06,
      "loss": 0.0019,
      "step": 108750
    },
    {
      "epoch": 7.250666666666667,
      "grad_norm": 0.7367048859596252,
      "learning_rate": 4.683333333333333e-06,
      "loss": 0.0023,
      "step": 108760
    },
    {
      "epoch": 7.251333333333333,
      "grad_norm": 0.471187949180603,
      "learning_rate": 4.6791666666666675e-06,
      "loss": 0.0017,
      "step": 108770
    },
    {
      "epoch": 7.252,
      "grad_norm": 0.3670027554035187,
      "learning_rate": 4.675e-06,
      "loss": 0.0027,
      "step": 108780
    },
    {
      "epoch": 7.252666666666666,
      "grad_norm": 0.19935385882854462,
      "learning_rate": 4.670833333333334e-06,
      "loss": 0.0016,
      "step": 108790
    },
    {
      "epoch": 7.253333333333333,
      "grad_norm": 0.11897017806768417,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.0016,
      "step": 108800
    },
    {
      "epoch": 7.254,
      "grad_norm": 0.2352253645658493,
      "learning_rate": 4.6625e-06,
      "loss": 0.0015,
      "step": 108810
    },
    {
      "epoch": 7.254666666666667,
      "grad_norm": 0.08368343859910965,
      "learning_rate": 4.658333333333333e-06,
      "loss": 0.0025,
      "step": 108820
    },
    {
      "epoch": 7.255333333333334,
      "grad_norm": 0.30234667658805847,
      "learning_rate": 4.654166666666667e-06,
      "loss": 0.0018,
      "step": 108830
    },
    {
      "epoch": 7.256,
      "grad_norm": 0.3312649130821228,
      "learning_rate": 4.65e-06,
      "loss": 0.0012,
      "step": 108840
    },
    {
      "epoch": 7.256666666666667,
      "grad_norm": 0.05755291134119034,
      "learning_rate": 4.645833333333333e-06,
      "loss": 0.0016,
      "step": 108850
    },
    {
      "epoch": 7.257333333333333,
      "grad_norm": 0.1148003339767456,
      "learning_rate": 4.641666666666667e-06,
      "loss": 0.0027,
      "step": 108860
    },
    {
      "epoch": 7.258,
      "grad_norm": 0.07042789459228516,
      "learning_rate": 4.6375e-06,
      "loss": 0.0021,
      "step": 108870
    },
    {
      "epoch": 7.258666666666667,
      "grad_norm": 0.10632062703371048,
      "learning_rate": 4.633333333333334e-06,
      "loss": 0.0023,
      "step": 108880
    },
    {
      "epoch": 7.259333333333333,
      "grad_norm": 0.0723225399851799,
      "learning_rate": 4.629166666666667e-06,
      "loss": 0.0015,
      "step": 108890
    },
    {
      "epoch": 7.26,
      "grad_norm": 0.4359510540962219,
      "learning_rate": 4.625e-06,
      "loss": 0.0017,
      "step": 108900
    },
    {
      "epoch": 7.260666666666666,
      "grad_norm": 0.436654269695282,
      "learning_rate": 4.620833333333333e-06,
      "loss": 0.0018,
      "step": 108910
    },
    {
      "epoch": 7.261333333333333,
      "grad_norm": 0.10303487628698349,
      "learning_rate": 4.616666666666667e-06,
      "loss": 0.0013,
      "step": 108920
    },
    {
      "epoch": 7.2620000000000005,
      "grad_norm": 0.19223067164421082,
      "learning_rate": 4.6125e-06,
      "loss": 0.0017,
      "step": 108930
    },
    {
      "epoch": 7.262666666666667,
      "grad_norm": 0.5005595684051514,
      "learning_rate": 4.608333333333333e-06,
      "loss": 0.0012,
      "step": 108940
    },
    {
      "epoch": 7.263333333333334,
      "grad_norm": 0.3127959370613098,
      "learning_rate": 4.604166666666667e-06,
      "loss": 0.0016,
      "step": 108950
    },
    {
      "epoch": 7.264,
      "grad_norm": 0.16352778673171997,
      "learning_rate": 4.6e-06,
      "loss": 0.0015,
      "step": 108960
    },
    {
      "epoch": 7.264666666666667,
      "grad_norm": 0.08440065383911133,
      "learning_rate": 4.5958333333333335e-06,
      "loss": 0.0016,
      "step": 108970
    },
    {
      "epoch": 7.265333333333333,
      "grad_norm": 0.33459779620170593,
      "learning_rate": 4.591666666666667e-06,
      "loss": 0.0015,
      "step": 108980
    },
    {
      "epoch": 7.266,
      "grad_norm": 0.3996941149234772,
      "learning_rate": 4.5875000000000005e-06,
      "loss": 0.0012,
      "step": 108990
    },
    {
      "epoch": 7.266666666666667,
      "grad_norm": 0.0822099968791008,
      "learning_rate": 4.583333333333333e-06,
      "loss": 0.0019,
      "step": 109000
    },
    {
      "epoch": 7.267333333333333,
      "grad_norm": 0.42612090706825256,
      "learning_rate": 4.579166666666667e-06,
      "loss": 0.0021,
      "step": 109010
    },
    {
      "epoch": 7.268,
      "grad_norm": 0.31310534477233887,
      "learning_rate": 4.575e-06,
      "loss": 0.0013,
      "step": 109020
    },
    {
      "epoch": 7.268666666666666,
      "grad_norm": 0.536014974117279,
      "learning_rate": 4.570833333333333e-06,
      "loss": 0.0015,
      "step": 109030
    },
    {
      "epoch": 7.269333333333333,
      "grad_norm": 0.3955269455909729,
      "learning_rate": 4.566666666666667e-06,
      "loss": 0.0023,
      "step": 109040
    },
    {
      "epoch": 7.27,
      "grad_norm": 0.09245055913925171,
      "learning_rate": 4.5625e-06,
      "loss": 0.002,
      "step": 109050
    },
    {
      "epoch": 7.270666666666667,
      "grad_norm": 0.11412427574396133,
      "learning_rate": 4.5583333333333335e-06,
      "loss": 0.002,
      "step": 109060
    },
    {
      "epoch": 7.271333333333334,
      "grad_norm": 0.25368309020996094,
      "learning_rate": 4.554166666666667e-06,
      "loss": 0.0025,
      "step": 109070
    },
    {
      "epoch": 7.272,
      "grad_norm": 0.43227988481521606,
      "learning_rate": 4.5500000000000005e-06,
      "loss": 0.0015,
      "step": 109080
    },
    {
      "epoch": 7.272666666666667,
      "grad_norm": 0.17551328241825104,
      "learning_rate": 4.545833333333334e-06,
      "loss": 0.002,
      "step": 109090
    },
    {
      "epoch": 7.273333333333333,
      "grad_norm": 0.11114373803138733,
      "learning_rate": 4.541666666666667e-06,
      "loss": 0.0017,
      "step": 109100
    },
    {
      "epoch": 7.274,
      "grad_norm": 0.4116065502166748,
      "learning_rate": 4.5375e-06,
      "loss": 0.0024,
      "step": 109110
    },
    {
      "epoch": 7.274666666666667,
      "grad_norm": 0.27321216464042664,
      "learning_rate": 4.533333333333334e-06,
      "loss": 0.0019,
      "step": 109120
    },
    {
      "epoch": 7.275333333333333,
      "grad_norm": 0.2975446581840515,
      "learning_rate": 4.529166666666667e-06,
      "loss": 0.0015,
      "step": 109130
    },
    {
      "epoch": 7.276,
      "grad_norm": 0.4392770826816559,
      "learning_rate": 4.525e-06,
      "loss": 0.0025,
      "step": 109140
    },
    {
      "epoch": 7.276666666666666,
      "grad_norm": 0.3365245461463928,
      "learning_rate": 4.520833333333334e-06,
      "loss": 0.0015,
      "step": 109150
    },
    {
      "epoch": 7.277333333333333,
      "grad_norm": 0.11296526342630386,
      "learning_rate": 4.516666666666667e-06,
      "loss": 0.0021,
      "step": 109160
    },
    {
      "epoch": 7.2780000000000005,
      "grad_norm": 0.20085440576076508,
      "learning_rate": 4.5125e-06,
      "loss": 0.0014,
      "step": 109170
    },
    {
      "epoch": 7.278666666666667,
      "grad_norm": 0.23256826400756836,
      "learning_rate": 4.508333333333334e-06,
      "loss": 0.002,
      "step": 109180
    },
    {
      "epoch": 7.279333333333334,
      "grad_norm": 0.39786678552627563,
      "learning_rate": 4.504166666666667e-06,
      "loss": 0.0027,
      "step": 109190
    },
    {
      "epoch": 7.28,
      "grad_norm": 0.20396985113620758,
      "learning_rate": 4.5e-06,
      "loss": 0.0014,
      "step": 109200
    },
    {
      "epoch": 7.280666666666667,
      "grad_norm": 0.2177475541830063,
      "learning_rate": 4.495833333333334e-06,
      "loss": 0.0022,
      "step": 109210
    },
    {
      "epoch": 7.281333333333333,
      "grad_norm": 0.12591023743152618,
      "learning_rate": 4.491666666666667e-06,
      "loss": 0.002,
      "step": 109220
    },
    {
      "epoch": 7.282,
      "grad_norm": 0.4413054883480072,
      "learning_rate": 4.4875e-06,
      "loss": 0.0021,
      "step": 109230
    },
    {
      "epoch": 7.282666666666667,
      "grad_norm": 0.3958362638950348,
      "learning_rate": 4.483333333333334e-06,
      "loss": 0.0017,
      "step": 109240
    },
    {
      "epoch": 7.283333333333333,
      "grad_norm": 0.101447194814682,
      "learning_rate": 4.479166666666667e-06,
      "loss": 0.0017,
      "step": 109250
    },
    {
      "epoch": 7.284,
      "grad_norm": 0.13617917895317078,
      "learning_rate": 4.475e-06,
      "loss": 0.0016,
      "step": 109260
    },
    {
      "epoch": 7.284666666666666,
      "grad_norm": 0.10786879807710648,
      "learning_rate": 4.470833333333334e-06,
      "loss": 0.0015,
      "step": 109270
    },
    {
      "epoch": 7.285333333333333,
      "grad_norm": 0.3965792655944824,
      "learning_rate": 4.4666666666666665e-06,
      "loss": 0.0018,
      "step": 109280
    },
    {
      "epoch": 7.286,
      "grad_norm": 0.49892932176589966,
      "learning_rate": 4.4625e-06,
      "loss": 0.0019,
      "step": 109290
    },
    {
      "epoch": 7.286666666666667,
      "grad_norm": 0.4139906167984009,
      "learning_rate": 4.4583333333333336e-06,
      "loss": 0.0014,
      "step": 109300
    },
    {
      "epoch": 7.287333333333334,
      "grad_norm": 0.5196722149848938,
      "learning_rate": 4.454166666666667e-06,
      "loss": 0.0021,
      "step": 109310
    },
    {
      "epoch": 7.288,
      "grad_norm": 0.2019057422876358,
      "learning_rate": 4.45e-06,
      "loss": 0.0021,
      "step": 109320
    },
    {
      "epoch": 7.288666666666667,
      "grad_norm": 0.043576810508966446,
      "learning_rate": 4.445833333333334e-06,
      "loss": 0.0021,
      "step": 109330
    },
    {
      "epoch": 7.289333333333333,
      "grad_norm": 0.11969587206840515,
      "learning_rate": 4.441666666666667e-06,
      "loss": 0.0012,
      "step": 109340
    },
    {
      "epoch": 7.29,
      "grad_norm": 0.6522577404975891,
      "learning_rate": 4.4375e-06,
      "loss": 0.0013,
      "step": 109350
    },
    {
      "epoch": 7.290666666666667,
      "grad_norm": 0.06747280806303024,
      "learning_rate": 4.433333333333334e-06,
      "loss": 0.0025,
      "step": 109360
    },
    {
      "epoch": 7.291333333333333,
      "grad_norm": 0.1977451890707016,
      "learning_rate": 4.4291666666666665e-06,
      "loss": 0.0015,
      "step": 109370
    },
    {
      "epoch": 7.292,
      "grad_norm": 0.21667863428592682,
      "learning_rate": 4.425e-06,
      "loss": 0.002,
      "step": 109380
    },
    {
      "epoch": 7.292666666666666,
      "grad_norm": 0.4035393297672272,
      "learning_rate": 4.4208333333333335e-06,
      "loss": 0.0017,
      "step": 109390
    },
    {
      "epoch": 7.293333333333333,
      "grad_norm": 0.504382848739624,
      "learning_rate": 4.416666666666667e-06,
      "loss": 0.0016,
      "step": 109400
    },
    {
      "epoch": 7.294,
      "grad_norm": 0.2703906297683716,
      "learning_rate": 4.4125e-06,
      "loss": 0.0015,
      "step": 109410
    },
    {
      "epoch": 7.294666666666667,
      "grad_norm": 0.6917673945426941,
      "learning_rate": 4.408333333333334e-06,
      "loss": 0.0028,
      "step": 109420
    },
    {
      "epoch": 7.295333333333334,
      "grad_norm": 0.31085556745529175,
      "learning_rate": 4.404166666666667e-06,
      "loss": 0.0015,
      "step": 109430
    },
    {
      "epoch": 7.296,
      "grad_norm": 0.34185919165611267,
      "learning_rate": 4.4e-06,
      "loss": 0.0015,
      "step": 109440
    },
    {
      "epoch": 7.296666666666667,
      "grad_norm": 0.4029751121997833,
      "learning_rate": 4.395833333333334e-06,
      "loss": 0.0017,
      "step": 109450
    },
    {
      "epoch": 7.2973333333333334,
      "grad_norm": 0.23699574172496796,
      "learning_rate": 4.391666666666667e-06,
      "loss": 0.0025,
      "step": 109460
    },
    {
      "epoch": 7.298,
      "grad_norm": 0.05478846654295921,
      "learning_rate": 4.3875e-06,
      "loss": 0.0019,
      "step": 109470
    },
    {
      "epoch": 7.298666666666667,
      "grad_norm": 0.2016722857952118,
      "learning_rate": 4.3833333333333334e-06,
      "loss": 0.0019,
      "step": 109480
    },
    {
      "epoch": 7.299333333333333,
      "grad_norm": 0.40250205993652344,
      "learning_rate": 4.379166666666667e-06,
      "loss": 0.0014,
      "step": 109490
    },
    {
      "epoch": 7.3,
      "grad_norm": 0.046651534736156464,
      "learning_rate": 4.375e-06,
      "loss": 0.0014,
      "step": 109500
    },
    {
      "epoch": 7.300666666666666,
      "grad_norm": 0.20432066917419434,
      "learning_rate": 4.370833333333334e-06,
      "loss": 0.002,
      "step": 109510
    },
    {
      "epoch": 7.301333333333333,
      "grad_norm": 0.07425267994403839,
      "learning_rate": 4.366666666666667e-06,
      "loss": 0.002,
      "step": 109520
    },
    {
      "epoch": 7.302,
      "grad_norm": 0.16988331079483032,
      "learning_rate": 4.3625e-06,
      "loss": 0.0022,
      "step": 109530
    },
    {
      "epoch": 7.302666666666667,
      "grad_norm": 0.13665266335010529,
      "learning_rate": 4.358333333333334e-06,
      "loss": 0.0016,
      "step": 109540
    },
    {
      "epoch": 7.303333333333334,
      "grad_norm": 0.14093852043151855,
      "learning_rate": 4.354166666666667e-06,
      "loss": 0.0013,
      "step": 109550
    },
    {
      "epoch": 7.304,
      "grad_norm": 0.04748234525322914,
      "learning_rate": 4.35e-06,
      "loss": 0.0015,
      "step": 109560
    },
    {
      "epoch": 7.304666666666667,
      "grad_norm": 0.08056840300559998,
      "learning_rate": 4.345833333333333e-06,
      "loss": 0.0015,
      "step": 109570
    },
    {
      "epoch": 7.3053333333333335,
      "grad_norm": 0.19513435661792755,
      "learning_rate": 4.341666666666667e-06,
      "loss": 0.0017,
      "step": 109580
    },
    {
      "epoch": 7.306,
      "grad_norm": 0.07801484316587448,
      "learning_rate": 4.3374999999999996e-06,
      "loss": 0.0018,
      "step": 109590
    },
    {
      "epoch": 7.306666666666667,
      "grad_norm": 0.23419347405433655,
      "learning_rate": 4.333333333333334e-06,
      "loss": 0.0014,
      "step": 109600
    },
    {
      "epoch": 7.307333333333333,
      "grad_norm": 0.13805626332759857,
      "learning_rate": 4.329166666666667e-06,
      "loss": 0.0014,
      "step": 109610
    },
    {
      "epoch": 7.308,
      "grad_norm": 0.33410438895225525,
      "learning_rate": 4.325e-06,
      "loss": 0.0013,
      "step": 109620
    },
    {
      "epoch": 7.308666666666666,
      "grad_norm": 0.23876497149467468,
      "learning_rate": 4.320833333333334e-06,
      "loss": 0.002,
      "step": 109630
    },
    {
      "epoch": 7.309333333333333,
      "grad_norm": 0.051032308489084244,
      "learning_rate": 4.316666666666667e-06,
      "loss": 0.002,
      "step": 109640
    },
    {
      "epoch": 7.31,
      "grad_norm": 0.2330567091703415,
      "learning_rate": 4.3125e-06,
      "loss": 0.0023,
      "step": 109650
    },
    {
      "epoch": 7.310666666666666,
      "grad_norm": 0.18457581102848053,
      "learning_rate": 4.308333333333333e-06,
      "loss": 0.0015,
      "step": 109660
    },
    {
      "epoch": 7.311333333333334,
      "grad_norm": 0.6572853922843933,
      "learning_rate": 4.304166666666667e-06,
      "loss": 0.0014,
      "step": 109670
    },
    {
      "epoch": 7.312,
      "grad_norm": 0.17143017053604126,
      "learning_rate": 4.2999999999999995e-06,
      "loss": 0.0016,
      "step": 109680
    },
    {
      "epoch": 7.312666666666667,
      "grad_norm": 0.36389264464378357,
      "learning_rate": 4.295833333333334e-06,
      "loss": 0.0028,
      "step": 109690
    },
    {
      "epoch": 7.3133333333333335,
      "grad_norm": 0.07586124539375305,
      "learning_rate": 4.2916666666666665e-06,
      "loss": 0.0019,
      "step": 109700
    },
    {
      "epoch": 7.314,
      "grad_norm": 0.4119482934474945,
      "learning_rate": 4.287500000000001e-06,
      "loss": 0.0013,
      "step": 109710
    },
    {
      "epoch": 7.314666666666667,
      "grad_norm": 0.23258782923221588,
      "learning_rate": 4.2833333333333335e-06,
      "loss": 0.0022,
      "step": 109720
    },
    {
      "epoch": 7.315333333333333,
      "grad_norm": 0.33749687671661377,
      "learning_rate": 4.279166666666667e-06,
      "loss": 0.0031,
      "step": 109730
    },
    {
      "epoch": 7.316,
      "grad_norm": 0.13470658659934998,
      "learning_rate": 4.2750000000000006e-06,
      "loss": 0.0016,
      "step": 109740
    },
    {
      "epoch": 7.316666666666666,
      "grad_norm": 0.36450275778770447,
      "learning_rate": 4.270833333333333e-06,
      "loss": 0.0012,
      "step": 109750
    },
    {
      "epoch": 7.317333333333333,
      "grad_norm": 0.11003264039754868,
      "learning_rate": 4.266666666666667e-06,
      "loss": 0.0015,
      "step": 109760
    },
    {
      "epoch": 7.318,
      "grad_norm": 0.17117071151733398,
      "learning_rate": 4.2625e-06,
      "loss": 0.0018,
      "step": 109770
    },
    {
      "epoch": 7.318666666666667,
      "grad_norm": 0.10598400235176086,
      "learning_rate": 4.258333333333334e-06,
      "loss": 0.002,
      "step": 109780
    },
    {
      "epoch": 7.319333333333334,
      "grad_norm": 0.1435040533542633,
      "learning_rate": 4.2541666666666665e-06,
      "loss": 0.0013,
      "step": 109790
    },
    {
      "epoch": 7.32,
      "grad_norm": 0.2668643295764923,
      "learning_rate": 4.250000000000001e-06,
      "loss": 0.0017,
      "step": 109800
    },
    {
      "epoch": 7.320666666666667,
      "grad_norm": 0.22005333006381989,
      "learning_rate": 4.2458333333333335e-06,
      "loss": 0.0015,
      "step": 109810
    },
    {
      "epoch": 7.3213333333333335,
      "grad_norm": 0.14688333868980408,
      "learning_rate": 4.241666666666667e-06,
      "loss": 0.0013,
      "step": 109820
    },
    {
      "epoch": 7.322,
      "grad_norm": 0.17235985398292542,
      "learning_rate": 4.2375000000000005e-06,
      "loss": 0.0017,
      "step": 109830
    },
    {
      "epoch": 7.322666666666667,
      "grad_norm": 0.13936404883861542,
      "learning_rate": 4.233333333333333e-06,
      "loss": 0.0017,
      "step": 109840
    },
    {
      "epoch": 7.323333333333333,
      "grad_norm": 0.09111522138118744,
      "learning_rate": 4.229166666666667e-06,
      "loss": 0.0029,
      "step": 109850
    },
    {
      "epoch": 7.324,
      "grad_norm": 0.10940206050872803,
      "learning_rate": 4.225e-06,
      "loss": 0.0014,
      "step": 109860
    },
    {
      "epoch": 7.324666666666666,
      "grad_norm": 0.45980003476142883,
      "learning_rate": 4.220833333333334e-06,
      "loss": 0.0024,
      "step": 109870
    },
    {
      "epoch": 7.325333333333333,
      "grad_norm": 0.07695622742176056,
      "learning_rate": 4.216666666666666e-06,
      "loss": 0.0015,
      "step": 109880
    },
    {
      "epoch": 7.326,
      "grad_norm": 0.3985287845134735,
      "learning_rate": 4.212500000000001e-06,
      "loss": 0.0017,
      "step": 109890
    },
    {
      "epoch": 7.326666666666666,
      "grad_norm": 0.23194000124931335,
      "learning_rate": 4.208333333333333e-06,
      "loss": 0.0016,
      "step": 109900
    },
    {
      "epoch": 7.327333333333334,
      "grad_norm": 0.09504013508558273,
      "learning_rate": 4.204166666666667e-06,
      "loss": 0.0019,
      "step": 109910
    },
    {
      "epoch": 7.328,
      "grad_norm": 0.03941209614276886,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 0.0022,
      "step": 109920
    },
    {
      "epoch": 7.328666666666667,
      "grad_norm": 0.17334964871406555,
      "learning_rate": 4.195833333333334e-06,
      "loss": 0.0029,
      "step": 109930
    },
    {
      "epoch": 7.3293333333333335,
      "grad_norm": 0.439460426568985,
      "learning_rate": 4.191666666666667e-06,
      "loss": 0.0022,
      "step": 109940
    },
    {
      "epoch": 7.33,
      "grad_norm": 0.2338387370109558,
      "learning_rate": 4.1875e-06,
      "loss": 0.0014,
      "step": 109950
    },
    {
      "epoch": 7.330666666666667,
      "grad_norm": 0.17679698765277863,
      "learning_rate": 4.183333333333334e-06,
      "loss": 0.0017,
      "step": 109960
    },
    {
      "epoch": 7.331333333333333,
      "grad_norm": 0.2462639957666397,
      "learning_rate": 4.179166666666666e-06,
      "loss": 0.0024,
      "step": 109970
    },
    {
      "epoch": 7.332,
      "grad_norm": 0.296115517616272,
      "learning_rate": 4.175000000000001e-06,
      "loss": 0.001,
      "step": 109980
    },
    {
      "epoch": 7.332666666666666,
      "grad_norm": 0.21156226098537445,
      "learning_rate": 4.170833333333333e-06,
      "loss": 0.0022,
      "step": 109990
    },
    {
      "epoch": 7.333333333333333,
      "grad_norm": 0.1475941687822342,
      "learning_rate": 4.166666666666667e-06,
      "loss": 0.0013,
      "step": 110000
    },
    {
      "epoch": 7.334,
      "grad_norm": 0.43751585483551025,
      "learning_rate": 4.1625e-06,
      "loss": 0.0017,
      "step": 110010
    },
    {
      "epoch": 7.334666666666667,
      "grad_norm": 0.2663988173007965,
      "learning_rate": 4.158333333333334e-06,
      "loss": 0.0027,
      "step": 110020
    },
    {
      "epoch": 7.335333333333334,
      "grad_norm": 0.053378839045763016,
      "learning_rate": 4.1541666666666666e-06,
      "loss": 0.0027,
      "step": 110030
    },
    {
      "epoch": 7.336,
      "grad_norm": 0.23773932456970215,
      "learning_rate": 4.15e-06,
      "loss": 0.0017,
      "step": 110040
    },
    {
      "epoch": 7.336666666666667,
      "grad_norm": 0.08110110461711884,
      "learning_rate": 4.145833333333334e-06,
      "loss": 0.0018,
      "step": 110050
    },
    {
      "epoch": 7.3373333333333335,
      "grad_norm": 0.2310170829296112,
      "learning_rate": 4.141666666666666e-06,
      "loss": 0.0019,
      "step": 110060
    },
    {
      "epoch": 7.338,
      "grad_norm": 0.17881081998348236,
      "learning_rate": 4.137500000000001e-06,
      "loss": 0.0013,
      "step": 110070
    },
    {
      "epoch": 7.338666666666667,
      "grad_norm": 0.2019815742969513,
      "learning_rate": 4.133333333333333e-06,
      "loss": 0.0022,
      "step": 110080
    },
    {
      "epoch": 7.339333333333333,
      "grad_norm": 0.23330660164356232,
      "learning_rate": 4.129166666666667e-06,
      "loss": 0.0021,
      "step": 110090
    },
    {
      "epoch": 7.34,
      "grad_norm": 0.49861106276512146,
      "learning_rate": 4.125e-06,
      "loss": 0.0021,
      "step": 110100
    },
    {
      "epoch": 7.3406666666666665,
      "grad_norm": 0.2046085149049759,
      "learning_rate": 4.120833333333334e-06,
      "loss": 0.0019,
      "step": 110110
    },
    {
      "epoch": 7.341333333333333,
      "grad_norm": 0.05295536667108536,
      "learning_rate": 4.1166666666666665e-06,
      "loss": 0.0015,
      "step": 110120
    },
    {
      "epoch": 7.342,
      "grad_norm": 0.16574032604694366,
      "learning_rate": 4.1125e-06,
      "loss": 0.002,
      "step": 110130
    },
    {
      "epoch": 7.342666666666666,
      "grad_norm": 0.24441972374916077,
      "learning_rate": 4.1083333333333335e-06,
      "loss": 0.0016,
      "step": 110140
    },
    {
      "epoch": 7.343333333333334,
      "grad_norm": 0.37283286452293396,
      "learning_rate": 4.104166666666666e-06,
      "loss": 0.0022,
      "step": 110150
    },
    {
      "epoch": 7.344,
      "grad_norm": 0.07130921632051468,
      "learning_rate": 4.1000000000000006e-06,
      "loss": 0.0014,
      "step": 110160
    },
    {
      "epoch": 7.344666666666667,
      "grad_norm": 0.10401315987110138,
      "learning_rate": 4.095833333333333e-06,
      "loss": 0.0015,
      "step": 110170
    },
    {
      "epoch": 7.3453333333333335,
      "grad_norm": 0.04590162634849548,
      "learning_rate": 4.091666666666667e-06,
      "loss": 0.002,
      "step": 110180
    },
    {
      "epoch": 7.346,
      "grad_norm": 0.17225465178489685,
      "learning_rate": 4.0875e-06,
      "loss": 0.0012,
      "step": 110190
    },
    {
      "epoch": 7.346666666666667,
      "grad_norm": 0.21220184862613678,
      "learning_rate": 4.083333333333334e-06,
      "loss": 0.0015,
      "step": 110200
    },
    {
      "epoch": 7.347333333333333,
      "grad_norm": 0.23717544972896576,
      "learning_rate": 4.0791666666666664e-06,
      "loss": 0.0015,
      "step": 110210
    },
    {
      "epoch": 7.348,
      "grad_norm": 0.24089315533638,
      "learning_rate": 4.075e-06,
      "loss": 0.0016,
      "step": 110220
    },
    {
      "epoch": 7.3486666666666665,
      "grad_norm": 0.33779507875442505,
      "learning_rate": 4.0708333333333335e-06,
      "loss": 0.0015,
      "step": 110230
    },
    {
      "epoch": 7.349333333333333,
      "grad_norm": 0.059477049857378006,
      "learning_rate": 4.066666666666666e-06,
      "loss": 0.0024,
      "step": 110240
    },
    {
      "epoch": 7.35,
      "grad_norm": 0.10425707697868347,
      "learning_rate": 4.0625000000000005e-06,
      "loss": 0.0016,
      "step": 110250
    },
    {
      "epoch": 7.350666666666667,
      "grad_norm": 0.2428734004497528,
      "learning_rate": 4.058333333333333e-06,
      "loss": 0.0014,
      "step": 110260
    },
    {
      "epoch": 7.351333333333334,
      "grad_norm": 0.13254210352897644,
      "learning_rate": 4.054166666666667e-06,
      "loss": 0.0021,
      "step": 110270
    },
    {
      "epoch": 7.352,
      "grad_norm": 0.3325361907482147,
      "learning_rate": 4.05e-06,
      "loss": 0.0024,
      "step": 110280
    },
    {
      "epoch": 7.352666666666667,
      "grad_norm": 0.10671155154705048,
      "learning_rate": 4.045833333333334e-06,
      "loss": 0.0021,
      "step": 110290
    },
    {
      "epoch": 7.3533333333333335,
      "grad_norm": 0.039254698902368546,
      "learning_rate": 4.041666666666666e-06,
      "loss": 0.0015,
      "step": 110300
    },
    {
      "epoch": 7.354,
      "grad_norm": 0.04240207374095917,
      "learning_rate": 4.037500000000001e-06,
      "loss": 0.0014,
      "step": 110310
    },
    {
      "epoch": 7.354666666666667,
      "grad_norm": 0.04371458664536476,
      "learning_rate": 4.033333333333333e-06,
      "loss": 0.0019,
      "step": 110320
    },
    {
      "epoch": 7.355333333333333,
      "grad_norm": 0.040616605430841446,
      "learning_rate": 4.029166666666667e-06,
      "loss": 0.0018,
      "step": 110330
    },
    {
      "epoch": 7.356,
      "grad_norm": 0.5475339293479919,
      "learning_rate": 4.0250000000000004e-06,
      "loss": 0.0021,
      "step": 110340
    },
    {
      "epoch": 7.3566666666666665,
      "grad_norm": 0.08272269368171692,
      "learning_rate": 4.020833333333333e-06,
      "loss": 0.0028,
      "step": 110350
    },
    {
      "epoch": 7.357333333333333,
      "grad_norm": 0.050994645804166794,
      "learning_rate": 4.0166666666666675e-06,
      "loss": 0.0028,
      "step": 110360
    },
    {
      "epoch": 7.358,
      "grad_norm": 0.06691435724496841,
      "learning_rate": 4.0125e-06,
      "loss": 0.0016,
      "step": 110370
    },
    {
      "epoch": 7.358666666666666,
      "grad_norm": 0.07553205639123917,
      "learning_rate": 4.008333333333334e-06,
      "loss": 0.0012,
      "step": 110380
    },
    {
      "epoch": 7.359333333333334,
      "grad_norm": 0.15360479056835175,
      "learning_rate": 4.004166666666667e-06,
      "loss": 0.0017,
      "step": 110390
    },
    {
      "epoch": 7.36,
      "grad_norm": 0.3766464293003082,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.0017,
      "step": 110400
    },
    {
      "epoch": 7.360666666666667,
      "grad_norm": 0.1038953959941864,
      "learning_rate": 3.995833333333333e-06,
      "loss": 0.0017,
      "step": 110410
    },
    {
      "epoch": 7.3613333333333335,
      "grad_norm": 0.23589594662189484,
      "learning_rate": 3.991666666666667e-06,
      "loss": 0.0023,
      "step": 110420
    },
    {
      "epoch": 7.362,
      "grad_norm": 0.2999025583267212,
      "learning_rate": 3.9875e-06,
      "loss": 0.0016,
      "step": 110430
    },
    {
      "epoch": 7.362666666666667,
      "grad_norm": 0.33540767431259155,
      "learning_rate": 3.983333333333333e-06,
      "loss": 0.0021,
      "step": 110440
    },
    {
      "epoch": 7.363333333333333,
      "grad_norm": 0.3995119035243988,
      "learning_rate": 3.979166666666667e-06,
      "loss": 0.0015,
      "step": 110450
    },
    {
      "epoch": 7.364,
      "grad_norm": 0.10791502892971039,
      "learning_rate": 3.975e-06,
      "loss": 0.0017,
      "step": 110460
    },
    {
      "epoch": 7.3646666666666665,
      "grad_norm": 0.4358951449394226,
      "learning_rate": 3.9708333333333336e-06,
      "loss": 0.0019,
      "step": 110470
    },
    {
      "epoch": 7.365333333333333,
      "grad_norm": 0.061513982713222504,
      "learning_rate": 3.966666666666667e-06,
      "loss": 0.0017,
      "step": 110480
    },
    {
      "epoch": 7.366,
      "grad_norm": 0.10589428246021271,
      "learning_rate": 3.962500000000001e-06,
      "loss": 0.0018,
      "step": 110490
    },
    {
      "epoch": 7.366666666666666,
      "grad_norm": 0.23965489864349365,
      "learning_rate": 3.958333333333333e-06,
      "loss": 0.0014,
      "step": 110500
    },
    {
      "epoch": 7.367333333333334,
      "grad_norm": 0.3045348823070526,
      "learning_rate": 3.954166666666667e-06,
      "loss": 0.0019,
      "step": 110510
    },
    {
      "epoch": 7.368,
      "grad_norm": 0.5737335085868835,
      "learning_rate": 3.95e-06,
      "loss": 0.0013,
      "step": 110520
    },
    {
      "epoch": 7.368666666666667,
      "grad_norm": 0.16426599025726318,
      "learning_rate": 3.945833333333333e-06,
      "loss": 0.0022,
      "step": 110530
    },
    {
      "epoch": 7.3693333333333335,
      "grad_norm": 0.04369010403752327,
      "learning_rate": 3.941666666666667e-06,
      "loss": 0.0022,
      "step": 110540
    },
    {
      "epoch": 7.37,
      "grad_norm": 0.07105088979005814,
      "learning_rate": 3.9375e-06,
      "loss": 0.0012,
      "step": 110550
    },
    {
      "epoch": 7.370666666666667,
      "grad_norm": 0.04703705757856369,
      "learning_rate": 3.9333333333333335e-06,
      "loss": 0.0019,
      "step": 110560
    },
    {
      "epoch": 7.371333333333333,
      "grad_norm": 0.050400394946336746,
      "learning_rate": 3.929166666666667e-06,
      "loss": 0.0022,
      "step": 110570
    },
    {
      "epoch": 7.372,
      "grad_norm": 0.04313276708126068,
      "learning_rate": 3.9250000000000005e-06,
      "loss": 0.0018,
      "step": 110580
    },
    {
      "epoch": 7.3726666666666665,
      "grad_norm": 0.24022641777992249,
      "learning_rate": 3.920833333333333e-06,
      "loss": 0.0014,
      "step": 110590
    },
    {
      "epoch": 7.373333333333333,
      "grad_norm": 0.23241087794303894,
      "learning_rate": 3.916666666666667e-06,
      "loss": 0.0015,
      "step": 110600
    },
    {
      "epoch": 7.374,
      "grad_norm": 0.11252103745937347,
      "learning_rate": 3.9125e-06,
      "loss": 0.0011,
      "step": 110610
    },
    {
      "epoch": 7.374666666666666,
      "grad_norm": 0.17261840403079987,
      "learning_rate": 3.908333333333333e-06,
      "loss": 0.0014,
      "step": 110620
    },
    {
      "epoch": 7.375333333333334,
      "grad_norm": 0.05583180859684944,
      "learning_rate": 3.904166666666667e-06,
      "loss": 0.0012,
      "step": 110630
    },
    {
      "epoch": 7.376,
      "grad_norm": 0.07752671092748642,
      "learning_rate": 3.9e-06,
      "loss": 0.0015,
      "step": 110640
    },
    {
      "epoch": 7.376666666666667,
      "grad_norm": 0.19844065606594086,
      "learning_rate": 3.8958333333333334e-06,
      "loss": 0.0018,
      "step": 110650
    },
    {
      "epoch": 7.3773333333333335,
      "grad_norm": 0.06771670281887054,
      "learning_rate": 3.891666666666667e-06,
      "loss": 0.0014,
      "step": 110660
    },
    {
      "epoch": 7.378,
      "grad_norm": 0.04341892898082733,
      "learning_rate": 3.8875000000000005e-06,
      "loss": 0.0018,
      "step": 110670
    },
    {
      "epoch": 7.378666666666667,
      "grad_norm": 0.16483061015605927,
      "learning_rate": 3.883333333333333e-06,
      "loss": 0.0022,
      "step": 110680
    },
    {
      "epoch": 7.379333333333333,
      "grad_norm": 0.1979951709508896,
      "learning_rate": 3.879166666666667e-06,
      "loss": 0.0016,
      "step": 110690
    },
    {
      "epoch": 7.38,
      "grad_norm": 0.058735769242048264,
      "learning_rate": 3.875e-06,
      "loss": 0.0017,
      "step": 110700
    },
    {
      "epoch": 7.3806666666666665,
      "grad_norm": 0.04355310648679733,
      "learning_rate": 3.870833333333333e-06,
      "loss": 0.0013,
      "step": 110710
    },
    {
      "epoch": 7.381333333333333,
      "grad_norm": 0.1054820865392685,
      "learning_rate": 3.866666666666667e-06,
      "loss": 0.0028,
      "step": 110720
    },
    {
      "epoch": 7.382,
      "grad_norm": 0.3352854549884796,
      "learning_rate": 3.8625e-06,
      "loss": 0.0024,
      "step": 110730
    },
    {
      "epoch": 7.382666666666666,
      "grad_norm": 0.2750929892063141,
      "learning_rate": 3.858333333333333e-06,
      "loss": 0.0014,
      "step": 110740
    },
    {
      "epoch": 7.383333333333334,
      "grad_norm": 0.20882700383663177,
      "learning_rate": 3.854166666666667e-06,
      "loss": 0.0015,
      "step": 110750
    },
    {
      "epoch": 7.384,
      "grad_norm": 0.3052802085876465,
      "learning_rate": 3.85e-06,
      "loss": 0.0015,
      "step": 110760
    },
    {
      "epoch": 7.384666666666667,
      "grad_norm": 0.23619386553764343,
      "learning_rate": 3.845833333333333e-06,
      "loss": 0.0019,
      "step": 110770
    },
    {
      "epoch": 7.3853333333333335,
      "grad_norm": 0.199055477976799,
      "learning_rate": 3.8416666666666674e-06,
      "loss": 0.0012,
      "step": 110780
    },
    {
      "epoch": 7.386,
      "grad_norm": 0.09010200202465057,
      "learning_rate": 3.8375e-06,
      "loss": 0.0017,
      "step": 110790
    },
    {
      "epoch": 7.386666666666667,
      "grad_norm": 0.4031742811203003,
      "learning_rate": 3.833333333333334e-06,
      "loss": 0.0013,
      "step": 110800
    },
    {
      "epoch": 7.387333333333333,
      "grad_norm": 0.267042338848114,
      "learning_rate": 3.829166666666667e-06,
      "loss": 0.0018,
      "step": 110810
    },
    {
      "epoch": 7.388,
      "grad_norm": 0.07864897698163986,
      "learning_rate": 3.825e-06,
      "loss": 0.0018,
      "step": 110820
    },
    {
      "epoch": 7.3886666666666665,
      "grad_norm": 0.14654161036014557,
      "learning_rate": 3.820833333333333e-06,
      "loss": 0.0014,
      "step": 110830
    },
    {
      "epoch": 7.389333333333333,
      "grad_norm": 0.21442316472530365,
      "learning_rate": 3.816666666666667e-06,
      "loss": 0.0012,
      "step": 110840
    },
    {
      "epoch": 7.39,
      "grad_norm": 0.4816266596317291,
      "learning_rate": 3.8125e-06,
      "loss": 0.0016,
      "step": 110850
    },
    {
      "epoch": 7.390666666666666,
      "grad_norm": 0.07077087461948395,
      "learning_rate": 3.808333333333333e-06,
      "loss": 0.0014,
      "step": 110860
    },
    {
      "epoch": 7.391333333333334,
      "grad_norm": 0.23489168286323547,
      "learning_rate": 3.804166666666667e-06,
      "loss": 0.0017,
      "step": 110870
    },
    {
      "epoch": 7.392,
      "grad_norm": 0.2053932547569275,
      "learning_rate": 3.8e-06,
      "loss": 0.0017,
      "step": 110880
    },
    {
      "epoch": 7.392666666666667,
      "grad_norm": 0.1366649866104126,
      "learning_rate": 3.795833333333333e-06,
      "loss": 0.0017,
      "step": 110890
    },
    {
      "epoch": 7.3933333333333335,
      "grad_norm": 0.4394605755805969,
      "learning_rate": 3.791666666666667e-06,
      "loss": 0.0011,
      "step": 110900
    },
    {
      "epoch": 7.394,
      "grad_norm": 0.3310394883155823,
      "learning_rate": 3.7875e-06,
      "loss": 0.0018,
      "step": 110910
    },
    {
      "epoch": 7.394666666666667,
      "grad_norm": 0.23964624106884003,
      "learning_rate": 3.7833333333333333e-06,
      "loss": 0.0013,
      "step": 110920
    },
    {
      "epoch": 7.395333333333333,
      "grad_norm": 0.10857532918453217,
      "learning_rate": 3.7791666666666668e-06,
      "loss": 0.0021,
      "step": 110930
    },
    {
      "epoch": 7.396,
      "grad_norm": 0.10073694586753845,
      "learning_rate": 3.775e-06,
      "loss": 0.0012,
      "step": 110940
    },
    {
      "epoch": 7.3966666666666665,
      "grad_norm": 0.10704822838306427,
      "learning_rate": 3.770833333333334e-06,
      "loss": 0.0014,
      "step": 110950
    },
    {
      "epoch": 7.397333333333333,
      "grad_norm": 0.36556193232536316,
      "learning_rate": 3.766666666666667e-06,
      "loss": 0.0016,
      "step": 110960
    },
    {
      "epoch": 7.398,
      "grad_norm": 0.07595299184322357,
      "learning_rate": 3.7625e-06,
      "loss": 0.002,
      "step": 110970
    },
    {
      "epoch": 7.398666666666666,
      "grad_norm": 0.469571053981781,
      "learning_rate": 3.758333333333334e-06,
      "loss": 0.0016,
      "step": 110980
    },
    {
      "epoch": 7.399333333333333,
      "grad_norm": 0.10520277917385101,
      "learning_rate": 3.754166666666667e-06,
      "loss": 0.0013,
      "step": 110990
    },
    {
      "epoch": 7.4,
      "grad_norm": 0.23762483894824982,
      "learning_rate": 3.75e-06,
      "loss": 0.0012,
      "step": 111000
    },
    {
      "epoch": 7.400666666666667,
      "grad_norm": 0.17144675552845,
      "learning_rate": 3.745833333333334e-06,
      "loss": 0.0017,
      "step": 111010
    },
    {
      "epoch": 7.4013333333333335,
      "grad_norm": 0.30469515919685364,
      "learning_rate": 3.741666666666667e-06,
      "loss": 0.002,
      "step": 111020
    },
    {
      "epoch": 7.402,
      "grad_norm": 0.053652238100767136,
      "learning_rate": 3.7375000000000002e-06,
      "loss": 0.0017,
      "step": 111030
    },
    {
      "epoch": 7.402666666666667,
      "grad_norm": 0.2671661376953125,
      "learning_rate": 3.7333333333333337e-06,
      "loss": 0.002,
      "step": 111040
    },
    {
      "epoch": 7.403333333333333,
      "grad_norm": 0.017632074654102325,
      "learning_rate": 3.729166666666667e-06,
      "loss": 0.0014,
      "step": 111050
    },
    {
      "epoch": 7.404,
      "grad_norm": 0.10424704849720001,
      "learning_rate": 3.725e-06,
      "loss": 0.0029,
      "step": 111060
    },
    {
      "epoch": 7.4046666666666665,
      "grad_norm": 0.3360002338886261,
      "learning_rate": 3.720833333333334e-06,
      "loss": 0.0018,
      "step": 111070
    },
    {
      "epoch": 7.405333333333333,
      "grad_norm": 0.1687045693397522,
      "learning_rate": 3.716666666666667e-06,
      "loss": 0.0021,
      "step": 111080
    },
    {
      "epoch": 7.406,
      "grad_norm": 0.3309939503669739,
      "learning_rate": 3.7125e-06,
      "loss": 0.0016,
      "step": 111090
    },
    {
      "epoch": 7.406666666666666,
      "grad_norm": 0.10317020863294601,
      "learning_rate": 3.708333333333334e-06,
      "loss": 0.0016,
      "step": 111100
    },
    {
      "epoch": 7.407333333333334,
      "grad_norm": 0.10045643895864487,
      "learning_rate": 3.704166666666667e-06,
      "loss": 0.0014,
      "step": 111110
    },
    {
      "epoch": 7.408,
      "grad_norm": 0.034649401903152466,
      "learning_rate": 3.7e-06,
      "loss": 0.0016,
      "step": 111120
    },
    {
      "epoch": 7.408666666666667,
      "grad_norm": 0.4080045819282532,
      "learning_rate": 3.6958333333333337e-06,
      "loss": 0.0014,
      "step": 111130
    },
    {
      "epoch": 7.4093333333333335,
      "grad_norm": 0.4538716971874237,
      "learning_rate": 3.6916666666666668e-06,
      "loss": 0.0014,
      "step": 111140
    },
    {
      "epoch": 7.41,
      "grad_norm": 0.07081323117017746,
      "learning_rate": 3.6875e-06,
      "loss": 0.0017,
      "step": 111150
    },
    {
      "epoch": 7.410666666666667,
      "grad_norm": 0.07895034551620483,
      "learning_rate": 3.6833333333333338e-06,
      "loss": 0.0016,
      "step": 111160
    },
    {
      "epoch": 7.411333333333333,
      "grad_norm": 0.5864026546478271,
      "learning_rate": 3.679166666666667e-06,
      "loss": 0.0014,
      "step": 111170
    },
    {
      "epoch": 7.412,
      "grad_norm": 0.10991716384887695,
      "learning_rate": 3.675e-06,
      "loss": 0.0014,
      "step": 111180
    },
    {
      "epoch": 7.4126666666666665,
      "grad_norm": 0.20717404782772064,
      "learning_rate": 3.670833333333334e-06,
      "loss": 0.0023,
      "step": 111190
    },
    {
      "epoch": 7.413333333333333,
      "grad_norm": 0.6368850469589233,
      "learning_rate": 3.666666666666667e-06,
      "loss": 0.0026,
      "step": 111200
    },
    {
      "epoch": 7.414,
      "grad_norm": 0.07596618682146072,
      "learning_rate": 3.6625e-06,
      "loss": 0.0013,
      "step": 111210
    },
    {
      "epoch": 7.414666666666666,
      "grad_norm": 0.36067789793014526,
      "learning_rate": 3.6583333333333336e-06,
      "loss": 0.0016,
      "step": 111220
    },
    {
      "epoch": 7.415333333333333,
      "grad_norm": 0.5670024156570435,
      "learning_rate": 3.6541666666666667e-06,
      "loss": 0.0014,
      "step": 111230
    },
    {
      "epoch": 7.416,
      "grad_norm": 0.30845144391059875,
      "learning_rate": 3.6499999999999998e-06,
      "loss": 0.0018,
      "step": 111240
    },
    {
      "epoch": 7.416666666666667,
      "grad_norm": 0.2031029611825943,
      "learning_rate": 3.6458333333333337e-06,
      "loss": 0.0024,
      "step": 111250
    },
    {
      "epoch": 7.417333333333334,
      "grad_norm": 0.31452620029449463,
      "learning_rate": 3.641666666666667e-06,
      "loss": 0.0017,
      "step": 111260
    },
    {
      "epoch": 7.418,
      "grad_norm": 0.04933719336986542,
      "learning_rate": 3.6375e-06,
      "loss": 0.002,
      "step": 111270
    },
    {
      "epoch": 7.418666666666667,
      "grad_norm": 0.4011414349079132,
      "learning_rate": 3.633333333333334e-06,
      "loss": 0.0012,
      "step": 111280
    },
    {
      "epoch": 7.419333333333333,
      "grad_norm": 0.08072040230035782,
      "learning_rate": 3.629166666666667e-06,
      "loss": 0.0014,
      "step": 111290
    },
    {
      "epoch": 7.42,
      "grad_norm": 0.23586250841617584,
      "learning_rate": 3.625e-06,
      "loss": 0.0027,
      "step": 111300
    },
    {
      "epoch": 7.4206666666666665,
      "grad_norm": 0.3031068742275238,
      "learning_rate": 3.6208333333333335e-06,
      "loss": 0.0021,
      "step": 111310
    },
    {
      "epoch": 7.421333333333333,
      "grad_norm": 0.36399325728416443,
      "learning_rate": 3.6166666666666666e-06,
      "loss": 0.0032,
      "step": 111320
    },
    {
      "epoch": 7.422,
      "grad_norm": 0.5259525775909424,
      "learning_rate": 3.6124999999999997e-06,
      "loss": 0.0022,
      "step": 111330
    },
    {
      "epoch": 7.422666666666666,
      "grad_norm": 0.5012325644493103,
      "learning_rate": 3.6083333333333337e-06,
      "loss": 0.0025,
      "step": 111340
    },
    {
      "epoch": 7.423333333333334,
      "grad_norm": 0.17220912873744965,
      "learning_rate": 3.6041666666666667e-06,
      "loss": 0.002,
      "step": 111350
    },
    {
      "epoch": 7.424,
      "grad_norm": 0.33132871985435486,
      "learning_rate": 3.6e-06,
      "loss": 0.0021,
      "step": 111360
    },
    {
      "epoch": 7.424666666666667,
      "grad_norm": 0.13838478922843933,
      "learning_rate": 3.5958333333333338e-06,
      "loss": 0.0015,
      "step": 111370
    },
    {
      "epoch": 7.425333333333334,
      "grad_norm": 0.05844990909099579,
      "learning_rate": 3.591666666666667e-06,
      "loss": 0.0017,
      "step": 111380
    },
    {
      "epoch": 7.426,
      "grad_norm": 0.0566706620156765,
      "learning_rate": 3.5875e-06,
      "loss": 0.0021,
      "step": 111390
    },
    {
      "epoch": 7.426666666666667,
      "grad_norm": 0.35864540934562683,
      "learning_rate": 3.5833333333333335e-06,
      "loss": 0.0014,
      "step": 111400
    },
    {
      "epoch": 7.427333333333333,
      "grad_norm": 0.20117582380771637,
      "learning_rate": 3.579166666666667e-06,
      "loss": 0.0023,
      "step": 111410
    },
    {
      "epoch": 7.428,
      "grad_norm": 0.1973462551832199,
      "learning_rate": 3.575e-06,
      "loss": 0.002,
      "step": 111420
    },
    {
      "epoch": 7.4286666666666665,
      "grad_norm": 0.3694671392440796,
      "learning_rate": 3.5708333333333336e-06,
      "loss": 0.0015,
      "step": 111430
    },
    {
      "epoch": 7.429333333333333,
      "grad_norm": 0.04601363092660904,
      "learning_rate": 3.5666666666666667e-06,
      "loss": 0.0011,
      "step": 111440
    },
    {
      "epoch": 7.43,
      "grad_norm": 0.11448296159505844,
      "learning_rate": 3.5624999999999998e-06,
      "loss": 0.0017,
      "step": 111450
    },
    {
      "epoch": 7.430666666666666,
      "grad_norm": 0.15008187294006348,
      "learning_rate": 3.5583333333333337e-06,
      "loss": 0.0021,
      "step": 111460
    },
    {
      "epoch": 7.431333333333333,
      "grad_norm": 0.5350683927536011,
      "learning_rate": 3.554166666666667e-06,
      "loss": 0.0024,
      "step": 111470
    },
    {
      "epoch": 7.432,
      "grad_norm": 0.19962163269519806,
      "learning_rate": 3.55e-06,
      "loss": 0.0015,
      "step": 111480
    },
    {
      "epoch": 7.432666666666667,
      "grad_norm": 0.23710089921951294,
      "learning_rate": 3.545833333333334e-06,
      "loss": 0.0018,
      "step": 111490
    },
    {
      "epoch": 7.433333333333334,
      "grad_norm": 0.03815920278429985,
      "learning_rate": 3.541666666666667e-06,
      "loss": 0.0025,
      "step": 111500
    },
    {
      "epoch": 7.434,
      "grad_norm": 0.2331732213497162,
      "learning_rate": 3.5375e-06,
      "loss": 0.0012,
      "step": 111510
    },
    {
      "epoch": 7.434666666666667,
      "grad_norm": 0.2977483868598938,
      "learning_rate": 3.5333333333333335e-06,
      "loss": 0.0019,
      "step": 111520
    },
    {
      "epoch": 7.435333333333333,
      "grad_norm": 0.07033173739910126,
      "learning_rate": 3.5291666666666666e-06,
      "loss": 0.0021,
      "step": 111530
    },
    {
      "epoch": 7.436,
      "grad_norm": 0.2816821336746216,
      "learning_rate": 3.5249999999999997e-06,
      "loss": 0.0019,
      "step": 111540
    },
    {
      "epoch": 7.4366666666666665,
      "grad_norm": 0.10946213454008102,
      "learning_rate": 3.5208333333333336e-06,
      "loss": 0.002,
      "step": 111550
    },
    {
      "epoch": 7.437333333333333,
      "grad_norm": 0.16580301523208618,
      "learning_rate": 3.5166666666666667e-06,
      "loss": 0.0019,
      "step": 111560
    },
    {
      "epoch": 7.438,
      "grad_norm": 0.14304861426353455,
      "learning_rate": 3.5125000000000007e-06,
      "loss": 0.0015,
      "step": 111570
    },
    {
      "epoch": 7.438666666666666,
      "grad_norm": 0.15308667719364166,
      "learning_rate": 3.5083333333333338e-06,
      "loss": 0.0012,
      "step": 111580
    },
    {
      "epoch": 7.439333333333334,
      "grad_norm": 0.07710418105125427,
      "learning_rate": 3.504166666666667e-06,
      "loss": 0.0015,
      "step": 111590
    },
    {
      "epoch": 7.44,
      "grad_norm": 0.37189435958862305,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 0.0015,
      "step": 111600
    },
    {
      "epoch": 7.440666666666667,
      "grad_norm": 0.2542405426502228,
      "learning_rate": 3.4958333333333335e-06,
      "loss": 0.0027,
      "step": 111610
    },
    {
      "epoch": 7.441333333333334,
      "grad_norm": 0.050954658538103104,
      "learning_rate": 3.4916666666666666e-06,
      "loss": 0.0022,
      "step": 111620
    },
    {
      "epoch": 7.442,
      "grad_norm": 0.30156630277633667,
      "learning_rate": 3.4875000000000005e-06,
      "loss": 0.0023,
      "step": 111630
    },
    {
      "epoch": 7.442666666666667,
      "grad_norm": 0.10401768237352371,
      "learning_rate": 3.4833333333333336e-06,
      "loss": 0.0012,
      "step": 111640
    },
    {
      "epoch": 7.443333333333333,
      "grad_norm": 0.5328556299209595,
      "learning_rate": 3.4791666666666667e-06,
      "loss": 0.0017,
      "step": 111650
    },
    {
      "epoch": 7.444,
      "grad_norm": 0.38472646474838257,
      "learning_rate": 3.4750000000000006e-06,
      "loss": 0.0022,
      "step": 111660
    },
    {
      "epoch": 7.4446666666666665,
      "grad_norm": 0.19642773270606995,
      "learning_rate": 3.4708333333333337e-06,
      "loss": 0.0015,
      "step": 111670
    },
    {
      "epoch": 7.445333333333333,
      "grad_norm": 0.20933905243873596,
      "learning_rate": 3.466666666666667e-06,
      "loss": 0.0027,
      "step": 111680
    },
    {
      "epoch": 7.446,
      "grad_norm": 0.41502898931503296,
      "learning_rate": 3.4625000000000003e-06,
      "loss": 0.0033,
      "step": 111690
    },
    {
      "epoch": 7.446666666666666,
      "grad_norm": 0.20871040225028992,
      "learning_rate": 3.4583333333333334e-06,
      "loss": 0.0011,
      "step": 111700
    },
    {
      "epoch": 7.447333333333333,
      "grad_norm": 0.278015673160553,
      "learning_rate": 3.4541666666666665e-06,
      "loss": 0.0033,
      "step": 111710
    },
    {
      "epoch": 7.448,
      "grad_norm": 0.20317721366882324,
      "learning_rate": 3.4500000000000004e-06,
      "loss": 0.002,
      "step": 111720
    },
    {
      "epoch": 7.448666666666667,
      "grad_norm": 0.13813862204551697,
      "learning_rate": 3.4458333333333335e-06,
      "loss": 0.0016,
      "step": 111730
    },
    {
      "epoch": 7.449333333333334,
      "grad_norm": 0.13909845054149628,
      "learning_rate": 3.4416666666666666e-06,
      "loss": 0.0018,
      "step": 111740
    },
    {
      "epoch": 7.45,
      "grad_norm": 0.053374964743852615,
      "learning_rate": 3.4375000000000005e-06,
      "loss": 0.0027,
      "step": 111750
    },
    {
      "epoch": 7.450666666666667,
      "grad_norm": 0.34779831767082214,
      "learning_rate": 3.4333333333333336e-06,
      "loss": 0.0013,
      "step": 111760
    },
    {
      "epoch": 7.451333333333333,
      "grad_norm": 0.29924458265304565,
      "learning_rate": 3.4291666666666667e-06,
      "loss": 0.0021,
      "step": 111770
    },
    {
      "epoch": 7.452,
      "grad_norm": 0.3683053255081177,
      "learning_rate": 3.4250000000000002e-06,
      "loss": 0.0016,
      "step": 111780
    },
    {
      "epoch": 7.4526666666666666,
      "grad_norm": 0.05906350910663605,
      "learning_rate": 3.4208333333333333e-06,
      "loss": 0.0015,
      "step": 111790
    },
    {
      "epoch": 7.453333333333333,
      "grad_norm": 0.24408742785453796,
      "learning_rate": 3.4166666666666664e-06,
      "loss": 0.0019,
      "step": 111800
    },
    {
      "epoch": 7.454,
      "grad_norm": 0.4369047284126282,
      "learning_rate": 3.4125000000000004e-06,
      "loss": 0.0021,
      "step": 111810
    },
    {
      "epoch": 7.454666666666666,
      "grad_norm": 0.4700816571712494,
      "learning_rate": 3.4083333333333335e-06,
      "loss": 0.0025,
      "step": 111820
    },
    {
      "epoch": 7.455333333333333,
      "grad_norm": 0.37698715925216675,
      "learning_rate": 3.4041666666666665e-06,
      "loss": 0.0015,
      "step": 111830
    },
    {
      "epoch": 7.456,
      "grad_norm": 0.5629466772079468,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 0.002,
      "step": 111840
    },
    {
      "epoch": 7.456666666666667,
      "grad_norm": 0.051369231194257736,
      "learning_rate": 3.3958333333333336e-06,
      "loss": 0.0017,
      "step": 111850
    },
    {
      "epoch": 7.457333333333334,
      "grad_norm": 0.33621376752853394,
      "learning_rate": 3.3916666666666667e-06,
      "loss": 0.0013,
      "step": 111860
    },
    {
      "epoch": 7.458,
      "grad_norm": 0.23725561797618866,
      "learning_rate": 3.3875000000000006e-06,
      "loss": 0.0014,
      "step": 111870
    },
    {
      "epoch": 7.458666666666667,
      "grad_norm": 0.23589712381362915,
      "learning_rate": 3.3833333333333337e-06,
      "loss": 0.0019,
      "step": 111880
    },
    {
      "epoch": 7.459333333333333,
      "grad_norm": 0.3256063163280487,
      "learning_rate": 3.3791666666666668e-06,
      "loss": 0.0018,
      "step": 111890
    },
    {
      "epoch": 7.46,
      "grad_norm": 0.47733190655708313,
      "learning_rate": 3.3750000000000003e-06,
      "loss": 0.0022,
      "step": 111900
    },
    {
      "epoch": 7.460666666666667,
      "grad_norm": 0.2741374671459198,
      "learning_rate": 3.3708333333333334e-06,
      "loss": 0.0029,
      "step": 111910
    },
    {
      "epoch": 7.461333333333333,
      "grad_norm": 0.3521256148815155,
      "learning_rate": 3.3666666666666665e-06,
      "loss": 0.0017,
      "step": 111920
    },
    {
      "epoch": 7.462,
      "grad_norm": 0.5343565344810486,
      "learning_rate": 3.3625000000000004e-06,
      "loss": 0.0011,
      "step": 111930
    },
    {
      "epoch": 7.462666666666666,
      "grad_norm": 0.164444699883461,
      "learning_rate": 3.3583333333333335e-06,
      "loss": 0.0014,
      "step": 111940
    },
    {
      "epoch": 7.463333333333333,
      "grad_norm": 0.09893260896205902,
      "learning_rate": 3.3541666666666666e-06,
      "loss": 0.0019,
      "step": 111950
    },
    {
      "epoch": 7.464,
      "grad_norm": 0.15866565704345703,
      "learning_rate": 3.3500000000000005e-06,
      "loss": 0.0018,
      "step": 111960
    },
    {
      "epoch": 7.464666666666667,
      "grad_norm": 0.28132176399230957,
      "learning_rate": 3.3458333333333336e-06,
      "loss": 0.0013,
      "step": 111970
    },
    {
      "epoch": 7.465333333333334,
      "grad_norm": 0.5389938354492188,
      "learning_rate": 3.3416666666666667e-06,
      "loss": 0.0047,
      "step": 111980
    },
    {
      "epoch": 7.466,
      "grad_norm": 0.19878341257572174,
      "learning_rate": 3.3375000000000002e-06,
      "loss": 0.0012,
      "step": 111990
    },
    {
      "epoch": 7.466666666666667,
      "grad_norm": 0.0556948259472847,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.0011,
      "step": 112000
    },
    {
      "epoch": 7.467333333333333,
      "grad_norm": 0.15394340455532074,
      "learning_rate": 3.3291666666666664e-06,
      "loss": 0.0015,
      "step": 112010
    },
    {
      "epoch": 7.468,
      "grad_norm": 0.6553360819816589,
      "learning_rate": 3.3250000000000004e-06,
      "loss": 0.0019,
      "step": 112020
    },
    {
      "epoch": 7.468666666666667,
      "grad_norm": 0.406157523393631,
      "learning_rate": 3.3208333333333334e-06,
      "loss": 0.0017,
      "step": 112030
    },
    {
      "epoch": 7.469333333333333,
      "grad_norm": 0.6337115168571472,
      "learning_rate": 3.3166666666666665e-06,
      "loss": 0.002,
      "step": 112040
    },
    {
      "epoch": 7.47,
      "grad_norm": 0.36934953927993774,
      "learning_rate": 3.3125000000000005e-06,
      "loss": 0.0012,
      "step": 112050
    },
    {
      "epoch": 7.470666666666666,
      "grad_norm": 0.07530713081359863,
      "learning_rate": 3.3083333333333336e-06,
      "loss": 0.002,
      "step": 112060
    },
    {
      "epoch": 7.471333333333333,
      "grad_norm": 0.17168860137462616,
      "learning_rate": 3.3041666666666667e-06,
      "loss": 0.0015,
      "step": 112070
    },
    {
      "epoch": 7.4719999999999995,
      "grad_norm": 0.1742868423461914,
      "learning_rate": 3.3e-06,
      "loss": 0.0018,
      "step": 112080
    },
    {
      "epoch": 7.472666666666667,
      "grad_norm": 0.30558308959007263,
      "learning_rate": 3.2958333333333333e-06,
      "loss": 0.0017,
      "step": 112090
    },
    {
      "epoch": 7.473333333333334,
      "grad_norm": 0.2362666130065918,
      "learning_rate": 3.2916666666666664e-06,
      "loss": 0.0021,
      "step": 112100
    },
    {
      "epoch": 7.474,
      "grad_norm": 0.10720355063676834,
      "learning_rate": 3.2875000000000003e-06,
      "loss": 0.0025,
      "step": 112110
    },
    {
      "epoch": 7.474666666666667,
      "grad_norm": 0.09488950669765472,
      "learning_rate": 3.2833333333333334e-06,
      "loss": 0.0014,
      "step": 112120
    },
    {
      "epoch": 7.475333333333333,
      "grad_norm": 0.20692913234233856,
      "learning_rate": 3.2791666666666665e-06,
      "loss": 0.0015,
      "step": 112130
    },
    {
      "epoch": 7.476,
      "grad_norm": 0.07705288380384445,
      "learning_rate": 3.2750000000000004e-06,
      "loss": 0.0013,
      "step": 112140
    },
    {
      "epoch": 7.476666666666667,
      "grad_norm": 0.495882123708725,
      "learning_rate": 3.2708333333333335e-06,
      "loss": 0.003,
      "step": 112150
    },
    {
      "epoch": 7.477333333333333,
      "grad_norm": 0.13934388756752014,
      "learning_rate": 3.2666666666666666e-06,
      "loss": 0.002,
      "step": 112160
    },
    {
      "epoch": 7.478,
      "grad_norm": 0.4999171495437622,
      "learning_rate": 3.2625e-06,
      "loss": 0.0026,
      "step": 112170
    },
    {
      "epoch": 7.478666666666666,
      "grad_norm": 0.6468265056610107,
      "learning_rate": 3.258333333333333e-06,
      "loss": 0.0037,
      "step": 112180
    },
    {
      "epoch": 7.479333333333333,
      "grad_norm": 0.1120392456650734,
      "learning_rate": 3.254166666666667e-06,
      "loss": 0.0015,
      "step": 112190
    },
    {
      "epoch": 7.48,
      "grad_norm": 0.13629841804504395,
      "learning_rate": 3.2500000000000002e-06,
      "loss": 0.0013,
      "step": 112200
    },
    {
      "epoch": 7.480666666666667,
      "grad_norm": 0.3265322744846344,
      "learning_rate": 3.2458333333333333e-06,
      "loss": 0.0012,
      "step": 112210
    },
    {
      "epoch": 7.481333333333334,
      "grad_norm": 0.2410437911748886,
      "learning_rate": 3.2416666666666673e-06,
      "loss": 0.0021,
      "step": 112220
    },
    {
      "epoch": 7.482,
      "grad_norm": 0.07588329166173935,
      "learning_rate": 3.2375000000000003e-06,
      "loss": 0.0012,
      "step": 112230
    },
    {
      "epoch": 7.482666666666667,
      "grad_norm": 0.21782782673835754,
      "learning_rate": 3.2333333333333334e-06,
      "loss": 0.0025,
      "step": 112240
    },
    {
      "epoch": 7.483333333333333,
      "grad_norm": 0.3403841257095337,
      "learning_rate": 3.229166666666667e-06,
      "loss": 0.0016,
      "step": 112250
    },
    {
      "epoch": 7.484,
      "grad_norm": 0.291117787361145,
      "learning_rate": 3.225e-06,
      "loss": 0.0019,
      "step": 112260
    },
    {
      "epoch": 7.484666666666667,
      "grad_norm": 0.29986026883125305,
      "learning_rate": 3.220833333333333e-06,
      "loss": 0.0014,
      "step": 112270
    },
    {
      "epoch": 7.485333333333333,
      "grad_norm": 0.04268810525536537,
      "learning_rate": 3.216666666666667e-06,
      "loss": 0.0018,
      "step": 112280
    },
    {
      "epoch": 7.486,
      "grad_norm": 0.27733442187309265,
      "learning_rate": 3.2125e-06,
      "loss": 0.0016,
      "step": 112290
    },
    {
      "epoch": 7.486666666666666,
      "grad_norm": 0.23953115940093994,
      "learning_rate": 3.2083333333333332e-06,
      "loss": 0.0019,
      "step": 112300
    },
    {
      "epoch": 7.487333333333333,
      "grad_norm": 0.23050707578659058,
      "learning_rate": 3.204166666666667e-06,
      "loss": 0.0013,
      "step": 112310
    },
    {
      "epoch": 7.4879999999999995,
      "grad_norm": 0.20297715067863464,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.0014,
      "step": 112320
    },
    {
      "epoch": 7.488666666666667,
      "grad_norm": 0.24434834718704224,
      "learning_rate": 3.1958333333333334e-06,
      "loss": 0.0014,
      "step": 112330
    },
    {
      "epoch": 7.489333333333334,
      "grad_norm": 0.33524563908576965,
      "learning_rate": 3.1916666666666673e-06,
      "loss": 0.0017,
      "step": 112340
    },
    {
      "epoch": 7.49,
      "grad_norm": 0.07558827847242355,
      "learning_rate": 3.1875000000000004e-06,
      "loss": 0.0019,
      "step": 112350
    },
    {
      "epoch": 7.490666666666667,
      "grad_norm": 0.07474713027477264,
      "learning_rate": 3.1833333333333335e-06,
      "loss": 0.0012,
      "step": 112360
    },
    {
      "epoch": 7.491333333333333,
      "grad_norm": 0.23875553905963898,
      "learning_rate": 3.179166666666667e-06,
      "loss": 0.0014,
      "step": 112370
    },
    {
      "epoch": 7.492,
      "grad_norm": 0.049501560628414154,
      "learning_rate": 3.175e-06,
      "loss": 0.0021,
      "step": 112380
    },
    {
      "epoch": 7.492666666666667,
      "grad_norm": 0.042423348873853683,
      "learning_rate": 3.170833333333333e-06,
      "loss": 0.003,
      "step": 112390
    },
    {
      "epoch": 7.493333333333333,
      "grad_norm": 0.20453976094722748,
      "learning_rate": 3.166666666666667e-06,
      "loss": 0.0023,
      "step": 112400
    },
    {
      "epoch": 7.494,
      "grad_norm": 0.10192206501960754,
      "learning_rate": 3.1625000000000002e-06,
      "loss": 0.0026,
      "step": 112410
    },
    {
      "epoch": 7.494666666666666,
      "grad_norm": 0.46301203966140747,
      "learning_rate": 3.1583333333333333e-06,
      "loss": 0.0013,
      "step": 112420
    },
    {
      "epoch": 7.495333333333333,
      "grad_norm": 0.23278969526290894,
      "learning_rate": 3.1541666666666672e-06,
      "loss": 0.0015,
      "step": 112430
    },
    {
      "epoch": 7.496,
      "grad_norm": 0.2034664899110794,
      "learning_rate": 3.1500000000000003e-06,
      "loss": 0.0022,
      "step": 112440
    },
    {
      "epoch": 7.496666666666667,
      "grad_norm": 0.2828538119792938,
      "learning_rate": 3.1458333333333334e-06,
      "loss": 0.0025,
      "step": 112450
    },
    {
      "epoch": 7.497333333333334,
      "grad_norm": 0.2015073597431183,
      "learning_rate": 3.141666666666667e-06,
      "loss": 0.0024,
      "step": 112460
    },
    {
      "epoch": 7.498,
      "grad_norm": 0.07544772326946259,
      "learning_rate": 3.1375e-06,
      "loss": 0.0016,
      "step": 112470
    },
    {
      "epoch": 7.498666666666667,
      "grad_norm": 0.0717330127954483,
      "learning_rate": 3.133333333333333e-06,
      "loss": 0.002,
      "step": 112480
    },
    {
      "epoch": 7.499333333333333,
      "grad_norm": 0.13383296132087708,
      "learning_rate": 3.129166666666667e-06,
      "loss": 0.0021,
      "step": 112490
    },
    {
      "epoch": 7.5,
      "grad_norm": 0.11173555999994278,
      "learning_rate": 3.125e-06,
      "loss": 0.0016,
      "step": 112500
    },
    {
      "epoch": 7.500666666666667,
      "grad_norm": 0.20478393137454987,
      "learning_rate": 3.1208333333333337e-06,
      "loss": 0.0015,
      "step": 112510
    },
    {
      "epoch": 7.501333333333333,
      "grad_norm": 0.0767921730875969,
      "learning_rate": 3.1166666666666668e-06,
      "loss": 0.0026,
      "step": 112520
    },
    {
      "epoch": 7.502,
      "grad_norm": 0.13755902647972107,
      "learning_rate": 3.1125000000000003e-06,
      "loss": 0.0015,
      "step": 112530
    },
    {
      "epoch": 7.502666666666666,
      "grad_norm": 0.1403714269399643,
      "learning_rate": 3.1083333333333338e-06,
      "loss": 0.0027,
      "step": 112540
    },
    {
      "epoch": 7.503333333333333,
      "grad_norm": 0.33454325795173645,
      "learning_rate": 3.104166666666667e-06,
      "loss": 0.0015,
      "step": 112550
    },
    {
      "epoch": 7.504,
      "grad_norm": 0.10147092491388321,
      "learning_rate": 3.1e-06,
      "loss": 0.0015,
      "step": 112560
    },
    {
      "epoch": 7.504666666666667,
      "grad_norm": 0.49899962544441223,
      "learning_rate": 3.0958333333333335e-06,
      "loss": 0.0014,
      "step": 112570
    },
    {
      "epoch": 7.505333333333334,
      "grad_norm": 0.13835640251636505,
      "learning_rate": 3.0916666666666666e-06,
      "loss": 0.002,
      "step": 112580
    },
    {
      "epoch": 7.506,
      "grad_norm": 0.5351704955101013,
      "learning_rate": 3.0875e-06,
      "loss": 0.0013,
      "step": 112590
    },
    {
      "epoch": 7.506666666666667,
      "grad_norm": 0.5136666893959045,
      "learning_rate": 3.0833333333333336e-06,
      "loss": 0.002,
      "step": 112600
    },
    {
      "epoch": 7.507333333333333,
      "grad_norm": 0.14552250504493713,
      "learning_rate": 3.0791666666666667e-06,
      "loss": 0.0019,
      "step": 112610
    },
    {
      "epoch": 7.508,
      "grad_norm": 0.14095047116279602,
      "learning_rate": 3.075e-06,
      "loss": 0.0017,
      "step": 112620
    },
    {
      "epoch": 7.508666666666667,
      "grad_norm": 0.8812108039855957,
      "learning_rate": 3.0708333333333337e-06,
      "loss": 0.0023,
      "step": 112630
    },
    {
      "epoch": 7.509333333333333,
      "grad_norm": 0.3330443203449249,
      "learning_rate": 3.066666666666667e-06,
      "loss": 0.0019,
      "step": 112640
    },
    {
      "epoch": 7.51,
      "grad_norm": 0.14101754128932953,
      "learning_rate": 3.0625e-06,
      "loss": 0.0017,
      "step": 112650
    },
    {
      "epoch": 7.510666666666666,
      "grad_norm": 0.2267855703830719,
      "learning_rate": 3.0583333333333334e-06,
      "loss": 0.0022,
      "step": 112660
    },
    {
      "epoch": 7.511333333333333,
      "grad_norm": 0.2644347548484802,
      "learning_rate": 3.054166666666667e-06,
      "loss": 0.0017,
      "step": 112670
    },
    {
      "epoch": 7.5120000000000005,
      "grad_norm": 0.33829376101493835,
      "learning_rate": 3.05e-06,
      "loss": 0.0013,
      "step": 112680
    },
    {
      "epoch": 7.512666666666667,
      "grad_norm": 0.24513782560825348,
      "learning_rate": 3.0458333333333335e-06,
      "loss": 0.0012,
      "step": 112690
    },
    {
      "epoch": 7.513333333333334,
      "grad_norm": 0.14843779802322388,
      "learning_rate": 3.041666666666667e-06,
      "loss": 0.0018,
      "step": 112700
    },
    {
      "epoch": 7.514,
      "grad_norm": 0.08083579689264297,
      "learning_rate": 3.0375e-06,
      "loss": 0.0011,
      "step": 112710
    },
    {
      "epoch": 7.514666666666667,
      "grad_norm": 0.07146526128053665,
      "learning_rate": 3.0333333333333337e-06,
      "loss": 0.0019,
      "step": 112720
    },
    {
      "epoch": 7.515333333333333,
      "grad_norm": 0.0734560415148735,
      "learning_rate": 3.029166666666667e-06,
      "loss": 0.0014,
      "step": 112730
    },
    {
      "epoch": 7.516,
      "grad_norm": 0.27185648679733276,
      "learning_rate": 3.0250000000000003e-06,
      "loss": 0.0017,
      "step": 112740
    },
    {
      "epoch": 7.516666666666667,
      "grad_norm": 0.09435245394706726,
      "learning_rate": 3.0208333333333334e-06,
      "loss": 0.0028,
      "step": 112750
    },
    {
      "epoch": 7.517333333333333,
      "grad_norm": 0.13482995331287384,
      "learning_rate": 3.016666666666667e-06,
      "loss": 0.002,
      "step": 112760
    },
    {
      "epoch": 7.518,
      "grad_norm": 0.10900703072547913,
      "learning_rate": 3.0125e-06,
      "loss": 0.0019,
      "step": 112770
    },
    {
      "epoch": 7.518666666666666,
      "grad_norm": 0.05078187957406044,
      "learning_rate": 3.0083333333333335e-06,
      "loss": 0.0016,
      "step": 112780
    },
    {
      "epoch": 7.519333333333333,
      "grad_norm": 0.3995978534221649,
      "learning_rate": 3.004166666666667e-06,
      "loss": 0.0019,
      "step": 112790
    },
    {
      "epoch": 7.52,
      "grad_norm": 0.23411034047603607,
      "learning_rate": 3e-06,
      "loss": 0.0018,
      "step": 112800
    },
    {
      "epoch": 7.520666666666667,
      "grad_norm": 0.1510922610759735,
      "learning_rate": 2.9958333333333336e-06,
      "loss": 0.0016,
      "step": 112810
    },
    {
      "epoch": 7.521333333333334,
      "grad_norm": 0.14219601452350616,
      "learning_rate": 2.991666666666667e-06,
      "loss": 0.0014,
      "step": 112820
    },
    {
      "epoch": 7.522,
      "grad_norm": 0.3929917812347412,
      "learning_rate": 2.9875e-06,
      "loss": 0.0013,
      "step": 112830
    },
    {
      "epoch": 7.522666666666667,
      "grad_norm": 0.22239577770233154,
      "learning_rate": 2.9833333333333333e-06,
      "loss": 0.0016,
      "step": 112840
    },
    {
      "epoch": 7.523333333333333,
      "grad_norm": 0.1065649539232254,
      "learning_rate": 2.979166666666667e-06,
      "loss": 0.0016,
      "step": 112850
    },
    {
      "epoch": 7.524,
      "grad_norm": 0.3094695508480072,
      "learning_rate": 2.975e-06,
      "loss": 0.0015,
      "step": 112860
    },
    {
      "epoch": 7.524666666666667,
      "grad_norm": 0.4172068238258362,
      "learning_rate": 2.9708333333333334e-06,
      "loss": 0.0014,
      "step": 112870
    },
    {
      "epoch": 7.525333333333333,
      "grad_norm": 0.5126405358314514,
      "learning_rate": 2.966666666666667e-06,
      "loss": 0.0018,
      "step": 112880
    },
    {
      "epoch": 7.526,
      "grad_norm": 0.0656939372420311,
      "learning_rate": 2.9625e-06,
      "loss": 0.0017,
      "step": 112890
    },
    {
      "epoch": 7.526666666666666,
      "grad_norm": 0.0865287184715271,
      "learning_rate": 2.9583333333333335e-06,
      "loss": 0.0017,
      "step": 112900
    },
    {
      "epoch": 7.527333333333333,
      "grad_norm": 0.20264282822608948,
      "learning_rate": 2.954166666666667e-06,
      "loss": 0.0018,
      "step": 112910
    },
    {
      "epoch": 7.5280000000000005,
      "grad_norm": 0.36293908953666687,
      "learning_rate": 2.95e-06,
      "loss": 0.0011,
      "step": 112920
    },
    {
      "epoch": 7.528666666666666,
      "grad_norm": 0.10865701735019684,
      "learning_rate": 2.9458333333333332e-06,
      "loss": 0.0026,
      "step": 112930
    },
    {
      "epoch": 7.529333333333334,
      "grad_norm": 0.299296498298645,
      "learning_rate": 2.9416666666666667e-06,
      "loss": 0.0017,
      "step": 112940
    },
    {
      "epoch": 7.53,
      "grad_norm": 0.043322108685970306,
      "learning_rate": 2.9375e-06,
      "loss": 0.0021,
      "step": 112950
    },
    {
      "epoch": 7.530666666666667,
      "grad_norm": 0.10194062441587448,
      "learning_rate": 2.9333333333333333e-06,
      "loss": 0.0016,
      "step": 112960
    },
    {
      "epoch": 7.531333333333333,
      "grad_norm": 0.44649770855903625,
      "learning_rate": 2.929166666666667e-06,
      "loss": 0.0018,
      "step": 112970
    },
    {
      "epoch": 7.532,
      "grad_norm": 0.08214815706014633,
      "learning_rate": 2.9250000000000004e-06,
      "loss": 0.002,
      "step": 112980
    },
    {
      "epoch": 7.532666666666667,
      "grad_norm": 0.10062865167856216,
      "learning_rate": 2.9208333333333335e-06,
      "loss": 0.0012,
      "step": 112990
    },
    {
      "epoch": 7.533333333333333,
      "grad_norm": 0.06538382172584534,
      "learning_rate": 2.916666666666667e-06,
      "loss": 0.0012,
      "step": 113000
    },
    {
      "epoch": 7.534,
      "grad_norm": 0.0689391940832138,
      "learning_rate": 2.9125000000000005e-06,
      "loss": 0.0021,
      "step": 113010
    },
    {
      "epoch": 7.534666666666666,
      "grad_norm": 0.3043159544467926,
      "learning_rate": 2.9083333333333336e-06,
      "loss": 0.0014,
      "step": 113020
    },
    {
      "epoch": 7.535333333333333,
      "grad_norm": 0.06918245553970337,
      "learning_rate": 2.9041666666666667e-06,
      "loss": 0.0017,
      "step": 113030
    },
    {
      "epoch": 7.536,
      "grad_norm": 0.04488437995314598,
      "learning_rate": 2.9e-06,
      "loss": 0.0013,
      "step": 113040
    },
    {
      "epoch": 7.536666666666667,
      "grad_norm": 0.029106074944138527,
      "learning_rate": 2.8958333333333333e-06,
      "loss": 0.0012,
      "step": 113050
    },
    {
      "epoch": 7.537333333333334,
      "grad_norm": 0.17569789290428162,
      "learning_rate": 2.891666666666667e-06,
      "loss": 0.0019,
      "step": 113060
    },
    {
      "epoch": 7.538,
      "grad_norm": 0.16869689524173737,
      "learning_rate": 2.8875000000000003e-06,
      "loss": 0.0016,
      "step": 113070
    },
    {
      "epoch": 7.538666666666667,
      "grad_norm": 0.2254859060049057,
      "learning_rate": 2.8833333333333334e-06,
      "loss": 0.0026,
      "step": 113080
    },
    {
      "epoch": 7.539333333333333,
      "grad_norm": 0.06910181790590286,
      "learning_rate": 2.879166666666667e-06,
      "loss": 0.0019,
      "step": 113090
    },
    {
      "epoch": 7.54,
      "grad_norm": 0.4007170796394348,
      "learning_rate": 2.8750000000000004e-06,
      "loss": 0.0023,
      "step": 113100
    },
    {
      "epoch": 7.540666666666667,
      "grad_norm": 0.6749446392059326,
      "learning_rate": 2.8708333333333335e-06,
      "loss": 0.0015,
      "step": 113110
    },
    {
      "epoch": 7.541333333333333,
      "grad_norm": 0.1640506237745285,
      "learning_rate": 2.8666666666666666e-06,
      "loss": 0.0013,
      "step": 113120
    },
    {
      "epoch": 7.542,
      "grad_norm": 0.1710028350353241,
      "learning_rate": 2.8625e-06,
      "loss": 0.0015,
      "step": 113130
    },
    {
      "epoch": 7.542666666666666,
      "grad_norm": 0.085737444460392,
      "learning_rate": 2.8583333333333332e-06,
      "loss": 0.0019,
      "step": 113140
    },
    {
      "epoch": 7.543333333333333,
      "grad_norm": 0.2635137140750885,
      "learning_rate": 2.8541666666666667e-06,
      "loss": 0.0014,
      "step": 113150
    },
    {
      "epoch": 7.5440000000000005,
      "grad_norm": 0.13562704622745514,
      "learning_rate": 2.8500000000000002e-06,
      "loss": 0.0018,
      "step": 113160
    },
    {
      "epoch": 7.544666666666666,
      "grad_norm": 0.27244216203689575,
      "learning_rate": 2.8458333333333333e-06,
      "loss": 0.0026,
      "step": 113170
    },
    {
      "epoch": 7.545333333333334,
      "grad_norm": 0.23964428901672363,
      "learning_rate": 2.841666666666667e-06,
      "loss": 0.0016,
      "step": 113180
    },
    {
      "epoch": 7.546,
      "grad_norm": 0.13844257593154907,
      "learning_rate": 2.8375000000000004e-06,
      "loss": 0.0028,
      "step": 113190
    },
    {
      "epoch": 7.546666666666667,
      "grad_norm": 0.1406000405550003,
      "learning_rate": 2.8333333333333335e-06,
      "loss": 0.0027,
      "step": 113200
    },
    {
      "epoch": 7.5473333333333334,
      "grad_norm": 0.16882513463497162,
      "learning_rate": 2.829166666666667e-06,
      "loss": 0.0016,
      "step": 113210
    },
    {
      "epoch": 7.548,
      "grad_norm": 0.1459616869688034,
      "learning_rate": 2.825e-06,
      "loss": 0.0015,
      "step": 113220
    },
    {
      "epoch": 7.548666666666667,
      "grad_norm": 0.1754995584487915,
      "learning_rate": 2.820833333333333e-06,
      "loss": 0.0017,
      "step": 113230
    },
    {
      "epoch": 7.549333333333333,
      "grad_norm": 0.2671976685523987,
      "learning_rate": 2.8166666666666667e-06,
      "loss": 0.0018,
      "step": 113240
    },
    {
      "epoch": 7.55,
      "grad_norm": 0.1961275190114975,
      "learning_rate": 2.8125e-06,
      "loss": 0.002,
      "step": 113250
    },
    {
      "epoch": 7.550666666666666,
      "grad_norm": 0.24054056406021118,
      "learning_rate": 2.8083333333333333e-06,
      "loss": 0.002,
      "step": 113260
    },
    {
      "epoch": 7.551333333333333,
      "grad_norm": 0.21819819509983063,
      "learning_rate": 2.8041666666666668e-06,
      "loss": 0.0015,
      "step": 113270
    },
    {
      "epoch": 7.552,
      "grad_norm": 0.33231624960899353,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.0028,
      "step": 113280
    },
    {
      "epoch": 7.552666666666667,
      "grad_norm": 0.10458911955356598,
      "learning_rate": 2.795833333333334e-06,
      "loss": 0.0018,
      "step": 113290
    },
    {
      "epoch": 7.553333333333334,
      "grad_norm": 0.16668276488780975,
      "learning_rate": 2.791666666666667e-06,
      "loss": 0.0015,
      "step": 113300
    },
    {
      "epoch": 7.554,
      "grad_norm": 0.1718328446149826,
      "learning_rate": 2.7875e-06,
      "loss": 0.0028,
      "step": 113310
    },
    {
      "epoch": 7.554666666666667,
      "grad_norm": 0.2442227452993393,
      "learning_rate": 2.7833333333333335e-06,
      "loss": 0.0021,
      "step": 113320
    },
    {
      "epoch": 7.5553333333333335,
      "grad_norm": 0.06809044629335403,
      "learning_rate": 2.7791666666666666e-06,
      "loss": 0.0017,
      "step": 113330
    },
    {
      "epoch": 7.556,
      "grad_norm": 0.43707793951034546,
      "learning_rate": 2.775e-06,
      "loss": 0.0016,
      "step": 113340
    },
    {
      "epoch": 7.556666666666667,
      "grad_norm": 0.24335899949073792,
      "learning_rate": 2.7708333333333336e-06,
      "loss": 0.0025,
      "step": 113350
    },
    {
      "epoch": 7.557333333333333,
      "grad_norm": 0.10225065052509308,
      "learning_rate": 2.7666666666666667e-06,
      "loss": 0.002,
      "step": 113360
    },
    {
      "epoch": 7.558,
      "grad_norm": 0.365845263004303,
      "learning_rate": 2.7625000000000002e-06,
      "loss": 0.002,
      "step": 113370
    },
    {
      "epoch": 7.558666666666666,
      "grad_norm": 0.3383772671222687,
      "learning_rate": 2.7583333333333337e-06,
      "loss": 0.0015,
      "step": 113380
    },
    {
      "epoch": 7.559333333333333,
      "grad_norm": 0.13820213079452515,
      "learning_rate": 2.754166666666667e-06,
      "loss": 0.002,
      "step": 113390
    },
    {
      "epoch": 7.5600000000000005,
      "grad_norm": 0.66167151927948,
      "learning_rate": 2.7500000000000004e-06,
      "loss": 0.0034,
      "step": 113400
    },
    {
      "epoch": 7.560666666666666,
      "grad_norm": 0.07198354601860046,
      "learning_rate": 2.7458333333333334e-06,
      "loss": 0.0017,
      "step": 113410
    },
    {
      "epoch": 7.561333333333334,
      "grad_norm": 0.16291853785514832,
      "learning_rate": 2.7416666666666665e-06,
      "loss": 0.0027,
      "step": 113420
    },
    {
      "epoch": 7.562,
      "grad_norm": 0.09828203916549683,
      "learning_rate": 2.7375e-06,
      "loss": 0.0017,
      "step": 113430
    },
    {
      "epoch": 7.562666666666667,
      "grad_norm": 0.4029232859611511,
      "learning_rate": 2.7333333333333336e-06,
      "loss": 0.0024,
      "step": 113440
    },
    {
      "epoch": 7.5633333333333335,
      "grad_norm": 0.20803262293338776,
      "learning_rate": 2.7291666666666667e-06,
      "loss": 0.002,
      "step": 113450
    },
    {
      "epoch": 7.564,
      "grad_norm": 0.23617587983608246,
      "learning_rate": 2.725e-06,
      "loss": 0.0029,
      "step": 113460
    },
    {
      "epoch": 7.564666666666667,
      "grad_norm": 0.30451086163520813,
      "learning_rate": 2.7208333333333337e-06,
      "loss": 0.0016,
      "step": 113470
    },
    {
      "epoch": 7.565333333333333,
      "grad_norm": 0.08117219060659409,
      "learning_rate": 2.7166666666666668e-06,
      "loss": 0.0015,
      "step": 113480
    },
    {
      "epoch": 7.566,
      "grad_norm": 0.043470680713653564,
      "learning_rate": 2.7125000000000003e-06,
      "loss": 0.0019,
      "step": 113490
    },
    {
      "epoch": 7.566666666666666,
      "grad_norm": 0.05999549478292465,
      "learning_rate": 2.7083333333333334e-06,
      "loss": 0.0017,
      "step": 113500
    },
    {
      "epoch": 7.567333333333333,
      "grad_norm": 0.027106279507279396,
      "learning_rate": 2.7041666666666665e-06,
      "loss": 0.0014,
      "step": 113510
    },
    {
      "epoch": 7.568,
      "grad_norm": 0.14165692031383514,
      "learning_rate": 2.7e-06,
      "loss": 0.002,
      "step": 113520
    },
    {
      "epoch": 7.568666666666667,
      "grad_norm": 0.16936080157756805,
      "learning_rate": 2.6958333333333335e-06,
      "loss": 0.0021,
      "step": 113530
    },
    {
      "epoch": 7.569333333333334,
      "grad_norm": 0.3037468194961548,
      "learning_rate": 2.6916666666666666e-06,
      "loss": 0.0021,
      "step": 113540
    },
    {
      "epoch": 7.57,
      "grad_norm": 0.30852746963500977,
      "learning_rate": 2.6875e-06,
      "loss": 0.002,
      "step": 113550
    },
    {
      "epoch": 7.570666666666667,
      "grad_norm": 0.203420490026474,
      "learning_rate": 2.6833333333333336e-06,
      "loss": 0.0017,
      "step": 113560
    },
    {
      "epoch": 7.5713333333333335,
      "grad_norm": 0.4667782187461853,
      "learning_rate": 2.6791666666666667e-06,
      "loss": 0.0017,
      "step": 113570
    },
    {
      "epoch": 7.572,
      "grad_norm": 0.07941081374883652,
      "learning_rate": 2.6750000000000002e-06,
      "loss": 0.0013,
      "step": 113580
    },
    {
      "epoch": 7.572666666666667,
      "grad_norm": 0.036692164838314056,
      "learning_rate": 2.6708333333333333e-06,
      "loss": 0.002,
      "step": 113590
    },
    {
      "epoch": 7.573333333333333,
      "grad_norm": 0.4178779423236847,
      "learning_rate": 2.666666666666667e-06,
      "loss": 0.0015,
      "step": 113600
    },
    {
      "epoch": 7.574,
      "grad_norm": 0.4048239588737488,
      "learning_rate": 2.6625e-06,
      "loss": 0.0016,
      "step": 113610
    },
    {
      "epoch": 7.574666666666666,
      "grad_norm": 0.0490230955183506,
      "learning_rate": 2.6583333333333334e-06,
      "loss": 0.0019,
      "step": 113620
    },
    {
      "epoch": 7.575333333333333,
      "grad_norm": 0.05103493481874466,
      "learning_rate": 2.654166666666667e-06,
      "loss": 0.0014,
      "step": 113630
    },
    {
      "epoch": 7.576,
      "grad_norm": 0.1673811674118042,
      "learning_rate": 2.65e-06,
      "loss": 0.0012,
      "step": 113640
    },
    {
      "epoch": 7.576666666666666,
      "grad_norm": 0.1341150403022766,
      "learning_rate": 2.6458333333333336e-06,
      "loss": 0.0013,
      "step": 113650
    },
    {
      "epoch": 7.577333333333334,
      "grad_norm": 0.4012725353240967,
      "learning_rate": 2.641666666666667e-06,
      "loss": 0.0014,
      "step": 113660
    },
    {
      "epoch": 7.578,
      "grad_norm": 0.3626980483531952,
      "learning_rate": 2.6375e-06,
      "loss": 0.0016,
      "step": 113670
    },
    {
      "epoch": 7.578666666666667,
      "grad_norm": 0.06882257759571075,
      "learning_rate": 2.6333333333333337e-06,
      "loss": 0.0017,
      "step": 113680
    },
    {
      "epoch": 7.5793333333333335,
      "grad_norm": 0.17608840763568878,
      "learning_rate": 2.6291666666666668e-06,
      "loss": 0.0022,
      "step": 113690
    },
    {
      "epoch": 7.58,
      "grad_norm": 0.07913298904895782,
      "learning_rate": 2.625e-06,
      "loss": 0.0017,
      "step": 113700
    },
    {
      "epoch": 7.580666666666667,
      "grad_norm": 0.4130643904209137,
      "learning_rate": 2.6208333333333334e-06,
      "loss": 0.0014,
      "step": 113710
    },
    {
      "epoch": 7.581333333333333,
      "grad_norm": 0.36206483840942383,
      "learning_rate": 2.616666666666667e-06,
      "loss": 0.0016,
      "step": 113720
    },
    {
      "epoch": 7.582,
      "grad_norm": 0.05495543032884598,
      "learning_rate": 2.6125e-06,
      "loss": 0.0013,
      "step": 113730
    },
    {
      "epoch": 7.582666666666666,
      "grad_norm": 0.03506871685385704,
      "learning_rate": 2.6083333333333335e-06,
      "loss": 0.0017,
      "step": 113740
    },
    {
      "epoch": 7.583333333333333,
      "grad_norm": 0.2672896683216095,
      "learning_rate": 2.604166666666667e-06,
      "loss": 0.0036,
      "step": 113750
    },
    {
      "epoch": 7.584,
      "grad_norm": 0.049982234835624695,
      "learning_rate": 2.6e-06,
      "loss": 0.0022,
      "step": 113760
    },
    {
      "epoch": 7.584666666666667,
      "grad_norm": 0.39792317152023315,
      "learning_rate": 2.5958333333333336e-06,
      "loss": 0.0015,
      "step": 113770
    },
    {
      "epoch": 7.585333333333334,
      "grad_norm": 0.10781354457139969,
      "learning_rate": 2.5916666666666667e-06,
      "loss": 0.0015,
      "step": 113780
    },
    {
      "epoch": 7.586,
      "grad_norm": 0.20757001638412476,
      "learning_rate": 2.5875e-06,
      "loss": 0.002,
      "step": 113790
    },
    {
      "epoch": 7.586666666666667,
      "grad_norm": 0.11419447511434555,
      "learning_rate": 2.5833333333333333e-06,
      "loss": 0.0019,
      "step": 113800
    },
    {
      "epoch": 7.5873333333333335,
      "grad_norm": 0.13775044679641724,
      "learning_rate": 2.579166666666667e-06,
      "loss": 0.0016,
      "step": 113810
    },
    {
      "epoch": 7.588,
      "grad_norm": 0.17681467533111572,
      "learning_rate": 2.575e-06,
      "loss": 0.0019,
      "step": 113820
    },
    {
      "epoch": 7.588666666666667,
      "grad_norm": 0.19407148659229279,
      "learning_rate": 2.5708333333333334e-06,
      "loss": 0.0015,
      "step": 113830
    },
    {
      "epoch": 7.589333333333333,
      "grad_norm": 0.061086345463991165,
      "learning_rate": 2.566666666666667e-06,
      "loss": 0.0027,
      "step": 113840
    },
    {
      "epoch": 7.59,
      "grad_norm": 0.15572616457939148,
      "learning_rate": 2.5625e-06,
      "loss": 0.0014,
      "step": 113850
    },
    {
      "epoch": 7.5906666666666665,
      "grad_norm": 0.5086953639984131,
      "learning_rate": 2.5583333333333335e-06,
      "loss": 0.0021,
      "step": 113860
    },
    {
      "epoch": 7.591333333333333,
      "grad_norm": 0.7662193775177002,
      "learning_rate": 2.554166666666667e-06,
      "loss": 0.0021,
      "step": 113870
    },
    {
      "epoch": 7.592,
      "grad_norm": 0.049630384892225266,
      "learning_rate": 2.55e-06,
      "loss": 0.0015,
      "step": 113880
    },
    {
      "epoch": 7.592666666666666,
      "grad_norm": 0.365734726190567,
      "learning_rate": 2.5458333333333332e-06,
      "loss": 0.0015,
      "step": 113890
    },
    {
      "epoch": 7.593333333333334,
      "grad_norm": 0.08652162551879883,
      "learning_rate": 2.5416666666666668e-06,
      "loss": 0.0022,
      "step": 113900
    },
    {
      "epoch": 7.594,
      "grad_norm": 0.04922249913215637,
      "learning_rate": 2.5375000000000003e-06,
      "loss": 0.0023,
      "step": 113910
    },
    {
      "epoch": 7.594666666666667,
      "grad_norm": 0.3008594810962677,
      "learning_rate": 2.5333333333333334e-06,
      "loss": 0.0016,
      "step": 113920
    },
    {
      "epoch": 7.5953333333333335,
      "grad_norm": 0.04552631452679634,
      "learning_rate": 2.529166666666667e-06,
      "loss": 0.0019,
      "step": 113930
    },
    {
      "epoch": 7.596,
      "grad_norm": 0.30548161268234253,
      "learning_rate": 2.5250000000000004e-06,
      "loss": 0.0016,
      "step": 113940
    },
    {
      "epoch": 7.596666666666667,
      "grad_norm": 0.17120693624019623,
      "learning_rate": 2.5208333333333335e-06,
      "loss": 0.0019,
      "step": 113950
    },
    {
      "epoch": 7.597333333333333,
      "grad_norm": 0.10012631118297577,
      "learning_rate": 2.516666666666667e-06,
      "loss": 0.0017,
      "step": 113960
    },
    {
      "epoch": 7.598,
      "grad_norm": 0.16674266755580902,
      "learning_rate": 2.5125e-06,
      "loss": 0.0023,
      "step": 113970
    },
    {
      "epoch": 7.5986666666666665,
      "grad_norm": 0.16912394762039185,
      "learning_rate": 2.508333333333333e-06,
      "loss": 0.0015,
      "step": 113980
    },
    {
      "epoch": 7.599333333333333,
      "grad_norm": 0.13535283505916595,
      "learning_rate": 2.5041666666666667e-06,
      "loss": 0.0011,
      "step": 113990
    },
    {
      "epoch": 7.6,
      "grad_norm": 0.03620361164212227,
      "learning_rate": 2.5e-06,
      "loss": 0.0014,
      "step": 114000
    },
    {
      "epoch": 7.600666666666667,
      "grad_norm": 0.04673417657613754,
      "learning_rate": 2.4958333333333333e-06,
      "loss": 0.0023,
      "step": 114010
    },
    {
      "epoch": 7.601333333333334,
      "grad_norm": 0.0773167535662651,
      "learning_rate": 2.491666666666667e-06,
      "loss": 0.0014,
      "step": 114020
    },
    {
      "epoch": 7.602,
      "grad_norm": 0.04278959706425667,
      "learning_rate": 2.4875000000000003e-06,
      "loss": 0.0016,
      "step": 114030
    },
    {
      "epoch": 7.602666666666667,
      "grad_norm": 0.3299235999584198,
      "learning_rate": 2.4833333333333334e-06,
      "loss": 0.0012,
      "step": 114040
    },
    {
      "epoch": 7.6033333333333335,
      "grad_norm": 0.26598411798477173,
      "learning_rate": 2.479166666666667e-06,
      "loss": 0.0014,
      "step": 114050
    },
    {
      "epoch": 7.604,
      "grad_norm": 0.1326955109834671,
      "learning_rate": 2.4750000000000004e-06,
      "loss": 0.0014,
      "step": 114060
    },
    {
      "epoch": 7.604666666666667,
      "grad_norm": 0.0504591204226017,
      "learning_rate": 2.4708333333333335e-06,
      "loss": 0.0019,
      "step": 114070
    },
    {
      "epoch": 7.605333333333333,
      "grad_norm": 0.0694945752620697,
      "learning_rate": 2.4666666666666666e-06,
      "loss": 0.0021,
      "step": 114080
    },
    {
      "epoch": 7.606,
      "grad_norm": 0.10589371621608734,
      "learning_rate": 2.4625e-06,
      "loss": 0.0015,
      "step": 114090
    },
    {
      "epoch": 7.6066666666666665,
      "grad_norm": 0.048336856067180634,
      "learning_rate": 2.4583333333333332e-06,
      "loss": 0.0014,
      "step": 114100
    },
    {
      "epoch": 7.607333333333333,
      "grad_norm": 0.07241766154766083,
      "learning_rate": 2.4541666666666667e-06,
      "loss": 0.0015,
      "step": 114110
    },
    {
      "epoch": 7.608,
      "grad_norm": 0.05032572150230408,
      "learning_rate": 2.4500000000000003e-06,
      "loss": 0.0014,
      "step": 114120
    },
    {
      "epoch": 7.608666666666666,
      "grad_norm": 0.1701914668083191,
      "learning_rate": 2.4458333333333334e-06,
      "loss": 0.0024,
      "step": 114130
    },
    {
      "epoch": 7.609333333333334,
      "grad_norm": 0.1711592674255371,
      "learning_rate": 2.441666666666667e-06,
      "loss": 0.0014,
      "step": 114140
    },
    {
      "epoch": 7.61,
      "grad_norm": 0.10064616054296494,
      "learning_rate": 2.4375000000000004e-06,
      "loss": 0.0014,
      "step": 114150
    },
    {
      "epoch": 7.610666666666667,
      "grad_norm": 0.30235567688941956,
      "learning_rate": 2.4333333333333335e-06,
      "loss": 0.0018,
      "step": 114160
    },
    {
      "epoch": 7.6113333333333335,
      "grad_norm": 0.05719460919499397,
      "learning_rate": 2.4291666666666666e-06,
      "loss": 0.0017,
      "step": 114170
    },
    {
      "epoch": 7.612,
      "grad_norm": 0.16527041792869568,
      "learning_rate": 2.425e-06,
      "loss": 0.0018,
      "step": 114180
    },
    {
      "epoch": 7.612666666666667,
      "grad_norm": 0.5596438050270081,
      "learning_rate": 2.420833333333333e-06,
      "loss": 0.0017,
      "step": 114190
    },
    {
      "epoch": 7.613333333333333,
      "grad_norm": 0.44609400629997253,
      "learning_rate": 2.4166666666666667e-06,
      "loss": 0.0032,
      "step": 114200
    },
    {
      "epoch": 7.614,
      "grad_norm": 0.17028015851974487,
      "learning_rate": 2.4125e-06,
      "loss": 0.0015,
      "step": 114210
    },
    {
      "epoch": 7.6146666666666665,
      "grad_norm": 0.20695045590400696,
      "learning_rate": 2.4083333333333337e-06,
      "loss": 0.0015,
      "step": 114220
    },
    {
      "epoch": 7.615333333333333,
      "grad_norm": 0.13484488427639008,
      "learning_rate": 2.404166666666667e-06,
      "loss": 0.0014,
      "step": 114230
    },
    {
      "epoch": 7.616,
      "grad_norm": 0.20007076859474182,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.0012,
      "step": 114240
    },
    {
      "epoch": 7.616666666666667,
      "grad_norm": 0.203544482588768,
      "learning_rate": 2.3958333333333334e-06,
      "loss": 0.0014,
      "step": 114250
    },
    {
      "epoch": 7.617333333333333,
      "grad_norm": 0.44549548625946045,
      "learning_rate": 2.3916666666666665e-06,
      "loss": 0.0017,
      "step": 114260
    },
    {
      "epoch": 7.618,
      "grad_norm": 0.12034425884485245,
      "learning_rate": 2.3875e-06,
      "loss": 0.0021,
      "step": 114270
    },
    {
      "epoch": 7.618666666666667,
      "grad_norm": 0.1672801971435547,
      "learning_rate": 2.3833333333333335e-06,
      "loss": 0.0026,
      "step": 114280
    },
    {
      "epoch": 7.6193333333333335,
      "grad_norm": 0.17231939733028412,
      "learning_rate": 2.3791666666666666e-06,
      "loss": 0.0012,
      "step": 114290
    },
    {
      "epoch": 7.62,
      "grad_norm": 0.10490797460079193,
      "learning_rate": 2.375e-06,
      "loss": 0.0015,
      "step": 114300
    },
    {
      "epoch": 7.620666666666667,
      "grad_norm": 0.5058146119117737,
      "learning_rate": 2.3708333333333336e-06,
      "loss": 0.0025,
      "step": 114310
    },
    {
      "epoch": 7.621333333333333,
      "grad_norm": 0.33772504329681396,
      "learning_rate": 2.3666666666666667e-06,
      "loss": 0.0014,
      "step": 114320
    },
    {
      "epoch": 7.622,
      "grad_norm": 0.4273172616958618,
      "learning_rate": 2.3625000000000003e-06,
      "loss": 0.002,
      "step": 114330
    },
    {
      "epoch": 7.6226666666666665,
      "grad_norm": 0.5345783829689026,
      "learning_rate": 2.3583333333333338e-06,
      "loss": 0.0011,
      "step": 114340
    },
    {
      "epoch": 7.623333333333333,
      "grad_norm": 0.2649001181125641,
      "learning_rate": 2.354166666666667e-06,
      "loss": 0.0024,
      "step": 114350
    },
    {
      "epoch": 7.624,
      "grad_norm": 0.03861451521515846,
      "learning_rate": 2.35e-06,
      "loss": 0.0014,
      "step": 114360
    },
    {
      "epoch": 7.624666666666666,
      "grad_norm": 0.18082380294799805,
      "learning_rate": 2.3458333333333335e-06,
      "loss": 0.0016,
      "step": 114370
    },
    {
      "epoch": 7.625333333333334,
      "grad_norm": 0.04454871267080307,
      "learning_rate": 2.3416666666666666e-06,
      "loss": 0.002,
      "step": 114380
    },
    {
      "epoch": 7.626,
      "grad_norm": 0.04771003499627113,
      "learning_rate": 2.3375e-06,
      "loss": 0.0015,
      "step": 114390
    },
    {
      "epoch": 7.626666666666667,
      "grad_norm": 0.2698298990726471,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 0.0024,
      "step": 114400
    },
    {
      "epoch": 7.6273333333333335,
      "grad_norm": 0.33267754316329956,
      "learning_rate": 2.3291666666666667e-06,
      "loss": 0.0019,
      "step": 114410
    },
    {
      "epoch": 7.628,
      "grad_norm": 0.1661703735589981,
      "learning_rate": 2.325e-06,
      "loss": 0.0019,
      "step": 114420
    },
    {
      "epoch": 7.628666666666667,
      "grad_norm": 0.2356673777103424,
      "learning_rate": 2.3208333333333337e-06,
      "loss": 0.0021,
      "step": 114430
    },
    {
      "epoch": 7.629333333333333,
      "grad_norm": 0.2260972559452057,
      "learning_rate": 2.316666666666667e-06,
      "loss": 0.002,
      "step": 114440
    },
    {
      "epoch": 7.63,
      "grad_norm": 0.025877943262457848,
      "learning_rate": 2.3125e-06,
      "loss": 0.0013,
      "step": 114450
    },
    {
      "epoch": 7.6306666666666665,
      "grad_norm": 0.045691099017858505,
      "learning_rate": 2.3083333333333334e-06,
      "loss": 0.0016,
      "step": 114460
    },
    {
      "epoch": 7.631333333333333,
      "grad_norm": 0.1979176253080368,
      "learning_rate": 2.3041666666666665e-06,
      "loss": 0.0015,
      "step": 114470
    },
    {
      "epoch": 7.632,
      "grad_norm": 0.31152039766311646,
      "learning_rate": 2.3e-06,
      "loss": 0.0011,
      "step": 114480
    },
    {
      "epoch": 7.632666666666667,
      "grad_norm": 0.23248836398124695,
      "learning_rate": 2.2958333333333335e-06,
      "loss": 0.0029,
      "step": 114490
    },
    {
      "epoch": 7.633333333333333,
      "grad_norm": 0.0509018711745739,
      "learning_rate": 2.2916666666666666e-06,
      "loss": 0.0021,
      "step": 114500
    },
    {
      "epoch": 7.634,
      "grad_norm": 0.14277297258377075,
      "learning_rate": 2.2875e-06,
      "loss": 0.0017,
      "step": 114510
    },
    {
      "epoch": 7.634666666666667,
      "grad_norm": 0.13656756281852722,
      "learning_rate": 2.2833333333333336e-06,
      "loss": 0.0014,
      "step": 114520
    },
    {
      "epoch": 7.6353333333333335,
      "grad_norm": 0.15513134002685547,
      "learning_rate": 2.2791666666666667e-06,
      "loss": 0.0012,
      "step": 114530
    },
    {
      "epoch": 7.636,
      "grad_norm": 0.07408144325017929,
      "learning_rate": 2.2750000000000002e-06,
      "loss": 0.0014,
      "step": 114540
    },
    {
      "epoch": 7.636666666666667,
      "grad_norm": 0.07342077791690826,
      "learning_rate": 2.2708333333333333e-06,
      "loss": 0.0019,
      "step": 114550
    },
    {
      "epoch": 7.637333333333333,
      "grad_norm": 0.08037178963422775,
      "learning_rate": 2.266666666666667e-06,
      "loss": 0.0017,
      "step": 114560
    },
    {
      "epoch": 7.638,
      "grad_norm": 0.17330831289291382,
      "learning_rate": 2.2625e-06,
      "loss": 0.0013,
      "step": 114570
    },
    {
      "epoch": 7.6386666666666665,
      "grad_norm": 0.3044794797897339,
      "learning_rate": 2.2583333333333335e-06,
      "loss": 0.0016,
      "step": 114580
    },
    {
      "epoch": 7.639333333333333,
      "grad_norm": 0.07307300716638565,
      "learning_rate": 2.254166666666667e-06,
      "loss": 0.0018,
      "step": 114590
    },
    {
      "epoch": 7.64,
      "grad_norm": 0.10585537552833557,
      "learning_rate": 2.25e-06,
      "loss": 0.002,
      "step": 114600
    },
    {
      "epoch": 7.640666666666666,
      "grad_norm": 0.13761544227600098,
      "learning_rate": 2.2458333333333336e-06,
      "loss": 0.0011,
      "step": 114610
    },
    {
      "epoch": 7.641333333333334,
      "grad_norm": 0.13775257766246796,
      "learning_rate": 2.241666666666667e-06,
      "loss": 0.0014,
      "step": 114620
    },
    {
      "epoch": 7.642,
      "grad_norm": 0.43434402346611023,
      "learning_rate": 2.2375e-06,
      "loss": 0.0011,
      "step": 114630
    },
    {
      "epoch": 7.642666666666667,
      "grad_norm": 0.20317316055297852,
      "learning_rate": 2.2333333333333333e-06,
      "loss": 0.0022,
      "step": 114640
    },
    {
      "epoch": 7.6433333333333335,
      "grad_norm": 0.21304656565189362,
      "learning_rate": 2.2291666666666668e-06,
      "loss": 0.0021,
      "step": 114650
    },
    {
      "epoch": 7.644,
      "grad_norm": 0.14266632497310638,
      "learning_rate": 2.225e-06,
      "loss": 0.0025,
      "step": 114660
    },
    {
      "epoch": 7.644666666666667,
      "grad_norm": 0.46827566623687744,
      "learning_rate": 2.2208333333333334e-06,
      "loss": 0.002,
      "step": 114670
    },
    {
      "epoch": 7.645333333333333,
      "grad_norm": 0.541534423828125,
      "learning_rate": 2.216666666666667e-06,
      "loss": 0.0013,
      "step": 114680
    },
    {
      "epoch": 7.646,
      "grad_norm": 0.042554233223199844,
      "learning_rate": 2.2125e-06,
      "loss": 0.0023,
      "step": 114690
    },
    {
      "epoch": 7.6466666666666665,
      "grad_norm": 0.23405134677886963,
      "learning_rate": 2.2083333333333335e-06,
      "loss": 0.0016,
      "step": 114700
    },
    {
      "epoch": 7.647333333333333,
      "grad_norm": 0.062151774764060974,
      "learning_rate": 2.204166666666667e-06,
      "loss": 0.0015,
      "step": 114710
    },
    {
      "epoch": 7.648,
      "grad_norm": 0.3147731125354767,
      "learning_rate": 2.2e-06,
      "loss": 0.0026,
      "step": 114720
    },
    {
      "epoch": 7.648666666666666,
      "grad_norm": 0.16706903278827667,
      "learning_rate": 2.1958333333333336e-06,
      "loss": 0.0024,
      "step": 114730
    },
    {
      "epoch": 7.649333333333333,
      "grad_norm": 0.46965277194976807,
      "learning_rate": 2.1916666666666667e-06,
      "loss": 0.0021,
      "step": 114740
    },
    {
      "epoch": 7.65,
      "grad_norm": 0.07541657984256744,
      "learning_rate": 2.1875e-06,
      "loss": 0.0016,
      "step": 114750
    },
    {
      "epoch": 7.650666666666667,
      "grad_norm": 0.2436707615852356,
      "learning_rate": 2.1833333333333333e-06,
      "loss": 0.0023,
      "step": 114760
    },
    {
      "epoch": 7.6513333333333335,
      "grad_norm": 0.40129992365837097,
      "learning_rate": 2.179166666666667e-06,
      "loss": 0.0018,
      "step": 114770
    },
    {
      "epoch": 7.652,
      "grad_norm": 0.3779049515724182,
      "learning_rate": 2.175e-06,
      "loss": 0.0018,
      "step": 114780
    },
    {
      "epoch": 7.652666666666667,
      "grad_norm": 0.1399356871843338,
      "learning_rate": 2.1708333333333334e-06,
      "loss": 0.0021,
      "step": 114790
    },
    {
      "epoch": 7.653333333333333,
      "grad_norm": 0.5278031826019287,
      "learning_rate": 2.166666666666667e-06,
      "loss": 0.002,
      "step": 114800
    },
    {
      "epoch": 7.654,
      "grad_norm": 0.1003662496805191,
      "learning_rate": 2.1625e-06,
      "loss": 0.0018,
      "step": 114810
    },
    {
      "epoch": 7.6546666666666665,
      "grad_norm": 0.08236805349588394,
      "learning_rate": 2.1583333333333336e-06,
      "loss": 0.0026,
      "step": 114820
    },
    {
      "epoch": 7.655333333333333,
      "grad_norm": 0.2471623718738556,
      "learning_rate": 2.1541666666666667e-06,
      "loss": 0.0021,
      "step": 114830
    },
    {
      "epoch": 7.656,
      "grad_norm": 0.16872377693653107,
      "learning_rate": 2.1499999999999997e-06,
      "loss": 0.0013,
      "step": 114840
    },
    {
      "epoch": 7.656666666666666,
      "grad_norm": 0.07197614014148712,
      "learning_rate": 2.1458333333333333e-06,
      "loss": 0.0014,
      "step": 114850
    },
    {
      "epoch": 7.657333333333334,
      "grad_norm": 0.23989152908325195,
      "learning_rate": 2.1416666666666668e-06,
      "loss": 0.0022,
      "step": 114860
    },
    {
      "epoch": 7.658,
      "grad_norm": 0.04724283888936043,
      "learning_rate": 2.1375000000000003e-06,
      "loss": 0.0023,
      "step": 114870
    },
    {
      "epoch": 7.658666666666667,
      "grad_norm": 0.08332374691963196,
      "learning_rate": 2.1333333333333334e-06,
      "loss": 0.0018,
      "step": 114880
    },
    {
      "epoch": 7.6593333333333335,
      "grad_norm": 0.3345639705657959,
      "learning_rate": 2.129166666666667e-06,
      "loss": 0.0018,
      "step": 114890
    },
    {
      "epoch": 7.66,
      "grad_norm": 0.14121972024440765,
      "learning_rate": 2.1250000000000004e-06,
      "loss": 0.0015,
      "step": 114900
    },
    {
      "epoch": 7.660666666666667,
      "grad_norm": 0.4153120517730713,
      "learning_rate": 2.1208333333333335e-06,
      "loss": 0.0011,
      "step": 114910
    },
    {
      "epoch": 7.661333333333333,
      "grad_norm": 0.2085028439760208,
      "learning_rate": 2.1166666666666666e-06,
      "loss": 0.0019,
      "step": 114920
    },
    {
      "epoch": 7.662,
      "grad_norm": 0.1585126519203186,
      "learning_rate": 2.1125e-06,
      "loss": 0.0013,
      "step": 114930
    },
    {
      "epoch": 7.6626666666666665,
      "grad_norm": 0.08244676887989044,
      "learning_rate": 2.108333333333333e-06,
      "loss": 0.0022,
      "step": 114940
    },
    {
      "epoch": 7.663333333333333,
      "grad_norm": 0.16715852916240692,
      "learning_rate": 2.1041666666666667e-06,
      "loss": 0.0016,
      "step": 114950
    },
    {
      "epoch": 7.664,
      "grad_norm": 0.04204133525490761,
      "learning_rate": 2.1000000000000002e-06,
      "loss": 0.0019,
      "step": 114960
    },
    {
      "epoch": 7.664666666666666,
      "grad_norm": 0.08481565117835999,
      "learning_rate": 2.0958333333333333e-06,
      "loss": 0.0018,
      "step": 114970
    },
    {
      "epoch": 7.665333333333333,
      "grad_norm": 0.3009417653083801,
      "learning_rate": 2.091666666666667e-06,
      "loss": 0.0027,
      "step": 114980
    },
    {
      "epoch": 7.666,
      "grad_norm": 0.33891797065734863,
      "learning_rate": 2.0875000000000003e-06,
      "loss": 0.0019,
      "step": 114990
    },
    {
      "epoch": 7.666666666666667,
      "grad_norm": 0.1254703253507614,
      "learning_rate": 2.0833333333333334e-06,
      "loss": 0.002,
      "step": 115000
    },
    {
      "epoch": 7.667333333333334,
      "grad_norm": 0.13943909108638763,
      "learning_rate": 2.079166666666667e-06,
      "loss": 0.0019,
      "step": 115010
    },
    {
      "epoch": 7.668,
      "grad_norm": 0.7112028002738953,
      "learning_rate": 2.075e-06,
      "loss": 0.002,
      "step": 115020
    },
    {
      "epoch": 7.668666666666667,
      "grad_norm": 0.10648886114358902,
      "learning_rate": 2.070833333333333e-06,
      "loss": 0.0017,
      "step": 115030
    },
    {
      "epoch": 7.669333333333333,
      "grad_norm": 0.2077137678861618,
      "learning_rate": 2.0666666666666666e-06,
      "loss": 0.002,
      "step": 115040
    },
    {
      "epoch": 7.67,
      "grad_norm": 0.27357104420661926,
      "learning_rate": 2.0625e-06,
      "loss": 0.0015,
      "step": 115050
    },
    {
      "epoch": 7.6706666666666665,
      "grad_norm": 0.07194704562425613,
      "learning_rate": 2.0583333333333332e-06,
      "loss": 0.0016,
      "step": 115060
    },
    {
      "epoch": 7.671333333333333,
      "grad_norm": 0.14386412501335144,
      "learning_rate": 2.0541666666666668e-06,
      "loss": 0.0014,
      "step": 115070
    },
    {
      "epoch": 7.672,
      "grad_norm": 0.4475597143173218,
      "learning_rate": 2.0500000000000003e-06,
      "loss": 0.0019,
      "step": 115080
    },
    {
      "epoch": 7.672666666666666,
      "grad_norm": 0.3028668165206909,
      "learning_rate": 2.0458333333333334e-06,
      "loss": 0.0019,
      "step": 115090
    },
    {
      "epoch": 7.673333333333334,
      "grad_norm": 0.14306344091892242,
      "learning_rate": 2.041666666666667e-06,
      "loss": 0.0016,
      "step": 115100
    },
    {
      "epoch": 7.674,
      "grad_norm": 0.46882307529449463,
      "learning_rate": 2.0375e-06,
      "loss": 0.0017,
      "step": 115110
    },
    {
      "epoch": 7.674666666666667,
      "grad_norm": 0.10417646914720535,
      "learning_rate": 2.033333333333333e-06,
      "loss": 0.0026,
      "step": 115120
    },
    {
      "epoch": 7.675333333333334,
      "grad_norm": 0.22889938950538635,
      "learning_rate": 2.0291666666666666e-06,
      "loss": 0.0012,
      "step": 115130
    },
    {
      "epoch": 7.676,
      "grad_norm": 0.4041948914527893,
      "learning_rate": 2.025e-06,
      "loss": 0.002,
      "step": 115140
    },
    {
      "epoch": 7.676666666666667,
      "grad_norm": 0.03928175941109657,
      "learning_rate": 2.020833333333333e-06,
      "loss": 0.0013,
      "step": 115150
    },
    {
      "epoch": 7.677333333333333,
      "grad_norm": 0.13234248757362366,
      "learning_rate": 2.0166666666666667e-06,
      "loss": 0.0014,
      "step": 115160
    },
    {
      "epoch": 7.678,
      "grad_norm": 0.36568576097488403,
      "learning_rate": 2.0125000000000002e-06,
      "loss": 0.0014,
      "step": 115170
    },
    {
      "epoch": 7.6786666666666665,
      "grad_norm": 0.10420208424329758,
      "learning_rate": 2.0083333333333337e-06,
      "loss": 0.0022,
      "step": 115180
    },
    {
      "epoch": 7.679333333333333,
      "grad_norm": 0.10404866188764572,
      "learning_rate": 2.004166666666667e-06,
      "loss": 0.0019,
      "step": 115190
    },
    {
      "epoch": 7.68,
      "grad_norm": 0.10668179392814636,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.0022,
      "step": 115200
    },
    {
      "epoch": 7.680666666666666,
      "grad_norm": 0.20404179394245148,
      "learning_rate": 1.9958333333333334e-06,
      "loss": 0.0023,
      "step": 115210
    },
    {
      "epoch": 7.681333333333333,
      "grad_norm": 0.342033326625824,
      "learning_rate": 1.9916666666666665e-06,
      "loss": 0.0015,
      "step": 115220
    },
    {
      "epoch": 7.682,
      "grad_norm": 0.1286139190196991,
      "learning_rate": 1.9875e-06,
      "loss": 0.0015,
      "step": 115230
    },
    {
      "epoch": 7.682666666666667,
      "grad_norm": 0.19797484576702118,
      "learning_rate": 1.9833333333333335e-06,
      "loss": 0.0014,
      "step": 115240
    },
    {
      "epoch": 7.683333333333334,
      "grad_norm": 0.1767290234565735,
      "learning_rate": 1.9791666666666666e-06,
      "loss": 0.0013,
      "step": 115250
    },
    {
      "epoch": 7.684,
      "grad_norm": 0.13342462480068207,
      "learning_rate": 1.975e-06,
      "loss": 0.0012,
      "step": 115260
    },
    {
      "epoch": 7.684666666666667,
      "grad_norm": 0.10344315320253372,
      "learning_rate": 1.9708333333333337e-06,
      "loss": 0.0021,
      "step": 115270
    },
    {
      "epoch": 7.685333333333333,
      "grad_norm": 0.39853158593177795,
      "learning_rate": 1.9666666666666668e-06,
      "loss": 0.0014,
      "step": 115280
    },
    {
      "epoch": 7.686,
      "grad_norm": 0.10918846726417542,
      "learning_rate": 1.9625000000000003e-06,
      "loss": 0.0015,
      "step": 115290
    },
    {
      "epoch": 7.6866666666666665,
      "grad_norm": 0.36555829644203186,
      "learning_rate": 1.9583333333333334e-06,
      "loss": 0.0015,
      "step": 115300
    },
    {
      "epoch": 7.687333333333333,
      "grad_norm": 0.6331576704978943,
      "learning_rate": 1.9541666666666665e-06,
      "loss": 0.0022,
      "step": 115310
    },
    {
      "epoch": 7.688,
      "grad_norm": 0.07066775113344193,
      "learning_rate": 1.95e-06,
      "loss": 0.0012,
      "step": 115320
    },
    {
      "epoch": 7.688666666666666,
      "grad_norm": 0.1853678822517395,
      "learning_rate": 1.9458333333333335e-06,
      "loss": 0.0013,
      "step": 115330
    },
    {
      "epoch": 7.689333333333334,
      "grad_norm": 0.0653720572590828,
      "learning_rate": 1.9416666666666666e-06,
      "loss": 0.0012,
      "step": 115340
    },
    {
      "epoch": 7.6899999999999995,
      "grad_norm": 0.04420517385005951,
      "learning_rate": 1.9375e-06,
      "loss": 0.002,
      "step": 115350
    },
    {
      "epoch": 7.690666666666667,
      "grad_norm": 0.05002978816628456,
      "learning_rate": 1.9333333333333336e-06,
      "loss": 0.0016,
      "step": 115360
    },
    {
      "epoch": 7.691333333333334,
      "grad_norm": 0.23988699913024902,
      "learning_rate": 1.9291666666666667e-06,
      "loss": 0.0016,
      "step": 115370
    },
    {
      "epoch": 7.692,
      "grad_norm": 0.5674712657928467,
      "learning_rate": 1.925e-06,
      "loss": 0.002,
      "step": 115380
    },
    {
      "epoch": 7.692666666666667,
      "grad_norm": 0.11221596598625183,
      "learning_rate": 1.9208333333333337e-06,
      "loss": 0.0014,
      "step": 115390
    },
    {
      "epoch": 7.693333333333333,
      "grad_norm": 0.17255933582782745,
      "learning_rate": 1.916666666666667e-06,
      "loss": 0.0023,
      "step": 115400
    },
    {
      "epoch": 7.694,
      "grad_norm": 0.36868172883987427,
      "learning_rate": 1.9125e-06,
      "loss": 0.0018,
      "step": 115410
    },
    {
      "epoch": 7.6946666666666665,
      "grad_norm": 0.18348035216331482,
      "learning_rate": 1.9083333333333334e-06,
      "loss": 0.0015,
      "step": 115420
    },
    {
      "epoch": 7.695333333333333,
      "grad_norm": 0.08184531331062317,
      "learning_rate": 1.9041666666666665e-06,
      "loss": 0.0021,
      "step": 115430
    },
    {
      "epoch": 7.696,
      "grad_norm": 0.7018299102783203,
      "learning_rate": 1.9e-06,
      "loss": 0.002,
      "step": 115440
    },
    {
      "epoch": 7.696666666666666,
      "grad_norm": 0.622586727142334,
      "learning_rate": 1.8958333333333335e-06,
      "loss": 0.002,
      "step": 115450
    },
    {
      "epoch": 7.697333333333333,
      "grad_norm": 0.134257510304451,
      "learning_rate": 1.8916666666666666e-06,
      "loss": 0.0019,
      "step": 115460
    },
    {
      "epoch": 7.698,
      "grad_norm": 0.3035748302936554,
      "learning_rate": 1.8875e-06,
      "loss": 0.002,
      "step": 115470
    },
    {
      "epoch": 7.698666666666667,
      "grad_norm": 0.2694168984889984,
      "learning_rate": 1.8833333333333334e-06,
      "loss": 0.0015,
      "step": 115480
    },
    {
      "epoch": 7.699333333333334,
      "grad_norm": 0.28956151008605957,
      "learning_rate": 1.879166666666667e-06,
      "loss": 0.0024,
      "step": 115490
    },
    {
      "epoch": 7.7,
      "grad_norm": 0.2739223539829254,
      "learning_rate": 1.875e-06,
      "loss": 0.0023,
      "step": 115500
    },
    {
      "epoch": 7.700666666666667,
      "grad_norm": 0.16891366243362427,
      "learning_rate": 1.8708333333333336e-06,
      "loss": 0.0017,
      "step": 115510
    },
    {
      "epoch": 7.701333333333333,
      "grad_norm": 0.06241779029369354,
      "learning_rate": 1.8666666666666669e-06,
      "loss": 0.0025,
      "step": 115520
    },
    {
      "epoch": 7.702,
      "grad_norm": 0.07313459366559982,
      "learning_rate": 1.8625e-06,
      "loss": 0.0019,
      "step": 115530
    },
    {
      "epoch": 7.7026666666666666,
      "grad_norm": 0.03377969563007355,
      "learning_rate": 1.8583333333333335e-06,
      "loss": 0.0013,
      "step": 115540
    },
    {
      "epoch": 7.703333333333333,
      "grad_norm": 0.14014975726604462,
      "learning_rate": 1.854166666666667e-06,
      "loss": 0.0014,
      "step": 115550
    },
    {
      "epoch": 7.704,
      "grad_norm": 0.4662656784057617,
      "learning_rate": 1.85e-06,
      "loss": 0.0027,
      "step": 115560
    },
    {
      "epoch": 7.704666666666666,
      "grad_norm": 0.4165723919868469,
      "learning_rate": 1.8458333333333334e-06,
      "loss": 0.0023,
      "step": 115570
    },
    {
      "epoch": 7.705333333333334,
      "grad_norm": 0.40660035610198975,
      "learning_rate": 1.8416666666666669e-06,
      "loss": 0.0022,
      "step": 115580
    },
    {
      "epoch": 7.7059999999999995,
      "grad_norm": 0.3070729374885559,
      "learning_rate": 1.8375e-06,
      "loss": 0.0022,
      "step": 115590
    },
    {
      "epoch": 7.706666666666667,
      "grad_norm": 0.2067398577928543,
      "learning_rate": 1.8333333333333335e-06,
      "loss": 0.0015,
      "step": 115600
    },
    {
      "epoch": 7.707333333333334,
      "grad_norm": 0.3366977572441101,
      "learning_rate": 1.8291666666666668e-06,
      "loss": 0.0014,
      "step": 115610
    },
    {
      "epoch": 7.708,
      "grad_norm": 0.16992975771427155,
      "learning_rate": 1.8249999999999999e-06,
      "loss": 0.0018,
      "step": 115620
    },
    {
      "epoch": 7.708666666666667,
      "grad_norm": 0.41529911756515503,
      "learning_rate": 1.8208333333333334e-06,
      "loss": 0.0022,
      "step": 115630
    },
    {
      "epoch": 7.709333333333333,
      "grad_norm": 0.04761390760540962,
      "learning_rate": 1.816666666666667e-06,
      "loss": 0.0015,
      "step": 115640
    },
    {
      "epoch": 7.71,
      "grad_norm": 0.16616304218769073,
      "learning_rate": 1.8125e-06,
      "loss": 0.0014,
      "step": 115650
    },
    {
      "epoch": 7.710666666666667,
      "grad_norm": 0.30710458755493164,
      "learning_rate": 1.8083333333333333e-06,
      "loss": 0.0015,
      "step": 115660
    },
    {
      "epoch": 7.711333333333333,
      "grad_norm": 0.33590278029441833,
      "learning_rate": 1.8041666666666668e-06,
      "loss": 0.0022,
      "step": 115670
    },
    {
      "epoch": 7.712,
      "grad_norm": 0.16436190903186798,
      "learning_rate": 1.8e-06,
      "loss": 0.0018,
      "step": 115680
    },
    {
      "epoch": 7.712666666666666,
      "grad_norm": 0.059933990240097046,
      "learning_rate": 1.7958333333333334e-06,
      "loss": 0.0019,
      "step": 115690
    },
    {
      "epoch": 7.713333333333333,
      "grad_norm": 0.19975027441978455,
      "learning_rate": 1.7916666666666667e-06,
      "loss": 0.0023,
      "step": 115700
    },
    {
      "epoch": 7.714,
      "grad_norm": 0.18611149489879608,
      "learning_rate": 1.7875e-06,
      "loss": 0.0014,
      "step": 115710
    },
    {
      "epoch": 7.714666666666667,
      "grad_norm": 0.48503342270851135,
      "learning_rate": 1.7833333333333333e-06,
      "loss": 0.0016,
      "step": 115720
    },
    {
      "epoch": 7.715333333333334,
      "grad_norm": 0.26205235719680786,
      "learning_rate": 1.7791666666666669e-06,
      "loss": 0.0013,
      "step": 115730
    },
    {
      "epoch": 7.716,
      "grad_norm": 0.28560835123062134,
      "learning_rate": 1.775e-06,
      "loss": 0.0019,
      "step": 115740
    },
    {
      "epoch": 7.716666666666667,
      "grad_norm": 0.23919358849525452,
      "learning_rate": 1.7708333333333335e-06,
      "loss": 0.0033,
      "step": 115750
    },
    {
      "epoch": 7.717333333333333,
      "grad_norm": 0.270706444978714,
      "learning_rate": 1.7666666666666668e-06,
      "loss": 0.0018,
      "step": 115760
    },
    {
      "epoch": 7.718,
      "grad_norm": 0.5630092620849609,
      "learning_rate": 1.7624999999999999e-06,
      "loss": 0.002,
      "step": 115770
    },
    {
      "epoch": 7.718666666666667,
      "grad_norm": 0.12592829763889313,
      "learning_rate": 1.7583333333333334e-06,
      "loss": 0.0015,
      "step": 115780
    },
    {
      "epoch": 7.719333333333333,
      "grad_norm": 0.14585433900356293,
      "learning_rate": 1.7541666666666669e-06,
      "loss": 0.0015,
      "step": 115790
    },
    {
      "epoch": 7.72,
      "grad_norm": 0.3719766139984131,
      "learning_rate": 1.7500000000000002e-06,
      "loss": 0.0026,
      "step": 115800
    },
    {
      "epoch": 7.720666666666666,
      "grad_norm": 0.2715878188610077,
      "learning_rate": 1.7458333333333333e-06,
      "loss": 0.0013,
      "step": 115810
    },
    {
      "epoch": 7.721333333333334,
      "grad_norm": 0.2682626247406006,
      "learning_rate": 1.7416666666666668e-06,
      "loss": 0.0012,
      "step": 115820
    },
    {
      "epoch": 7.7219999999999995,
      "grad_norm": 0.3252906799316406,
      "learning_rate": 1.7375000000000003e-06,
      "loss": 0.0013,
      "step": 115830
    },
    {
      "epoch": 7.722666666666667,
      "grad_norm": 0.3055635690689087,
      "learning_rate": 1.7333333333333334e-06,
      "loss": 0.0018,
      "step": 115840
    },
    {
      "epoch": 7.723333333333334,
      "grad_norm": 0.1364588886499405,
      "learning_rate": 1.7291666666666667e-06,
      "loss": 0.0015,
      "step": 115850
    },
    {
      "epoch": 7.724,
      "grad_norm": 0.23181109130382538,
      "learning_rate": 1.7250000000000002e-06,
      "loss": 0.0018,
      "step": 115860
    },
    {
      "epoch": 7.724666666666667,
      "grad_norm": 0.11143085360527039,
      "learning_rate": 1.7208333333333333e-06,
      "loss": 0.0012,
      "step": 115870
    },
    {
      "epoch": 7.725333333333333,
      "grad_norm": 0.6026754975318909,
      "learning_rate": 1.7166666666666668e-06,
      "loss": 0.0019,
      "step": 115880
    },
    {
      "epoch": 7.726,
      "grad_norm": 0.10485205799341202,
      "learning_rate": 1.7125000000000001e-06,
      "loss": 0.0012,
      "step": 115890
    },
    {
      "epoch": 7.726666666666667,
      "grad_norm": 0.15532682836055756,
      "learning_rate": 1.7083333333333332e-06,
      "loss": 0.0014,
      "step": 115900
    },
    {
      "epoch": 7.727333333333333,
      "grad_norm": 0.29003992676734924,
      "learning_rate": 1.7041666666666667e-06,
      "loss": 0.0024,
      "step": 115910
    },
    {
      "epoch": 7.728,
      "grad_norm": 0.3999956250190735,
      "learning_rate": 1.7000000000000002e-06,
      "loss": 0.003,
      "step": 115920
    },
    {
      "epoch": 7.728666666666666,
      "grad_norm": 0.10726847499608994,
      "learning_rate": 1.6958333333333333e-06,
      "loss": 0.0014,
      "step": 115930
    },
    {
      "epoch": 7.729333333333333,
      "grad_norm": 0.2707890570163727,
      "learning_rate": 1.6916666666666668e-06,
      "loss": 0.0013,
      "step": 115940
    },
    {
      "epoch": 7.73,
      "grad_norm": 0.5077996850013733,
      "learning_rate": 1.6875000000000001e-06,
      "loss": 0.0036,
      "step": 115950
    },
    {
      "epoch": 7.730666666666667,
      "grad_norm": 0.10392702370882034,
      "learning_rate": 1.6833333333333332e-06,
      "loss": 0.0019,
      "step": 115960
    },
    {
      "epoch": 7.731333333333334,
      "grad_norm": 0.09851086884737015,
      "learning_rate": 1.6791666666666668e-06,
      "loss": 0.0021,
      "step": 115970
    },
    {
      "epoch": 7.732,
      "grad_norm": 0.08324912935495377,
      "learning_rate": 1.6750000000000003e-06,
      "loss": 0.0017,
      "step": 115980
    },
    {
      "epoch": 7.732666666666667,
      "grad_norm": 0.04558584466576576,
      "learning_rate": 1.6708333333333334e-06,
      "loss": 0.0014,
      "step": 115990
    },
    {
      "epoch": 7.733333333333333,
      "grad_norm": 0.0790696069598198,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 0.0013,
      "step": 116000
    },
    {
      "epoch": 7.734,
      "grad_norm": 0.15412868559360504,
      "learning_rate": 1.6625000000000002e-06,
      "loss": 0.0024,
      "step": 116010
    },
    {
      "epoch": 7.734666666666667,
      "grad_norm": 0.2565869390964508,
      "learning_rate": 1.6583333333333333e-06,
      "loss": 0.0016,
      "step": 116020
    },
    {
      "epoch": 7.735333333333333,
      "grad_norm": 0.052803970873355865,
      "learning_rate": 1.6541666666666668e-06,
      "loss": 0.0023,
      "step": 116030
    },
    {
      "epoch": 7.736,
      "grad_norm": 0.11300018429756165,
      "learning_rate": 1.65e-06,
      "loss": 0.0015,
      "step": 116040
    },
    {
      "epoch": 7.736666666666666,
      "grad_norm": 0.2279372215270996,
      "learning_rate": 1.6458333333333332e-06,
      "loss": 0.0019,
      "step": 116050
    },
    {
      "epoch": 7.737333333333333,
      "grad_norm": 0.07342760264873505,
      "learning_rate": 1.6416666666666667e-06,
      "loss": 0.0023,
      "step": 116060
    },
    {
      "epoch": 7.7379999999999995,
      "grad_norm": 0.20436419546604156,
      "learning_rate": 1.6375000000000002e-06,
      "loss": 0.0022,
      "step": 116070
    },
    {
      "epoch": 7.738666666666667,
      "grad_norm": 0.11318657547235489,
      "learning_rate": 1.6333333333333333e-06,
      "loss": 0.0018,
      "step": 116080
    },
    {
      "epoch": 7.739333333333334,
      "grad_norm": 0.04457319155335426,
      "learning_rate": 1.6291666666666666e-06,
      "loss": 0.0013,
      "step": 116090
    },
    {
      "epoch": 7.74,
      "grad_norm": 0.2315024882555008,
      "learning_rate": 1.6250000000000001e-06,
      "loss": 0.0018,
      "step": 116100
    },
    {
      "epoch": 7.740666666666667,
      "grad_norm": 0.058385059237480164,
      "learning_rate": 1.6208333333333336e-06,
      "loss": 0.0017,
      "step": 116110
    },
    {
      "epoch": 7.741333333333333,
      "grad_norm": 0.07120388746261597,
      "learning_rate": 1.6166666666666667e-06,
      "loss": 0.0021,
      "step": 116120
    },
    {
      "epoch": 7.742,
      "grad_norm": 0.13549910485744476,
      "learning_rate": 1.6125e-06,
      "loss": 0.0018,
      "step": 116130
    },
    {
      "epoch": 7.742666666666667,
      "grad_norm": 0.07250169664621353,
      "learning_rate": 1.6083333333333335e-06,
      "loss": 0.0015,
      "step": 116140
    },
    {
      "epoch": 7.743333333333333,
      "grad_norm": 0.3363629877567291,
      "learning_rate": 1.6041666666666666e-06,
      "loss": 0.0017,
      "step": 116150
    },
    {
      "epoch": 7.744,
      "grad_norm": 0.21394088864326477,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.0012,
      "step": 116160
    },
    {
      "epoch": 7.744666666666666,
      "grad_norm": 0.10640610754489899,
      "learning_rate": 1.5958333333333337e-06,
      "loss": 0.0018,
      "step": 116170
    },
    {
      "epoch": 7.745333333333333,
      "grad_norm": 0.23599642515182495,
      "learning_rate": 1.5916666666666667e-06,
      "loss": 0.0014,
      "step": 116180
    },
    {
      "epoch": 7.746,
      "grad_norm": 0.2638157904148102,
      "learning_rate": 1.5875e-06,
      "loss": 0.0028,
      "step": 116190
    },
    {
      "epoch": 7.746666666666667,
      "grad_norm": 0.5994303822517395,
      "learning_rate": 1.5833333333333336e-06,
      "loss": 0.0023,
      "step": 116200
    },
    {
      "epoch": 7.747333333333334,
      "grad_norm": 0.0308730099350214,
      "learning_rate": 1.5791666666666667e-06,
      "loss": 0.0022,
      "step": 116210
    },
    {
      "epoch": 7.748,
      "grad_norm": 0.42971882224082947,
      "learning_rate": 1.5750000000000002e-06,
      "loss": 0.0015,
      "step": 116220
    },
    {
      "epoch": 7.748666666666667,
      "grad_norm": 0.05912849307060242,
      "learning_rate": 1.5708333333333335e-06,
      "loss": 0.0018,
      "step": 116230
    },
    {
      "epoch": 7.749333333333333,
      "grad_norm": 0.11516959220170975,
      "learning_rate": 1.5666666666666666e-06,
      "loss": 0.0018,
      "step": 116240
    },
    {
      "epoch": 7.75,
      "grad_norm": 0.09858694672584534,
      "learning_rate": 1.5625e-06,
      "loss": 0.0017,
      "step": 116250
    },
    {
      "epoch": 7.750666666666667,
      "grad_norm": 0.17062625288963318,
      "learning_rate": 1.5583333333333334e-06,
      "loss": 0.0037,
      "step": 116260
    },
    {
      "epoch": 7.751333333333333,
      "grad_norm": 0.3224565088748932,
      "learning_rate": 1.5541666666666669e-06,
      "loss": 0.0013,
      "step": 116270
    },
    {
      "epoch": 7.752,
      "grad_norm": 0.7763355374336243,
      "learning_rate": 1.55e-06,
      "loss": 0.0017,
      "step": 116280
    },
    {
      "epoch": 7.752666666666666,
      "grad_norm": 0.10993793606758118,
      "learning_rate": 1.5458333333333333e-06,
      "loss": 0.0012,
      "step": 116290
    },
    {
      "epoch": 7.753333333333333,
      "grad_norm": 0.3027712106704712,
      "learning_rate": 1.5416666666666668e-06,
      "loss": 0.0012,
      "step": 116300
    },
    {
      "epoch": 7.754,
      "grad_norm": 0.03995901718735695,
      "learning_rate": 1.5375e-06,
      "loss": 0.002,
      "step": 116310
    },
    {
      "epoch": 7.754666666666667,
      "grad_norm": 0.43446433544158936,
      "learning_rate": 1.5333333333333334e-06,
      "loss": 0.0013,
      "step": 116320
    },
    {
      "epoch": 7.755333333333334,
      "grad_norm": 0.23233725130558014,
      "learning_rate": 1.5291666666666667e-06,
      "loss": 0.0014,
      "step": 116330
    },
    {
      "epoch": 7.756,
      "grad_norm": 0.02992754802107811,
      "learning_rate": 1.525e-06,
      "loss": 0.0013,
      "step": 116340
    },
    {
      "epoch": 7.756666666666667,
      "grad_norm": 0.3682829737663269,
      "learning_rate": 1.5208333333333335e-06,
      "loss": 0.0015,
      "step": 116350
    },
    {
      "epoch": 7.757333333333333,
      "grad_norm": 0.10547901690006256,
      "learning_rate": 1.5166666666666668e-06,
      "loss": 0.0022,
      "step": 116360
    },
    {
      "epoch": 7.758,
      "grad_norm": 0.19439184665679932,
      "learning_rate": 1.5125000000000001e-06,
      "loss": 0.002,
      "step": 116370
    },
    {
      "epoch": 7.758666666666667,
      "grad_norm": 0.334312379360199,
      "learning_rate": 1.5083333333333334e-06,
      "loss": 0.0012,
      "step": 116380
    },
    {
      "epoch": 7.759333333333333,
      "grad_norm": 0.3683696687221527,
      "learning_rate": 1.5041666666666667e-06,
      "loss": 0.0015,
      "step": 116390
    },
    {
      "epoch": 7.76,
      "grad_norm": 0.10265789180994034,
      "learning_rate": 1.5e-06,
      "loss": 0.0013,
      "step": 116400
    },
    {
      "epoch": 7.760666666666666,
      "grad_norm": 0.20688316226005554,
      "learning_rate": 1.4958333333333336e-06,
      "loss": 0.0024,
      "step": 116410
    },
    {
      "epoch": 7.761333333333333,
      "grad_norm": 0.32529136538505554,
      "learning_rate": 1.4916666666666666e-06,
      "loss": 0.0017,
      "step": 116420
    },
    {
      "epoch": 7.7620000000000005,
      "grad_norm": 0.12685264647006989,
      "learning_rate": 1.4875e-06,
      "loss": 0.0014,
      "step": 116430
    },
    {
      "epoch": 7.762666666666667,
      "grad_norm": 0.07542925328016281,
      "learning_rate": 1.4833333333333335e-06,
      "loss": 0.0031,
      "step": 116440
    },
    {
      "epoch": 7.763333333333334,
      "grad_norm": 0.06769782304763794,
      "learning_rate": 1.4791666666666668e-06,
      "loss": 0.0021,
      "step": 116450
    },
    {
      "epoch": 7.764,
      "grad_norm": 0.3083905875682831,
      "learning_rate": 1.475e-06,
      "loss": 0.0023,
      "step": 116460
    },
    {
      "epoch": 7.764666666666667,
      "grad_norm": 0.37155547738075256,
      "learning_rate": 1.4708333333333334e-06,
      "loss": 0.0016,
      "step": 116470
    },
    {
      "epoch": 7.765333333333333,
      "grad_norm": 0.23679423332214355,
      "learning_rate": 1.4666666666666667e-06,
      "loss": 0.0024,
      "step": 116480
    },
    {
      "epoch": 7.766,
      "grad_norm": 0.048249706625938416,
      "learning_rate": 1.4625000000000002e-06,
      "loss": 0.0027,
      "step": 116490
    },
    {
      "epoch": 7.766666666666667,
      "grad_norm": 0.2113020122051239,
      "learning_rate": 1.4583333333333335e-06,
      "loss": 0.0025,
      "step": 116500
    },
    {
      "epoch": 7.767333333333333,
      "grad_norm": 0.10570915788412094,
      "learning_rate": 1.4541666666666668e-06,
      "loss": 0.0011,
      "step": 116510
    },
    {
      "epoch": 7.768,
      "grad_norm": 0.4391840696334839,
      "learning_rate": 1.45e-06,
      "loss": 0.0013,
      "step": 116520
    },
    {
      "epoch": 7.768666666666666,
      "grad_norm": 0.1702314168214798,
      "learning_rate": 1.4458333333333334e-06,
      "loss": 0.0018,
      "step": 116530
    },
    {
      "epoch": 7.769333333333333,
      "grad_norm": 0.07066955417394638,
      "learning_rate": 1.4416666666666667e-06,
      "loss": 0.0015,
      "step": 116540
    },
    {
      "epoch": 7.77,
      "grad_norm": 0.4371265470981598,
      "learning_rate": 1.4375000000000002e-06,
      "loss": 0.0012,
      "step": 116550
    },
    {
      "epoch": 7.770666666666667,
      "grad_norm": 0.2833416759967804,
      "learning_rate": 1.4333333333333333e-06,
      "loss": 0.002,
      "step": 116560
    },
    {
      "epoch": 7.771333333333334,
      "grad_norm": 0.23454636335372925,
      "learning_rate": 1.4291666666666666e-06,
      "loss": 0.0013,
      "step": 116570
    },
    {
      "epoch": 7.772,
      "grad_norm": 0.1611737757921219,
      "learning_rate": 1.4250000000000001e-06,
      "loss": 0.0021,
      "step": 116580
    },
    {
      "epoch": 7.772666666666667,
      "grad_norm": 0.14399918913841248,
      "learning_rate": 1.4208333333333334e-06,
      "loss": 0.0016,
      "step": 116590
    },
    {
      "epoch": 7.773333333333333,
      "grad_norm": 0.36438852548599243,
      "learning_rate": 1.4166666666666667e-06,
      "loss": 0.0014,
      "step": 116600
    },
    {
      "epoch": 7.774,
      "grad_norm": 0.2684502601623535,
      "learning_rate": 1.4125e-06,
      "loss": 0.0014,
      "step": 116610
    },
    {
      "epoch": 7.774666666666667,
      "grad_norm": 0.23701810836791992,
      "learning_rate": 1.4083333333333333e-06,
      "loss": 0.0025,
      "step": 116620
    },
    {
      "epoch": 7.775333333333333,
      "grad_norm": 0.18699553608894348,
      "learning_rate": 1.4041666666666666e-06,
      "loss": 0.0015,
      "step": 116630
    },
    {
      "epoch": 7.776,
      "grad_norm": 0.17301709949970245,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 0.0015,
      "step": 116640
    },
    {
      "epoch": 7.776666666666666,
      "grad_norm": 0.13856880366802216,
      "learning_rate": 1.3958333333333335e-06,
      "loss": 0.002,
      "step": 116650
    },
    {
      "epoch": 7.777333333333333,
      "grad_norm": 0.20347924530506134,
      "learning_rate": 1.3916666666666668e-06,
      "loss": 0.0015,
      "step": 116660
    },
    {
      "epoch": 7.7780000000000005,
      "grad_norm": 0.1733279824256897,
      "learning_rate": 1.3875e-06,
      "loss": 0.0012,
      "step": 116670
    },
    {
      "epoch": 7.778666666666666,
      "grad_norm": 0.1976805478334427,
      "learning_rate": 1.3833333333333334e-06,
      "loss": 0.0016,
      "step": 116680
    },
    {
      "epoch": 7.779333333333334,
      "grad_norm": 0.4971418082714081,
      "learning_rate": 1.3791666666666669e-06,
      "loss": 0.0019,
      "step": 116690
    },
    {
      "epoch": 7.78,
      "grad_norm": 0.17071673274040222,
      "learning_rate": 1.3750000000000002e-06,
      "loss": 0.0013,
      "step": 116700
    },
    {
      "epoch": 7.780666666666667,
      "grad_norm": 0.16905391216278076,
      "learning_rate": 1.3708333333333333e-06,
      "loss": 0.0011,
      "step": 116710
    },
    {
      "epoch": 7.781333333333333,
      "grad_norm": 0.3744705617427826,
      "learning_rate": 1.3666666666666668e-06,
      "loss": 0.0015,
      "step": 116720
    },
    {
      "epoch": 7.782,
      "grad_norm": 0.05602482333779335,
      "learning_rate": 1.3625e-06,
      "loss": 0.0019,
      "step": 116730
    },
    {
      "epoch": 7.782666666666667,
      "grad_norm": 0.05073006451129913,
      "learning_rate": 1.3583333333333334e-06,
      "loss": 0.0012,
      "step": 116740
    },
    {
      "epoch": 7.783333333333333,
      "grad_norm": 0.0862882062792778,
      "learning_rate": 1.3541666666666667e-06,
      "loss": 0.0016,
      "step": 116750
    },
    {
      "epoch": 7.784,
      "grad_norm": 0.0706852525472641,
      "learning_rate": 1.35e-06,
      "loss": 0.0013,
      "step": 116760
    },
    {
      "epoch": 7.784666666666666,
      "grad_norm": 0.308839350938797,
      "learning_rate": 1.3458333333333333e-06,
      "loss": 0.0023,
      "step": 116770
    },
    {
      "epoch": 7.785333333333333,
      "grad_norm": 0.37100619077682495,
      "learning_rate": 1.3416666666666668e-06,
      "loss": 0.0018,
      "step": 116780
    },
    {
      "epoch": 7.786,
      "grad_norm": 0.6327939033508301,
      "learning_rate": 1.3375000000000001e-06,
      "loss": 0.0012,
      "step": 116790
    },
    {
      "epoch": 7.786666666666667,
      "grad_norm": 0.5326521396636963,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.0014,
      "step": 116800
    },
    {
      "epoch": 7.787333333333334,
      "grad_norm": 0.5654447674751282,
      "learning_rate": 1.3291666666666667e-06,
      "loss": 0.0022,
      "step": 116810
    },
    {
      "epoch": 7.788,
      "grad_norm": 0.3048545718193054,
      "learning_rate": 1.325e-06,
      "loss": 0.0019,
      "step": 116820
    },
    {
      "epoch": 7.788666666666667,
      "grad_norm": 0.40063098073005676,
      "learning_rate": 1.3208333333333335e-06,
      "loss": 0.0026,
      "step": 116830
    },
    {
      "epoch": 7.789333333333333,
      "grad_norm": 0.2310802936553955,
      "learning_rate": 1.3166666666666668e-06,
      "loss": 0.0018,
      "step": 116840
    },
    {
      "epoch": 7.79,
      "grad_norm": 0.4581710696220398,
      "learning_rate": 1.3125e-06,
      "loss": 0.002,
      "step": 116850
    },
    {
      "epoch": 7.790666666666667,
      "grad_norm": 0.31862249970436096,
      "learning_rate": 1.3083333333333334e-06,
      "loss": 0.0019,
      "step": 116860
    },
    {
      "epoch": 7.791333333333333,
      "grad_norm": 0.33468368649482727,
      "learning_rate": 1.3041666666666667e-06,
      "loss": 0.0017,
      "step": 116870
    },
    {
      "epoch": 7.792,
      "grad_norm": 0.12179899215698242,
      "learning_rate": 1.3e-06,
      "loss": 0.0017,
      "step": 116880
    },
    {
      "epoch": 7.792666666666666,
      "grad_norm": 0.2988114058971405,
      "learning_rate": 1.2958333333333333e-06,
      "loss": 0.0018,
      "step": 116890
    },
    {
      "epoch": 7.793333333333333,
      "grad_norm": 0.0732431635260582,
      "learning_rate": 1.2916666666666667e-06,
      "loss": 0.0025,
      "step": 116900
    },
    {
      "epoch": 7.7940000000000005,
      "grad_norm": 0.30548110604286194,
      "learning_rate": 1.2875e-06,
      "loss": 0.0013,
      "step": 116910
    },
    {
      "epoch": 7.794666666666666,
      "grad_norm": 0.2026398926973343,
      "learning_rate": 1.2833333333333335e-06,
      "loss": 0.0012,
      "step": 116920
    },
    {
      "epoch": 7.795333333333334,
      "grad_norm": 0.04224531725049019,
      "learning_rate": 1.2791666666666668e-06,
      "loss": 0.0014,
      "step": 116930
    },
    {
      "epoch": 7.796,
      "grad_norm": 0.30159589648246765,
      "learning_rate": 1.275e-06,
      "loss": 0.0026,
      "step": 116940
    },
    {
      "epoch": 7.796666666666667,
      "grad_norm": 0.5336131453514099,
      "learning_rate": 1.2708333333333334e-06,
      "loss": 0.003,
      "step": 116950
    },
    {
      "epoch": 7.7973333333333334,
      "grad_norm": 0.3649466931819916,
      "learning_rate": 1.2666666666666667e-06,
      "loss": 0.0017,
      "step": 116960
    },
    {
      "epoch": 7.798,
      "grad_norm": 0.16926108300685883,
      "learning_rate": 1.2625000000000002e-06,
      "loss": 0.0016,
      "step": 116970
    },
    {
      "epoch": 7.798666666666667,
      "grad_norm": 0.043419331312179565,
      "learning_rate": 1.2583333333333335e-06,
      "loss": 0.0012,
      "step": 116980
    },
    {
      "epoch": 7.799333333333333,
      "grad_norm": 0.20105110108852386,
      "learning_rate": 1.2541666666666666e-06,
      "loss": 0.0024,
      "step": 116990
    },
    {
      "epoch": 7.8,
      "grad_norm": 0.3358924388885498,
      "learning_rate": 1.25e-06,
      "loss": 0.0016,
      "step": 117000
    },
    {
      "epoch": 7.800666666666666,
      "grad_norm": 0.04546824097633362,
      "learning_rate": 1.2458333333333334e-06,
      "loss": 0.0016,
      "step": 117010
    },
    {
      "epoch": 7.801333333333333,
      "grad_norm": 0.25261062383651733,
      "learning_rate": 1.2416666666666667e-06,
      "loss": 0.002,
      "step": 117020
    },
    {
      "epoch": 7.802,
      "grad_norm": 0.04331369698047638,
      "learning_rate": 1.2375000000000002e-06,
      "loss": 0.0012,
      "step": 117030
    },
    {
      "epoch": 7.802666666666667,
      "grad_norm": 0.13691742718219757,
      "learning_rate": 1.2333333333333333e-06,
      "loss": 0.0017,
      "step": 117040
    },
    {
      "epoch": 7.803333333333334,
      "grad_norm": 0.16590633988380432,
      "learning_rate": 1.2291666666666666e-06,
      "loss": 0.0018,
      "step": 117050
    },
    {
      "epoch": 7.804,
      "grad_norm": 0.13730338215827942,
      "learning_rate": 1.2250000000000001e-06,
      "loss": 0.002,
      "step": 117060
    },
    {
      "epoch": 7.804666666666667,
      "grad_norm": 0.059728413820266724,
      "learning_rate": 1.2208333333333334e-06,
      "loss": 0.0018,
      "step": 117070
    },
    {
      "epoch": 7.8053333333333335,
      "grad_norm": 0.0405362993478775,
      "learning_rate": 1.2166666666666667e-06,
      "loss": 0.0025,
      "step": 117080
    },
    {
      "epoch": 7.806,
      "grad_norm": 0.0484575554728508,
      "learning_rate": 1.2125e-06,
      "loss": 0.0019,
      "step": 117090
    },
    {
      "epoch": 7.806666666666667,
      "grad_norm": 0.16515250504016876,
      "learning_rate": 1.2083333333333333e-06,
      "loss": 0.0012,
      "step": 117100
    },
    {
      "epoch": 7.807333333333333,
      "grad_norm": 0.03158838301897049,
      "learning_rate": 1.2041666666666669e-06,
      "loss": 0.0014,
      "step": 117110
    },
    {
      "epoch": 7.808,
      "grad_norm": 0.13529928028583527,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 0.002,
      "step": 117120
    },
    {
      "epoch": 7.808666666666666,
      "grad_norm": 0.056735605001449585,
      "learning_rate": 1.1958333333333332e-06,
      "loss": 0.0016,
      "step": 117130
    },
    {
      "epoch": 7.809333333333333,
      "grad_norm": 0.12083294242620468,
      "learning_rate": 1.1916666666666668e-06,
      "loss": 0.0015,
      "step": 117140
    },
    {
      "epoch": 7.8100000000000005,
      "grad_norm": 0.11104124039411545,
      "learning_rate": 1.1875e-06,
      "loss": 0.0017,
      "step": 117150
    },
    {
      "epoch": 7.810666666666666,
      "grad_norm": 0.346848726272583,
      "learning_rate": 1.1833333333333334e-06,
      "loss": 0.0014,
      "step": 117160
    },
    {
      "epoch": 7.811333333333334,
      "grad_norm": 0.17243418097496033,
      "learning_rate": 1.1791666666666669e-06,
      "loss": 0.0013,
      "step": 117170
    },
    {
      "epoch": 7.812,
      "grad_norm": 0.3962610960006714,
      "learning_rate": 1.175e-06,
      "loss": 0.0021,
      "step": 117180
    },
    {
      "epoch": 7.812666666666667,
      "grad_norm": 0.30322057008743286,
      "learning_rate": 1.1708333333333333e-06,
      "loss": 0.0013,
      "step": 117190
    },
    {
      "epoch": 7.8133333333333335,
      "grad_norm": 0.20413503050804138,
      "learning_rate": 1.1666666666666668e-06,
      "loss": 0.0015,
      "step": 117200
    },
    {
      "epoch": 7.814,
      "grad_norm": 0.4954995810985565,
      "learning_rate": 1.1625e-06,
      "loss": 0.0017,
      "step": 117210
    },
    {
      "epoch": 7.814666666666667,
      "grad_norm": 0.13763728737831116,
      "learning_rate": 1.1583333333333334e-06,
      "loss": 0.0021,
      "step": 117220
    },
    {
      "epoch": 7.815333333333333,
      "grad_norm": 0.04471200704574585,
      "learning_rate": 1.1541666666666667e-06,
      "loss": 0.002,
      "step": 117230
    },
    {
      "epoch": 7.816,
      "grad_norm": 0.17690721154212952,
      "learning_rate": 1.15e-06,
      "loss": 0.0014,
      "step": 117240
    },
    {
      "epoch": 7.816666666666666,
      "grad_norm": 0.511385977268219,
      "learning_rate": 1.1458333333333333e-06,
      "loss": 0.0027,
      "step": 117250
    },
    {
      "epoch": 7.817333333333333,
      "grad_norm": 0.11479586362838745,
      "learning_rate": 1.1416666666666668e-06,
      "loss": 0.0013,
      "step": 117260
    },
    {
      "epoch": 7.818,
      "grad_norm": 0.23180682957172394,
      "learning_rate": 1.1375000000000001e-06,
      "loss": 0.002,
      "step": 117270
    },
    {
      "epoch": 7.818666666666667,
      "grad_norm": 0.27018845081329346,
      "learning_rate": 1.1333333333333334e-06,
      "loss": 0.0024,
      "step": 117280
    },
    {
      "epoch": 7.819333333333334,
      "grad_norm": 0.05052934214472771,
      "learning_rate": 1.1291666666666667e-06,
      "loss": 0.0016,
      "step": 117290
    },
    {
      "epoch": 7.82,
      "grad_norm": 0.3063397705554962,
      "learning_rate": 1.125e-06,
      "loss": 0.0032,
      "step": 117300
    },
    {
      "epoch": 7.820666666666667,
      "grad_norm": 0.21407189965248108,
      "learning_rate": 1.1208333333333335e-06,
      "loss": 0.0019,
      "step": 117310
    },
    {
      "epoch": 7.8213333333333335,
      "grad_norm": 0.2971203327178955,
      "learning_rate": 1.1166666666666666e-06,
      "loss": 0.0018,
      "step": 117320
    },
    {
      "epoch": 7.822,
      "grad_norm": 0.17279008030891418,
      "learning_rate": 1.1125e-06,
      "loss": 0.0013,
      "step": 117330
    },
    {
      "epoch": 7.822666666666667,
      "grad_norm": 0.13643759489059448,
      "learning_rate": 1.1083333333333335e-06,
      "loss": 0.0013,
      "step": 117340
    },
    {
      "epoch": 7.823333333333333,
      "grad_norm": 0.039079584181308746,
      "learning_rate": 1.1041666666666668e-06,
      "loss": 0.0015,
      "step": 117350
    },
    {
      "epoch": 7.824,
      "grad_norm": 0.43255260586738586,
      "learning_rate": 1.1e-06,
      "loss": 0.0017,
      "step": 117360
    },
    {
      "epoch": 7.824666666666666,
      "grad_norm": 0.20638787746429443,
      "learning_rate": 1.0958333333333334e-06,
      "loss": 0.0021,
      "step": 117370
    },
    {
      "epoch": 7.825333333333333,
      "grad_norm": 0.1752125471830368,
      "learning_rate": 1.0916666666666667e-06,
      "loss": 0.0021,
      "step": 117380
    },
    {
      "epoch": 7.826,
      "grad_norm": 0.07057318836450577,
      "learning_rate": 1.0875e-06,
      "loss": 0.0019,
      "step": 117390
    },
    {
      "epoch": 7.826666666666666,
      "grad_norm": 0.37875768542289734,
      "learning_rate": 1.0833333333333335e-06,
      "loss": 0.0014,
      "step": 117400
    },
    {
      "epoch": 7.827333333333334,
      "grad_norm": 0.36187294125556946,
      "learning_rate": 1.0791666666666668e-06,
      "loss": 0.0025,
      "step": 117410
    },
    {
      "epoch": 7.828,
      "grad_norm": 0.10858040302991867,
      "learning_rate": 1.0749999999999999e-06,
      "loss": 0.0021,
      "step": 117420
    },
    {
      "epoch": 7.828666666666667,
      "grad_norm": 0.5062031745910645,
      "learning_rate": 1.0708333333333334e-06,
      "loss": 0.0021,
      "step": 117430
    },
    {
      "epoch": 7.8293333333333335,
      "grad_norm": 0.233896404504776,
      "learning_rate": 1.0666666666666667e-06,
      "loss": 0.0027,
      "step": 117440
    },
    {
      "epoch": 7.83,
      "grad_norm": 0.07169605791568756,
      "learning_rate": 1.0625000000000002e-06,
      "loss": 0.0017,
      "step": 117450
    },
    {
      "epoch": 7.830666666666667,
      "grad_norm": 0.279646635055542,
      "learning_rate": 1.0583333333333333e-06,
      "loss": 0.0023,
      "step": 117460
    },
    {
      "epoch": 7.831333333333333,
      "grad_norm": 0.2026718258857727,
      "learning_rate": 1.0541666666666666e-06,
      "loss": 0.0012,
      "step": 117470
    },
    {
      "epoch": 7.832,
      "grad_norm": 0.44019922614097595,
      "learning_rate": 1.0500000000000001e-06,
      "loss": 0.0018,
      "step": 117480
    },
    {
      "epoch": 7.832666666666666,
      "grad_norm": 0.0735684186220169,
      "learning_rate": 1.0458333333333334e-06,
      "loss": 0.0014,
      "step": 117490
    },
    {
      "epoch": 7.833333333333333,
      "grad_norm": 0.35962367057800293,
      "learning_rate": 1.0416666666666667e-06,
      "loss": 0.0013,
      "step": 117500
    },
    {
      "epoch": 7.834,
      "grad_norm": 0.20378540456295013,
      "learning_rate": 1.0375e-06,
      "loss": 0.0024,
      "step": 117510
    },
    {
      "epoch": 7.834666666666667,
      "grad_norm": 0.5353914499282837,
      "learning_rate": 1.0333333333333333e-06,
      "loss": 0.0013,
      "step": 117520
    },
    {
      "epoch": 7.835333333333334,
      "grad_norm": 0.3372458219528198,
      "learning_rate": 1.0291666666666666e-06,
      "loss": 0.0017,
      "step": 117530
    },
    {
      "epoch": 7.836,
      "grad_norm": 0.05820414051413536,
      "learning_rate": 1.0250000000000001e-06,
      "loss": 0.0017,
      "step": 117540
    },
    {
      "epoch": 7.836666666666667,
      "grad_norm": 0.20602484047412872,
      "learning_rate": 1.0208333333333334e-06,
      "loss": 0.0013,
      "step": 117550
    },
    {
      "epoch": 7.8373333333333335,
      "grad_norm": 0.08623569458723068,
      "learning_rate": 1.0166666666666665e-06,
      "loss": 0.0019,
      "step": 117560
    },
    {
      "epoch": 7.838,
      "grad_norm": 0.10315937548875809,
      "learning_rate": 1.0125e-06,
      "loss": 0.0016,
      "step": 117570
    },
    {
      "epoch": 7.838666666666667,
      "grad_norm": 0.4592658281326294,
      "learning_rate": 1.0083333333333333e-06,
      "loss": 0.0022,
      "step": 117580
    },
    {
      "epoch": 7.839333333333333,
      "grad_norm": 0.07595694065093994,
      "learning_rate": 1.0041666666666669e-06,
      "loss": 0.0019,
      "step": 117590
    },
    {
      "epoch": 7.84,
      "grad_norm": 0.623591959476471,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.0022,
      "step": 117600
    },
    {
      "epoch": 7.8406666666666665,
      "grad_norm": 0.4647597670555115,
      "learning_rate": 9.958333333333333e-07,
      "loss": 0.0012,
      "step": 117610
    },
    {
      "epoch": 7.841333333333333,
      "grad_norm": 0.11531461775302887,
      "learning_rate": 9.916666666666668e-07,
      "loss": 0.0022,
      "step": 117620
    },
    {
      "epoch": 7.842,
      "grad_norm": 0.10123828798532486,
      "learning_rate": 9.875e-07,
      "loss": 0.0014,
      "step": 117630
    },
    {
      "epoch": 7.842666666666666,
      "grad_norm": 0.10859742760658264,
      "learning_rate": 9.833333333333334e-07,
      "loss": 0.0012,
      "step": 117640
    },
    {
      "epoch": 7.843333333333334,
      "grad_norm": 0.21164850890636444,
      "learning_rate": 9.791666666666667e-07,
      "loss": 0.0021,
      "step": 117650
    },
    {
      "epoch": 7.844,
      "grad_norm": 0.17396114766597748,
      "learning_rate": 9.75e-07,
      "loss": 0.0024,
      "step": 117660
    },
    {
      "epoch": 7.844666666666667,
      "grad_norm": 0.06938885152339935,
      "learning_rate": 9.708333333333333e-07,
      "loss": 0.0025,
      "step": 117670
    },
    {
      "epoch": 7.8453333333333335,
      "grad_norm": 0.048210907727479935,
      "learning_rate": 9.666666666666668e-07,
      "loss": 0.0012,
      "step": 117680
    },
    {
      "epoch": 7.846,
      "grad_norm": 0.10426510125398636,
      "learning_rate": 9.625e-07,
      "loss": 0.0024,
      "step": 117690
    },
    {
      "epoch": 7.846666666666667,
      "grad_norm": 0.10927881300449371,
      "learning_rate": 9.583333333333334e-07,
      "loss": 0.0022,
      "step": 117700
    },
    {
      "epoch": 7.847333333333333,
      "grad_norm": 0.023200100287795067,
      "learning_rate": 9.541666666666667e-07,
      "loss": 0.002,
      "step": 117710
    },
    {
      "epoch": 7.848,
      "grad_norm": 0.5374584794044495,
      "learning_rate": 9.5e-07,
      "loss": 0.002,
      "step": 117720
    },
    {
      "epoch": 7.8486666666666665,
      "grad_norm": 0.5654312372207642,
      "learning_rate": 9.458333333333333e-07,
      "loss": 0.0014,
      "step": 117730
    },
    {
      "epoch": 7.849333333333333,
      "grad_norm": 0.17122411727905273,
      "learning_rate": 9.416666666666667e-07,
      "loss": 0.0019,
      "step": 117740
    },
    {
      "epoch": 7.85,
      "grad_norm": 0.17344732582569122,
      "learning_rate": 9.375e-07,
      "loss": 0.0014,
      "step": 117750
    },
    {
      "epoch": 7.850666666666667,
      "grad_norm": 0.20128965377807617,
      "learning_rate": 9.333333333333334e-07,
      "loss": 0.0016,
      "step": 117760
    },
    {
      "epoch": 7.851333333333334,
      "grad_norm": 0.16988743841648102,
      "learning_rate": 9.291666666666667e-07,
      "loss": 0.0018,
      "step": 117770
    },
    {
      "epoch": 7.852,
      "grad_norm": 0.11112059652805328,
      "learning_rate": 9.25e-07,
      "loss": 0.0015,
      "step": 117780
    },
    {
      "epoch": 7.852666666666667,
      "grad_norm": 0.1675054281949997,
      "learning_rate": 9.208333333333334e-07,
      "loss": 0.0021,
      "step": 117790
    },
    {
      "epoch": 7.8533333333333335,
      "grad_norm": 0.0671514943242073,
      "learning_rate": 9.166666666666667e-07,
      "loss": 0.0013,
      "step": 117800
    },
    {
      "epoch": 7.854,
      "grad_norm": 0.14079716801643372,
      "learning_rate": 9.124999999999999e-07,
      "loss": 0.0018,
      "step": 117810
    },
    {
      "epoch": 7.854666666666667,
      "grad_norm": 0.36236104369163513,
      "learning_rate": 9.083333333333335e-07,
      "loss": 0.0015,
      "step": 117820
    },
    {
      "epoch": 7.855333333333333,
      "grad_norm": 0.10509930551052094,
      "learning_rate": 9.041666666666667e-07,
      "loss": 0.0023,
      "step": 117830
    },
    {
      "epoch": 7.856,
      "grad_norm": 0.1083570048213005,
      "learning_rate": 9e-07,
      "loss": 0.0018,
      "step": 117840
    },
    {
      "epoch": 7.8566666666666665,
      "grad_norm": 0.17328329384326935,
      "learning_rate": 8.958333333333334e-07,
      "loss": 0.0026,
      "step": 117850
    },
    {
      "epoch": 7.857333333333333,
      "grad_norm": 0.14536267518997192,
      "learning_rate": 8.916666666666667e-07,
      "loss": 0.0013,
      "step": 117860
    },
    {
      "epoch": 7.858,
      "grad_norm": 0.13748086988925934,
      "learning_rate": 8.875e-07,
      "loss": 0.0028,
      "step": 117870
    },
    {
      "epoch": 7.858666666666666,
      "grad_norm": 0.16758659482002258,
      "learning_rate": 8.833333333333334e-07,
      "loss": 0.0014,
      "step": 117880
    },
    {
      "epoch": 7.859333333333334,
      "grad_norm": 0.33776116371154785,
      "learning_rate": 8.791666666666667e-07,
      "loss": 0.0016,
      "step": 117890
    },
    {
      "epoch": 7.86,
      "grad_norm": 0.21366119384765625,
      "learning_rate": 8.750000000000001e-07,
      "loss": 0.0018,
      "step": 117900
    },
    {
      "epoch": 7.860666666666667,
      "grad_norm": 0.05916116386651993,
      "learning_rate": 8.708333333333334e-07,
      "loss": 0.002,
      "step": 117910
    },
    {
      "epoch": 7.8613333333333335,
      "grad_norm": 0.20318904519081116,
      "learning_rate": 8.666666666666667e-07,
      "loss": 0.0024,
      "step": 117920
    },
    {
      "epoch": 7.862,
      "grad_norm": 0.07748771458864212,
      "learning_rate": 8.625000000000001e-07,
      "loss": 0.0015,
      "step": 117930
    },
    {
      "epoch": 7.862666666666667,
      "grad_norm": 0.15245665609836578,
      "learning_rate": 8.583333333333334e-07,
      "loss": 0.0021,
      "step": 117940
    },
    {
      "epoch": 7.863333333333333,
      "grad_norm": 0.04841146618127823,
      "learning_rate": 8.541666666666666e-07,
      "loss": 0.0014,
      "step": 117950
    },
    {
      "epoch": 7.864,
      "grad_norm": 0.2673032283782959,
      "learning_rate": 8.500000000000001e-07,
      "loss": 0.0014,
      "step": 117960
    },
    {
      "epoch": 7.8646666666666665,
      "grad_norm": 0.23911146819591522,
      "learning_rate": 8.458333333333334e-07,
      "loss": 0.0021,
      "step": 117970
    },
    {
      "epoch": 7.865333333333333,
      "grad_norm": 0.4813366234302521,
      "learning_rate": 8.416666666666666e-07,
      "loss": 0.0012,
      "step": 117980
    },
    {
      "epoch": 7.866,
      "grad_norm": 0.43100717663764954,
      "learning_rate": 8.375000000000001e-07,
      "loss": 0.0016,
      "step": 117990
    },
    {
      "epoch": 7.866666666666667,
      "grad_norm": 0.1067638024687767,
      "learning_rate": 8.333333333333333e-07,
      "loss": 0.0012,
      "step": 118000
    },
    {
      "epoch": 7.867333333333333,
      "grad_norm": 0.23088963329792023,
      "learning_rate": 8.291666666666666e-07,
      "loss": 0.0015,
      "step": 118010
    },
    {
      "epoch": 7.868,
      "grad_norm": 0.07671239972114563,
      "learning_rate": 8.25e-07,
      "loss": 0.0022,
      "step": 118020
    },
    {
      "epoch": 7.868666666666667,
      "grad_norm": 0.4311401844024658,
      "learning_rate": 8.208333333333333e-07,
      "loss": 0.0015,
      "step": 118030
    },
    {
      "epoch": 7.8693333333333335,
      "grad_norm": 0.20713777840137482,
      "learning_rate": 8.166666666666666e-07,
      "loss": 0.0016,
      "step": 118040
    },
    {
      "epoch": 7.87,
      "grad_norm": 0.13972729444503784,
      "learning_rate": 8.125000000000001e-07,
      "loss": 0.0017,
      "step": 118050
    },
    {
      "epoch": 7.870666666666667,
      "grad_norm": 0.20405356585979462,
      "learning_rate": 8.083333333333334e-07,
      "loss": 0.0012,
      "step": 118060
    },
    {
      "epoch": 7.871333333333333,
      "grad_norm": 0.1409725695848465,
      "learning_rate": 8.041666666666668e-07,
      "loss": 0.0024,
      "step": 118070
    },
    {
      "epoch": 7.872,
      "grad_norm": 0.236784428358078,
      "learning_rate": 8.000000000000001e-07,
      "loss": 0.0021,
      "step": 118080
    },
    {
      "epoch": 7.8726666666666665,
      "grad_norm": 0.4733845591545105,
      "learning_rate": 7.958333333333334e-07,
      "loss": 0.0023,
      "step": 118090
    },
    {
      "epoch": 7.873333333333333,
      "grad_norm": 0.10600022226572037,
      "learning_rate": 7.916666666666668e-07,
      "loss": 0.0014,
      "step": 118100
    },
    {
      "epoch": 7.874,
      "grad_norm": 0.11731216311454773,
      "learning_rate": 7.875000000000001e-07,
      "loss": 0.0015,
      "step": 118110
    },
    {
      "epoch": 7.874666666666666,
      "grad_norm": 0.41368231177330017,
      "learning_rate": 7.833333333333333e-07,
      "loss": 0.0016,
      "step": 118120
    },
    {
      "epoch": 7.875333333333334,
      "grad_norm": 0.137363001704216,
      "learning_rate": 7.791666666666667e-07,
      "loss": 0.0022,
      "step": 118130
    },
    {
      "epoch": 7.876,
      "grad_norm": 0.07350216805934906,
      "learning_rate": 7.75e-07,
      "loss": 0.0014,
      "step": 118140
    },
    {
      "epoch": 7.876666666666667,
      "grad_norm": 0.04513651877641678,
      "learning_rate": 7.708333333333334e-07,
      "loss": 0.0018,
      "step": 118150
    },
    {
      "epoch": 7.8773333333333335,
      "grad_norm": 0.2996676564216614,
      "learning_rate": 7.666666666666667e-07,
      "loss": 0.0014,
      "step": 118160
    },
    {
      "epoch": 7.878,
      "grad_norm": 0.23983179032802582,
      "learning_rate": 7.625e-07,
      "loss": 0.0015,
      "step": 118170
    },
    {
      "epoch": 7.878666666666667,
      "grad_norm": 0.4105549156665802,
      "learning_rate": 7.583333333333334e-07,
      "loss": 0.0014,
      "step": 118180
    },
    {
      "epoch": 7.879333333333333,
      "grad_norm": 0.22545883059501648,
      "learning_rate": 7.541666666666667e-07,
      "loss": 0.0016,
      "step": 118190
    },
    {
      "epoch": 7.88,
      "grad_norm": 0.13923297822475433,
      "learning_rate": 7.5e-07,
      "loss": 0.0014,
      "step": 118200
    },
    {
      "epoch": 7.8806666666666665,
      "grad_norm": 0.21627357602119446,
      "learning_rate": 7.458333333333333e-07,
      "loss": 0.0015,
      "step": 118210
    },
    {
      "epoch": 7.881333333333333,
      "grad_norm": 0.20159679651260376,
      "learning_rate": 7.416666666666667e-07,
      "loss": 0.0013,
      "step": 118220
    },
    {
      "epoch": 7.882,
      "grad_norm": 0.07289110869169235,
      "learning_rate": 7.375e-07,
      "loss": 0.0014,
      "step": 118230
    },
    {
      "epoch": 7.882666666666667,
      "grad_norm": 0.04313046112656593,
      "learning_rate": 7.333333333333333e-07,
      "loss": 0.0019,
      "step": 118240
    },
    {
      "epoch": 7.883333333333333,
      "grad_norm": 0.4142283797264099,
      "learning_rate": 7.291666666666667e-07,
      "loss": 0.0028,
      "step": 118250
    },
    {
      "epoch": 7.884,
      "grad_norm": 0.3335801959037781,
      "learning_rate": 7.25e-07,
      "loss": 0.0017,
      "step": 118260
    },
    {
      "epoch": 7.884666666666667,
      "grad_norm": 0.20105351507663727,
      "learning_rate": 7.208333333333333e-07,
      "loss": 0.0022,
      "step": 118270
    },
    {
      "epoch": 7.8853333333333335,
      "grad_norm": 0.13505293428897858,
      "learning_rate": 7.166666666666667e-07,
      "loss": 0.0023,
      "step": 118280
    },
    {
      "epoch": 7.886,
      "grad_norm": 0.1603890061378479,
      "learning_rate": 7.125000000000001e-07,
      "loss": 0.0019,
      "step": 118290
    },
    {
      "epoch": 7.886666666666667,
      "grad_norm": 0.40126898884773254,
      "learning_rate": 7.083333333333334e-07,
      "loss": 0.0023,
      "step": 118300
    },
    {
      "epoch": 7.887333333333333,
      "grad_norm": 0.2991465926170349,
      "learning_rate": 7.041666666666667e-07,
      "loss": 0.0019,
      "step": 118310
    },
    {
      "epoch": 7.888,
      "grad_norm": 0.32682257890701294,
      "learning_rate": 7.000000000000001e-07,
      "loss": 0.0019,
      "step": 118320
    },
    {
      "epoch": 7.8886666666666665,
      "grad_norm": 0.1757015883922577,
      "learning_rate": 6.958333333333334e-07,
      "loss": 0.0013,
      "step": 118330
    },
    {
      "epoch": 7.889333333333333,
      "grad_norm": 0.4715205132961273,
      "learning_rate": 6.916666666666667e-07,
      "loss": 0.0021,
      "step": 118340
    },
    {
      "epoch": 7.89,
      "grad_norm": 0.134130597114563,
      "learning_rate": 6.875000000000001e-07,
      "loss": 0.0014,
      "step": 118350
    },
    {
      "epoch": 7.890666666666666,
      "grad_norm": 0.30455371737480164,
      "learning_rate": 6.833333333333334e-07,
      "loss": 0.002,
      "step": 118360
    },
    {
      "epoch": 7.891333333333334,
      "grad_norm": 0.336271196603775,
      "learning_rate": 6.791666666666667e-07,
      "loss": 0.0027,
      "step": 118370
    },
    {
      "epoch": 7.892,
      "grad_norm": 0.04911147058010101,
      "learning_rate": 6.75e-07,
      "loss": 0.0015,
      "step": 118380
    },
    {
      "epoch": 7.892666666666667,
      "grad_norm": 0.04540107399225235,
      "learning_rate": 6.708333333333334e-07,
      "loss": 0.0021,
      "step": 118390
    },
    {
      "epoch": 7.8933333333333335,
      "grad_norm": 0.20328781008720398,
      "learning_rate": 6.666666666666667e-07,
      "loss": 0.0024,
      "step": 118400
    },
    {
      "epoch": 7.894,
      "grad_norm": 0.07312798500061035,
      "learning_rate": 6.625e-07,
      "loss": 0.0022,
      "step": 118410
    },
    {
      "epoch": 7.894666666666667,
      "grad_norm": 0.39418068528175354,
      "learning_rate": 6.583333333333334e-07,
      "loss": 0.0019,
      "step": 118420
    },
    {
      "epoch": 7.895333333333333,
      "grad_norm": 0.1983727216720581,
      "learning_rate": 6.541666666666667e-07,
      "loss": 0.0016,
      "step": 118430
    },
    {
      "epoch": 7.896,
      "grad_norm": 0.26783785223960876,
      "learning_rate": 6.5e-07,
      "loss": 0.0012,
      "step": 118440
    },
    {
      "epoch": 7.8966666666666665,
      "grad_norm": 0.5012375712394714,
      "learning_rate": 6.458333333333333e-07,
      "loss": 0.0023,
      "step": 118450
    },
    {
      "epoch": 7.897333333333333,
      "grad_norm": 0.2697267234325409,
      "learning_rate": 6.416666666666667e-07,
      "loss": 0.0016,
      "step": 118460
    },
    {
      "epoch": 7.898,
      "grad_norm": 0.07850667089223862,
      "learning_rate": 6.375e-07,
      "loss": 0.0014,
      "step": 118470
    },
    {
      "epoch": 7.898666666666666,
      "grad_norm": 0.3613894283771515,
      "learning_rate": 6.333333333333333e-07,
      "loss": 0.0014,
      "step": 118480
    },
    {
      "epoch": 7.899333333333333,
      "grad_norm": 0.028851021081209183,
      "learning_rate": 6.291666666666667e-07,
      "loss": 0.0014,
      "step": 118490
    },
    {
      "epoch": 7.9,
      "grad_norm": 0.3672942519187927,
      "learning_rate": 6.25e-07,
      "loss": 0.0023,
      "step": 118500
    },
    {
      "epoch": 7.900666666666667,
      "grad_norm": 0.29545173048973083,
      "learning_rate": 6.208333333333334e-07,
      "loss": 0.0019,
      "step": 118510
    },
    {
      "epoch": 7.9013333333333335,
      "grad_norm": 0.12283650040626526,
      "learning_rate": 6.166666666666667e-07,
      "loss": 0.0017,
      "step": 118520
    },
    {
      "epoch": 7.902,
      "grad_norm": 0.297902375459671,
      "learning_rate": 6.125000000000001e-07,
      "loss": 0.0013,
      "step": 118530
    },
    {
      "epoch": 7.902666666666667,
      "grad_norm": 0.23333027958869934,
      "learning_rate": 6.083333333333334e-07,
      "loss": 0.0021,
      "step": 118540
    },
    {
      "epoch": 7.903333333333333,
      "grad_norm": 0.26798000931739807,
      "learning_rate": 6.041666666666667e-07,
      "loss": 0.0014,
      "step": 118550
    },
    {
      "epoch": 7.904,
      "grad_norm": 0.14695492386817932,
      "learning_rate": 6.000000000000001e-07,
      "loss": 0.0017,
      "step": 118560
    },
    {
      "epoch": 7.9046666666666665,
      "grad_norm": 0.031722430139780045,
      "learning_rate": 5.958333333333334e-07,
      "loss": 0.002,
      "step": 118570
    },
    {
      "epoch": 7.905333333333333,
      "grad_norm": 0.19879399240016937,
      "learning_rate": 5.916666666666667e-07,
      "loss": 0.0015,
      "step": 118580
    },
    {
      "epoch": 7.906,
      "grad_norm": 0.36198297142982483,
      "learning_rate": 5.875e-07,
      "loss": 0.0025,
      "step": 118590
    },
    {
      "epoch": 7.906666666666666,
      "grad_norm": 0.03331007435917854,
      "learning_rate": 5.833333333333334e-07,
      "loss": 0.0022,
      "step": 118600
    },
    {
      "epoch": 7.907333333333334,
      "grad_norm": 0.35251957178115845,
      "learning_rate": 5.791666666666667e-07,
      "loss": 0.002,
      "step": 118610
    },
    {
      "epoch": 7.908,
      "grad_norm": 0.43620505928993225,
      "learning_rate": 5.75e-07,
      "loss": 0.0018,
      "step": 118620
    },
    {
      "epoch": 7.908666666666667,
      "grad_norm": 0.03911076858639717,
      "learning_rate": 5.708333333333334e-07,
      "loss": 0.002,
      "step": 118630
    },
    {
      "epoch": 7.9093333333333335,
      "grad_norm": 0.06211051344871521,
      "learning_rate": 5.666666666666667e-07,
      "loss": 0.0018,
      "step": 118640
    },
    {
      "epoch": 7.91,
      "grad_norm": 0.3641080856323242,
      "learning_rate": 5.625e-07,
      "loss": 0.002,
      "step": 118650
    },
    {
      "epoch": 7.910666666666667,
      "grad_norm": 0.07422145456075668,
      "learning_rate": 5.583333333333333e-07,
      "loss": 0.0019,
      "step": 118660
    },
    {
      "epoch": 7.911333333333333,
      "grad_norm": 0.43089795112609863,
      "learning_rate": 5.541666666666667e-07,
      "loss": 0.0014,
      "step": 118670
    },
    {
      "epoch": 7.912,
      "grad_norm": 0.044190913438797,
      "learning_rate": 5.5e-07,
      "loss": 0.0026,
      "step": 118680
    },
    {
      "epoch": 7.9126666666666665,
      "grad_norm": 0.10079828649759293,
      "learning_rate": 5.458333333333333e-07,
      "loss": 0.0029,
      "step": 118690
    },
    {
      "epoch": 7.913333333333333,
      "grad_norm": 0.3704925775527954,
      "learning_rate": 5.416666666666667e-07,
      "loss": 0.0016,
      "step": 118700
    },
    {
      "epoch": 7.914,
      "grad_norm": 0.17496556043624878,
      "learning_rate": 5.374999999999999e-07,
      "loss": 0.0012,
      "step": 118710
    },
    {
      "epoch": 7.914666666666666,
      "grad_norm": 0.18878452479839325,
      "learning_rate": 5.333333333333333e-07,
      "loss": 0.0012,
      "step": 118720
    },
    {
      "epoch": 7.915333333333333,
      "grad_norm": 0.4772421717643738,
      "learning_rate": 5.291666666666666e-07,
      "loss": 0.0025,
      "step": 118730
    },
    {
      "epoch": 7.916,
      "grad_norm": 0.41783684492111206,
      "learning_rate": 5.250000000000001e-07,
      "loss": 0.0025,
      "step": 118740
    },
    {
      "epoch": 7.916666666666667,
      "grad_norm": 0.0733318030834198,
      "learning_rate": 5.208333333333334e-07,
      "loss": 0.0021,
      "step": 118750
    },
    {
      "epoch": 7.917333333333334,
      "grad_norm": 0.35664546489715576,
      "learning_rate": 5.166666666666667e-07,
      "loss": 0.0021,
      "step": 118760
    },
    {
      "epoch": 7.918,
      "grad_norm": 0.23299606144428253,
      "learning_rate": 5.125000000000001e-07,
      "loss": 0.0016,
      "step": 118770
    },
    {
      "epoch": 7.918666666666667,
      "grad_norm": 0.11265259236097336,
      "learning_rate": 5.083333333333333e-07,
      "loss": 0.002,
      "step": 118780
    },
    {
      "epoch": 7.919333333333333,
      "grad_norm": 0.054179076105356216,
      "learning_rate": 5.041666666666667e-07,
      "loss": 0.0024,
      "step": 118790
    },
    {
      "epoch": 7.92,
      "grad_norm": 0.04656461253762245,
      "learning_rate": 5.000000000000001e-07,
      "loss": 0.0015,
      "step": 118800
    },
    {
      "epoch": 7.9206666666666665,
      "grad_norm": 0.05582188442349434,
      "learning_rate": 4.958333333333334e-07,
      "loss": 0.0021,
      "step": 118810
    },
    {
      "epoch": 7.921333333333333,
      "grad_norm": 0.10426853597164154,
      "learning_rate": 4.916666666666667e-07,
      "loss": 0.0026,
      "step": 118820
    },
    {
      "epoch": 7.922,
      "grad_norm": 0.10999476164579391,
      "learning_rate": 4.875e-07,
      "loss": 0.0018,
      "step": 118830
    },
    {
      "epoch": 7.922666666666666,
      "grad_norm": 0.20924249291419983,
      "learning_rate": 4.833333333333334e-07,
      "loss": 0.002,
      "step": 118840
    },
    {
      "epoch": 7.923333333333334,
      "grad_norm": 0.3027491569519043,
      "learning_rate": 4.791666666666667e-07,
      "loss": 0.0014,
      "step": 118850
    },
    {
      "epoch": 7.924,
      "grad_norm": 0.23854774236679077,
      "learning_rate": 4.75e-07,
      "loss": 0.0016,
      "step": 118860
    },
    {
      "epoch": 7.924666666666667,
      "grad_norm": 0.3750653862953186,
      "learning_rate": 4.7083333333333336e-07,
      "loss": 0.0014,
      "step": 118870
    },
    {
      "epoch": 7.925333333333334,
      "grad_norm": 0.04589162394404411,
      "learning_rate": 4.666666666666667e-07,
      "loss": 0.0024,
      "step": 118880
    },
    {
      "epoch": 7.926,
      "grad_norm": 0.13615234196186066,
      "learning_rate": 4.625e-07,
      "loss": 0.0019,
      "step": 118890
    },
    {
      "epoch": 7.926666666666667,
      "grad_norm": 0.07506529241800308,
      "learning_rate": 4.583333333333334e-07,
      "loss": 0.0015,
      "step": 118900
    },
    {
      "epoch": 7.927333333333333,
      "grad_norm": 0.39836427569389343,
      "learning_rate": 4.5416666666666673e-07,
      "loss": 0.0023,
      "step": 118910
    },
    {
      "epoch": 7.928,
      "grad_norm": 0.14177924394607544,
      "learning_rate": 4.5e-07,
      "loss": 0.0017,
      "step": 118920
    },
    {
      "epoch": 7.9286666666666665,
      "grad_norm": 0.36149320006370544,
      "learning_rate": 4.4583333333333334e-07,
      "loss": 0.0021,
      "step": 118930
    },
    {
      "epoch": 7.929333333333333,
      "grad_norm": 0.2390819638967514,
      "learning_rate": 4.416666666666667e-07,
      "loss": 0.0017,
      "step": 118940
    },
    {
      "epoch": 7.93,
      "grad_norm": 0.23684120178222656,
      "learning_rate": 4.3750000000000005e-07,
      "loss": 0.0015,
      "step": 118950
    },
    {
      "epoch": 7.930666666666666,
      "grad_norm": 0.2299833595752716,
      "learning_rate": 4.3333333333333335e-07,
      "loss": 0.002,
      "step": 118960
    },
    {
      "epoch": 7.931333333333333,
      "grad_norm": 0.36839792132377625,
      "learning_rate": 4.291666666666667e-07,
      "loss": 0.0021,
      "step": 118970
    },
    {
      "epoch": 7.932,
      "grad_norm": 0.507476270198822,
      "learning_rate": 4.2500000000000006e-07,
      "loss": 0.0014,
      "step": 118980
    },
    {
      "epoch": 7.932666666666667,
      "grad_norm": 0.08182267844676971,
      "learning_rate": 4.208333333333333e-07,
      "loss": 0.0015,
      "step": 118990
    },
    {
      "epoch": 7.933333333333334,
      "grad_norm": 0.1368570178747177,
      "learning_rate": 4.1666666666666667e-07,
      "loss": 0.0018,
      "step": 119000
    },
    {
      "epoch": 7.934,
      "grad_norm": 0.11627859622240067,
      "learning_rate": 4.125e-07,
      "loss": 0.0021,
      "step": 119010
    },
    {
      "epoch": 7.934666666666667,
      "grad_norm": 0.043817006051540375,
      "learning_rate": 4.083333333333333e-07,
      "loss": 0.0017,
      "step": 119020
    },
    {
      "epoch": 7.935333333333333,
      "grad_norm": 0.08170811086893082,
      "learning_rate": 4.041666666666667e-07,
      "loss": 0.0021,
      "step": 119030
    },
    {
      "epoch": 7.936,
      "grad_norm": 0.18510815501213074,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 0.002,
      "step": 119040
    },
    {
      "epoch": 7.9366666666666665,
      "grad_norm": 0.5338173508644104,
      "learning_rate": 3.958333333333334e-07,
      "loss": 0.0019,
      "step": 119050
    },
    {
      "epoch": 7.937333333333333,
      "grad_norm": 0.4623206853866577,
      "learning_rate": 3.9166666666666664e-07,
      "loss": 0.0019,
      "step": 119060
    },
    {
      "epoch": 7.938,
      "grad_norm": 0.19920289516448975,
      "learning_rate": 3.875e-07,
      "loss": 0.0016,
      "step": 119070
    },
    {
      "epoch": 7.938666666666666,
      "grad_norm": 0.3624499440193176,
      "learning_rate": 3.8333333333333335e-07,
      "loss": 0.002,
      "step": 119080
    },
    {
      "epoch": 7.939333333333334,
      "grad_norm": 0.08056192100048065,
      "learning_rate": 3.791666666666667e-07,
      "loss": 0.0013,
      "step": 119090
    },
    {
      "epoch": 7.9399999999999995,
      "grad_norm": 0.3048754632472992,
      "learning_rate": 3.75e-07,
      "loss": 0.0022,
      "step": 119100
    },
    {
      "epoch": 7.940666666666667,
      "grad_norm": 0.17015428841114044,
      "learning_rate": 3.7083333333333337e-07,
      "loss": 0.0017,
      "step": 119110
    },
    {
      "epoch": 7.941333333333334,
      "grad_norm": 0.3303849995136261,
      "learning_rate": 3.6666666666666667e-07,
      "loss": 0.002,
      "step": 119120
    },
    {
      "epoch": 7.942,
      "grad_norm": 0.23621220886707306,
      "learning_rate": 3.625e-07,
      "loss": 0.0018,
      "step": 119130
    },
    {
      "epoch": 7.942666666666667,
      "grad_norm": 0.2397266924381256,
      "learning_rate": 3.583333333333333e-07,
      "loss": 0.002,
      "step": 119140
    },
    {
      "epoch": 7.943333333333333,
      "grad_norm": 0.20995786786079407,
      "learning_rate": 3.541666666666667e-07,
      "loss": 0.0022,
      "step": 119150
    },
    {
      "epoch": 7.944,
      "grad_norm": 0.45219939947128296,
      "learning_rate": 3.5000000000000004e-07,
      "loss": 0.0015,
      "step": 119160
    },
    {
      "epoch": 7.9446666666666665,
      "grad_norm": 0.10920821130275726,
      "learning_rate": 3.4583333333333334e-07,
      "loss": 0.0019,
      "step": 119170
    },
    {
      "epoch": 7.945333333333333,
      "grad_norm": 0.5859558582305908,
      "learning_rate": 3.416666666666667e-07,
      "loss": 0.0014,
      "step": 119180
    },
    {
      "epoch": 7.946,
      "grad_norm": 0.12084696441888809,
      "learning_rate": 3.375e-07,
      "loss": 0.0013,
      "step": 119190
    },
    {
      "epoch": 7.946666666666666,
      "grad_norm": 0.1723307967185974,
      "learning_rate": 3.3333333333333335e-07,
      "loss": 0.0011,
      "step": 119200
    },
    {
      "epoch": 7.947333333333333,
      "grad_norm": 0.053022220730781555,
      "learning_rate": 3.291666666666667e-07,
      "loss": 0.002,
      "step": 119210
    },
    {
      "epoch": 7.948,
      "grad_norm": 0.2650126516819,
      "learning_rate": 3.25e-07,
      "loss": 0.0014,
      "step": 119220
    },
    {
      "epoch": 7.948666666666667,
      "grad_norm": 0.2046644240617752,
      "learning_rate": 3.2083333333333337e-07,
      "loss": 0.0017,
      "step": 119230
    },
    {
      "epoch": 7.949333333333334,
      "grad_norm": 0.2724965214729309,
      "learning_rate": 3.1666666666666667e-07,
      "loss": 0.002,
      "step": 119240
    },
    {
      "epoch": 7.95,
      "grad_norm": 0.050924770534038544,
      "learning_rate": 3.125e-07,
      "loss": 0.0016,
      "step": 119250
    },
    {
      "epoch": 7.950666666666667,
      "grad_norm": 0.1560782492160797,
      "learning_rate": 3.0833333333333333e-07,
      "loss": 0.002,
      "step": 119260
    },
    {
      "epoch": 7.951333333333333,
      "grad_norm": 0.21073734760284424,
      "learning_rate": 3.041666666666667e-07,
      "loss": 0.0019,
      "step": 119270
    },
    {
      "epoch": 7.952,
      "grad_norm": 0.20823940634727478,
      "learning_rate": 3.0000000000000004e-07,
      "loss": 0.0017,
      "step": 119280
    },
    {
      "epoch": 7.9526666666666666,
      "grad_norm": 0.4467773735523224,
      "learning_rate": 2.9583333333333334e-07,
      "loss": 0.0024,
      "step": 119290
    },
    {
      "epoch": 7.953333333333333,
      "grad_norm": 0.045485883951187134,
      "learning_rate": 2.916666666666667e-07,
      "loss": 0.0023,
      "step": 119300
    },
    {
      "epoch": 7.954,
      "grad_norm": 0.05269661918282509,
      "learning_rate": 2.875e-07,
      "loss": 0.0019,
      "step": 119310
    },
    {
      "epoch": 7.954666666666666,
      "grad_norm": 0.2090626060962677,
      "learning_rate": 2.8333333333333336e-07,
      "loss": 0.0021,
      "step": 119320
    },
    {
      "epoch": 7.955333333333334,
      "grad_norm": 0.4170290231704712,
      "learning_rate": 2.7916666666666666e-07,
      "loss": 0.0013,
      "step": 119330
    },
    {
      "epoch": 7.9559999999999995,
      "grad_norm": 0.06774050742387772,
      "learning_rate": 2.75e-07,
      "loss": 0.0015,
      "step": 119340
    },
    {
      "epoch": 7.956666666666667,
      "grad_norm": 0.2958203852176666,
      "learning_rate": 2.7083333333333337e-07,
      "loss": 0.0015,
      "step": 119350
    },
    {
      "epoch": 7.957333333333334,
      "grad_norm": 0.039940208196640015,
      "learning_rate": 2.6666666666666667e-07,
      "loss": 0.0019,
      "step": 119360
    },
    {
      "epoch": 7.958,
      "grad_norm": 0.046910710632801056,
      "learning_rate": 2.6250000000000003e-07,
      "loss": 0.0026,
      "step": 119370
    },
    {
      "epoch": 7.958666666666667,
      "grad_norm": 0.35499200224876404,
      "learning_rate": 2.5833333333333333e-07,
      "loss": 0.0019,
      "step": 119380
    },
    {
      "epoch": 7.959333333333333,
      "grad_norm": 0.07063759863376617,
      "learning_rate": 2.5416666666666663e-07,
      "loss": 0.0028,
      "step": 119390
    },
    {
      "epoch": 7.96,
      "grad_norm": 0.1083376482129097,
      "learning_rate": 2.5000000000000004e-07,
      "loss": 0.0015,
      "step": 119400
    },
    {
      "epoch": 7.960666666666667,
      "grad_norm": 0.4195629954338074,
      "learning_rate": 2.4583333333333334e-07,
      "loss": 0.0018,
      "step": 119410
    },
    {
      "epoch": 7.961333333333333,
      "grad_norm": 0.2748662531375885,
      "learning_rate": 2.416666666666667e-07,
      "loss": 0.0021,
      "step": 119420
    },
    {
      "epoch": 7.962,
      "grad_norm": 0.16677479445934296,
      "learning_rate": 2.375e-07,
      "loss": 0.0023,
      "step": 119430
    },
    {
      "epoch": 7.962666666666666,
      "grad_norm": 0.3699880540370941,
      "learning_rate": 2.3333333333333336e-07,
      "loss": 0.0016,
      "step": 119440
    },
    {
      "epoch": 7.963333333333333,
      "grad_norm": 0.3136977255344391,
      "learning_rate": 2.291666666666667e-07,
      "loss": 0.0017,
      "step": 119450
    },
    {
      "epoch": 7.964,
      "grad_norm": 0.0708513855934143,
      "learning_rate": 2.25e-07,
      "loss": 0.0015,
      "step": 119460
    },
    {
      "epoch": 7.964666666666667,
      "grad_norm": 0.08402609080076218,
      "learning_rate": 2.2083333333333335e-07,
      "loss": 0.0015,
      "step": 119470
    },
    {
      "epoch": 7.965333333333334,
      "grad_norm": 0.10366591066122055,
      "learning_rate": 2.1666666666666667e-07,
      "loss": 0.0014,
      "step": 119480
    },
    {
      "epoch": 7.966,
      "grad_norm": 0.1415659636259079,
      "learning_rate": 2.1250000000000003e-07,
      "loss": 0.0031,
      "step": 119490
    },
    {
      "epoch": 7.966666666666667,
      "grad_norm": 0.07419995963573456,
      "learning_rate": 2.0833333333333333e-07,
      "loss": 0.0019,
      "step": 119500
    },
    {
      "epoch": 7.967333333333333,
      "grad_norm": 0.7223125696182251,
      "learning_rate": 2.0416666666666666e-07,
      "loss": 0.0019,
      "step": 119510
    },
    {
      "epoch": 7.968,
      "grad_norm": 0.37001198530197144,
      "learning_rate": 2.0000000000000002e-07,
      "loss": 0.0013,
      "step": 119520
    },
    {
      "epoch": 7.968666666666667,
      "grad_norm": 0.5684213042259216,
      "learning_rate": 1.9583333333333332e-07,
      "loss": 0.0018,
      "step": 119530
    },
    {
      "epoch": 7.969333333333333,
      "grad_norm": 0.3006192147731781,
      "learning_rate": 1.9166666666666668e-07,
      "loss": 0.0018,
      "step": 119540
    },
    {
      "epoch": 7.97,
      "grad_norm": 0.02815483696758747,
      "learning_rate": 1.875e-07,
      "loss": 0.0029,
      "step": 119550
    },
    {
      "epoch": 7.970666666666666,
      "grad_norm": 0.16713185608386993,
      "learning_rate": 1.8333333333333333e-07,
      "loss": 0.0024,
      "step": 119560
    },
    {
      "epoch": 7.971333333333334,
      "grad_norm": 0.3680321276187897,
      "learning_rate": 1.7916666666666666e-07,
      "loss": 0.0023,
      "step": 119570
    },
    {
      "epoch": 7.9719999999999995,
      "grad_norm": 0.17166821658611298,
      "learning_rate": 1.7500000000000002e-07,
      "loss": 0.0019,
      "step": 119580
    },
    {
      "epoch": 7.972666666666667,
      "grad_norm": 0.1369519829750061,
      "learning_rate": 1.7083333333333335e-07,
      "loss": 0.0015,
      "step": 119590
    },
    {
      "epoch": 7.973333333333334,
      "grad_norm": 0.1595880389213562,
      "learning_rate": 1.6666666666666668e-07,
      "loss": 0.0021,
      "step": 119600
    },
    {
      "epoch": 7.974,
      "grad_norm": 0.43395960330963135,
      "learning_rate": 1.625e-07,
      "loss": 0.0028,
      "step": 119610
    },
    {
      "epoch": 7.974666666666667,
      "grad_norm": 0.13910172879695892,
      "learning_rate": 1.5833333333333333e-07,
      "loss": 0.0022,
      "step": 119620
    },
    {
      "epoch": 7.975333333333333,
      "grad_norm": 0.14100834727287292,
      "learning_rate": 1.5416666666666666e-07,
      "loss": 0.0015,
      "step": 119630
    },
    {
      "epoch": 7.976,
      "grad_norm": 0.0754486620426178,
      "learning_rate": 1.5000000000000002e-07,
      "loss": 0.0014,
      "step": 119640
    },
    {
      "epoch": 7.976666666666667,
      "grad_norm": 0.2439107745885849,
      "learning_rate": 1.4583333333333335e-07,
      "loss": 0.0017,
      "step": 119650
    },
    {
      "epoch": 7.977333333333333,
      "grad_norm": 0.07749604433774948,
      "learning_rate": 1.4166666666666668e-07,
      "loss": 0.0022,
      "step": 119660
    },
    {
      "epoch": 7.978,
      "grad_norm": 0.5295325517654419,
      "learning_rate": 1.375e-07,
      "loss": 0.0018,
      "step": 119670
    },
    {
      "epoch": 7.978666666666666,
      "grad_norm": 0.5026654601097107,
      "learning_rate": 1.3333333333333334e-07,
      "loss": 0.0019,
      "step": 119680
    },
    {
      "epoch": 7.979333333333333,
      "grad_norm": 0.138020321726799,
      "learning_rate": 1.2916666666666667e-07,
      "loss": 0.002,
      "step": 119690
    },
    {
      "epoch": 7.98,
      "grad_norm": 0.2828995883464813,
      "learning_rate": 1.2500000000000002e-07,
      "loss": 0.0016,
      "step": 119700
    },
    {
      "epoch": 7.980666666666667,
      "grad_norm": 0.05857975035905838,
      "learning_rate": 1.2083333333333335e-07,
      "loss": 0.0018,
      "step": 119710
    },
    {
      "epoch": 7.981333333333334,
      "grad_norm": 0.08110179752111435,
      "learning_rate": 1.1666666666666668e-07,
      "loss": 0.002,
      "step": 119720
    },
    {
      "epoch": 7.982,
      "grad_norm": 0.3394795060157776,
      "learning_rate": 1.125e-07,
      "loss": 0.0023,
      "step": 119730
    },
    {
      "epoch": 7.982666666666667,
      "grad_norm": 0.33511409163475037,
      "learning_rate": 1.0833333333333334e-07,
      "loss": 0.0014,
      "step": 119740
    },
    {
      "epoch": 7.983333333333333,
      "grad_norm": 0.13699807226657867,
      "learning_rate": 1.0416666666666667e-07,
      "loss": 0.002,
      "step": 119750
    },
    {
      "epoch": 7.984,
      "grad_norm": 0.08243662118911743,
      "learning_rate": 1.0000000000000001e-07,
      "loss": 0.0015,
      "step": 119760
    },
    {
      "epoch": 7.984666666666667,
      "grad_norm": 0.0499715618789196,
      "learning_rate": 9.583333333333334e-08,
      "loss": 0.002,
      "step": 119770
    },
    {
      "epoch": 7.985333333333333,
      "grad_norm": 0.08492676913738251,
      "learning_rate": 9.166666666666667e-08,
      "loss": 0.002,
      "step": 119780
    },
    {
      "epoch": 7.986,
      "grad_norm": 0.23427912592887878,
      "learning_rate": 8.750000000000001e-08,
      "loss": 0.0017,
      "step": 119790
    },
    {
      "epoch": 7.986666666666666,
      "grad_norm": 0.3854394555091858,
      "learning_rate": 8.333333333333334e-08,
      "loss": 0.0025,
      "step": 119800
    },
    {
      "epoch": 7.987333333333333,
      "grad_norm": 0.11562331765890121,
      "learning_rate": 7.916666666666667e-08,
      "loss": 0.0014,
      "step": 119810
    },
    {
      "epoch": 7.9879999999999995,
      "grad_norm": 0.08334054797887802,
      "learning_rate": 7.500000000000001e-08,
      "loss": 0.0025,
      "step": 119820
    },
    {
      "epoch": 7.988666666666667,
      "grad_norm": 0.08938081562519073,
      "learning_rate": 7.083333333333334e-08,
      "loss": 0.0018,
      "step": 119830
    },
    {
      "epoch": 7.989333333333334,
      "grad_norm": 0.5690155029296875,
      "learning_rate": 6.666666666666667e-08,
      "loss": 0.0015,
      "step": 119840
    },
    {
      "epoch": 7.99,
      "grad_norm": 0.04962611570954323,
      "learning_rate": 6.250000000000001e-08,
      "loss": 0.0014,
      "step": 119850
    },
    {
      "epoch": 7.990666666666667,
      "grad_norm": 0.23417580127716064,
      "learning_rate": 5.833333333333334e-08,
      "loss": 0.0021,
      "step": 119860
    },
    {
      "epoch": 7.991333333333333,
      "grad_norm": 0.16861355304718018,
      "learning_rate": 5.416666666666667e-08,
      "loss": 0.0015,
      "step": 119870
    },
    {
      "epoch": 7.992,
      "grad_norm": 0.07803519070148468,
      "learning_rate": 5.0000000000000004e-08,
      "loss": 0.0023,
      "step": 119880
    },
    {
      "epoch": 7.992666666666667,
      "grad_norm": 0.17155173420906067,
      "learning_rate": 4.5833333333333333e-08,
      "loss": 0.0012,
      "step": 119890
    },
    {
      "epoch": 7.993333333333333,
      "grad_norm": 0.14576177299022675,
      "learning_rate": 4.166666666666667e-08,
      "loss": 0.0022,
      "step": 119900
    },
    {
      "epoch": 7.994,
      "grad_norm": 0.037480298429727554,
      "learning_rate": 3.7500000000000005e-08,
      "loss": 0.0012,
      "step": 119910
    },
    {
      "epoch": 7.994666666666666,
      "grad_norm": 0.2676650285720825,
      "learning_rate": 3.3333333333333334e-08,
      "loss": 0.002,
      "step": 119920
    },
    {
      "epoch": 7.995333333333333,
      "grad_norm": 0.08221422880887985,
      "learning_rate": 2.916666666666667e-08,
      "loss": 0.0015,
      "step": 119930
    },
    {
      "epoch": 7.996,
      "grad_norm": 0.13079184293746948,
      "learning_rate": 2.5000000000000002e-08,
      "loss": 0.0019,
      "step": 119940
    },
    {
      "epoch": 7.996666666666667,
      "grad_norm": 0.26721927523612976,
      "learning_rate": 2.0833333333333335e-08,
      "loss": 0.0015,
      "step": 119950
    },
    {
      "epoch": 7.997333333333334,
      "grad_norm": 0.0548417828977108,
      "learning_rate": 1.6666666666666667e-08,
      "loss": 0.0013,
      "step": 119960
    },
    {
      "epoch": 7.998,
      "grad_norm": 0.5211466550827026,
      "learning_rate": 1.2500000000000001e-08,
      "loss": 0.0024,
      "step": 119970
    },
    {
      "epoch": 7.998666666666667,
      "grad_norm": 0.11854124814271927,
      "learning_rate": 8.333333333333334e-09,
      "loss": 0.0012,
      "step": 119980
    },
    {
      "epoch": 7.999333333333333,
      "grad_norm": 0.24754449725151062,
      "learning_rate": 4.166666666666667e-09,
      "loss": 0.0019,
      "step": 119990
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.1456594318151474,
      "learning_rate": 0.0,
      "loss": 0.0014,
      "step": 120000
    }
  ],
  "logging_steps": 10,
  "max_steps": 120000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 8,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 40,
  "trial_name": null,
  "trial_params": null
}

{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9733333333333334,
  "eval_steps": 500,
  "global_step": 37000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0005333333333333334,
      "grad_norm": 0.8569134473800659,
      "learning_rate": 4.9986666666666674e-05,
      "loss": 0.0025,
      "step": 10
    },
    {
      "epoch": 0.0010666666666666667,
      "grad_norm": 0.165547177195549,
      "learning_rate": 4.997333333333333e-05,
      "loss": 0.0029,
      "step": 20
    },
    {
      "epoch": 0.0016,
      "grad_norm": 0.3923609256744385,
      "learning_rate": 4.996e-05,
      "loss": 0.003,
      "step": 30
    },
    {
      "epoch": 0.0021333333333333334,
      "grad_norm": 0.1866934895515442,
      "learning_rate": 4.994666666666667e-05,
      "loss": 0.003,
      "step": 40
    },
    {
      "epoch": 0.0026666666666666666,
      "grad_norm": 0.24192090332508087,
      "learning_rate": 4.993333333333334e-05,
      "loss": 0.0035,
      "step": 50
    },
    {
      "epoch": 0.0032,
      "grad_norm": 0.5942729115486145,
      "learning_rate": 4.992e-05,
      "loss": 0.0043,
      "step": 60
    },
    {
      "epoch": 0.0037333333333333333,
      "grad_norm": 0.2648719251155853,
      "learning_rate": 4.990666666666667e-05,
      "loss": 0.0043,
      "step": 70
    },
    {
      "epoch": 0.004266666666666667,
      "grad_norm": 0.4968264400959015,
      "learning_rate": 4.989333333333334e-05,
      "loss": 0.0038,
      "step": 80
    },
    {
      "epoch": 0.0048,
      "grad_norm": 0.46066176891326904,
      "learning_rate": 4.9880000000000004e-05,
      "loss": 0.0033,
      "step": 90
    },
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 0.10030889511108398,
      "learning_rate": 4.986666666666667e-05,
      "loss": 0.0031,
      "step": 100
    },
    {
      "epoch": 0.005866666666666667,
      "grad_norm": 0.6516956686973572,
      "learning_rate": 4.985333333333333e-05,
      "loss": 0.0032,
      "step": 110
    },
    {
      "epoch": 0.0064,
      "grad_norm": 0.30214688181877136,
      "learning_rate": 4.9840000000000004e-05,
      "loss": 0.0036,
      "step": 120
    },
    {
      "epoch": 0.006933333333333333,
      "grad_norm": 0.4975658655166626,
      "learning_rate": 4.982666666666667e-05,
      "loss": 0.004,
      "step": 130
    },
    {
      "epoch": 0.007466666666666667,
      "grad_norm": 0.792850911617279,
      "learning_rate": 4.981333333333333e-05,
      "loss": 0.0035,
      "step": 140
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.33229222893714905,
      "learning_rate": 4.9800000000000004e-05,
      "loss": 0.003,
      "step": 150
    },
    {
      "epoch": 0.008533333333333334,
      "grad_norm": 0.6219444274902344,
      "learning_rate": 4.978666666666667e-05,
      "loss": 0.0034,
      "step": 160
    },
    {
      "epoch": 0.009066666666666667,
      "grad_norm": 0.295170396566391,
      "learning_rate": 4.977333333333334e-05,
      "loss": 0.0027,
      "step": 170
    },
    {
      "epoch": 0.0096,
      "grad_norm": 0.4950981140136719,
      "learning_rate": 4.976e-05,
      "loss": 0.0048,
      "step": 180
    },
    {
      "epoch": 0.010133333333333333,
      "grad_norm": 0.22129465639591217,
      "learning_rate": 4.974666666666667e-05,
      "loss": 0.0025,
      "step": 190
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 0.17659486830234528,
      "learning_rate": 4.973333333333334e-05,
      "loss": 0.0028,
      "step": 200
    },
    {
      "epoch": 0.0112,
      "grad_norm": 0.26850688457489014,
      "learning_rate": 4.972e-05,
      "loss": 0.0022,
      "step": 210
    },
    {
      "epoch": 0.011733333333333333,
      "grad_norm": 0.4162222445011139,
      "learning_rate": 4.970666666666667e-05,
      "loss": 0.0029,
      "step": 220
    },
    {
      "epoch": 0.012266666666666667,
      "grad_norm": 0.32360926270484924,
      "learning_rate": 4.9693333333333334e-05,
      "loss": 0.0044,
      "step": 230
    },
    {
      "epoch": 0.0128,
      "grad_norm": 1.0045634508132935,
      "learning_rate": 4.9680000000000005e-05,
      "loss": 0.0033,
      "step": 240
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.9793700575828552,
      "learning_rate": 4.966666666666667e-05,
      "loss": 0.0029,
      "step": 250
    },
    {
      "epoch": 0.013866666666666666,
      "grad_norm": 0.7534539103507996,
      "learning_rate": 4.9653333333333335e-05,
      "loss": 0.0035,
      "step": 260
    },
    {
      "epoch": 0.0144,
      "grad_norm": 0.29890307784080505,
      "learning_rate": 4.9640000000000006e-05,
      "loss": 0.0034,
      "step": 270
    },
    {
      "epoch": 0.014933333333333333,
      "grad_norm": 0.7847545742988586,
      "learning_rate": 4.962666666666667e-05,
      "loss": 0.0034,
      "step": 280
    },
    {
      "epoch": 0.015466666666666667,
      "grad_norm": 0.3706609010696411,
      "learning_rate": 4.9613333333333335e-05,
      "loss": 0.0035,
      "step": 290
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.2824493646621704,
      "learning_rate": 4.96e-05,
      "loss": 0.0021,
      "step": 300
    },
    {
      "epoch": 0.016533333333333334,
      "grad_norm": 0.18391135334968567,
      "learning_rate": 4.958666666666667e-05,
      "loss": 0.0028,
      "step": 310
    },
    {
      "epoch": 0.017066666666666667,
      "grad_norm": 0.13914239406585693,
      "learning_rate": 4.9573333333333335e-05,
      "loss": 0.0029,
      "step": 320
    },
    {
      "epoch": 0.0176,
      "grad_norm": 0.24757854640483856,
      "learning_rate": 4.956e-05,
      "loss": 0.0027,
      "step": 330
    },
    {
      "epoch": 0.018133333333333335,
      "grad_norm": 0.2772384285926819,
      "learning_rate": 4.954666666666667e-05,
      "loss": 0.0035,
      "step": 340
    },
    {
      "epoch": 0.018666666666666668,
      "grad_norm": 0.3570190668106079,
      "learning_rate": 4.9533333333333336e-05,
      "loss": 0.0026,
      "step": 350
    },
    {
      "epoch": 0.0192,
      "grad_norm": 0.4505486786365509,
      "learning_rate": 4.952e-05,
      "loss": 0.0042,
      "step": 360
    },
    {
      "epoch": 0.019733333333333332,
      "grad_norm": 0.32081034779548645,
      "learning_rate": 4.9506666666666665e-05,
      "loss": 0.0027,
      "step": 370
    },
    {
      "epoch": 0.020266666666666665,
      "grad_norm": 0.3172169625759125,
      "learning_rate": 4.9493333333333336e-05,
      "loss": 0.0029,
      "step": 380
    },
    {
      "epoch": 0.0208,
      "grad_norm": 0.13584744930267334,
      "learning_rate": 4.948000000000001e-05,
      "loss": 0.0037,
      "step": 390
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 0.07729235291481018,
      "learning_rate": 4.9466666666666665e-05,
      "loss": 0.0024,
      "step": 400
    },
    {
      "epoch": 0.021866666666666666,
      "grad_norm": 0.4843331575393677,
      "learning_rate": 4.9453333333333336e-05,
      "loss": 0.0026,
      "step": 410
    },
    {
      "epoch": 0.0224,
      "grad_norm": 0.5476429462432861,
      "learning_rate": 4.944e-05,
      "loss": 0.0031,
      "step": 420
    },
    {
      "epoch": 0.022933333333333333,
      "grad_norm": 0.31950345635414124,
      "learning_rate": 4.942666666666667e-05,
      "loss": 0.003,
      "step": 430
    },
    {
      "epoch": 0.023466666666666667,
      "grad_norm": 0.7205737829208374,
      "learning_rate": 4.941333333333334e-05,
      "loss": 0.0025,
      "step": 440
    },
    {
      "epoch": 0.024,
      "grad_norm": 0.13329513370990753,
      "learning_rate": 4.94e-05,
      "loss": 0.0021,
      "step": 450
    },
    {
      "epoch": 0.024533333333333334,
      "grad_norm": 0.26941192150115967,
      "learning_rate": 4.938666666666667e-05,
      "loss": 0.0027,
      "step": 460
    },
    {
      "epoch": 0.025066666666666668,
      "grad_norm": 0.8547675609588623,
      "learning_rate": 4.937333333333334e-05,
      "loss": 0.0027,
      "step": 470
    },
    {
      "epoch": 0.0256,
      "grad_norm": 0.2749357223510742,
      "learning_rate": 4.936e-05,
      "loss": 0.0027,
      "step": 480
    },
    {
      "epoch": 0.026133333333333335,
      "grad_norm": 0.6044763326644897,
      "learning_rate": 4.9346666666666666e-05,
      "loss": 0.0025,
      "step": 490
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.5281338095664978,
      "learning_rate": 4.933333333333334e-05,
      "loss": 0.0023,
      "step": 500
    },
    {
      "epoch": 0.0272,
      "grad_norm": 0.07195479422807693,
      "learning_rate": 4.932e-05,
      "loss": 0.0036,
      "step": 510
    },
    {
      "epoch": 0.027733333333333332,
      "grad_norm": 0.3099339008331299,
      "learning_rate": 4.930666666666667e-05,
      "loss": 0.0024,
      "step": 520
    },
    {
      "epoch": 0.028266666666666666,
      "grad_norm": 0.533376157283783,
      "learning_rate": 4.929333333333334e-05,
      "loss": 0.0027,
      "step": 530
    },
    {
      "epoch": 0.0288,
      "grad_norm": 0.7731208801269531,
      "learning_rate": 4.928e-05,
      "loss": 0.0047,
      "step": 540
    },
    {
      "epoch": 0.029333333333333333,
      "grad_norm": 0.28454428911209106,
      "learning_rate": 4.926666666666667e-05,
      "loss": 0.0037,
      "step": 550
    },
    {
      "epoch": 0.029866666666666666,
      "grad_norm": 0.179282084107399,
      "learning_rate": 4.925333333333333e-05,
      "loss": 0.0023,
      "step": 560
    },
    {
      "epoch": 0.0304,
      "grad_norm": 0.4311743974685669,
      "learning_rate": 4.924e-05,
      "loss": 0.0038,
      "step": 570
    },
    {
      "epoch": 0.030933333333333334,
      "grad_norm": 0.49095436930656433,
      "learning_rate": 4.9226666666666674e-05,
      "loss": 0.0026,
      "step": 580
    },
    {
      "epoch": 0.031466666666666664,
      "grad_norm": 0.172687366604805,
      "learning_rate": 4.921333333333333e-05,
      "loss": 0.0025,
      "step": 590
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.1529303640127182,
      "learning_rate": 4.92e-05,
      "loss": 0.0028,
      "step": 600
    },
    {
      "epoch": 0.03253333333333333,
      "grad_norm": 0.7582141757011414,
      "learning_rate": 4.918666666666667e-05,
      "loss": 0.0038,
      "step": 610
    },
    {
      "epoch": 0.03306666666666667,
      "grad_norm": 0.2479592114686966,
      "learning_rate": 4.917333333333334e-05,
      "loss": 0.0038,
      "step": 620
    },
    {
      "epoch": 0.0336,
      "grad_norm": 0.3753317594528198,
      "learning_rate": 4.9160000000000004e-05,
      "loss": 0.0033,
      "step": 630
    },
    {
      "epoch": 0.034133333333333335,
      "grad_norm": 0.5674053430557251,
      "learning_rate": 4.914666666666667e-05,
      "loss": 0.0033,
      "step": 640
    },
    {
      "epoch": 0.034666666666666665,
      "grad_norm": 0.5542289018630981,
      "learning_rate": 4.913333333333334e-05,
      "loss": 0.005,
      "step": 650
    },
    {
      "epoch": 0.0352,
      "grad_norm": 0.1697385311126709,
      "learning_rate": 4.9120000000000004e-05,
      "loss": 0.0038,
      "step": 660
    },
    {
      "epoch": 0.03573333333333333,
      "grad_norm": 0.7438129782676697,
      "learning_rate": 4.910666666666667e-05,
      "loss": 0.0025,
      "step": 670
    },
    {
      "epoch": 0.03626666666666667,
      "grad_norm": 0.24438531696796417,
      "learning_rate": 4.909333333333333e-05,
      "loss": 0.0027,
      "step": 680
    },
    {
      "epoch": 0.0368,
      "grad_norm": 0.8421363830566406,
      "learning_rate": 4.9080000000000004e-05,
      "loss": 0.0057,
      "step": 690
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 0.7371668815612793,
      "learning_rate": 4.906666666666667e-05,
      "loss": 0.0043,
      "step": 700
    },
    {
      "epoch": 0.037866666666666667,
      "grad_norm": 0.3562130033969879,
      "learning_rate": 4.9053333333333333e-05,
      "loss": 0.0032,
      "step": 710
    },
    {
      "epoch": 0.0384,
      "grad_norm": 0.40474098920822144,
      "learning_rate": 4.9040000000000005e-05,
      "loss": 0.0029,
      "step": 720
    },
    {
      "epoch": 0.038933333333333334,
      "grad_norm": 0.1647869497537613,
      "learning_rate": 4.902666666666667e-05,
      "loss": 0.0027,
      "step": 730
    },
    {
      "epoch": 0.039466666666666664,
      "grad_norm": 0.4434388279914856,
      "learning_rate": 4.9013333333333334e-05,
      "loss": 0.0033,
      "step": 740
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.23374946415424347,
      "learning_rate": 4.9e-05,
      "loss": 0.0025,
      "step": 750
    },
    {
      "epoch": 0.04053333333333333,
      "grad_norm": 0.719330370426178,
      "learning_rate": 4.898666666666667e-05,
      "loss": 0.0022,
      "step": 760
    },
    {
      "epoch": 0.04106666666666667,
      "grad_norm": 0.9668005704879761,
      "learning_rate": 4.897333333333334e-05,
      "loss": 0.0023,
      "step": 770
    },
    {
      "epoch": 0.0416,
      "grad_norm": 0.3633289039134979,
      "learning_rate": 4.896e-05,
      "loss": 0.0032,
      "step": 780
    },
    {
      "epoch": 0.042133333333333335,
      "grad_norm": 0.4208815097808838,
      "learning_rate": 4.894666666666667e-05,
      "loss": 0.0026,
      "step": 790
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 0.24275630712509155,
      "learning_rate": 4.8933333333333335e-05,
      "loss": 0.0028,
      "step": 800
    },
    {
      "epoch": 0.0432,
      "grad_norm": 0.5929837226867676,
      "learning_rate": 4.8920000000000006e-05,
      "loss": 0.0034,
      "step": 810
    },
    {
      "epoch": 0.04373333333333333,
      "grad_norm": 0.9321101307868958,
      "learning_rate": 4.890666666666667e-05,
      "loss": 0.0054,
      "step": 820
    },
    {
      "epoch": 0.04426666666666667,
      "grad_norm": 0.4453345537185669,
      "learning_rate": 4.8893333333333335e-05,
      "loss": 0.0028,
      "step": 830
    },
    {
      "epoch": 0.0448,
      "grad_norm": 0.483163058757782,
      "learning_rate": 4.8880000000000006e-05,
      "loss": 0.0034,
      "step": 840
    },
    {
      "epoch": 0.04533333333333334,
      "grad_norm": 0.43676701188087463,
      "learning_rate": 4.886666666666667e-05,
      "loss": 0.0029,
      "step": 850
    },
    {
      "epoch": 0.04586666666666667,
      "grad_norm": 0.1959824562072754,
      "learning_rate": 4.8853333333333335e-05,
      "loss": 0.0028,
      "step": 860
    },
    {
      "epoch": 0.0464,
      "grad_norm": 0.13623955845832825,
      "learning_rate": 4.884e-05,
      "loss": 0.0031,
      "step": 870
    },
    {
      "epoch": 0.046933333333333334,
      "grad_norm": 0.11732050031423569,
      "learning_rate": 4.882666666666667e-05,
      "loss": 0.0022,
      "step": 880
    },
    {
      "epoch": 0.047466666666666664,
      "grad_norm": 0.3382578194141388,
      "learning_rate": 4.8813333333333336e-05,
      "loss": 0.0046,
      "step": 890
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.1848827749490738,
      "learning_rate": 4.88e-05,
      "loss": 0.0033,
      "step": 900
    },
    {
      "epoch": 0.04853333333333333,
      "grad_norm": 0.15518991649150848,
      "learning_rate": 4.878666666666667e-05,
      "loss": 0.0029,
      "step": 910
    },
    {
      "epoch": 0.04906666666666667,
      "grad_norm": 0.6604090929031372,
      "learning_rate": 4.8773333333333336e-05,
      "loss": 0.0033,
      "step": 920
    },
    {
      "epoch": 0.0496,
      "grad_norm": 0.35185736417770386,
      "learning_rate": 4.876e-05,
      "loss": 0.0022,
      "step": 930
    },
    {
      "epoch": 0.050133333333333335,
      "grad_norm": 0.3348753750324249,
      "learning_rate": 4.8746666666666665e-05,
      "loss": 0.0032,
      "step": 940
    },
    {
      "epoch": 0.050666666666666665,
      "grad_norm": 0.21296820044517517,
      "learning_rate": 4.8733333333333337e-05,
      "loss": 0.002,
      "step": 950
    },
    {
      "epoch": 0.0512,
      "grad_norm": 0.6276938915252686,
      "learning_rate": 4.872000000000001e-05,
      "loss": 0.0047,
      "step": 960
    },
    {
      "epoch": 0.05173333333333333,
      "grad_norm": 0.31949400901794434,
      "learning_rate": 4.8706666666666666e-05,
      "loss": 0.0031,
      "step": 970
    },
    {
      "epoch": 0.05226666666666667,
      "grad_norm": 0.2895923852920532,
      "learning_rate": 4.869333333333334e-05,
      "loss": 0.0039,
      "step": 980
    },
    {
      "epoch": 0.0528,
      "grad_norm": 0.22924107313156128,
      "learning_rate": 4.868e-05,
      "loss": 0.0033,
      "step": 990
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.4106357991695404,
      "learning_rate": 4.866666666666667e-05,
      "loss": 0.0028,
      "step": 1000
    },
    {
      "epoch": 0.05386666666666667,
      "grad_norm": 0.40157759189605713,
      "learning_rate": 4.865333333333334e-05,
      "loss": 0.0034,
      "step": 1010
    },
    {
      "epoch": 0.0544,
      "grad_norm": 0.6408479809761047,
      "learning_rate": 4.864e-05,
      "loss": 0.004,
      "step": 1020
    },
    {
      "epoch": 0.054933333333333334,
      "grad_norm": 0.1613844782114029,
      "learning_rate": 4.862666666666667e-05,
      "loss": 0.0027,
      "step": 1030
    },
    {
      "epoch": 0.055466666666666664,
      "grad_norm": 0.5432507991790771,
      "learning_rate": 4.861333333333333e-05,
      "loss": 0.0033,
      "step": 1040
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.49123525619506836,
      "learning_rate": 4.86e-05,
      "loss": 0.0026,
      "step": 1050
    },
    {
      "epoch": 0.05653333333333333,
      "grad_norm": 0.1887708455324173,
      "learning_rate": 4.858666666666667e-05,
      "loss": 0.0027,
      "step": 1060
    },
    {
      "epoch": 0.05706666666666667,
      "grad_norm": 0.7337992191314697,
      "learning_rate": 4.857333333333334e-05,
      "loss": 0.0022,
      "step": 1070
    },
    {
      "epoch": 0.0576,
      "grad_norm": 0.2651500701904297,
      "learning_rate": 4.856e-05,
      "loss": 0.0025,
      "step": 1080
    },
    {
      "epoch": 0.058133333333333335,
      "grad_norm": 0.2055148482322693,
      "learning_rate": 4.854666666666667e-05,
      "loss": 0.0027,
      "step": 1090
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 0.47469741106033325,
      "learning_rate": 4.853333333333334e-05,
      "loss": 0.0037,
      "step": 1100
    },
    {
      "epoch": 0.0592,
      "grad_norm": 0.25788164138793945,
      "learning_rate": 4.852e-05,
      "loss": 0.0028,
      "step": 1110
    },
    {
      "epoch": 0.05973333333333333,
      "grad_norm": 0.07768900692462921,
      "learning_rate": 4.850666666666667e-05,
      "loss": 0.0022,
      "step": 1120
    },
    {
      "epoch": 0.06026666666666667,
      "grad_norm": 0.15577884018421173,
      "learning_rate": 4.849333333333333e-05,
      "loss": 0.0023,
      "step": 1130
    },
    {
      "epoch": 0.0608,
      "grad_norm": 0.6161590218544006,
      "learning_rate": 4.8480000000000003e-05,
      "loss": 0.0019,
      "step": 1140
    },
    {
      "epoch": 0.06133333333333333,
      "grad_norm": 0.849264919757843,
      "learning_rate": 4.8466666666666675e-05,
      "loss": 0.0021,
      "step": 1150
    },
    {
      "epoch": 0.06186666666666667,
      "grad_norm": 0.16773976385593414,
      "learning_rate": 4.845333333333333e-05,
      "loss": 0.0025,
      "step": 1160
    },
    {
      "epoch": 0.0624,
      "grad_norm": 0.24510622024536133,
      "learning_rate": 4.8440000000000004e-05,
      "loss": 0.0023,
      "step": 1170
    },
    {
      "epoch": 0.06293333333333333,
      "grad_norm": 0.32576701045036316,
      "learning_rate": 4.842666666666667e-05,
      "loss": 0.0024,
      "step": 1180
    },
    {
      "epoch": 0.06346666666666667,
      "grad_norm": 0.11464136838912964,
      "learning_rate": 4.841333333333334e-05,
      "loss": 0.0027,
      "step": 1190
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.39578133821487427,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 0.004,
      "step": 1200
    },
    {
      "epoch": 0.06453333333333333,
      "grad_norm": 0.4205925464630127,
      "learning_rate": 4.838666666666667e-05,
      "loss": 0.0022,
      "step": 1210
    },
    {
      "epoch": 0.06506666666666666,
      "grad_norm": 0.987023115158081,
      "learning_rate": 4.837333333333334e-05,
      "loss": 0.0038,
      "step": 1220
    },
    {
      "epoch": 0.0656,
      "grad_norm": 0.2124917060136795,
      "learning_rate": 4.836e-05,
      "loss": 0.0041,
      "step": 1230
    },
    {
      "epoch": 0.06613333333333334,
      "grad_norm": 0.26187431812286377,
      "learning_rate": 4.834666666666667e-05,
      "loss": 0.0028,
      "step": 1240
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.17168496549129486,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 0.0034,
      "step": 1250
    },
    {
      "epoch": 0.0672,
      "grad_norm": 0.2770196199417114,
      "learning_rate": 4.8320000000000005e-05,
      "loss": 0.0029,
      "step": 1260
    },
    {
      "epoch": 0.06773333333333334,
      "grad_norm": 0.1292072981595993,
      "learning_rate": 4.830666666666667e-05,
      "loss": 0.0045,
      "step": 1270
    },
    {
      "epoch": 0.06826666666666667,
      "grad_norm": 0.3231717348098755,
      "learning_rate": 4.8293333333333334e-05,
      "loss": 0.0043,
      "step": 1280
    },
    {
      "epoch": 0.0688,
      "grad_norm": 0.6488388776779175,
      "learning_rate": 4.8280000000000005e-05,
      "loss": 0.0042,
      "step": 1290
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 0.2081412971019745,
      "learning_rate": 4.826666666666667e-05,
      "loss": 0.0024,
      "step": 1300
    },
    {
      "epoch": 0.06986666666666666,
      "grad_norm": 0.22458307445049286,
      "learning_rate": 4.8253333333333334e-05,
      "loss": 0.0029,
      "step": 1310
    },
    {
      "epoch": 0.0704,
      "grad_norm": 0.3728905916213989,
      "learning_rate": 4.824e-05,
      "loss": 0.0028,
      "step": 1320
    },
    {
      "epoch": 0.07093333333333333,
      "grad_norm": 0.4733801484107971,
      "learning_rate": 4.822666666666667e-05,
      "loss": 0.0026,
      "step": 1330
    },
    {
      "epoch": 0.07146666666666666,
      "grad_norm": 0.22999636828899384,
      "learning_rate": 4.8213333333333335e-05,
      "loss": 0.0029,
      "step": 1340
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.11764386296272278,
      "learning_rate": 4.82e-05,
      "loss": 0.0018,
      "step": 1350
    },
    {
      "epoch": 0.07253333333333334,
      "grad_norm": 0.3785252571105957,
      "learning_rate": 4.818666666666667e-05,
      "loss": 0.0029,
      "step": 1360
    },
    {
      "epoch": 0.07306666666666667,
      "grad_norm": 0.34275150299072266,
      "learning_rate": 4.8173333333333335e-05,
      "loss": 0.0029,
      "step": 1370
    },
    {
      "epoch": 0.0736,
      "grad_norm": 0.39925187826156616,
      "learning_rate": 4.816e-05,
      "loss": 0.0019,
      "step": 1380
    },
    {
      "epoch": 0.07413333333333333,
      "grad_norm": 0.15447549521923065,
      "learning_rate": 4.814666666666667e-05,
      "loss": 0.0024,
      "step": 1390
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 0.49881410598754883,
      "learning_rate": 4.8133333333333336e-05,
      "loss": 0.0039,
      "step": 1400
    },
    {
      "epoch": 0.0752,
      "grad_norm": 0.28925466537475586,
      "learning_rate": 4.812000000000001e-05,
      "loss": 0.0028,
      "step": 1410
    },
    {
      "epoch": 0.07573333333333333,
      "grad_norm": 0.3524087071418762,
      "learning_rate": 4.8106666666666665e-05,
      "loss": 0.0034,
      "step": 1420
    },
    {
      "epoch": 0.07626666666666666,
      "grad_norm": 0.4478517472743988,
      "learning_rate": 4.8093333333333336e-05,
      "loss": 0.0033,
      "step": 1430
    },
    {
      "epoch": 0.0768,
      "grad_norm": 0.46367210149765015,
      "learning_rate": 4.808e-05,
      "loss": 0.0033,
      "step": 1440
    },
    {
      "epoch": 0.07733333333333334,
      "grad_norm": 0.1334480196237564,
      "learning_rate": 4.806666666666667e-05,
      "loss": 0.0029,
      "step": 1450
    },
    {
      "epoch": 0.07786666666666667,
      "grad_norm": 0.5665440559387207,
      "learning_rate": 4.8053333333333336e-05,
      "loss": 0.0026,
      "step": 1460
    },
    {
      "epoch": 0.0784,
      "grad_norm": 0.25378650426864624,
      "learning_rate": 4.804e-05,
      "loss": 0.003,
      "step": 1470
    },
    {
      "epoch": 0.07893333333333333,
      "grad_norm": 0.2524696886539459,
      "learning_rate": 4.802666666666667e-05,
      "loss": 0.0034,
      "step": 1480
    },
    {
      "epoch": 0.07946666666666667,
      "grad_norm": 0.19664493203163147,
      "learning_rate": 4.801333333333334e-05,
      "loss": 0.0033,
      "step": 1490
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.13472579419612885,
      "learning_rate": 4.8e-05,
      "loss": 0.0039,
      "step": 1500
    },
    {
      "epoch": 0.08053333333333333,
      "grad_norm": 0.6876638531684875,
      "learning_rate": 4.7986666666666666e-05,
      "loss": 0.0023,
      "step": 1510
    },
    {
      "epoch": 0.08106666666666666,
      "grad_norm": 0.952999472618103,
      "learning_rate": 4.797333333333334e-05,
      "loss": 0.0029,
      "step": 1520
    },
    {
      "epoch": 0.0816,
      "grad_norm": 0.42832595109939575,
      "learning_rate": 4.796e-05,
      "loss": 0.0035,
      "step": 1530
    },
    {
      "epoch": 0.08213333333333334,
      "grad_norm": 0.22868572175502777,
      "learning_rate": 4.7946666666666666e-05,
      "loss": 0.0025,
      "step": 1540
    },
    {
      "epoch": 0.08266666666666667,
      "grad_norm": 0.10953418910503387,
      "learning_rate": 4.793333333333334e-05,
      "loss": 0.0028,
      "step": 1550
    },
    {
      "epoch": 0.0832,
      "grad_norm": 0.537878155708313,
      "learning_rate": 4.792e-05,
      "loss": 0.0035,
      "step": 1560
    },
    {
      "epoch": 0.08373333333333334,
      "grad_norm": 1.1963757276535034,
      "learning_rate": 4.7906666666666667e-05,
      "loss": 0.0036,
      "step": 1570
    },
    {
      "epoch": 0.08426666666666667,
      "grad_norm": 0.4679369330406189,
      "learning_rate": 4.789333333333334e-05,
      "loss": 0.0029,
      "step": 1580
    },
    {
      "epoch": 0.0848,
      "grad_norm": 0.572830319404602,
      "learning_rate": 4.788e-05,
      "loss": 0.0029,
      "step": 1590
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 0.3024875819683075,
      "learning_rate": 4.7866666666666674e-05,
      "loss": 0.0027,
      "step": 1600
    },
    {
      "epoch": 0.08586666666666666,
      "grad_norm": 0.23271405696868896,
      "learning_rate": 4.785333333333333e-05,
      "loss": 0.002,
      "step": 1610
    },
    {
      "epoch": 0.0864,
      "grad_norm": 0.4110378921031952,
      "learning_rate": 4.784e-05,
      "loss": 0.0029,
      "step": 1620
    },
    {
      "epoch": 0.08693333333333333,
      "grad_norm": 0.7861271500587463,
      "learning_rate": 4.782666666666667e-05,
      "loss": 0.0034,
      "step": 1630
    },
    {
      "epoch": 0.08746666666666666,
      "grad_norm": 0.25084030628204346,
      "learning_rate": 4.781333333333334e-05,
      "loss": 0.0022,
      "step": 1640
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.5954703688621521,
      "learning_rate": 4.78e-05,
      "loss": 0.0034,
      "step": 1650
    },
    {
      "epoch": 0.08853333333333334,
      "grad_norm": 0.18286198377609253,
      "learning_rate": 4.778666666666667e-05,
      "loss": 0.0024,
      "step": 1660
    },
    {
      "epoch": 0.08906666666666667,
      "grad_norm": 0.6795284152030945,
      "learning_rate": 4.777333333333334e-05,
      "loss": 0.0029,
      "step": 1670
    },
    {
      "epoch": 0.0896,
      "grad_norm": 0.1864231526851654,
      "learning_rate": 4.7760000000000004e-05,
      "loss": 0.0029,
      "step": 1680
    },
    {
      "epoch": 0.09013333333333333,
      "grad_norm": 0.10275661945343018,
      "learning_rate": 4.774666666666667e-05,
      "loss": 0.0027,
      "step": 1690
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 0.23006033897399902,
      "learning_rate": 4.773333333333333e-05,
      "loss": 0.0027,
      "step": 1700
    },
    {
      "epoch": 0.0912,
      "grad_norm": 0.5405067205429077,
      "learning_rate": 4.7720000000000004e-05,
      "loss": 0.0048,
      "step": 1710
    },
    {
      "epoch": 0.09173333333333333,
      "grad_norm": 0.6921253800392151,
      "learning_rate": 4.770666666666667e-05,
      "loss": 0.0026,
      "step": 1720
    },
    {
      "epoch": 0.09226666666666666,
      "grad_norm": 0.09900080412626266,
      "learning_rate": 4.769333333333333e-05,
      "loss": 0.0022,
      "step": 1730
    },
    {
      "epoch": 0.0928,
      "grad_norm": 0.18684126436710358,
      "learning_rate": 4.7680000000000004e-05,
      "loss": 0.0026,
      "step": 1740
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.11286252737045288,
      "learning_rate": 4.766666666666667e-05,
      "loss": 0.0031,
      "step": 1750
    },
    {
      "epoch": 0.09386666666666667,
      "grad_norm": 0.12937739491462708,
      "learning_rate": 4.765333333333333e-05,
      "loss": 0.0029,
      "step": 1760
    },
    {
      "epoch": 0.0944,
      "grad_norm": 0.6196271777153015,
      "learning_rate": 4.7640000000000005e-05,
      "loss": 0.0026,
      "step": 1770
    },
    {
      "epoch": 0.09493333333333333,
      "grad_norm": 0.3731424808502197,
      "learning_rate": 4.762666666666667e-05,
      "loss": 0.0024,
      "step": 1780
    },
    {
      "epoch": 0.09546666666666667,
      "grad_norm": 0.4269455075263977,
      "learning_rate": 4.761333333333334e-05,
      "loss": 0.0035,
      "step": 1790
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.748490571975708,
      "learning_rate": 4.76e-05,
      "loss": 0.0038,
      "step": 1800
    },
    {
      "epoch": 0.09653333333333333,
      "grad_norm": 0.2447681725025177,
      "learning_rate": 4.758666666666667e-05,
      "loss": 0.0023,
      "step": 1810
    },
    {
      "epoch": 0.09706666666666666,
      "grad_norm": 0.8055452704429626,
      "learning_rate": 4.7573333333333334e-05,
      "loss": 0.0028,
      "step": 1820
    },
    {
      "epoch": 0.0976,
      "grad_norm": 0.1272428184747696,
      "learning_rate": 4.7560000000000005e-05,
      "loss": 0.0033,
      "step": 1830
    },
    {
      "epoch": 0.09813333333333334,
      "grad_norm": 0.6197211742401123,
      "learning_rate": 4.754666666666667e-05,
      "loss": 0.0025,
      "step": 1840
    },
    {
      "epoch": 0.09866666666666667,
      "grad_norm": 0.08003919571638107,
      "learning_rate": 4.7533333333333334e-05,
      "loss": 0.0023,
      "step": 1850
    },
    {
      "epoch": 0.0992,
      "grad_norm": 0.1502358466386795,
      "learning_rate": 4.7520000000000006e-05,
      "loss": 0.0022,
      "step": 1860
    },
    {
      "epoch": 0.09973333333333333,
      "grad_norm": 0.683881402015686,
      "learning_rate": 4.750666666666667e-05,
      "loss": 0.0028,
      "step": 1870
    },
    {
      "epoch": 0.10026666666666667,
      "grad_norm": 0.7357789278030396,
      "learning_rate": 4.7493333333333335e-05,
      "loss": 0.0023,
      "step": 1880
    },
    {
      "epoch": 0.1008,
      "grad_norm": 0.108780138194561,
      "learning_rate": 4.748e-05,
      "loss": 0.0023,
      "step": 1890
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 0.3568432331085205,
      "learning_rate": 4.746666666666667e-05,
      "loss": 0.0034,
      "step": 1900
    },
    {
      "epoch": 0.10186666666666666,
      "grad_norm": 0.24486932158470154,
      "learning_rate": 4.7453333333333335e-05,
      "loss": 0.0032,
      "step": 1910
    },
    {
      "epoch": 0.1024,
      "grad_norm": 0.5050172805786133,
      "learning_rate": 4.744e-05,
      "loss": 0.0039,
      "step": 1920
    },
    {
      "epoch": 0.10293333333333334,
      "grad_norm": 0.5298570990562439,
      "learning_rate": 4.742666666666667e-05,
      "loss": 0.0034,
      "step": 1930
    },
    {
      "epoch": 0.10346666666666667,
      "grad_norm": 0.674466073513031,
      "learning_rate": 4.7413333333333336e-05,
      "loss": 0.0031,
      "step": 1940
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.4528420567512512,
      "learning_rate": 4.74e-05,
      "loss": 0.0021,
      "step": 1950
    },
    {
      "epoch": 0.10453333333333334,
      "grad_norm": 0.4510239064693451,
      "learning_rate": 4.7386666666666665e-05,
      "loss": 0.0021,
      "step": 1960
    },
    {
      "epoch": 0.10506666666666667,
      "grad_norm": 0.10699053108692169,
      "learning_rate": 4.7373333333333336e-05,
      "loss": 0.0027,
      "step": 1970
    },
    {
      "epoch": 0.1056,
      "grad_norm": 0.09713909775018692,
      "learning_rate": 4.736000000000001e-05,
      "loss": 0.0021,
      "step": 1980
    },
    {
      "epoch": 0.10613333333333333,
      "grad_norm": 0.9615420699119568,
      "learning_rate": 4.7346666666666665e-05,
      "loss": 0.0029,
      "step": 1990
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.3152288496494293,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 0.003,
      "step": 2000
    },
    {
      "epoch": 0.1072,
      "grad_norm": 0.28813713788986206,
      "learning_rate": 4.732e-05,
      "loss": 0.0035,
      "step": 2010
    },
    {
      "epoch": 0.10773333333333333,
      "grad_norm": 0.3135630786418915,
      "learning_rate": 4.730666666666667e-05,
      "loss": 0.0029,
      "step": 2020
    },
    {
      "epoch": 0.10826666666666666,
      "grad_norm": 0.07814823091030121,
      "learning_rate": 4.729333333333334e-05,
      "loss": 0.0027,
      "step": 2030
    },
    {
      "epoch": 0.1088,
      "grad_norm": 0.3775211572647095,
      "learning_rate": 4.728e-05,
      "loss": 0.0037,
      "step": 2040
    },
    {
      "epoch": 0.10933333333333334,
      "grad_norm": 0.7237042784690857,
      "learning_rate": 4.726666666666667e-05,
      "loss": 0.0033,
      "step": 2050
    },
    {
      "epoch": 0.10986666666666667,
      "grad_norm": 0.5486060380935669,
      "learning_rate": 4.725333333333334e-05,
      "loss": 0.0024,
      "step": 2060
    },
    {
      "epoch": 0.1104,
      "grad_norm": 0.3108642101287842,
      "learning_rate": 4.724e-05,
      "loss": 0.003,
      "step": 2070
    },
    {
      "epoch": 0.11093333333333333,
      "grad_norm": 0.4061506688594818,
      "learning_rate": 4.7226666666666666e-05,
      "loss": 0.0024,
      "step": 2080
    },
    {
      "epoch": 0.11146666666666667,
      "grad_norm": 0.3205346465110779,
      "learning_rate": 4.721333333333334e-05,
      "loss": 0.0022,
      "step": 2090
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.25387850403785706,
      "learning_rate": 4.72e-05,
      "loss": 0.0031,
      "step": 2100
    },
    {
      "epoch": 0.11253333333333333,
      "grad_norm": 0.4581379294395447,
      "learning_rate": 4.718666666666667e-05,
      "loss": 0.0028,
      "step": 2110
    },
    {
      "epoch": 0.11306666666666666,
      "grad_norm": 0.5515303015708923,
      "learning_rate": 4.717333333333334e-05,
      "loss": 0.004,
      "step": 2120
    },
    {
      "epoch": 0.1136,
      "grad_norm": 0.8500591516494751,
      "learning_rate": 4.716e-05,
      "loss": 0.0032,
      "step": 2130
    },
    {
      "epoch": 0.11413333333333334,
      "grad_norm": 0.7106558084487915,
      "learning_rate": 4.714666666666667e-05,
      "loss": 0.0041,
      "step": 2140
    },
    {
      "epoch": 0.11466666666666667,
      "grad_norm": 0.6567054986953735,
      "learning_rate": 4.713333333333333e-05,
      "loss": 0.0027,
      "step": 2150
    },
    {
      "epoch": 0.1152,
      "grad_norm": 0.5254905819892883,
      "learning_rate": 4.712e-05,
      "loss": 0.0037,
      "step": 2160
    },
    {
      "epoch": 0.11573333333333333,
      "grad_norm": 0.7661869525909424,
      "learning_rate": 4.7106666666666674e-05,
      "loss": 0.0031,
      "step": 2170
    },
    {
      "epoch": 0.11626666666666667,
      "grad_norm": 0.06465546786785126,
      "learning_rate": 4.709333333333333e-05,
      "loss": 0.0037,
      "step": 2180
    },
    {
      "epoch": 0.1168,
      "grad_norm": 0.1290244460105896,
      "learning_rate": 4.708e-05,
      "loss": 0.003,
      "step": 2190
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 0.2820350229740143,
      "learning_rate": 4.706666666666667e-05,
      "loss": 0.0026,
      "step": 2200
    },
    {
      "epoch": 0.11786666666666666,
      "grad_norm": 0.2709268033504486,
      "learning_rate": 4.705333333333334e-05,
      "loss": 0.0027,
      "step": 2210
    },
    {
      "epoch": 0.1184,
      "grad_norm": 0.5370136499404907,
      "learning_rate": 4.7040000000000004e-05,
      "loss": 0.0029,
      "step": 2220
    },
    {
      "epoch": 0.11893333333333334,
      "grad_norm": 0.8497585654258728,
      "learning_rate": 4.702666666666667e-05,
      "loss": 0.0038,
      "step": 2230
    },
    {
      "epoch": 0.11946666666666667,
      "grad_norm": 0.4575323760509491,
      "learning_rate": 4.701333333333334e-05,
      "loss": 0.0026,
      "step": 2240
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.16178129613399506,
      "learning_rate": 4.7e-05,
      "loss": 0.0022,
      "step": 2250
    },
    {
      "epoch": 0.12053333333333334,
      "grad_norm": 0.23229362070560455,
      "learning_rate": 4.698666666666667e-05,
      "loss": 0.0021,
      "step": 2260
    },
    {
      "epoch": 0.12106666666666667,
      "grad_norm": 0.49362167716026306,
      "learning_rate": 4.697333333333333e-05,
      "loss": 0.003,
      "step": 2270
    },
    {
      "epoch": 0.1216,
      "grad_norm": 0.1197570413351059,
      "learning_rate": 4.6960000000000004e-05,
      "loss": 0.0017,
      "step": 2280
    },
    {
      "epoch": 0.12213333333333333,
      "grad_norm": 0.6863293051719666,
      "learning_rate": 4.694666666666667e-05,
      "loss": 0.004,
      "step": 2290
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 0.5215160846710205,
      "learning_rate": 4.6933333333333333e-05,
      "loss": 0.0029,
      "step": 2300
    },
    {
      "epoch": 0.1232,
      "grad_norm": 0.49642130732536316,
      "learning_rate": 4.6920000000000005e-05,
      "loss": 0.0027,
      "step": 2310
    },
    {
      "epoch": 0.12373333333333333,
      "grad_norm": 0.23913466930389404,
      "learning_rate": 4.690666666666667e-05,
      "loss": 0.0033,
      "step": 2320
    },
    {
      "epoch": 0.12426666666666666,
      "grad_norm": 0.09663275629281998,
      "learning_rate": 4.6893333333333334e-05,
      "loss": 0.0026,
      "step": 2330
    },
    {
      "epoch": 0.1248,
      "grad_norm": 0.17380793392658234,
      "learning_rate": 4.688e-05,
      "loss": 0.0032,
      "step": 2340
    },
    {
      "epoch": 0.12533333333333332,
      "grad_norm": 0.08118601888418198,
      "learning_rate": 4.686666666666667e-05,
      "loss": 0.0022,
      "step": 2350
    },
    {
      "epoch": 0.12586666666666665,
      "grad_norm": 0.21131320297718048,
      "learning_rate": 4.685333333333334e-05,
      "loss": 0.0026,
      "step": 2360
    },
    {
      "epoch": 0.1264,
      "grad_norm": 0.41393813490867615,
      "learning_rate": 4.684e-05,
      "loss": 0.0038,
      "step": 2370
    },
    {
      "epoch": 0.12693333333333334,
      "grad_norm": 0.33484023809432983,
      "learning_rate": 4.682666666666667e-05,
      "loss": 0.0036,
      "step": 2380
    },
    {
      "epoch": 0.12746666666666667,
      "grad_norm": 0.3335557281970978,
      "learning_rate": 4.6813333333333335e-05,
      "loss": 0.0021,
      "step": 2390
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.11046194285154343,
      "learning_rate": 4.6800000000000006e-05,
      "loss": 0.0024,
      "step": 2400
    },
    {
      "epoch": 0.12853333333333333,
      "grad_norm": 0.691236674785614,
      "learning_rate": 4.678666666666667e-05,
      "loss": 0.0024,
      "step": 2410
    },
    {
      "epoch": 0.12906666666666666,
      "grad_norm": 0.1965969055891037,
      "learning_rate": 4.6773333333333335e-05,
      "loss": 0.0024,
      "step": 2420
    },
    {
      "epoch": 0.1296,
      "grad_norm": 0.46807458996772766,
      "learning_rate": 4.6760000000000006e-05,
      "loss": 0.0028,
      "step": 2430
    },
    {
      "epoch": 0.13013333333333332,
      "grad_norm": 0.47706353664398193,
      "learning_rate": 4.6746666666666664e-05,
      "loss": 0.0028,
      "step": 2440
    },
    {
      "epoch": 0.13066666666666665,
      "grad_norm": 0.32986587285995483,
      "learning_rate": 4.6733333333333335e-05,
      "loss": 0.0026,
      "step": 2450
    },
    {
      "epoch": 0.1312,
      "grad_norm": 0.4119008779525757,
      "learning_rate": 4.672e-05,
      "loss": 0.0026,
      "step": 2460
    },
    {
      "epoch": 0.13173333333333334,
      "grad_norm": 0.2464694231748581,
      "learning_rate": 4.670666666666667e-05,
      "loss": 0.0037,
      "step": 2470
    },
    {
      "epoch": 0.13226666666666667,
      "grad_norm": 0.07231118530035019,
      "learning_rate": 4.6693333333333336e-05,
      "loss": 0.0025,
      "step": 2480
    },
    {
      "epoch": 0.1328,
      "grad_norm": 0.465440571308136,
      "learning_rate": 4.668e-05,
      "loss": 0.003,
      "step": 2490
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.42353686690330505,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.0031,
      "step": 2500
    },
    {
      "epoch": 0.13386666666666666,
      "grad_norm": 0.10599105805158615,
      "learning_rate": 4.6653333333333336e-05,
      "loss": 0.0035,
      "step": 2510
    },
    {
      "epoch": 0.1344,
      "grad_norm": 0.43782171607017517,
      "learning_rate": 4.664e-05,
      "loss": 0.0035,
      "step": 2520
    },
    {
      "epoch": 0.13493333333333332,
      "grad_norm": 0.36306074261665344,
      "learning_rate": 4.6626666666666665e-05,
      "loss": 0.0038,
      "step": 2530
    },
    {
      "epoch": 0.13546666666666668,
      "grad_norm": 0.3795163333415985,
      "learning_rate": 4.6613333333333337e-05,
      "loss": 0.0023,
      "step": 2540
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.5456305146217346,
      "learning_rate": 4.660000000000001e-05,
      "loss": 0.0027,
      "step": 2550
    },
    {
      "epoch": 0.13653333333333334,
      "grad_norm": 0.19689522683620453,
      "learning_rate": 4.6586666666666666e-05,
      "loss": 0.0025,
      "step": 2560
    },
    {
      "epoch": 0.13706666666666667,
      "grad_norm": 0.3005858063697815,
      "learning_rate": 4.657333333333334e-05,
      "loss": 0.0034,
      "step": 2570
    },
    {
      "epoch": 0.1376,
      "grad_norm": 0.26314064860343933,
      "learning_rate": 4.656e-05,
      "loss": 0.0024,
      "step": 2580
    },
    {
      "epoch": 0.13813333333333333,
      "grad_norm": 0.21446868777275085,
      "learning_rate": 4.6546666666666666e-05,
      "loss": 0.0025,
      "step": 2590
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 0.29935139417648315,
      "learning_rate": 4.653333333333334e-05,
      "loss": 0.0021,
      "step": 2600
    },
    {
      "epoch": 0.1392,
      "grad_norm": 0.6204615831375122,
      "learning_rate": 4.652e-05,
      "loss": 0.0026,
      "step": 2610
    },
    {
      "epoch": 0.13973333333333332,
      "grad_norm": 0.5659193396568298,
      "learning_rate": 4.650666666666667e-05,
      "loss": 0.0026,
      "step": 2620
    },
    {
      "epoch": 0.14026666666666668,
      "grad_norm": 0.6020783185958862,
      "learning_rate": 4.649333333333333e-05,
      "loss": 0.004,
      "step": 2630
    },
    {
      "epoch": 0.1408,
      "grad_norm": 0.5521666407585144,
      "learning_rate": 4.648e-05,
      "loss": 0.0031,
      "step": 2640
    },
    {
      "epoch": 0.14133333333333334,
      "grad_norm": 0.2250622808933258,
      "learning_rate": 4.646666666666667e-05,
      "loss": 0.0035,
      "step": 2650
    },
    {
      "epoch": 0.14186666666666667,
      "grad_norm": 0.8554432988166809,
      "learning_rate": 4.645333333333334e-05,
      "loss": 0.0043,
      "step": 2660
    },
    {
      "epoch": 0.1424,
      "grad_norm": 0.353279709815979,
      "learning_rate": 4.644e-05,
      "loss": 0.0038,
      "step": 2670
    },
    {
      "epoch": 0.14293333333333333,
      "grad_norm": 0.4682242274284363,
      "learning_rate": 4.642666666666667e-05,
      "loss": 0.0024,
      "step": 2680
    },
    {
      "epoch": 0.14346666666666666,
      "grad_norm": 0.1811954379081726,
      "learning_rate": 4.641333333333334e-05,
      "loss": 0.0048,
      "step": 2690
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.44093528389930725,
      "learning_rate": 4.64e-05,
      "loss": 0.0027,
      "step": 2700
    },
    {
      "epoch": 0.14453333333333335,
      "grad_norm": 0.24724970757961273,
      "learning_rate": 4.638666666666667e-05,
      "loss": 0.0026,
      "step": 2710
    },
    {
      "epoch": 0.14506666666666668,
      "grad_norm": 0.8031225204467773,
      "learning_rate": 4.637333333333333e-05,
      "loss": 0.0025,
      "step": 2720
    },
    {
      "epoch": 0.1456,
      "grad_norm": 0.12537126243114471,
      "learning_rate": 4.636e-05,
      "loss": 0.0045,
      "step": 2730
    },
    {
      "epoch": 0.14613333333333334,
      "grad_norm": 0.280405730009079,
      "learning_rate": 4.6346666666666675e-05,
      "loss": 0.0034,
      "step": 2740
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.1836259663105011,
      "learning_rate": 4.633333333333333e-05,
      "loss": 0.0025,
      "step": 2750
    },
    {
      "epoch": 0.1472,
      "grad_norm": 0.22884871065616608,
      "learning_rate": 4.6320000000000004e-05,
      "loss": 0.0028,
      "step": 2760
    },
    {
      "epoch": 0.14773333333333333,
      "grad_norm": 0.6875852942466736,
      "learning_rate": 4.630666666666667e-05,
      "loss": 0.0041,
      "step": 2770
    },
    {
      "epoch": 0.14826666666666666,
      "grad_norm": 0.6096483469009399,
      "learning_rate": 4.629333333333333e-05,
      "loss": 0.0039,
      "step": 2780
    },
    {
      "epoch": 0.1488,
      "grad_norm": 0.24888373911380768,
      "learning_rate": 4.6280000000000004e-05,
      "loss": 0.0025,
      "step": 2790
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 0.6868245601654053,
      "learning_rate": 4.626666666666667e-05,
      "loss": 0.0027,
      "step": 2800
    },
    {
      "epoch": 0.14986666666666668,
      "grad_norm": 0.35292139649391174,
      "learning_rate": 4.625333333333334e-05,
      "loss": 0.0044,
      "step": 2810
    },
    {
      "epoch": 0.1504,
      "grad_norm": 0.22357851266860962,
      "learning_rate": 4.624e-05,
      "loss": 0.0024,
      "step": 2820
    },
    {
      "epoch": 0.15093333333333334,
      "grad_norm": 0.32875514030456543,
      "learning_rate": 4.622666666666667e-05,
      "loss": 0.0024,
      "step": 2830
    },
    {
      "epoch": 0.15146666666666667,
      "grad_norm": 0.4344446063041687,
      "learning_rate": 4.6213333333333334e-05,
      "loss": 0.0028,
      "step": 2840
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.18369217216968536,
      "learning_rate": 4.6200000000000005e-05,
      "loss": 0.0033,
      "step": 2850
    },
    {
      "epoch": 0.15253333333333333,
      "grad_norm": 0.3910137712955475,
      "learning_rate": 4.618666666666667e-05,
      "loss": 0.0054,
      "step": 2860
    },
    {
      "epoch": 0.15306666666666666,
      "grad_norm": 0.24344715476036072,
      "learning_rate": 4.6173333333333334e-05,
      "loss": 0.0028,
      "step": 2870
    },
    {
      "epoch": 0.1536,
      "grad_norm": 0.1858401894569397,
      "learning_rate": 4.6160000000000005e-05,
      "loss": 0.0034,
      "step": 2880
    },
    {
      "epoch": 0.15413333333333334,
      "grad_norm": 0.48586612939834595,
      "learning_rate": 4.614666666666667e-05,
      "loss": 0.0036,
      "step": 2890
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 0.12224577367305756,
      "learning_rate": 4.6133333333333334e-05,
      "loss": 0.0033,
      "step": 2900
    },
    {
      "epoch": 0.1552,
      "grad_norm": 0.40164539217948914,
      "learning_rate": 4.612e-05,
      "loss": 0.0033,
      "step": 2910
    },
    {
      "epoch": 0.15573333333333333,
      "grad_norm": 0.7159051299095154,
      "learning_rate": 4.610666666666667e-05,
      "loss": 0.0042,
      "step": 2920
    },
    {
      "epoch": 0.15626666666666666,
      "grad_norm": 0.25806471705436707,
      "learning_rate": 4.6093333333333335e-05,
      "loss": 0.0022,
      "step": 2930
    },
    {
      "epoch": 0.1568,
      "grad_norm": 0.285004585981369,
      "learning_rate": 4.608e-05,
      "loss": 0.0026,
      "step": 2940
    },
    {
      "epoch": 0.15733333333333333,
      "grad_norm": 0.382050484418869,
      "learning_rate": 4.606666666666667e-05,
      "loss": 0.0046,
      "step": 2950
    },
    {
      "epoch": 0.15786666666666666,
      "grad_norm": 0.3152928948402405,
      "learning_rate": 4.6053333333333335e-05,
      "loss": 0.0028,
      "step": 2960
    },
    {
      "epoch": 0.1584,
      "grad_norm": 0.199532151222229,
      "learning_rate": 4.604e-05,
      "loss": 0.0034,
      "step": 2970
    },
    {
      "epoch": 0.15893333333333334,
      "grad_norm": 0.2912793457508087,
      "learning_rate": 4.602666666666667e-05,
      "loss": 0.0028,
      "step": 2980
    },
    {
      "epoch": 0.15946666666666667,
      "grad_norm": 0.6218802332878113,
      "learning_rate": 4.6013333333333336e-05,
      "loss": 0.0023,
      "step": 2990
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.26281991600990295,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.0039,
      "step": 3000
    },
    {
      "epoch": 0.16053333333333333,
      "grad_norm": 0.1400424987077713,
      "learning_rate": 4.5986666666666665e-05,
      "loss": 0.0027,
      "step": 3010
    },
    {
      "epoch": 0.16106666666666666,
      "grad_norm": 0.22236165404319763,
      "learning_rate": 4.5973333333333336e-05,
      "loss": 0.0029,
      "step": 3020
    },
    {
      "epoch": 0.1616,
      "grad_norm": 0.35365742444992065,
      "learning_rate": 4.596e-05,
      "loss": 0.0023,
      "step": 3030
    },
    {
      "epoch": 0.16213333333333332,
      "grad_norm": 0.11834698915481567,
      "learning_rate": 4.594666666666667e-05,
      "loss": 0.0036,
      "step": 3040
    },
    {
      "epoch": 0.16266666666666665,
      "grad_norm": 0.2242625653743744,
      "learning_rate": 4.5933333333333336e-05,
      "loss": 0.0023,
      "step": 3050
    },
    {
      "epoch": 0.1632,
      "grad_norm": 0.5990450382232666,
      "learning_rate": 4.592e-05,
      "loss": 0.0037,
      "step": 3060
    },
    {
      "epoch": 0.16373333333333334,
      "grad_norm": 0.19804930686950684,
      "learning_rate": 4.590666666666667e-05,
      "loss": 0.0026,
      "step": 3070
    },
    {
      "epoch": 0.16426666666666667,
      "grad_norm": 0.6180476546287537,
      "learning_rate": 4.589333333333334e-05,
      "loss": 0.0033,
      "step": 3080
    },
    {
      "epoch": 0.1648,
      "grad_norm": 0.37988871335983276,
      "learning_rate": 4.588e-05,
      "loss": 0.0022,
      "step": 3090
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 0.5010351538658142,
      "learning_rate": 4.5866666666666666e-05,
      "loss": 0.0028,
      "step": 3100
    },
    {
      "epoch": 0.16586666666666666,
      "grad_norm": 0.08143100142478943,
      "learning_rate": 4.585333333333334e-05,
      "loss": 0.0046,
      "step": 3110
    },
    {
      "epoch": 0.1664,
      "grad_norm": 0.15415187180042267,
      "learning_rate": 4.584e-05,
      "loss": 0.003,
      "step": 3120
    },
    {
      "epoch": 0.16693333333333332,
      "grad_norm": 0.32694679498672485,
      "learning_rate": 4.5826666666666666e-05,
      "loss": 0.0025,
      "step": 3130
    },
    {
      "epoch": 0.16746666666666668,
      "grad_norm": 0.20069736242294312,
      "learning_rate": 4.581333333333334e-05,
      "loss": 0.004,
      "step": 3140
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.1780778169631958,
      "learning_rate": 4.58e-05,
      "loss": 0.0021,
      "step": 3150
    },
    {
      "epoch": 0.16853333333333334,
      "grad_norm": 0.43800243735313416,
      "learning_rate": 4.5786666666666666e-05,
      "loss": 0.0023,
      "step": 3160
    },
    {
      "epoch": 0.16906666666666667,
      "grad_norm": 0.26709309220314026,
      "learning_rate": 4.577333333333334e-05,
      "loss": 0.0034,
      "step": 3170
    },
    {
      "epoch": 0.1696,
      "grad_norm": 0.33505040407180786,
      "learning_rate": 4.576e-05,
      "loss": 0.0035,
      "step": 3180
    },
    {
      "epoch": 0.17013333333333333,
      "grad_norm": 0.12910053133964539,
      "learning_rate": 4.5746666666666674e-05,
      "loss": 0.0028,
      "step": 3190
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 0.22162151336669922,
      "learning_rate": 4.573333333333333e-05,
      "loss": 0.0025,
      "step": 3200
    },
    {
      "epoch": 0.1712,
      "grad_norm": 0.26997503638267517,
      "learning_rate": 4.572e-05,
      "loss": 0.0032,
      "step": 3210
    },
    {
      "epoch": 0.17173333333333332,
      "grad_norm": 0.30621588230133057,
      "learning_rate": 4.570666666666667e-05,
      "loss": 0.0042,
      "step": 3220
    },
    {
      "epoch": 0.17226666666666668,
      "grad_norm": 0.557181179523468,
      "learning_rate": 4.569333333333334e-05,
      "loss": 0.0034,
      "step": 3230
    },
    {
      "epoch": 0.1728,
      "grad_norm": 0.3146167993545532,
      "learning_rate": 4.568e-05,
      "loss": 0.0024,
      "step": 3240
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.4025207459926605,
      "learning_rate": 4.566666666666667e-05,
      "loss": 0.0019,
      "step": 3250
    },
    {
      "epoch": 0.17386666666666667,
      "grad_norm": 0.1509699672460556,
      "learning_rate": 4.565333333333334e-05,
      "loss": 0.002,
      "step": 3260
    },
    {
      "epoch": 0.1744,
      "grad_norm": 0.5598879456520081,
      "learning_rate": 4.564e-05,
      "loss": 0.0032,
      "step": 3270
    },
    {
      "epoch": 0.17493333333333333,
      "grad_norm": 0.6268028020858765,
      "learning_rate": 4.562666666666667e-05,
      "loss": 0.0032,
      "step": 3280
    },
    {
      "epoch": 0.17546666666666666,
      "grad_norm": 0.29061564803123474,
      "learning_rate": 4.561333333333333e-05,
      "loss": 0.0044,
      "step": 3290
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.5689219236373901,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 0.0024,
      "step": 3300
    },
    {
      "epoch": 0.17653333333333332,
      "grad_norm": 0.5131009221076965,
      "learning_rate": 4.558666666666667e-05,
      "loss": 0.0038,
      "step": 3310
    },
    {
      "epoch": 0.17706666666666668,
      "grad_norm": 0.1540200561285019,
      "learning_rate": 4.557333333333333e-05,
      "loss": 0.0021,
      "step": 3320
    },
    {
      "epoch": 0.1776,
      "grad_norm": 0.48172610998153687,
      "learning_rate": 4.5560000000000004e-05,
      "loss": 0.0024,
      "step": 3330
    },
    {
      "epoch": 0.17813333333333334,
      "grad_norm": 0.21729394793510437,
      "learning_rate": 4.554666666666667e-05,
      "loss": 0.0039,
      "step": 3340
    },
    {
      "epoch": 0.17866666666666667,
      "grad_norm": 0.45707160234451294,
      "learning_rate": 4.553333333333333e-05,
      "loss": 0.0029,
      "step": 3350
    },
    {
      "epoch": 0.1792,
      "grad_norm": 0.5099389553070068,
      "learning_rate": 4.5520000000000005e-05,
      "loss": 0.0033,
      "step": 3360
    },
    {
      "epoch": 0.17973333333333333,
      "grad_norm": 0.18226289749145508,
      "learning_rate": 4.550666666666667e-05,
      "loss": 0.0029,
      "step": 3370
    },
    {
      "epoch": 0.18026666666666666,
      "grad_norm": 0.22633056342601776,
      "learning_rate": 4.549333333333334e-05,
      "loss": 0.0024,
      "step": 3380
    },
    {
      "epoch": 0.1808,
      "grad_norm": 0.4695891737937927,
      "learning_rate": 4.548e-05,
      "loss": 0.0025,
      "step": 3390
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 0.23080407083034515,
      "learning_rate": 4.546666666666667e-05,
      "loss": 0.0019,
      "step": 3400
    },
    {
      "epoch": 0.18186666666666668,
      "grad_norm": 0.19596756994724274,
      "learning_rate": 4.5453333333333334e-05,
      "loss": 0.0027,
      "step": 3410
    },
    {
      "epoch": 0.1824,
      "grad_norm": 0.6697092652320862,
      "learning_rate": 4.5440000000000005e-05,
      "loss": 0.0039,
      "step": 3420
    },
    {
      "epoch": 0.18293333333333334,
      "grad_norm": 0.812555193901062,
      "learning_rate": 4.542666666666667e-05,
      "loss": 0.0032,
      "step": 3430
    },
    {
      "epoch": 0.18346666666666667,
      "grad_norm": 0.28782472014427185,
      "learning_rate": 4.5413333333333334e-05,
      "loss": 0.0022,
      "step": 3440
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.06955959647893906,
      "learning_rate": 4.5400000000000006e-05,
      "loss": 0.0034,
      "step": 3450
    },
    {
      "epoch": 0.18453333333333333,
      "grad_norm": 0.06043494865298271,
      "learning_rate": 4.5386666666666664e-05,
      "loss": 0.0024,
      "step": 3460
    },
    {
      "epoch": 0.18506666666666666,
      "grad_norm": 0.6004316210746765,
      "learning_rate": 4.5373333333333335e-05,
      "loss": 0.003,
      "step": 3470
    },
    {
      "epoch": 0.1856,
      "grad_norm": 0.15625809133052826,
      "learning_rate": 4.536e-05,
      "loss": 0.0026,
      "step": 3480
    },
    {
      "epoch": 0.18613333333333335,
      "grad_norm": 0.4892266094684601,
      "learning_rate": 4.534666666666667e-05,
      "loss": 0.0026,
      "step": 3490
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.26823532581329346,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 0.0023,
      "step": 3500
    },
    {
      "epoch": 0.1872,
      "grad_norm": 0.6227789521217346,
      "learning_rate": 4.532e-05,
      "loss": 0.0034,
      "step": 3510
    },
    {
      "epoch": 0.18773333333333334,
      "grad_norm": 0.6395810842514038,
      "learning_rate": 4.530666666666667e-05,
      "loss": 0.0042,
      "step": 3520
    },
    {
      "epoch": 0.18826666666666667,
      "grad_norm": 0.4123246371746063,
      "learning_rate": 4.5293333333333336e-05,
      "loss": 0.0038,
      "step": 3530
    },
    {
      "epoch": 0.1888,
      "grad_norm": 0.5719711780548096,
      "learning_rate": 4.528e-05,
      "loss": 0.0022,
      "step": 3540
    },
    {
      "epoch": 0.18933333333333333,
      "grad_norm": 0.22232262790203094,
      "learning_rate": 4.526666666666667e-05,
      "loss": 0.0028,
      "step": 3550
    },
    {
      "epoch": 0.18986666666666666,
      "grad_norm": 0.3600388467311859,
      "learning_rate": 4.5253333333333336e-05,
      "loss": 0.0027,
      "step": 3560
    },
    {
      "epoch": 0.1904,
      "grad_norm": 0.5872915387153625,
      "learning_rate": 4.524000000000001e-05,
      "loss": 0.0021,
      "step": 3570
    },
    {
      "epoch": 0.19093333333333334,
      "grad_norm": 0.2515815198421478,
      "learning_rate": 4.5226666666666665e-05,
      "loss": 0.0029,
      "step": 3580
    },
    {
      "epoch": 0.19146666666666667,
      "grad_norm": 0.10980591922998428,
      "learning_rate": 4.5213333333333336e-05,
      "loss": 0.0039,
      "step": 3590
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.1786292940378189,
      "learning_rate": 4.52e-05,
      "loss": 0.0039,
      "step": 3600
    },
    {
      "epoch": 0.19253333333333333,
      "grad_norm": 0.3500492572784424,
      "learning_rate": 4.518666666666667e-05,
      "loss": 0.0027,
      "step": 3610
    },
    {
      "epoch": 0.19306666666666666,
      "grad_norm": 0.25136038661003113,
      "learning_rate": 4.517333333333334e-05,
      "loss": 0.0022,
      "step": 3620
    },
    {
      "epoch": 0.1936,
      "grad_norm": 0.43391624093055725,
      "learning_rate": 4.516e-05,
      "loss": 0.0034,
      "step": 3630
    },
    {
      "epoch": 0.19413333333333332,
      "grad_norm": 0.32934755086898804,
      "learning_rate": 4.514666666666667e-05,
      "loss": 0.0036,
      "step": 3640
    },
    {
      "epoch": 0.19466666666666665,
      "grad_norm": 0.3949331045150757,
      "learning_rate": 4.513333333333333e-05,
      "loss": 0.002,
      "step": 3650
    },
    {
      "epoch": 0.1952,
      "grad_norm": 0.19290931522846222,
      "learning_rate": 4.512e-05,
      "loss": 0.0035,
      "step": 3660
    },
    {
      "epoch": 0.19573333333333334,
      "grad_norm": 0.1872197538614273,
      "learning_rate": 4.5106666666666666e-05,
      "loss": 0.003,
      "step": 3670
    },
    {
      "epoch": 0.19626666666666667,
      "grad_norm": 0.35754039883613586,
      "learning_rate": 4.509333333333334e-05,
      "loss": 0.0017,
      "step": 3680
    },
    {
      "epoch": 0.1968,
      "grad_norm": 0.32228562235832214,
      "learning_rate": 4.508e-05,
      "loss": 0.0037,
      "step": 3690
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 0.36225250363349915,
      "learning_rate": 4.5066666666666667e-05,
      "loss": 0.0027,
      "step": 3700
    },
    {
      "epoch": 0.19786666666666666,
      "grad_norm": 0.47039732336997986,
      "learning_rate": 4.505333333333334e-05,
      "loss": 0.0036,
      "step": 3710
    },
    {
      "epoch": 0.1984,
      "grad_norm": 0.47500547766685486,
      "learning_rate": 4.504e-05,
      "loss": 0.0027,
      "step": 3720
    },
    {
      "epoch": 0.19893333333333332,
      "grad_norm": 0.6645197868347168,
      "learning_rate": 4.502666666666667e-05,
      "loss": 0.0025,
      "step": 3730
    },
    {
      "epoch": 0.19946666666666665,
      "grad_norm": 0.430018812417984,
      "learning_rate": 4.501333333333334e-05,
      "loss": 0.0025,
      "step": 3740
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.21706703305244446,
      "learning_rate": 4.5e-05,
      "loss": 0.0026,
      "step": 3750
    },
    {
      "epoch": 0.20053333333333334,
      "grad_norm": 0.42038512229919434,
      "learning_rate": 4.4986666666666674e-05,
      "loss": 0.0023,
      "step": 3760
    },
    {
      "epoch": 0.20106666666666667,
      "grad_norm": 0.25650471448898315,
      "learning_rate": 4.497333333333333e-05,
      "loss": 0.0026,
      "step": 3770
    },
    {
      "epoch": 0.2016,
      "grad_norm": 0.5247319936752319,
      "learning_rate": 4.496e-05,
      "loss": 0.0023,
      "step": 3780
    },
    {
      "epoch": 0.20213333333333333,
      "grad_norm": 0.3822858929634094,
      "learning_rate": 4.494666666666667e-05,
      "loss": 0.0031,
      "step": 3790
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 0.6665855050086975,
      "learning_rate": 4.493333333333333e-05,
      "loss": 0.0028,
      "step": 3800
    },
    {
      "epoch": 0.2032,
      "grad_norm": 0.6422783136367798,
      "learning_rate": 4.4920000000000004e-05,
      "loss": 0.0035,
      "step": 3810
    },
    {
      "epoch": 0.20373333333333332,
      "grad_norm": 0.3271898925304413,
      "learning_rate": 4.490666666666667e-05,
      "loss": 0.0035,
      "step": 3820
    },
    {
      "epoch": 0.20426666666666668,
      "grad_norm": 0.5153881907463074,
      "learning_rate": 4.489333333333334e-05,
      "loss": 0.0027,
      "step": 3830
    },
    {
      "epoch": 0.2048,
      "grad_norm": 0.11215364933013916,
      "learning_rate": 4.488e-05,
      "loss": 0.0029,
      "step": 3840
    },
    {
      "epoch": 0.20533333333333334,
      "grad_norm": 0.49287739396095276,
      "learning_rate": 4.486666666666667e-05,
      "loss": 0.0034,
      "step": 3850
    },
    {
      "epoch": 0.20586666666666667,
      "grad_norm": 0.5669734477996826,
      "learning_rate": 4.485333333333333e-05,
      "loss": 0.003,
      "step": 3860
    },
    {
      "epoch": 0.2064,
      "grad_norm": 0.26148805022239685,
      "learning_rate": 4.4840000000000004e-05,
      "loss": 0.0026,
      "step": 3870
    },
    {
      "epoch": 0.20693333333333333,
      "grad_norm": 0.47604018449783325,
      "learning_rate": 4.482666666666667e-05,
      "loss": 0.0024,
      "step": 3880
    },
    {
      "epoch": 0.20746666666666666,
      "grad_norm": 0.2944319546222687,
      "learning_rate": 4.4813333333333333e-05,
      "loss": 0.0032,
      "step": 3890
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.2799566388130188,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 0.0025,
      "step": 3900
    },
    {
      "epoch": 0.20853333333333332,
      "grad_norm": 0.19594928622245789,
      "learning_rate": 4.478666666666667e-05,
      "loss": 0.0021,
      "step": 3910
    },
    {
      "epoch": 0.20906666666666668,
      "grad_norm": 0.13433411717414856,
      "learning_rate": 4.4773333333333334e-05,
      "loss": 0.0033,
      "step": 3920
    },
    {
      "epoch": 0.2096,
      "grad_norm": 0.45350492000579834,
      "learning_rate": 4.4760000000000005e-05,
      "loss": 0.0024,
      "step": 3930
    },
    {
      "epoch": 0.21013333333333334,
      "grad_norm": 0.2592759430408478,
      "learning_rate": 4.474666666666667e-05,
      "loss": 0.0031,
      "step": 3940
    },
    {
      "epoch": 0.21066666666666667,
      "grad_norm": 0.7621455788612366,
      "learning_rate": 4.473333333333334e-05,
      "loss": 0.0038,
      "step": 3950
    },
    {
      "epoch": 0.2112,
      "grad_norm": 0.1434108316898346,
      "learning_rate": 4.472e-05,
      "loss": 0.0034,
      "step": 3960
    },
    {
      "epoch": 0.21173333333333333,
      "grad_norm": 0.2805522680282593,
      "learning_rate": 4.470666666666667e-05,
      "loss": 0.003,
      "step": 3970
    },
    {
      "epoch": 0.21226666666666666,
      "grad_norm": 0.4971228241920471,
      "learning_rate": 4.4693333333333335e-05,
      "loss": 0.0024,
      "step": 3980
    },
    {
      "epoch": 0.2128,
      "grad_norm": 0.10141084343194962,
      "learning_rate": 4.468e-05,
      "loss": 0.0047,
      "step": 3990
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.5497998595237732,
      "learning_rate": 4.466666666666667e-05,
      "loss": 0.0029,
      "step": 4000
    },
    {
      "epoch": 0.21386666666666668,
      "grad_norm": 0.297708660364151,
      "learning_rate": 4.4653333333333335e-05,
      "loss": 0.003,
      "step": 4010
    },
    {
      "epoch": 0.2144,
      "grad_norm": 0.2912060618400574,
      "learning_rate": 4.4640000000000006e-05,
      "loss": 0.003,
      "step": 4020
    },
    {
      "epoch": 0.21493333333333334,
      "grad_norm": 0.374721497297287,
      "learning_rate": 4.4626666666666664e-05,
      "loss": 0.0024,
      "step": 4030
    },
    {
      "epoch": 0.21546666666666667,
      "grad_norm": 0.4960596263408661,
      "learning_rate": 4.4613333333333335e-05,
      "loss": 0.0027,
      "step": 4040
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.4000631868839264,
      "learning_rate": 4.46e-05,
      "loss": 0.0038,
      "step": 4050
    },
    {
      "epoch": 0.21653333333333333,
      "grad_norm": 0.49262043833732605,
      "learning_rate": 4.458666666666667e-05,
      "loss": 0.0034,
      "step": 4060
    },
    {
      "epoch": 0.21706666666666666,
      "grad_norm": 0.49113520979881287,
      "learning_rate": 4.4573333333333336e-05,
      "loss": 0.0036,
      "step": 4070
    },
    {
      "epoch": 0.2176,
      "grad_norm": 0.9708966612815857,
      "learning_rate": 4.456e-05,
      "loss": 0.004,
      "step": 4080
    },
    {
      "epoch": 0.21813333333333335,
      "grad_norm": 0.737212061882019,
      "learning_rate": 4.454666666666667e-05,
      "loss": 0.0027,
      "step": 4090
    },
    {
      "epoch": 0.21866666666666668,
      "grad_norm": 0.4045063853263855,
      "learning_rate": 4.4533333333333336e-05,
      "loss": 0.003,
      "step": 4100
    },
    {
      "epoch": 0.2192,
      "grad_norm": 0.4555279016494751,
      "learning_rate": 4.452e-05,
      "loss": 0.0035,
      "step": 4110
    },
    {
      "epoch": 0.21973333333333334,
      "grad_norm": 0.14542198181152344,
      "learning_rate": 4.450666666666667e-05,
      "loss": 0.0024,
      "step": 4120
    },
    {
      "epoch": 0.22026666666666667,
      "grad_norm": 0.18287359178066254,
      "learning_rate": 4.4493333333333337e-05,
      "loss": 0.003,
      "step": 4130
    },
    {
      "epoch": 0.2208,
      "grad_norm": 0.09132364392280579,
      "learning_rate": 4.448e-05,
      "loss": 0.0036,
      "step": 4140
    },
    {
      "epoch": 0.22133333333333333,
      "grad_norm": 0.3408432900905609,
      "learning_rate": 4.4466666666666666e-05,
      "loss": 0.0025,
      "step": 4150
    },
    {
      "epoch": 0.22186666666666666,
      "grad_norm": 0.24664363265037537,
      "learning_rate": 4.445333333333334e-05,
      "loss": 0.0027,
      "step": 4160
    },
    {
      "epoch": 0.2224,
      "grad_norm": 0.1218370720744133,
      "learning_rate": 4.444e-05,
      "loss": 0.0024,
      "step": 4170
    },
    {
      "epoch": 0.22293333333333334,
      "grad_norm": 0.146764874458313,
      "learning_rate": 4.4426666666666666e-05,
      "loss": 0.002,
      "step": 4180
    },
    {
      "epoch": 0.22346666666666667,
      "grad_norm": 0.12588104605674744,
      "learning_rate": 4.441333333333334e-05,
      "loss": 0.0033,
      "step": 4190
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.6021360158920288,
      "learning_rate": 4.44e-05,
      "loss": 0.0027,
      "step": 4200
    },
    {
      "epoch": 0.22453333333333333,
      "grad_norm": 0.06988280266523361,
      "learning_rate": 4.438666666666667e-05,
      "loss": 0.003,
      "step": 4210
    },
    {
      "epoch": 0.22506666666666666,
      "grad_norm": 0.14153499901294708,
      "learning_rate": 4.437333333333333e-05,
      "loss": 0.0018,
      "step": 4220
    },
    {
      "epoch": 0.2256,
      "grad_norm": 0.4220855236053467,
      "learning_rate": 4.436e-05,
      "loss": 0.0022,
      "step": 4230
    },
    {
      "epoch": 0.22613333333333333,
      "grad_norm": 0.7010286450386047,
      "learning_rate": 4.434666666666667e-05,
      "loss": 0.0025,
      "step": 4240
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.222062349319458,
      "learning_rate": 4.433333333333334e-05,
      "loss": 0.003,
      "step": 4250
    },
    {
      "epoch": 0.2272,
      "grad_norm": 0.5598747730255127,
      "learning_rate": 4.432e-05,
      "loss": 0.0032,
      "step": 4260
    },
    {
      "epoch": 0.22773333333333334,
      "grad_norm": 0.15743567049503326,
      "learning_rate": 4.430666666666667e-05,
      "loss": 0.0028,
      "step": 4270
    },
    {
      "epoch": 0.22826666666666667,
      "grad_norm": 0.8018814921379089,
      "learning_rate": 4.429333333333334e-05,
      "loss": 0.0032,
      "step": 4280
    },
    {
      "epoch": 0.2288,
      "grad_norm": 0.30556046962738037,
      "learning_rate": 4.428e-05,
      "loss": 0.0022,
      "step": 4290
    },
    {
      "epoch": 0.22933333333333333,
      "grad_norm": 0.14834225177764893,
      "learning_rate": 4.426666666666667e-05,
      "loss": 0.0018,
      "step": 4300
    },
    {
      "epoch": 0.22986666666666666,
      "grad_norm": 0.24845407903194427,
      "learning_rate": 4.425333333333334e-05,
      "loss": 0.0022,
      "step": 4310
    },
    {
      "epoch": 0.2304,
      "grad_norm": 0.09056466072797775,
      "learning_rate": 4.424e-05,
      "loss": 0.0023,
      "step": 4320
    },
    {
      "epoch": 0.23093333333333332,
      "grad_norm": 0.41002795100212097,
      "learning_rate": 4.422666666666667e-05,
      "loss": 0.0037,
      "step": 4330
    },
    {
      "epoch": 0.23146666666666665,
      "grad_norm": 0.36629030108451843,
      "learning_rate": 4.421333333333333e-05,
      "loss": 0.0044,
      "step": 4340
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.3327278792858124,
      "learning_rate": 4.4200000000000004e-05,
      "loss": 0.0023,
      "step": 4350
    },
    {
      "epoch": 0.23253333333333334,
      "grad_norm": 0.36101311445236206,
      "learning_rate": 4.418666666666667e-05,
      "loss": 0.0039,
      "step": 4360
    },
    {
      "epoch": 0.23306666666666667,
      "grad_norm": 0.4171048700809479,
      "learning_rate": 4.417333333333333e-05,
      "loss": 0.0025,
      "step": 4370
    },
    {
      "epoch": 0.2336,
      "grad_norm": 0.3446022868156433,
      "learning_rate": 4.4160000000000004e-05,
      "loss": 0.0023,
      "step": 4380
    },
    {
      "epoch": 0.23413333333333333,
      "grad_norm": 0.36665451526641846,
      "learning_rate": 4.414666666666667e-05,
      "loss": 0.0028,
      "step": 4390
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 0.4149685204029083,
      "learning_rate": 4.413333333333334e-05,
      "loss": 0.0024,
      "step": 4400
    },
    {
      "epoch": 0.2352,
      "grad_norm": 0.5797607898712158,
      "learning_rate": 4.412e-05,
      "loss": 0.0032,
      "step": 4410
    },
    {
      "epoch": 0.23573333333333332,
      "grad_norm": 0.32701575756073,
      "learning_rate": 4.410666666666667e-05,
      "loss": 0.002,
      "step": 4420
    },
    {
      "epoch": 0.23626666666666668,
      "grad_norm": 0.2762508690357208,
      "learning_rate": 4.4093333333333334e-05,
      "loss": 0.002,
      "step": 4430
    },
    {
      "epoch": 0.2368,
      "grad_norm": 0.23114895820617676,
      "learning_rate": 4.4080000000000005e-05,
      "loss": 0.0023,
      "step": 4440
    },
    {
      "epoch": 0.23733333333333334,
      "grad_norm": 0.49715927243232727,
      "learning_rate": 4.406666666666667e-05,
      "loss": 0.0021,
      "step": 4450
    },
    {
      "epoch": 0.23786666666666667,
      "grad_norm": 0.25581714510917664,
      "learning_rate": 4.4053333333333334e-05,
      "loss": 0.0043,
      "step": 4460
    },
    {
      "epoch": 0.2384,
      "grad_norm": 0.2919352948665619,
      "learning_rate": 4.4040000000000005e-05,
      "loss": 0.0023,
      "step": 4470
    },
    {
      "epoch": 0.23893333333333333,
      "grad_norm": 0.2750207185745239,
      "learning_rate": 4.402666666666666e-05,
      "loss": 0.0033,
      "step": 4480
    },
    {
      "epoch": 0.23946666666666666,
      "grad_norm": 0.22390811145305634,
      "learning_rate": 4.4013333333333334e-05,
      "loss": 0.0023,
      "step": 4490
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.13397102057933807,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.0026,
      "step": 4500
    },
    {
      "epoch": 0.24053333333333332,
      "grad_norm": 0.4931724965572357,
      "learning_rate": 4.398666666666667e-05,
      "loss": 0.0026,
      "step": 4510
    },
    {
      "epoch": 0.24106666666666668,
      "grad_norm": 0.28081217408180237,
      "learning_rate": 4.3973333333333335e-05,
      "loss": 0.0031,
      "step": 4520
    },
    {
      "epoch": 0.2416,
      "grad_norm": 0.2242477685213089,
      "learning_rate": 4.396e-05,
      "loss": 0.0028,
      "step": 4530
    },
    {
      "epoch": 0.24213333333333334,
      "grad_norm": 0.5058932900428772,
      "learning_rate": 4.394666666666667e-05,
      "loss": 0.0022,
      "step": 4540
    },
    {
      "epoch": 0.24266666666666667,
      "grad_norm": 0.43770256638526917,
      "learning_rate": 4.3933333333333335e-05,
      "loss": 0.0028,
      "step": 4550
    },
    {
      "epoch": 0.2432,
      "grad_norm": 0.4021463692188263,
      "learning_rate": 4.392e-05,
      "loss": 0.0021,
      "step": 4560
    },
    {
      "epoch": 0.24373333333333333,
      "grad_norm": 0.6238744258880615,
      "learning_rate": 4.390666666666667e-05,
      "loss": 0.0022,
      "step": 4570
    },
    {
      "epoch": 0.24426666666666666,
      "grad_norm": 0.4130648076534271,
      "learning_rate": 4.3893333333333335e-05,
      "loss": 0.0026,
      "step": 4580
    },
    {
      "epoch": 0.2448,
      "grad_norm": 0.6569857001304626,
      "learning_rate": 4.388000000000001e-05,
      "loss": 0.002,
      "step": 4590
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 0.6628051996231079,
      "learning_rate": 4.3866666666666665e-05,
      "loss": 0.0023,
      "step": 4600
    },
    {
      "epoch": 0.24586666666666668,
      "grad_norm": 0.7261273860931396,
      "learning_rate": 4.3853333333333336e-05,
      "loss": 0.0042,
      "step": 4610
    },
    {
      "epoch": 0.2464,
      "grad_norm": 0.5374911427497864,
      "learning_rate": 4.384e-05,
      "loss": 0.0022,
      "step": 4620
    },
    {
      "epoch": 0.24693333333333334,
      "grad_norm": 0.3058069050312042,
      "learning_rate": 4.382666666666667e-05,
      "loss": 0.0022,
      "step": 4630
    },
    {
      "epoch": 0.24746666666666667,
      "grad_norm": 0.10346844047307968,
      "learning_rate": 4.3813333333333336e-05,
      "loss": 0.0023,
      "step": 4640
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.07448385655879974,
      "learning_rate": 4.38e-05,
      "loss": 0.0029,
      "step": 4650
    },
    {
      "epoch": 0.24853333333333333,
      "grad_norm": 0.40624842047691345,
      "learning_rate": 4.378666666666667e-05,
      "loss": 0.0022,
      "step": 4660
    },
    {
      "epoch": 0.24906666666666666,
      "grad_norm": 0.4926183521747589,
      "learning_rate": 4.377333333333333e-05,
      "loss": 0.0023,
      "step": 4670
    },
    {
      "epoch": 0.2496,
      "grad_norm": 0.3571436405181885,
      "learning_rate": 4.376e-05,
      "loss": 0.0028,
      "step": 4680
    },
    {
      "epoch": 0.2501333333333333,
      "grad_norm": 0.11475048214197159,
      "learning_rate": 4.374666666666667e-05,
      "loss": 0.0027,
      "step": 4690
    },
    {
      "epoch": 0.25066666666666665,
      "grad_norm": 0.6697658896446228,
      "learning_rate": 4.373333333333334e-05,
      "loss": 0.003,
      "step": 4700
    },
    {
      "epoch": 0.2512,
      "grad_norm": 0.44533786177635193,
      "learning_rate": 4.372e-05,
      "loss": 0.0027,
      "step": 4710
    },
    {
      "epoch": 0.2517333333333333,
      "grad_norm": 0.28454479575157166,
      "learning_rate": 4.3706666666666666e-05,
      "loss": 0.0019,
      "step": 4720
    },
    {
      "epoch": 0.25226666666666664,
      "grad_norm": 0.3913210332393646,
      "learning_rate": 4.369333333333334e-05,
      "loss": 0.0022,
      "step": 4730
    },
    {
      "epoch": 0.2528,
      "grad_norm": 0.12641282379627228,
      "learning_rate": 4.368e-05,
      "loss": 0.0034,
      "step": 4740
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.09812038391828537,
      "learning_rate": 4.3666666666666666e-05,
      "loss": 0.0039,
      "step": 4750
    },
    {
      "epoch": 0.2538666666666667,
      "grad_norm": 0.6098076701164246,
      "learning_rate": 4.365333333333334e-05,
      "loss": 0.0023,
      "step": 4760
    },
    {
      "epoch": 0.2544,
      "grad_norm": 0.27175238728523254,
      "learning_rate": 4.364e-05,
      "loss": 0.0028,
      "step": 4770
    },
    {
      "epoch": 0.25493333333333335,
      "grad_norm": 0.23471130430698395,
      "learning_rate": 4.3626666666666674e-05,
      "loss": 0.0028,
      "step": 4780
    },
    {
      "epoch": 0.2554666666666667,
      "grad_norm": 0.6580936312675476,
      "learning_rate": 4.361333333333333e-05,
      "loss": 0.0028,
      "step": 4790
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.5357499718666077,
      "learning_rate": 4.36e-05,
      "loss": 0.0028,
      "step": 4800
    },
    {
      "epoch": 0.25653333333333334,
      "grad_norm": 0.19682367146015167,
      "learning_rate": 4.358666666666667e-05,
      "loss": 0.0036,
      "step": 4810
    },
    {
      "epoch": 0.25706666666666667,
      "grad_norm": 0.11516829580068588,
      "learning_rate": 4.357333333333333e-05,
      "loss": 0.0038,
      "step": 4820
    },
    {
      "epoch": 0.2576,
      "grad_norm": 0.10846329480409622,
      "learning_rate": 4.356e-05,
      "loss": 0.0034,
      "step": 4830
    },
    {
      "epoch": 0.2581333333333333,
      "grad_norm": 0.45386332273483276,
      "learning_rate": 4.354666666666667e-05,
      "loss": 0.0021,
      "step": 4840
    },
    {
      "epoch": 0.25866666666666666,
      "grad_norm": 0.511017382144928,
      "learning_rate": 4.353333333333334e-05,
      "loss": 0.0031,
      "step": 4850
    },
    {
      "epoch": 0.2592,
      "grad_norm": 0.5099149942398071,
      "learning_rate": 4.352e-05,
      "loss": 0.0032,
      "step": 4860
    },
    {
      "epoch": 0.2597333333333333,
      "grad_norm": 0.2690076231956482,
      "learning_rate": 4.350666666666667e-05,
      "loss": 0.0023,
      "step": 4870
    },
    {
      "epoch": 0.26026666666666665,
      "grad_norm": 0.26493534445762634,
      "learning_rate": 4.349333333333334e-05,
      "loss": 0.0034,
      "step": 4880
    },
    {
      "epoch": 0.2608,
      "grad_norm": 0.711715579032898,
      "learning_rate": 4.3480000000000004e-05,
      "loss": 0.0036,
      "step": 4890
    },
    {
      "epoch": 0.2613333333333333,
      "grad_norm": 0.7869184613227844,
      "learning_rate": 4.346666666666667e-05,
      "loss": 0.0022,
      "step": 4900
    },
    {
      "epoch": 0.2618666666666667,
      "grad_norm": 0.5459535717964172,
      "learning_rate": 4.345333333333333e-05,
      "loss": 0.0038,
      "step": 4910
    },
    {
      "epoch": 0.2624,
      "grad_norm": 1.0135228633880615,
      "learning_rate": 4.3440000000000004e-05,
      "loss": 0.0027,
      "step": 4920
    },
    {
      "epoch": 0.26293333333333335,
      "grad_norm": 0.12588173151016235,
      "learning_rate": 4.342666666666667e-05,
      "loss": 0.0019,
      "step": 4930
    },
    {
      "epoch": 0.2634666666666667,
      "grad_norm": 0.4273301362991333,
      "learning_rate": 4.341333333333333e-05,
      "loss": 0.0019,
      "step": 4940
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.20094150304794312,
      "learning_rate": 4.3400000000000005e-05,
      "loss": 0.0042,
      "step": 4950
    },
    {
      "epoch": 0.26453333333333334,
      "grad_norm": 0.272868812084198,
      "learning_rate": 4.338666666666667e-05,
      "loss": 0.0022,
      "step": 4960
    },
    {
      "epoch": 0.2650666666666667,
      "grad_norm": 0.3979061245918274,
      "learning_rate": 4.337333333333334e-05,
      "loss": 0.003,
      "step": 4970
    },
    {
      "epoch": 0.2656,
      "grad_norm": 0.13697178661823273,
      "learning_rate": 4.336e-05,
      "loss": 0.0023,
      "step": 4980
    },
    {
      "epoch": 0.26613333333333333,
      "grad_norm": 0.0903155580163002,
      "learning_rate": 4.334666666666667e-05,
      "loss": 0.002,
      "step": 4990
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.5024517774581909,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.003,
      "step": 5000
    },
    {
      "epoch": 0.2672,
      "grad_norm": 0.3926587700843811,
      "learning_rate": 4.332e-05,
      "loss": 0.0039,
      "step": 5010
    },
    {
      "epoch": 0.2677333333333333,
      "grad_norm": 0.4691435694694519,
      "learning_rate": 4.330666666666667e-05,
      "loss": 0.0042,
      "step": 5020
    },
    {
      "epoch": 0.26826666666666665,
      "grad_norm": 0.1608288437128067,
      "learning_rate": 4.3293333333333334e-05,
      "loss": 0.0021,
      "step": 5030
    },
    {
      "epoch": 0.2688,
      "grad_norm": 0.12032289057970047,
      "learning_rate": 4.3280000000000006e-05,
      "loss": 0.002,
      "step": 5040
    },
    {
      "epoch": 0.2693333333333333,
      "grad_norm": 0.22146070003509521,
      "learning_rate": 4.3266666666666664e-05,
      "loss": 0.0022,
      "step": 5050
    },
    {
      "epoch": 0.26986666666666664,
      "grad_norm": 0.6253778338432312,
      "learning_rate": 4.3253333333333335e-05,
      "loss": 0.0025,
      "step": 5060
    },
    {
      "epoch": 0.2704,
      "grad_norm": 0.347503125667572,
      "learning_rate": 4.324e-05,
      "loss": 0.0021,
      "step": 5070
    },
    {
      "epoch": 0.27093333333333336,
      "grad_norm": 0.6685950756072998,
      "learning_rate": 4.322666666666667e-05,
      "loss": 0.0022,
      "step": 5080
    },
    {
      "epoch": 0.2714666666666667,
      "grad_norm": 0.4446982741355896,
      "learning_rate": 4.3213333333333335e-05,
      "loss": 0.003,
      "step": 5090
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.12858054041862488,
      "learning_rate": 4.32e-05,
      "loss": 0.0022,
      "step": 5100
    },
    {
      "epoch": 0.27253333333333335,
      "grad_norm": 0.10789086669683456,
      "learning_rate": 4.318666666666667e-05,
      "loss": 0.0026,
      "step": 5110
    },
    {
      "epoch": 0.2730666666666667,
      "grad_norm": 0.20222190022468567,
      "learning_rate": 4.3173333333333336e-05,
      "loss": 0.0026,
      "step": 5120
    },
    {
      "epoch": 0.2736,
      "grad_norm": 0.2994396686553955,
      "learning_rate": 4.316e-05,
      "loss": 0.0024,
      "step": 5130
    },
    {
      "epoch": 0.27413333333333334,
      "grad_norm": 0.4482437074184418,
      "learning_rate": 4.314666666666667e-05,
      "loss": 0.0025,
      "step": 5140
    },
    {
      "epoch": 0.27466666666666667,
      "grad_norm": 0.3185678720474243,
      "learning_rate": 4.3133333333333336e-05,
      "loss": 0.0027,
      "step": 5150
    },
    {
      "epoch": 0.2752,
      "grad_norm": 0.19120337069034576,
      "learning_rate": 4.312000000000001e-05,
      "loss": 0.0021,
      "step": 5160
    },
    {
      "epoch": 0.27573333333333333,
      "grad_norm": 0.7623177766799927,
      "learning_rate": 4.3106666666666665e-05,
      "loss": 0.0019,
      "step": 5170
    },
    {
      "epoch": 0.27626666666666666,
      "grad_norm": 0.13132913410663605,
      "learning_rate": 4.3093333333333336e-05,
      "loss": 0.002,
      "step": 5180
    },
    {
      "epoch": 0.2768,
      "grad_norm": 0.1971605271100998,
      "learning_rate": 4.308e-05,
      "loss": 0.0026,
      "step": 5190
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 0.8085433840751648,
      "learning_rate": 4.3066666666666665e-05,
      "loss": 0.0024,
      "step": 5200
    },
    {
      "epoch": 0.27786666666666665,
      "grad_norm": 0.2298106998205185,
      "learning_rate": 4.305333333333334e-05,
      "loss": 0.0019,
      "step": 5210
    },
    {
      "epoch": 0.2784,
      "grad_norm": 0.2490088790655136,
      "learning_rate": 4.304e-05,
      "loss": 0.0028,
      "step": 5220
    },
    {
      "epoch": 0.2789333333333333,
      "grad_norm": 0.16385236382484436,
      "learning_rate": 4.302666666666667e-05,
      "loss": 0.0018,
      "step": 5230
    },
    {
      "epoch": 0.27946666666666664,
      "grad_norm": 0.2638193666934967,
      "learning_rate": 4.301333333333333e-05,
      "loss": 0.0041,
      "step": 5240
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.37467092275619507,
      "learning_rate": 4.3e-05,
      "loss": 0.0018,
      "step": 5250
    },
    {
      "epoch": 0.28053333333333336,
      "grad_norm": 0.3690793216228485,
      "learning_rate": 4.2986666666666666e-05,
      "loss": 0.0029,
      "step": 5260
    },
    {
      "epoch": 0.2810666666666667,
      "grad_norm": 0.7566252946853638,
      "learning_rate": 4.297333333333334e-05,
      "loss": 0.0026,
      "step": 5270
    },
    {
      "epoch": 0.2816,
      "grad_norm": 0.17549556493759155,
      "learning_rate": 4.296e-05,
      "loss": 0.0033,
      "step": 5280
    },
    {
      "epoch": 0.28213333333333335,
      "grad_norm": 0.34755659103393555,
      "learning_rate": 4.2946666666666667e-05,
      "loss": 0.002,
      "step": 5290
    },
    {
      "epoch": 0.2826666666666667,
      "grad_norm": 0.9912095069885254,
      "learning_rate": 4.293333333333334e-05,
      "loss": 0.0026,
      "step": 5300
    },
    {
      "epoch": 0.2832,
      "grad_norm": 0.5423659682273865,
      "learning_rate": 4.292e-05,
      "loss": 0.0037,
      "step": 5310
    },
    {
      "epoch": 0.28373333333333334,
      "grad_norm": 0.6623004674911499,
      "learning_rate": 4.290666666666667e-05,
      "loss": 0.0026,
      "step": 5320
    },
    {
      "epoch": 0.28426666666666667,
      "grad_norm": 0.48902326822280884,
      "learning_rate": 4.289333333333334e-05,
      "loss": 0.0024,
      "step": 5330
    },
    {
      "epoch": 0.2848,
      "grad_norm": 0.19886565208435059,
      "learning_rate": 4.288e-05,
      "loss": 0.0034,
      "step": 5340
    },
    {
      "epoch": 0.2853333333333333,
      "grad_norm": 0.2328662872314453,
      "learning_rate": 4.286666666666667e-05,
      "loss": 0.0023,
      "step": 5350
    },
    {
      "epoch": 0.28586666666666666,
      "grad_norm": 0.3192695677280426,
      "learning_rate": 4.285333333333333e-05,
      "loss": 0.0029,
      "step": 5360
    },
    {
      "epoch": 0.2864,
      "grad_norm": 0.4095083177089691,
      "learning_rate": 4.284e-05,
      "loss": 0.0028,
      "step": 5370
    },
    {
      "epoch": 0.2869333333333333,
      "grad_norm": 0.2856963574886322,
      "learning_rate": 4.282666666666667e-05,
      "loss": 0.0029,
      "step": 5380
    },
    {
      "epoch": 0.28746666666666665,
      "grad_norm": 0.7585778832435608,
      "learning_rate": 4.281333333333333e-05,
      "loss": 0.0023,
      "step": 5390
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.46947747468948364,
      "learning_rate": 4.2800000000000004e-05,
      "loss": 0.0027,
      "step": 5400
    },
    {
      "epoch": 0.2885333333333333,
      "grad_norm": 0.2853326201438904,
      "learning_rate": 4.278666666666667e-05,
      "loss": 0.0017,
      "step": 5410
    },
    {
      "epoch": 0.2890666666666667,
      "grad_norm": 0.19426225125789642,
      "learning_rate": 4.277333333333334e-05,
      "loss": 0.004,
      "step": 5420
    },
    {
      "epoch": 0.2896,
      "grad_norm": 0.5529329180717468,
      "learning_rate": 4.276e-05,
      "loss": 0.002,
      "step": 5430
    },
    {
      "epoch": 0.29013333333333335,
      "grad_norm": 0.4024152159690857,
      "learning_rate": 4.274666666666667e-05,
      "loss": 0.0028,
      "step": 5440
    },
    {
      "epoch": 0.2906666666666667,
      "grad_norm": 0.10019383579492569,
      "learning_rate": 4.273333333333333e-05,
      "loss": 0.0025,
      "step": 5450
    },
    {
      "epoch": 0.2912,
      "grad_norm": 0.43370282649993896,
      "learning_rate": 4.2720000000000004e-05,
      "loss": 0.0022,
      "step": 5460
    },
    {
      "epoch": 0.29173333333333334,
      "grad_norm": 0.2731049358844757,
      "learning_rate": 4.270666666666667e-05,
      "loss": 0.0021,
      "step": 5470
    },
    {
      "epoch": 0.2922666666666667,
      "grad_norm": 0.13195361196994781,
      "learning_rate": 4.2693333333333333e-05,
      "loss": 0.0023,
      "step": 5480
    },
    {
      "epoch": 0.2928,
      "grad_norm": 0.9837509393692017,
      "learning_rate": 4.2680000000000005e-05,
      "loss": 0.0034,
      "step": 5490
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.23009301722049713,
      "learning_rate": 4.266666666666667e-05,
      "loss": 0.0032,
      "step": 5500
    },
    {
      "epoch": 0.29386666666666666,
      "grad_norm": 0.14517134428024292,
      "learning_rate": 4.2653333333333334e-05,
      "loss": 0.0027,
      "step": 5510
    },
    {
      "epoch": 0.2944,
      "grad_norm": 0.43800610303878784,
      "learning_rate": 4.2640000000000005e-05,
      "loss": 0.0034,
      "step": 5520
    },
    {
      "epoch": 0.2949333333333333,
      "grad_norm": 0.2261269986629486,
      "learning_rate": 4.262666666666667e-05,
      "loss": 0.0027,
      "step": 5530
    },
    {
      "epoch": 0.29546666666666666,
      "grad_norm": 0.38405364751815796,
      "learning_rate": 4.2613333333333334e-05,
      "loss": 0.0023,
      "step": 5540
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.5460050702095032,
      "learning_rate": 4.26e-05,
      "loss": 0.0034,
      "step": 5550
    },
    {
      "epoch": 0.2965333333333333,
      "grad_norm": 0.13583780825138092,
      "learning_rate": 4.258666666666667e-05,
      "loss": 0.0024,
      "step": 5560
    },
    {
      "epoch": 0.29706666666666665,
      "grad_norm": 0.3985147476196289,
      "learning_rate": 4.2573333333333335e-05,
      "loss": 0.0042,
      "step": 5570
    },
    {
      "epoch": 0.2976,
      "grad_norm": 0.8034112453460693,
      "learning_rate": 4.256e-05,
      "loss": 0.0048,
      "step": 5580
    },
    {
      "epoch": 0.2981333333333333,
      "grad_norm": 0.27715864777565,
      "learning_rate": 4.254666666666667e-05,
      "loss": 0.0038,
      "step": 5590
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 0.25091350078582764,
      "learning_rate": 4.2533333333333335e-05,
      "loss": 0.0043,
      "step": 5600
    },
    {
      "epoch": 0.2992,
      "grad_norm": 0.6663274168968201,
      "learning_rate": 4.2520000000000006e-05,
      "loss": 0.005,
      "step": 5610
    },
    {
      "epoch": 0.29973333333333335,
      "grad_norm": 0.24242429435253143,
      "learning_rate": 4.2506666666666664e-05,
      "loss": 0.0034,
      "step": 5620
    },
    {
      "epoch": 0.3002666666666667,
      "grad_norm": 0.5321468710899353,
      "learning_rate": 4.2493333333333335e-05,
      "loss": 0.0034,
      "step": 5630
    },
    {
      "epoch": 0.3008,
      "grad_norm": 0.15730419754981995,
      "learning_rate": 4.248e-05,
      "loss": 0.0029,
      "step": 5640
    },
    {
      "epoch": 0.30133333333333334,
      "grad_norm": 0.5721936821937561,
      "learning_rate": 4.246666666666667e-05,
      "loss": 0.003,
      "step": 5650
    },
    {
      "epoch": 0.30186666666666667,
      "grad_norm": 0.3416871726512909,
      "learning_rate": 4.2453333333333336e-05,
      "loss": 0.0037,
      "step": 5660
    },
    {
      "epoch": 0.3024,
      "grad_norm": 0.5034658908843994,
      "learning_rate": 4.244e-05,
      "loss": 0.0042,
      "step": 5670
    },
    {
      "epoch": 0.30293333333333333,
      "grad_norm": 0.13400059938430786,
      "learning_rate": 4.242666666666667e-05,
      "loss": 0.0027,
      "step": 5680
    },
    {
      "epoch": 0.30346666666666666,
      "grad_norm": 0.6624288558959961,
      "learning_rate": 4.241333333333333e-05,
      "loss": 0.0034,
      "step": 5690
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.4472058117389679,
      "learning_rate": 4.24e-05,
      "loss": 0.0039,
      "step": 5700
    },
    {
      "epoch": 0.3045333333333333,
      "grad_norm": 0.6573841571807861,
      "learning_rate": 4.238666666666667e-05,
      "loss": 0.0019,
      "step": 5710
    },
    {
      "epoch": 0.30506666666666665,
      "grad_norm": 1.1083170175552368,
      "learning_rate": 4.2373333333333336e-05,
      "loss": 0.0018,
      "step": 5720
    },
    {
      "epoch": 0.3056,
      "grad_norm": 0.576977014541626,
      "learning_rate": 4.236e-05,
      "loss": 0.0019,
      "step": 5730
    },
    {
      "epoch": 0.3061333333333333,
      "grad_norm": 0.18466652929782867,
      "learning_rate": 4.2346666666666666e-05,
      "loss": 0.0032,
      "step": 5740
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.10811780393123627,
      "learning_rate": 4.233333333333334e-05,
      "loss": 0.0025,
      "step": 5750
    },
    {
      "epoch": 0.3072,
      "grad_norm": 0.2942865788936615,
      "learning_rate": 4.232e-05,
      "loss": 0.0033,
      "step": 5760
    },
    {
      "epoch": 0.30773333333333336,
      "grad_norm": 0.11117009818553925,
      "learning_rate": 4.2306666666666666e-05,
      "loss": 0.0023,
      "step": 5770
    },
    {
      "epoch": 0.3082666666666667,
      "grad_norm": 0.27328500151634216,
      "learning_rate": 4.229333333333334e-05,
      "loss": 0.0024,
      "step": 5780
    },
    {
      "epoch": 0.3088,
      "grad_norm": 0.48325660824775696,
      "learning_rate": 4.228e-05,
      "loss": 0.0028,
      "step": 5790
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 0.12408613413572311,
      "learning_rate": 4.226666666666667e-05,
      "loss": 0.002,
      "step": 5800
    },
    {
      "epoch": 0.3098666666666667,
      "grad_norm": 0.5678504705429077,
      "learning_rate": 4.225333333333333e-05,
      "loss": 0.0024,
      "step": 5810
    },
    {
      "epoch": 0.3104,
      "grad_norm": 0.43064817786216736,
      "learning_rate": 4.224e-05,
      "loss": 0.0024,
      "step": 5820
    },
    {
      "epoch": 0.31093333333333334,
      "grad_norm": 0.38660451769828796,
      "learning_rate": 4.222666666666667e-05,
      "loss": 0.0025,
      "step": 5830
    },
    {
      "epoch": 0.31146666666666667,
      "grad_norm": 0.28081443905830383,
      "learning_rate": 4.221333333333334e-05,
      "loss": 0.0031,
      "step": 5840
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.7180253863334656,
      "learning_rate": 4.22e-05,
      "loss": 0.0031,
      "step": 5850
    },
    {
      "epoch": 0.31253333333333333,
      "grad_norm": 0.3787771463394165,
      "learning_rate": 4.218666666666667e-05,
      "loss": 0.0018,
      "step": 5860
    },
    {
      "epoch": 0.31306666666666666,
      "grad_norm": 0.38750705122947693,
      "learning_rate": 4.217333333333334e-05,
      "loss": 0.0023,
      "step": 5870
    },
    {
      "epoch": 0.3136,
      "grad_norm": 0.5064195990562439,
      "learning_rate": 4.2159999999999996e-05,
      "loss": 0.0022,
      "step": 5880
    },
    {
      "epoch": 0.3141333333333333,
      "grad_norm": 0.2585527002811432,
      "learning_rate": 4.214666666666667e-05,
      "loss": 0.0023,
      "step": 5890
    },
    {
      "epoch": 0.31466666666666665,
      "grad_norm": 0.45402824878692627,
      "learning_rate": 4.213333333333334e-05,
      "loss": 0.0038,
      "step": 5900
    },
    {
      "epoch": 0.3152,
      "grad_norm": 0.8467156887054443,
      "learning_rate": 4.212e-05,
      "loss": 0.002,
      "step": 5910
    },
    {
      "epoch": 0.3157333333333333,
      "grad_norm": 0.18914639949798584,
      "learning_rate": 4.210666666666667e-05,
      "loss": 0.0025,
      "step": 5920
    },
    {
      "epoch": 0.31626666666666664,
      "grad_norm": 0.32005399465560913,
      "learning_rate": 4.209333333333333e-05,
      "loss": 0.0036,
      "step": 5930
    },
    {
      "epoch": 0.3168,
      "grad_norm": 0.35526514053344727,
      "learning_rate": 4.2080000000000004e-05,
      "loss": 0.004,
      "step": 5940
    },
    {
      "epoch": 0.31733333333333336,
      "grad_norm": 0.11573148518800735,
      "learning_rate": 4.206666666666667e-05,
      "loss": 0.0021,
      "step": 5950
    },
    {
      "epoch": 0.3178666666666667,
      "grad_norm": 0.10244444757699966,
      "learning_rate": 4.205333333333333e-05,
      "loss": 0.0021,
      "step": 5960
    },
    {
      "epoch": 0.3184,
      "grad_norm": 0.3018045723438263,
      "learning_rate": 4.2040000000000004e-05,
      "loss": 0.0034,
      "step": 5970
    },
    {
      "epoch": 0.31893333333333335,
      "grad_norm": 0.49906566739082336,
      "learning_rate": 4.202666666666667e-05,
      "loss": 0.0024,
      "step": 5980
    },
    {
      "epoch": 0.3194666666666667,
      "grad_norm": 0.6097710132598877,
      "learning_rate": 4.201333333333334e-05,
      "loss": 0.0028,
      "step": 5990
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.3605681359767914,
      "learning_rate": 4.2e-05,
      "loss": 0.003,
      "step": 6000
    },
    {
      "epoch": 0.32053333333333334,
      "grad_norm": 0.3762373626232147,
      "learning_rate": 4.198666666666667e-05,
      "loss": 0.0023,
      "step": 6010
    },
    {
      "epoch": 0.32106666666666667,
      "grad_norm": 0.18479353189468384,
      "learning_rate": 4.1973333333333334e-05,
      "loss": 0.0021,
      "step": 6020
    },
    {
      "epoch": 0.3216,
      "grad_norm": 0.3710678219795227,
      "learning_rate": 4.196e-05,
      "loss": 0.0023,
      "step": 6030
    },
    {
      "epoch": 0.3221333333333333,
      "grad_norm": 0.4263995885848999,
      "learning_rate": 4.194666666666667e-05,
      "loss": 0.0033,
      "step": 6040
    },
    {
      "epoch": 0.32266666666666666,
      "grad_norm": 0.3058091998100281,
      "learning_rate": 4.1933333333333334e-05,
      "loss": 0.0037,
      "step": 6050
    },
    {
      "epoch": 0.3232,
      "grad_norm": 0.4068586826324463,
      "learning_rate": 4.1920000000000005e-05,
      "loss": 0.0029,
      "step": 6060
    },
    {
      "epoch": 0.3237333333333333,
      "grad_norm": 0.06718755513429642,
      "learning_rate": 4.190666666666666e-05,
      "loss": 0.0022,
      "step": 6070
    },
    {
      "epoch": 0.32426666666666665,
      "grad_norm": 0.19241055846214294,
      "learning_rate": 4.1893333333333334e-05,
      "loss": 0.0027,
      "step": 6080
    },
    {
      "epoch": 0.3248,
      "grad_norm": 0.08543430268764496,
      "learning_rate": 4.1880000000000006e-05,
      "loss": 0.0023,
      "step": 6090
    },
    {
      "epoch": 0.3253333333333333,
      "grad_norm": 0.10312295705080032,
      "learning_rate": 4.186666666666667e-05,
      "loss": 0.0041,
      "step": 6100
    },
    {
      "epoch": 0.3258666666666667,
      "grad_norm": 0.49959444999694824,
      "learning_rate": 4.1853333333333335e-05,
      "loss": 0.0028,
      "step": 6110
    },
    {
      "epoch": 0.3264,
      "grad_norm": 0.6515896320343018,
      "learning_rate": 4.184e-05,
      "loss": 0.0034,
      "step": 6120
    },
    {
      "epoch": 0.32693333333333335,
      "grad_norm": 0.20024652779102325,
      "learning_rate": 4.182666666666667e-05,
      "loss": 0.0033,
      "step": 6130
    },
    {
      "epoch": 0.3274666666666667,
      "grad_norm": 0.23866455256938934,
      "learning_rate": 4.1813333333333335e-05,
      "loss": 0.0041,
      "step": 6140
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.7381758689880371,
      "learning_rate": 4.18e-05,
      "loss": 0.0027,
      "step": 6150
    },
    {
      "epoch": 0.32853333333333334,
      "grad_norm": 0.1376936286687851,
      "learning_rate": 4.178666666666667e-05,
      "loss": 0.0035,
      "step": 6160
    },
    {
      "epoch": 0.3290666666666667,
      "grad_norm": 0.2960319221019745,
      "learning_rate": 4.1773333333333335e-05,
      "loss": 0.0025,
      "step": 6170
    },
    {
      "epoch": 0.3296,
      "grad_norm": 0.37837615609169006,
      "learning_rate": 4.176000000000001e-05,
      "loss": 0.003,
      "step": 6180
    },
    {
      "epoch": 0.33013333333333333,
      "grad_norm": 0.0913725271821022,
      "learning_rate": 4.1746666666666665e-05,
      "loss": 0.002,
      "step": 6190
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 0.06524614989757538,
      "learning_rate": 4.1733333333333336e-05,
      "loss": 0.0033,
      "step": 6200
    },
    {
      "epoch": 0.3312,
      "grad_norm": 0.2415812462568283,
      "learning_rate": 4.172e-05,
      "loss": 0.0027,
      "step": 6210
    },
    {
      "epoch": 0.3317333333333333,
      "grad_norm": 0.5038621425628662,
      "learning_rate": 4.1706666666666665e-05,
      "loss": 0.0039,
      "step": 6220
    },
    {
      "epoch": 0.33226666666666665,
      "grad_norm": 0.2948876917362213,
      "learning_rate": 4.1693333333333336e-05,
      "loss": 0.002,
      "step": 6230
    },
    {
      "epoch": 0.3328,
      "grad_norm": 0.4484107196331024,
      "learning_rate": 4.168e-05,
      "loss": 0.0023,
      "step": 6240
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.1828925609588623,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.0025,
      "step": 6250
    },
    {
      "epoch": 0.33386666666666664,
      "grad_norm": 0.2467062920331955,
      "learning_rate": 4.165333333333333e-05,
      "loss": 0.003,
      "step": 6260
    },
    {
      "epoch": 0.3344,
      "grad_norm": 0.244415283203125,
      "learning_rate": 4.164e-05,
      "loss": 0.0037,
      "step": 6270
    },
    {
      "epoch": 0.33493333333333336,
      "grad_norm": 0.2300289124250412,
      "learning_rate": 4.162666666666667e-05,
      "loss": 0.0026,
      "step": 6280
    },
    {
      "epoch": 0.3354666666666667,
      "grad_norm": 0.4537268579006195,
      "learning_rate": 4.161333333333334e-05,
      "loss": 0.0033,
      "step": 6290
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.24879033863544464,
      "learning_rate": 4.16e-05,
      "loss": 0.002,
      "step": 6300
    },
    {
      "epoch": 0.33653333333333335,
      "grad_norm": 0.36152976751327515,
      "learning_rate": 4.1586666666666666e-05,
      "loss": 0.0028,
      "step": 6310
    },
    {
      "epoch": 0.3370666666666667,
      "grad_norm": 0.15038645267486572,
      "learning_rate": 4.157333333333334e-05,
      "loss": 0.0022,
      "step": 6320
    },
    {
      "epoch": 0.3376,
      "grad_norm": 0.23157146573066711,
      "learning_rate": 4.156e-05,
      "loss": 0.0043,
      "step": 6330
    },
    {
      "epoch": 0.33813333333333334,
      "grad_norm": 0.23921163380146027,
      "learning_rate": 4.1546666666666666e-05,
      "loss": 0.0032,
      "step": 6340
    },
    {
      "epoch": 0.33866666666666667,
      "grad_norm": 0.08601786196231842,
      "learning_rate": 4.153333333333334e-05,
      "loss": 0.0029,
      "step": 6350
    },
    {
      "epoch": 0.3392,
      "grad_norm": 0.5420705080032349,
      "learning_rate": 4.152e-05,
      "loss": 0.0023,
      "step": 6360
    },
    {
      "epoch": 0.33973333333333333,
      "grad_norm": 0.8260862827301025,
      "learning_rate": 4.150666666666667e-05,
      "loss": 0.0022,
      "step": 6370
    },
    {
      "epoch": 0.34026666666666666,
      "grad_norm": 0.6152356266975403,
      "learning_rate": 4.149333333333333e-05,
      "loss": 0.0028,
      "step": 6380
    },
    {
      "epoch": 0.3408,
      "grad_norm": 0.21652653813362122,
      "learning_rate": 4.148e-05,
      "loss": 0.0031,
      "step": 6390
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 0.20648279786109924,
      "learning_rate": 4.146666666666667e-05,
      "loss": 0.0035,
      "step": 6400
    },
    {
      "epoch": 0.34186666666666665,
      "grad_norm": 0.4517003297805786,
      "learning_rate": 4.145333333333333e-05,
      "loss": 0.0034,
      "step": 6410
    },
    {
      "epoch": 0.3424,
      "grad_norm": 0.06304483115673065,
      "learning_rate": 4.144e-05,
      "loss": 0.0028,
      "step": 6420
    },
    {
      "epoch": 0.3429333333333333,
      "grad_norm": 0.3230520188808441,
      "learning_rate": 4.142666666666667e-05,
      "loss": 0.0034,
      "step": 6430
    },
    {
      "epoch": 0.34346666666666664,
      "grad_norm": 0.36611226201057434,
      "learning_rate": 4.141333333333334e-05,
      "loss": 0.0028,
      "step": 6440
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.06312341243028641,
      "learning_rate": 4.14e-05,
      "loss": 0.0034,
      "step": 6450
    },
    {
      "epoch": 0.34453333333333336,
      "grad_norm": 0.10491131246089935,
      "learning_rate": 4.138666666666667e-05,
      "loss": 0.0025,
      "step": 6460
    },
    {
      "epoch": 0.3450666666666667,
      "grad_norm": 0.36285364627838135,
      "learning_rate": 4.137333333333334e-05,
      "loss": 0.0023,
      "step": 6470
    },
    {
      "epoch": 0.3456,
      "grad_norm": 0.7571901679039001,
      "learning_rate": 4.1360000000000004e-05,
      "loss": 0.0023,
      "step": 6480
    },
    {
      "epoch": 0.34613333333333335,
      "grad_norm": 0.2900954782962799,
      "learning_rate": 4.134666666666667e-05,
      "loss": 0.0022,
      "step": 6490
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.448967844247818,
      "learning_rate": 4.133333333333333e-05,
      "loss": 0.0024,
      "step": 6500
    },
    {
      "epoch": 0.3472,
      "grad_norm": 0.6260005831718445,
      "learning_rate": 4.1320000000000004e-05,
      "loss": 0.0032,
      "step": 6510
    },
    {
      "epoch": 0.34773333333333334,
      "grad_norm": 0.24366365373134613,
      "learning_rate": 4.130666666666667e-05,
      "loss": 0.0023,
      "step": 6520
    },
    {
      "epoch": 0.34826666666666667,
      "grad_norm": 0.32438910007476807,
      "learning_rate": 4.129333333333333e-05,
      "loss": 0.0032,
      "step": 6530
    },
    {
      "epoch": 0.3488,
      "grad_norm": 0.12329262495040894,
      "learning_rate": 4.1280000000000005e-05,
      "loss": 0.0034,
      "step": 6540
    },
    {
      "epoch": 0.34933333333333333,
      "grad_norm": 1.094565510749817,
      "learning_rate": 4.126666666666667e-05,
      "loss": 0.0031,
      "step": 6550
    },
    {
      "epoch": 0.34986666666666666,
      "grad_norm": 0.671664834022522,
      "learning_rate": 4.1253333333333334e-05,
      "loss": 0.0029,
      "step": 6560
    },
    {
      "epoch": 0.3504,
      "grad_norm": 0.32435619831085205,
      "learning_rate": 4.124e-05,
      "loss": 0.0025,
      "step": 6570
    },
    {
      "epoch": 0.3509333333333333,
      "grad_norm": 0.1866583228111267,
      "learning_rate": 4.122666666666667e-05,
      "loss": 0.0018,
      "step": 6580
    },
    {
      "epoch": 0.35146666666666665,
      "grad_norm": 0.3771894574165344,
      "learning_rate": 4.1213333333333334e-05,
      "loss": 0.0024,
      "step": 6590
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.19198603928089142,
      "learning_rate": 4.12e-05,
      "loss": 0.0025,
      "step": 6600
    },
    {
      "epoch": 0.3525333333333333,
      "grad_norm": 0.14597834646701813,
      "learning_rate": 4.118666666666667e-05,
      "loss": 0.002,
      "step": 6610
    },
    {
      "epoch": 0.35306666666666664,
      "grad_norm": 0.19841250777244568,
      "learning_rate": 4.1173333333333334e-05,
      "loss": 0.002,
      "step": 6620
    },
    {
      "epoch": 0.3536,
      "grad_norm": 0.15616373717784882,
      "learning_rate": 4.1160000000000006e-05,
      "loss": 0.0045,
      "step": 6630
    },
    {
      "epoch": 0.35413333333333336,
      "grad_norm": 0.2618345022201538,
      "learning_rate": 4.1146666666666663e-05,
      "loss": 0.0026,
      "step": 6640
    },
    {
      "epoch": 0.3546666666666667,
      "grad_norm": 0.6475197672843933,
      "learning_rate": 4.1133333333333335e-05,
      "loss": 0.0043,
      "step": 6650
    },
    {
      "epoch": 0.3552,
      "grad_norm": 0.3613017201423645,
      "learning_rate": 4.1120000000000006e-05,
      "loss": 0.0031,
      "step": 6660
    },
    {
      "epoch": 0.35573333333333335,
      "grad_norm": 0.38947156071662903,
      "learning_rate": 4.110666666666667e-05,
      "loss": 0.0022,
      "step": 6670
    },
    {
      "epoch": 0.3562666666666667,
      "grad_norm": 0.2961820363998413,
      "learning_rate": 4.1093333333333335e-05,
      "loss": 0.0032,
      "step": 6680
    },
    {
      "epoch": 0.3568,
      "grad_norm": 0.24743422865867615,
      "learning_rate": 4.108e-05,
      "loss": 0.0033,
      "step": 6690
    },
    {
      "epoch": 0.35733333333333334,
      "grad_norm": 0.3214722275733948,
      "learning_rate": 4.106666666666667e-05,
      "loss": 0.0027,
      "step": 6700
    },
    {
      "epoch": 0.35786666666666667,
      "grad_norm": 0.15009424090385437,
      "learning_rate": 4.1053333333333336e-05,
      "loss": 0.0045,
      "step": 6710
    },
    {
      "epoch": 0.3584,
      "grad_norm": 0.11710787564516068,
      "learning_rate": 4.104e-05,
      "loss": 0.0026,
      "step": 6720
    },
    {
      "epoch": 0.3589333333333333,
      "grad_norm": 0.7677014470100403,
      "learning_rate": 4.102666666666667e-05,
      "loss": 0.003,
      "step": 6730
    },
    {
      "epoch": 0.35946666666666666,
      "grad_norm": 0.41639482975006104,
      "learning_rate": 4.1013333333333336e-05,
      "loss": 0.0029,
      "step": 6740
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.18431426584720612,
      "learning_rate": 4.1e-05,
      "loss": 0.0027,
      "step": 6750
    },
    {
      "epoch": 0.3605333333333333,
      "grad_norm": 0.15672405064105988,
      "learning_rate": 4.0986666666666665e-05,
      "loss": 0.0045,
      "step": 6760
    },
    {
      "epoch": 0.36106666666666665,
      "grad_norm": 0.22006769478321075,
      "learning_rate": 4.0973333333333336e-05,
      "loss": 0.003,
      "step": 6770
    },
    {
      "epoch": 0.3616,
      "grad_norm": 0.2828127145767212,
      "learning_rate": 4.096e-05,
      "loss": 0.0022,
      "step": 6780
    },
    {
      "epoch": 0.3621333333333333,
      "grad_norm": 0.07767147570848465,
      "learning_rate": 4.0946666666666665e-05,
      "loss": 0.0027,
      "step": 6790
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 0.19301177561283112,
      "learning_rate": 4.093333333333334e-05,
      "loss": 0.0029,
      "step": 6800
    },
    {
      "epoch": 0.3632,
      "grad_norm": 0.7115186452865601,
      "learning_rate": 4.092e-05,
      "loss": 0.0027,
      "step": 6810
    },
    {
      "epoch": 0.36373333333333335,
      "grad_norm": 0.1925523579120636,
      "learning_rate": 4.090666666666667e-05,
      "loss": 0.0029,
      "step": 6820
    },
    {
      "epoch": 0.3642666666666667,
      "grad_norm": 0.44600406289100647,
      "learning_rate": 4.089333333333333e-05,
      "loss": 0.0022,
      "step": 6830
    },
    {
      "epoch": 0.3648,
      "grad_norm": 0.21052439510822296,
      "learning_rate": 4.088e-05,
      "loss": 0.0022,
      "step": 6840
    },
    {
      "epoch": 0.36533333333333334,
      "grad_norm": 0.9115062355995178,
      "learning_rate": 4.086666666666667e-05,
      "loss": 0.0031,
      "step": 6850
    },
    {
      "epoch": 0.3658666666666667,
      "grad_norm": 0.08287503570318222,
      "learning_rate": 4.085333333333334e-05,
      "loss": 0.0024,
      "step": 6860
    },
    {
      "epoch": 0.3664,
      "grad_norm": 0.2613498270511627,
      "learning_rate": 4.084e-05,
      "loss": 0.0026,
      "step": 6870
    },
    {
      "epoch": 0.36693333333333333,
      "grad_norm": 0.5026366710662842,
      "learning_rate": 4.0826666666666667e-05,
      "loss": 0.0025,
      "step": 6880
    },
    {
      "epoch": 0.36746666666666666,
      "grad_norm": 0.43282419443130493,
      "learning_rate": 4.081333333333334e-05,
      "loss": 0.0025,
      "step": 6890
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.26488715410232544,
      "learning_rate": 4.08e-05,
      "loss": 0.0027,
      "step": 6900
    },
    {
      "epoch": 0.3685333333333333,
      "grad_norm": 0.1661859005689621,
      "learning_rate": 4.078666666666667e-05,
      "loss": 0.0021,
      "step": 6910
    },
    {
      "epoch": 0.36906666666666665,
      "grad_norm": 0.271862268447876,
      "learning_rate": 4.077333333333334e-05,
      "loss": 0.0023,
      "step": 6920
    },
    {
      "epoch": 0.3696,
      "grad_norm": 0.07963758707046509,
      "learning_rate": 4.076e-05,
      "loss": 0.0023,
      "step": 6930
    },
    {
      "epoch": 0.3701333333333333,
      "grad_norm": 0.23171140253543854,
      "learning_rate": 4.074666666666667e-05,
      "loss": 0.003,
      "step": 6940
    },
    {
      "epoch": 0.37066666666666664,
      "grad_norm": 0.45267564058303833,
      "learning_rate": 4.073333333333333e-05,
      "loss": 0.0024,
      "step": 6950
    },
    {
      "epoch": 0.3712,
      "grad_norm": 0.2473250925540924,
      "learning_rate": 4.072e-05,
      "loss": 0.0023,
      "step": 6960
    },
    {
      "epoch": 0.37173333333333336,
      "grad_norm": 0.27108079195022583,
      "learning_rate": 4.070666666666667e-05,
      "loss": 0.0027,
      "step": 6970
    },
    {
      "epoch": 0.3722666666666667,
      "grad_norm": 0.5549588799476624,
      "learning_rate": 4.069333333333333e-05,
      "loss": 0.003,
      "step": 6980
    },
    {
      "epoch": 0.3728,
      "grad_norm": 0.5419651865959167,
      "learning_rate": 4.0680000000000004e-05,
      "loss": 0.004,
      "step": 6990
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.11785665154457092,
      "learning_rate": 4.066666666666667e-05,
      "loss": 0.0032,
      "step": 7000
    },
    {
      "epoch": 0.3738666666666667,
      "grad_norm": 0.48482006788253784,
      "learning_rate": 4.065333333333334e-05,
      "loss": 0.0035,
      "step": 7010
    },
    {
      "epoch": 0.3744,
      "grad_norm": 0.5471944808959961,
      "learning_rate": 4.064e-05,
      "loss": 0.0023,
      "step": 7020
    },
    {
      "epoch": 0.37493333333333334,
      "grad_norm": 0.32042741775512695,
      "learning_rate": 4.062666666666667e-05,
      "loss": 0.0023,
      "step": 7030
    },
    {
      "epoch": 0.37546666666666667,
      "grad_norm": 0.18832847476005554,
      "learning_rate": 4.061333333333334e-05,
      "loss": 0.0027,
      "step": 7040
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.3530927002429962,
      "learning_rate": 4.0600000000000004e-05,
      "loss": 0.0028,
      "step": 7050
    },
    {
      "epoch": 0.37653333333333333,
      "grad_norm": 0.5546818375587463,
      "learning_rate": 4.058666666666667e-05,
      "loss": 0.0024,
      "step": 7060
    },
    {
      "epoch": 0.37706666666666666,
      "grad_norm": 0.6515260934829712,
      "learning_rate": 4.057333333333333e-05,
      "loss": 0.0024,
      "step": 7070
    },
    {
      "epoch": 0.3776,
      "grad_norm": 0.1658201664686203,
      "learning_rate": 4.0560000000000005e-05,
      "loss": 0.0018,
      "step": 7080
    },
    {
      "epoch": 0.3781333333333333,
      "grad_norm": 0.3148490786552429,
      "learning_rate": 4.054666666666667e-05,
      "loss": 0.0035,
      "step": 7090
    },
    {
      "epoch": 0.37866666666666665,
      "grad_norm": 0.8502660989761353,
      "learning_rate": 4.0533333333333334e-05,
      "loss": 0.0039,
      "step": 7100
    },
    {
      "epoch": 0.3792,
      "grad_norm": 0.07144840806722641,
      "learning_rate": 4.0520000000000005e-05,
      "loss": 0.0025,
      "step": 7110
    },
    {
      "epoch": 0.3797333333333333,
      "grad_norm": 0.24243396520614624,
      "learning_rate": 4.050666666666667e-05,
      "loss": 0.0031,
      "step": 7120
    },
    {
      "epoch": 0.38026666666666664,
      "grad_norm": 0.2208961695432663,
      "learning_rate": 4.0493333333333334e-05,
      "loss": 0.0034,
      "step": 7130
    },
    {
      "epoch": 0.3808,
      "grad_norm": 0.4575865566730499,
      "learning_rate": 4.048e-05,
      "loss": 0.0028,
      "step": 7140
    },
    {
      "epoch": 0.38133333333333336,
      "grad_norm": 0.3030228018760681,
      "learning_rate": 4.046666666666667e-05,
      "loss": 0.0033,
      "step": 7150
    },
    {
      "epoch": 0.3818666666666667,
      "grad_norm": 0.5498692393302917,
      "learning_rate": 4.0453333333333335e-05,
      "loss": 0.0024,
      "step": 7160
    },
    {
      "epoch": 0.3824,
      "grad_norm": 0.0760292261838913,
      "learning_rate": 4.044e-05,
      "loss": 0.0035,
      "step": 7170
    },
    {
      "epoch": 0.38293333333333335,
      "grad_norm": 0.77093905210495,
      "learning_rate": 4.042666666666667e-05,
      "loss": 0.003,
      "step": 7180
    },
    {
      "epoch": 0.3834666666666667,
      "grad_norm": 0.3595946133136749,
      "learning_rate": 4.0413333333333335e-05,
      "loss": 0.0024,
      "step": 7190
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.20424529910087585,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 0.0026,
      "step": 7200
    },
    {
      "epoch": 0.38453333333333334,
      "grad_norm": 0.12375003099441528,
      "learning_rate": 4.0386666666666664e-05,
      "loss": 0.0022,
      "step": 7210
    },
    {
      "epoch": 0.38506666666666667,
      "grad_norm": 0.4996176064014435,
      "learning_rate": 4.0373333333333335e-05,
      "loss": 0.0032,
      "step": 7220
    },
    {
      "epoch": 0.3856,
      "grad_norm": 0.6873061656951904,
      "learning_rate": 4.0360000000000007e-05,
      "loss": 0.0023,
      "step": 7230
    },
    {
      "epoch": 0.38613333333333333,
      "grad_norm": 0.17995917797088623,
      "learning_rate": 4.0346666666666664e-05,
      "loss": 0.0032,
      "step": 7240
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.131523996591568,
      "learning_rate": 4.0333333333333336e-05,
      "loss": 0.0035,
      "step": 7250
    },
    {
      "epoch": 0.3872,
      "grad_norm": 0.48079270124435425,
      "learning_rate": 4.032e-05,
      "loss": 0.002,
      "step": 7260
    },
    {
      "epoch": 0.3877333333333333,
      "grad_norm": 0.16911867260932922,
      "learning_rate": 4.030666666666667e-05,
      "loss": 0.0023,
      "step": 7270
    },
    {
      "epoch": 0.38826666666666665,
      "grad_norm": 0.4538200795650482,
      "learning_rate": 4.0293333333333336e-05,
      "loss": 0.0025,
      "step": 7280
    },
    {
      "epoch": 0.3888,
      "grad_norm": 0.21856912970542908,
      "learning_rate": 4.028e-05,
      "loss": 0.0023,
      "step": 7290
    },
    {
      "epoch": 0.3893333333333333,
      "grad_norm": 0.26420971751213074,
      "learning_rate": 4.026666666666667e-05,
      "loss": 0.0024,
      "step": 7300
    },
    {
      "epoch": 0.38986666666666664,
      "grad_norm": 0.5012447834014893,
      "learning_rate": 4.0253333333333336e-05,
      "loss": 0.0022,
      "step": 7310
    },
    {
      "epoch": 0.3904,
      "grad_norm": 0.5092859268188477,
      "learning_rate": 4.024e-05,
      "loss": 0.002,
      "step": 7320
    },
    {
      "epoch": 0.39093333333333335,
      "grad_norm": 0.6959651708602905,
      "learning_rate": 4.0226666666666666e-05,
      "loss": 0.0021,
      "step": 7330
    },
    {
      "epoch": 0.3914666666666667,
      "grad_norm": 0.7505474090576172,
      "learning_rate": 4.021333333333334e-05,
      "loss": 0.0024,
      "step": 7340
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.35645627975463867,
      "learning_rate": 4.02e-05,
      "loss": 0.0024,
      "step": 7350
    },
    {
      "epoch": 0.39253333333333335,
      "grad_norm": 0.3036152124404907,
      "learning_rate": 4.0186666666666666e-05,
      "loss": 0.0037,
      "step": 7360
    },
    {
      "epoch": 0.3930666666666667,
      "grad_norm": 0.3004351258277893,
      "learning_rate": 4.017333333333334e-05,
      "loss": 0.0033,
      "step": 7370
    },
    {
      "epoch": 0.3936,
      "grad_norm": 0.4924675524234772,
      "learning_rate": 4.016e-05,
      "loss": 0.0037,
      "step": 7380
    },
    {
      "epoch": 0.39413333333333334,
      "grad_norm": 0.30354759097099304,
      "learning_rate": 4.014666666666667e-05,
      "loss": 0.0029,
      "step": 7390
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 0.2381724864244461,
      "learning_rate": 4.013333333333333e-05,
      "loss": 0.0019,
      "step": 7400
    },
    {
      "epoch": 0.3952,
      "grad_norm": 0.4645378291606903,
      "learning_rate": 4.012e-05,
      "loss": 0.0044,
      "step": 7410
    },
    {
      "epoch": 0.3957333333333333,
      "grad_norm": 0.10057147592306137,
      "learning_rate": 4.0106666666666673e-05,
      "loss": 0.0019,
      "step": 7420
    },
    {
      "epoch": 0.39626666666666666,
      "grad_norm": 0.1718864142894745,
      "learning_rate": 4.009333333333333e-05,
      "loss": 0.0029,
      "step": 7430
    },
    {
      "epoch": 0.3968,
      "grad_norm": 0.5129747986793518,
      "learning_rate": 4.008e-05,
      "loss": 0.003,
      "step": 7440
    },
    {
      "epoch": 0.3973333333333333,
      "grad_norm": 0.2247687429189682,
      "learning_rate": 4.006666666666667e-05,
      "loss": 0.0031,
      "step": 7450
    },
    {
      "epoch": 0.39786666666666665,
      "grad_norm": 0.517532229423523,
      "learning_rate": 4.005333333333334e-05,
      "loss": 0.0026,
      "step": 7460
    },
    {
      "epoch": 0.3984,
      "grad_norm": 0.16336768865585327,
      "learning_rate": 4.004e-05,
      "loss": 0.0021,
      "step": 7470
    },
    {
      "epoch": 0.3989333333333333,
      "grad_norm": 0.39775970578193665,
      "learning_rate": 4.002666666666667e-05,
      "loss": 0.0036,
      "step": 7480
    },
    {
      "epoch": 0.3994666666666667,
      "grad_norm": 0.5494894981384277,
      "learning_rate": 4.001333333333334e-05,
      "loss": 0.0023,
      "step": 7490
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.507032036781311,
      "learning_rate": 4e-05,
      "loss": 0.0025,
      "step": 7500
    },
    {
      "epoch": 0.40053333333333335,
      "grad_norm": 0.3347609341144562,
      "learning_rate": 3.998666666666667e-05,
      "loss": 0.0028,
      "step": 7510
    },
    {
      "epoch": 0.4010666666666667,
      "grad_norm": 0.45852130651474,
      "learning_rate": 3.997333333333333e-05,
      "loss": 0.0034,
      "step": 7520
    },
    {
      "epoch": 0.4016,
      "grad_norm": 0.6502587795257568,
      "learning_rate": 3.9960000000000004e-05,
      "loss": 0.004,
      "step": 7530
    },
    {
      "epoch": 0.40213333333333334,
      "grad_norm": 0.442920446395874,
      "learning_rate": 3.994666666666667e-05,
      "loss": 0.0029,
      "step": 7540
    },
    {
      "epoch": 0.4026666666666667,
      "grad_norm": 0.7374988198280334,
      "learning_rate": 3.993333333333333e-05,
      "loss": 0.0027,
      "step": 7550
    },
    {
      "epoch": 0.4032,
      "grad_norm": 0.1940835565328598,
      "learning_rate": 3.9920000000000004e-05,
      "loss": 0.0029,
      "step": 7560
    },
    {
      "epoch": 0.40373333333333333,
      "grad_norm": 0.2916249930858612,
      "learning_rate": 3.990666666666667e-05,
      "loss": 0.002,
      "step": 7570
    },
    {
      "epoch": 0.40426666666666666,
      "grad_norm": 0.5775622725486755,
      "learning_rate": 3.989333333333333e-05,
      "loss": 0.0025,
      "step": 7580
    },
    {
      "epoch": 0.4048,
      "grad_norm": 0.29194363951683044,
      "learning_rate": 3.988e-05,
      "loss": 0.0026,
      "step": 7590
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 0.328646183013916,
      "learning_rate": 3.986666666666667e-05,
      "loss": 0.0023,
      "step": 7600
    },
    {
      "epoch": 0.40586666666666665,
      "grad_norm": 0.2577880024909973,
      "learning_rate": 3.985333333333334e-05,
      "loss": 0.0024,
      "step": 7610
    },
    {
      "epoch": 0.4064,
      "grad_norm": 0.43566712737083435,
      "learning_rate": 3.984e-05,
      "loss": 0.0027,
      "step": 7620
    },
    {
      "epoch": 0.4069333333333333,
      "grad_norm": 0.16965025663375854,
      "learning_rate": 3.982666666666667e-05,
      "loss": 0.0028,
      "step": 7630
    },
    {
      "epoch": 0.40746666666666664,
      "grad_norm": 0.15309607982635498,
      "learning_rate": 3.9813333333333334e-05,
      "loss": 0.0029,
      "step": 7640
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.536308765411377,
      "learning_rate": 3.9800000000000005e-05,
      "loss": 0.0024,
      "step": 7650
    },
    {
      "epoch": 0.40853333333333336,
      "grad_norm": 0.8998664021492004,
      "learning_rate": 3.978666666666667e-05,
      "loss": 0.0028,
      "step": 7660
    },
    {
      "epoch": 0.4090666666666667,
      "grad_norm": 0.48426273465156555,
      "learning_rate": 3.9773333333333334e-05,
      "loss": 0.0032,
      "step": 7670
    },
    {
      "epoch": 0.4096,
      "grad_norm": 0.15224610269069672,
      "learning_rate": 3.9760000000000006e-05,
      "loss": 0.0027,
      "step": 7680
    },
    {
      "epoch": 0.41013333333333335,
      "grad_norm": 0.478201687335968,
      "learning_rate": 3.974666666666667e-05,
      "loss": 0.0033,
      "step": 7690
    },
    {
      "epoch": 0.4106666666666667,
      "grad_norm": 0.10892093181610107,
      "learning_rate": 3.9733333333333335e-05,
      "loss": 0.0022,
      "step": 7700
    },
    {
      "epoch": 0.4112,
      "grad_norm": 0.23736371099948883,
      "learning_rate": 3.972e-05,
      "loss": 0.0029,
      "step": 7710
    },
    {
      "epoch": 0.41173333333333334,
      "grad_norm": 0.48638829588890076,
      "learning_rate": 3.970666666666667e-05,
      "loss": 0.0026,
      "step": 7720
    },
    {
      "epoch": 0.41226666666666667,
      "grad_norm": 0.16528920829296112,
      "learning_rate": 3.9693333333333335e-05,
      "loss": 0.0029,
      "step": 7730
    },
    {
      "epoch": 0.4128,
      "grad_norm": 0.4330379068851471,
      "learning_rate": 3.968e-05,
      "loss": 0.0031,
      "step": 7740
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.9743055701255798,
      "learning_rate": 3.966666666666667e-05,
      "loss": 0.0031,
      "step": 7750
    },
    {
      "epoch": 0.41386666666666666,
      "grad_norm": 0.17222751677036285,
      "learning_rate": 3.9653333333333335e-05,
      "loss": 0.0031,
      "step": 7760
    },
    {
      "epoch": 0.4144,
      "grad_norm": 0.10229166597127914,
      "learning_rate": 3.964e-05,
      "loss": 0.0029,
      "step": 7770
    },
    {
      "epoch": 0.4149333333333333,
      "grad_norm": 0.0918712392449379,
      "learning_rate": 3.9626666666666664e-05,
      "loss": 0.004,
      "step": 7780
    },
    {
      "epoch": 0.41546666666666665,
      "grad_norm": 0.27295294404029846,
      "learning_rate": 3.9613333333333336e-05,
      "loss": 0.0032,
      "step": 7790
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.20399005711078644,
      "learning_rate": 3.960000000000001e-05,
      "loss": 0.0029,
      "step": 7800
    },
    {
      "epoch": 0.4165333333333333,
      "grad_norm": 0.46889376640319824,
      "learning_rate": 3.9586666666666665e-05,
      "loss": 0.0034,
      "step": 7810
    },
    {
      "epoch": 0.41706666666666664,
      "grad_norm": 0.43031829595565796,
      "learning_rate": 3.9573333333333336e-05,
      "loss": 0.0039,
      "step": 7820
    },
    {
      "epoch": 0.4176,
      "grad_norm": 0.23462320864200592,
      "learning_rate": 3.956e-05,
      "loss": 0.0024,
      "step": 7830
    },
    {
      "epoch": 0.41813333333333336,
      "grad_norm": 0.17261426150798798,
      "learning_rate": 3.954666666666667e-05,
      "loss": 0.0023,
      "step": 7840
    },
    {
      "epoch": 0.4186666666666667,
      "grad_norm": 0.2759440243244171,
      "learning_rate": 3.9533333333333337e-05,
      "loss": 0.0028,
      "step": 7850
    },
    {
      "epoch": 0.4192,
      "grad_norm": 0.242459237575531,
      "learning_rate": 3.952e-05,
      "loss": 0.0025,
      "step": 7860
    },
    {
      "epoch": 0.41973333333333335,
      "grad_norm": 0.14117132127285004,
      "learning_rate": 3.950666666666667e-05,
      "loss": 0.0031,
      "step": 7870
    },
    {
      "epoch": 0.4202666666666667,
      "grad_norm": 0.10015767812728882,
      "learning_rate": 3.949333333333334e-05,
      "loss": 0.0025,
      "step": 7880
    },
    {
      "epoch": 0.4208,
      "grad_norm": 0.2619751989841461,
      "learning_rate": 3.948e-05,
      "loss": 0.0026,
      "step": 7890
    },
    {
      "epoch": 0.42133333333333334,
      "grad_norm": 0.10286182165145874,
      "learning_rate": 3.9466666666666666e-05,
      "loss": 0.0027,
      "step": 7900
    },
    {
      "epoch": 0.42186666666666667,
      "grad_norm": 0.4859950840473175,
      "learning_rate": 3.945333333333334e-05,
      "loss": 0.0034,
      "step": 7910
    },
    {
      "epoch": 0.4224,
      "grad_norm": 0.4545287489891052,
      "learning_rate": 3.944e-05,
      "loss": 0.0033,
      "step": 7920
    },
    {
      "epoch": 0.42293333333333333,
      "grad_norm": 0.40913861989974976,
      "learning_rate": 3.9426666666666666e-05,
      "loss": 0.0016,
      "step": 7930
    },
    {
      "epoch": 0.42346666666666666,
      "grad_norm": 0.15464892983436584,
      "learning_rate": 3.941333333333334e-05,
      "loss": 0.0023,
      "step": 7940
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.15625283122062683,
      "learning_rate": 3.94e-05,
      "loss": 0.0025,
      "step": 7950
    },
    {
      "epoch": 0.4245333333333333,
      "grad_norm": 0.11823299527168274,
      "learning_rate": 3.938666666666667e-05,
      "loss": 0.0023,
      "step": 7960
    },
    {
      "epoch": 0.42506666666666665,
      "grad_norm": 0.43405452370643616,
      "learning_rate": 3.937333333333333e-05,
      "loss": 0.0021,
      "step": 7970
    },
    {
      "epoch": 0.4256,
      "grad_norm": 0.6016661524772644,
      "learning_rate": 3.936e-05,
      "loss": 0.0031,
      "step": 7980
    },
    {
      "epoch": 0.4261333333333333,
      "grad_norm": 0.7526372075080872,
      "learning_rate": 3.9346666666666674e-05,
      "loss": 0.0029,
      "step": 7990
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.08830692619085312,
      "learning_rate": 3.933333333333333e-05,
      "loss": 0.0027,
      "step": 8000
    },
    {
      "epoch": 0.4272,
      "grad_norm": 0.16989992558956146,
      "learning_rate": 3.932e-05,
      "loss": 0.0019,
      "step": 8010
    },
    {
      "epoch": 0.42773333333333335,
      "grad_norm": 0.676036536693573,
      "learning_rate": 3.930666666666667e-05,
      "loss": 0.0029,
      "step": 8020
    },
    {
      "epoch": 0.4282666666666667,
      "grad_norm": 0.22910696268081665,
      "learning_rate": 3.929333333333334e-05,
      "loss": 0.0036,
      "step": 8030
    },
    {
      "epoch": 0.4288,
      "grad_norm": 0.1849464476108551,
      "learning_rate": 3.9280000000000003e-05,
      "loss": 0.0022,
      "step": 8040
    },
    {
      "epoch": 0.42933333333333334,
      "grad_norm": 0.5437206029891968,
      "learning_rate": 3.926666666666667e-05,
      "loss": 0.0023,
      "step": 8050
    },
    {
      "epoch": 0.4298666666666667,
      "grad_norm": 0.2968844771385193,
      "learning_rate": 3.925333333333334e-05,
      "loss": 0.0026,
      "step": 8060
    },
    {
      "epoch": 0.4304,
      "grad_norm": 0.08034578710794449,
      "learning_rate": 3.9240000000000004e-05,
      "loss": 0.0031,
      "step": 8070
    },
    {
      "epoch": 0.43093333333333333,
      "grad_norm": 0.7786311507225037,
      "learning_rate": 3.922666666666667e-05,
      "loss": 0.0021,
      "step": 8080
    },
    {
      "epoch": 0.43146666666666667,
      "grad_norm": 0.5966071486473083,
      "learning_rate": 3.921333333333333e-05,
      "loss": 0.002,
      "step": 8090
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.14467300474643707,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 0.0027,
      "step": 8100
    },
    {
      "epoch": 0.4325333333333333,
      "grad_norm": 0.2538904547691345,
      "learning_rate": 3.918666666666667e-05,
      "loss": 0.0042,
      "step": 8110
    },
    {
      "epoch": 0.43306666666666666,
      "grad_norm": 0.27854523062705994,
      "learning_rate": 3.917333333333333e-05,
      "loss": 0.0036,
      "step": 8120
    },
    {
      "epoch": 0.4336,
      "grad_norm": 0.08237604796886444,
      "learning_rate": 3.9160000000000005e-05,
      "loss": 0.0037,
      "step": 8130
    },
    {
      "epoch": 0.4341333333333333,
      "grad_norm": 0.21478918194770813,
      "learning_rate": 3.914666666666667e-05,
      "loss": 0.0024,
      "step": 8140
    },
    {
      "epoch": 0.43466666666666665,
      "grad_norm": 0.12208199501037598,
      "learning_rate": 3.9133333333333334e-05,
      "loss": 0.0028,
      "step": 8150
    },
    {
      "epoch": 0.4352,
      "grad_norm": 0.15120559930801392,
      "learning_rate": 3.912e-05,
      "loss": 0.0022,
      "step": 8160
    },
    {
      "epoch": 0.4357333333333333,
      "grad_norm": 0.2777846157550812,
      "learning_rate": 3.910666666666667e-05,
      "loss": 0.0034,
      "step": 8170
    },
    {
      "epoch": 0.4362666666666667,
      "grad_norm": 0.17112620174884796,
      "learning_rate": 3.9093333333333334e-05,
      "loss": 0.003,
      "step": 8180
    },
    {
      "epoch": 0.4368,
      "grad_norm": 0.801760733127594,
      "learning_rate": 3.908e-05,
      "loss": 0.0029,
      "step": 8190
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 0.19267718493938446,
      "learning_rate": 3.906666666666667e-05,
      "loss": 0.0031,
      "step": 8200
    },
    {
      "epoch": 0.4378666666666667,
      "grad_norm": 0.4348323345184326,
      "learning_rate": 3.9053333333333334e-05,
      "loss": 0.0026,
      "step": 8210
    },
    {
      "epoch": 0.4384,
      "grad_norm": 0.294120192527771,
      "learning_rate": 3.9040000000000006e-05,
      "loss": 0.0031,
      "step": 8220
    },
    {
      "epoch": 0.43893333333333334,
      "grad_norm": 0.14176005125045776,
      "learning_rate": 3.902666666666667e-05,
      "loss": 0.003,
      "step": 8230
    },
    {
      "epoch": 0.43946666666666667,
      "grad_norm": 0.5336205959320068,
      "learning_rate": 3.9013333333333335e-05,
      "loss": 0.0029,
      "step": 8240
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.5351036787033081,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.0025,
      "step": 8250
    },
    {
      "epoch": 0.44053333333333333,
      "grad_norm": 0.7606545686721802,
      "learning_rate": 3.8986666666666664e-05,
      "loss": 0.002,
      "step": 8260
    },
    {
      "epoch": 0.44106666666666666,
      "grad_norm": 0.5272565484046936,
      "learning_rate": 3.8973333333333335e-05,
      "loss": 0.0026,
      "step": 8270
    },
    {
      "epoch": 0.4416,
      "grad_norm": 0.2036670297384262,
      "learning_rate": 3.896e-05,
      "loss": 0.0036,
      "step": 8280
    },
    {
      "epoch": 0.4421333333333333,
      "grad_norm": 0.19004346430301666,
      "learning_rate": 3.894666666666667e-05,
      "loss": 0.0032,
      "step": 8290
    },
    {
      "epoch": 0.44266666666666665,
      "grad_norm": 0.46530860662460327,
      "learning_rate": 3.8933333333333336e-05,
      "loss": 0.0029,
      "step": 8300
    },
    {
      "epoch": 0.4432,
      "grad_norm": 0.2822752594947815,
      "learning_rate": 3.892e-05,
      "loss": 0.0021,
      "step": 8310
    },
    {
      "epoch": 0.4437333333333333,
      "grad_norm": 0.10400716215372086,
      "learning_rate": 3.890666666666667e-05,
      "loss": 0.0023,
      "step": 8320
    },
    {
      "epoch": 0.44426666666666664,
      "grad_norm": 0.7168519496917725,
      "learning_rate": 3.8893333333333336e-05,
      "loss": 0.0032,
      "step": 8330
    },
    {
      "epoch": 0.4448,
      "grad_norm": 0.30133917927742004,
      "learning_rate": 3.888e-05,
      "loss": 0.002,
      "step": 8340
    },
    {
      "epoch": 0.44533333333333336,
      "grad_norm": 0.509201169013977,
      "learning_rate": 3.8866666666666665e-05,
      "loss": 0.0024,
      "step": 8350
    },
    {
      "epoch": 0.4458666666666667,
      "grad_norm": 0.4532574713230133,
      "learning_rate": 3.8853333333333336e-05,
      "loss": 0.0029,
      "step": 8360
    },
    {
      "epoch": 0.4464,
      "grad_norm": 0.08359424024820328,
      "learning_rate": 3.884e-05,
      "loss": 0.0035,
      "step": 8370
    },
    {
      "epoch": 0.44693333333333335,
      "grad_norm": 0.27353155612945557,
      "learning_rate": 3.8826666666666665e-05,
      "loss": 0.0023,
      "step": 8380
    },
    {
      "epoch": 0.4474666666666667,
      "grad_norm": 0.3626326322555542,
      "learning_rate": 3.881333333333334e-05,
      "loss": 0.0021,
      "step": 8390
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.14612115919589996,
      "learning_rate": 3.88e-05,
      "loss": 0.0028,
      "step": 8400
    },
    {
      "epoch": 0.44853333333333334,
      "grad_norm": 0.3180653154850006,
      "learning_rate": 3.878666666666667e-05,
      "loss": 0.0024,
      "step": 8410
    },
    {
      "epoch": 0.44906666666666667,
      "grad_norm": 0.2681024670600891,
      "learning_rate": 3.877333333333334e-05,
      "loss": 0.0022,
      "step": 8420
    },
    {
      "epoch": 0.4496,
      "grad_norm": 0.10464463382959366,
      "learning_rate": 3.876e-05,
      "loss": 0.003,
      "step": 8430
    },
    {
      "epoch": 0.45013333333333333,
      "grad_norm": 0.1764148473739624,
      "learning_rate": 3.874666666666667e-05,
      "loss": 0.0024,
      "step": 8440
    },
    {
      "epoch": 0.45066666666666666,
      "grad_norm": 0.21827585995197296,
      "learning_rate": 3.873333333333333e-05,
      "loss": 0.0022,
      "step": 8450
    },
    {
      "epoch": 0.4512,
      "grad_norm": 0.5119425654411316,
      "learning_rate": 3.872e-05,
      "loss": 0.0029,
      "step": 8460
    },
    {
      "epoch": 0.4517333333333333,
      "grad_norm": 0.3156832754611969,
      "learning_rate": 3.8706666666666667e-05,
      "loss": 0.0024,
      "step": 8470
    },
    {
      "epoch": 0.45226666666666665,
      "grad_norm": 0.13831374049186707,
      "learning_rate": 3.869333333333334e-05,
      "loss": 0.0031,
      "step": 8480
    },
    {
      "epoch": 0.4528,
      "grad_norm": 0.5713222622871399,
      "learning_rate": 3.868e-05,
      "loss": 0.0035,
      "step": 8490
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.1638394445180893,
      "learning_rate": 3.866666666666667e-05,
      "loss": 0.0022,
      "step": 8500
    },
    {
      "epoch": 0.45386666666666664,
      "grad_norm": 0.1902613788843155,
      "learning_rate": 3.865333333333334e-05,
      "loss": 0.0032,
      "step": 8510
    },
    {
      "epoch": 0.4544,
      "grad_norm": 0.3343486189842224,
      "learning_rate": 3.864e-05,
      "loss": 0.0022,
      "step": 8520
    },
    {
      "epoch": 0.45493333333333336,
      "grad_norm": 0.4550933241844177,
      "learning_rate": 3.862666666666667e-05,
      "loss": 0.0038,
      "step": 8530
    },
    {
      "epoch": 0.4554666666666667,
      "grad_norm": 0.13505645096302032,
      "learning_rate": 3.861333333333333e-05,
      "loss": 0.0021,
      "step": 8540
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.17312094569206238,
      "learning_rate": 3.86e-05,
      "loss": 0.0017,
      "step": 8550
    },
    {
      "epoch": 0.45653333333333335,
      "grad_norm": 0.3561553657054901,
      "learning_rate": 3.858666666666667e-05,
      "loss": 0.0032,
      "step": 8560
    },
    {
      "epoch": 0.4570666666666667,
      "grad_norm": 0.14043638110160828,
      "learning_rate": 3.857333333333333e-05,
      "loss": 0.0024,
      "step": 8570
    },
    {
      "epoch": 0.4576,
      "grad_norm": 0.37130245566368103,
      "learning_rate": 3.8560000000000004e-05,
      "loss": 0.0022,
      "step": 8580
    },
    {
      "epoch": 0.45813333333333334,
      "grad_norm": 0.36095502972602844,
      "learning_rate": 3.854666666666667e-05,
      "loss": 0.0025,
      "step": 8590
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 0.7017378211021423,
      "learning_rate": 3.853333333333334e-05,
      "loss": 0.0025,
      "step": 8600
    },
    {
      "epoch": 0.4592,
      "grad_norm": 0.6375042796134949,
      "learning_rate": 3.8520000000000004e-05,
      "loss": 0.0022,
      "step": 8610
    },
    {
      "epoch": 0.4597333333333333,
      "grad_norm": 0.2925524413585663,
      "learning_rate": 3.850666666666667e-05,
      "loss": 0.0038,
      "step": 8620
    },
    {
      "epoch": 0.46026666666666666,
      "grad_norm": 0.6420701146125793,
      "learning_rate": 3.849333333333334e-05,
      "loss": 0.0024,
      "step": 8630
    },
    {
      "epoch": 0.4608,
      "grad_norm": 0.14758186042308807,
      "learning_rate": 3.848e-05,
      "loss": 0.0028,
      "step": 8640
    },
    {
      "epoch": 0.4613333333333333,
      "grad_norm": 0.21793687343597412,
      "learning_rate": 3.846666666666667e-05,
      "loss": 0.0027,
      "step": 8650
    },
    {
      "epoch": 0.46186666666666665,
      "grad_norm": 0.17542093992233276,
      "learning_rate": 3.845333333333333e-05,
      "loss": 0.0029,
      "step": 8660
    },
    {
      "epoch": 0.4624,
      "grad_norm": 0.09798391163349152,
      "learning_rate": 3.8440000000000005e-05,
      "loss": 0.0022,
      "step": 8670
    },
    {
      "epoch": 0.4629333333333333,
      "grad_norm": 0.23583990335464478,
      "learning_rate": 3.842666666666667e-05,
      "loss": 0.0033,
      "step": 8680
    },
    {
      "epoch": 0.4634666666666667,
      "grad_norm": 0.476938933134079,
      "learning_rate": 3.8413333333333334e-05,
      "loss": 0.0022,
      "step": 8690
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.52516770362854,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 0.0041,
      "step": 8700
    },
    {
      "epoch": 0.46453333333333335,
      "grad_norm": 0.2963181436061859,
      "learning_rate": 3.838666666666667e-05,
      "loss": 0.0035,
      "step": 8710
    },
    {
      "epoch": 0.4650666666666667,
      "grad_norm": 0.34732961654663086,
      "learning_rate": 3.8373333333333334e-05,
      "loss": 0.0025,
      "step": 8720
    },
    {
      "epoch": 0.4656,
      "grad_norm": 0.7881629467010498,
      "learning_rate": 3.836e-05,
      "loss": 0.0025,
      "step": 8730
    },
    {
      "epoch": 0.46613333333333334,
      "grad_norm": 0.7644715309143066,
      "learning_rate": 3.834666666666667e-05,
      "loss": 0.0025,
      "step": 8740
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.27183812856674194,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 0.0028,
      "step": 8750
    },
    {
      "epoch": 0.4672,
      "grad_norm": 0.7740333080291748,
      "learning_rate": 3.832e-05,
      "loss": 0.0027,
      "step": 8760
    },
    {
      "epoch": 0.46773333333333333,
      "grad_norm": 0.23352861404418945,
      "learning_rate": 3.830666666666667e-05,
      "loss": 0.0021,
      "step": 8770
    },
    {
      "epoch": 0.46826666666666666,
      "grad_norm": 0.31585004925727844,
      "learning_rate": 3.8293333333333335e-05,
      "loss": 0.0035,
      "step": 8780
    },
    {
      "epoch": 0.4688,
      "grad_norm": 0.22436104714870453,
      "learning_rate": 3.828e-05,
      "loss": 0.0032,
      "step": 8790
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 0.4967515766620636,
      "learning_rate": 3.8266666666666664e-05,
      "loss": 0.0021,
      "step": 8800
    },
    {
      "epoch": 0.46986666666666665,
      "grad_norm": 0.19392618536949158,
      "learning_rate": 3.8253333333333335e-05,
      "loss": 0.0031,
      "step": 8810
    },
    {
      "epoch": 0.4704,
      "grad_norm": 0.41185304522514343,
      "learning_rate": 3.8240000000000007e-05,
      "loss": 0.0023,
      "step": 8820
    },
    {
      "epoch": 0.4709333333333333,
      "grad_norm": 0.7099930644035339,
      "learning_rate": 3.8226666666666664e-05,
      "loss": 0.0029,
      "step": 8830
    },
    {
      "epoch": 0.47146666666666665,
      "grad_norm": 0.42883050441741943,
      "learning_rate": 3.8213333333333336e-05,
      "loss": 0.0027,
      "step": 8840
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.3053798973560333,
      "learning_rate": 3.82e-05,
      "loss": 0.0021,
      "step": 8850
    },
    {
      "epoch": 0.47253333333333336,
      "grad_norm": 0.3395828902721405,
      "learning_rate": 3.818666666666667e-05,
      "loss": 0.0031,
      "step": 8860
    },
    {
      "epoch": 0.4730666666666667,
      "grad_norm": 0.18671540915966034,
      "learning_rate": 3.8173333333333336e-05,
      "loss": 0.0029,
      "step": 8870
    },
    {
      "epoch": 0.4736,
      "grad_norm": 0.29301294684410095,
      "learning_rate": 3.816e-05,
      "loss": 0.002,
      "step": 8880
    },
    {
      "epoch": 0.47413333333333335,
      "grad_norm": 0.22706055641174316,
      "learning_rate": 3.814666666666667e-05,
      "loss": 0.0025,
      "step": 8890
    },
    {
      "epoch": 0.4746666666666667,
      "grad_norm": 0.3408113420009613,
      "learning_rate": 3.8133333333333336e-05,
      "loss": 0.0021,
      "step": 8900
    },
    {
      "epoch": 0.4752,
      "grad_norm": 0.15844516456127167,
      "learning_rate": 3.812e-05,
      "loss": 0.0028,
      "step": 8910
    },
    {
      "epoch": 0.47573333333333334,
      "grad_norm": 0.40790802240371704,
      "learning_rate": 3.8106666666666665e-05,
      "loss": 0.0034,
      "step": 8920
    },
    {
      "epoch": 0.47626666666666667,
      "grad_norm": 0.4431661367416382,
      "learning_rate": 3.809333333333334e-05,
      "loss": 0.0031,
      "step": 8930
    },
    {
      "epoch": 0.4768,
      "grad_norm": 0.2437686324119568,
      "learning_rate": 3.808e-05,
      "loss": 0.0038,
      "step": 8940
    },
    {
      "epoch": 0.47733333333333333,
      "grad_norm": 0.3212558627128601,
      "learning_rate": 3.8066666666666666e-05,
      "loss": 0.0036,
      "step": 8950
    },
    {
      "epoch": 0.47786666666666666,
      "grad_norm": 0.16063742339611053,
      "learning_rate": 3.805333333333334e-05,
      "loss": 0.0024,
      "step": 8960
    },
    {
      "epoch": 0.4784,
      "grad_norm": 0.5395402908325195,
      "learning_rate": 3.804e-05,
      "loss": 0.0022,
      "step": 8970
    },
    {
      "epoch": 0.4789333333333333,
      "grad_norm": 0.11984232068061829,
      "learning_rate": 3.8026666666666666e-05,
      "loss": 0.0027,
      "step": 8980
    },
    {
      "epoch": 0.47946666666666665,
      "grad_norm": 0.3662670850753784,
      "learning_rate": 3.801333333333333e-05,
      "loss": 0.0031,
      "step": 8990
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.20720748603343964,
      "learning_rate": 3.8e-05,
      "loss": 0.0027,
      "step": 9000
    },
    {
      "epoch": 0.4805333333333333,
      "grad_norm": 0.820043683052063,
      "learning_rate": 3.7986666666666673e-05,
      "loss": 0.0019,
      "step": 9010
    },
    {
      "epoch": 0.48106666666666664,
      "grad_norm": 0.2822030782699585,
      "learning_rate": 3.797333333333333e-05,
      "loss": 0.0026,
      "step": 9020
    },
    {
      "epoch": 0.4816,
      "grad_norm": 0.12305495887994766,
      "learning_rate": 3.796e-05,
      "loss": 0.0026,
      "step": 9030
    },
    {
      "epoch": 0.48213333333333336,
      "grad_norm": 0.3542672395706177,
      "learning_rate": 3.794666666666667e-05,
      "loss": 0.0021,
      "step": 9040
    },
    {
      "epoch": 0.4826666666666667,
      "grad_norm": 0.15280495584011078,
      "learning_rate": 3.793333333333334e-05,
      "loss": 0.0026,
      "step": 9050
    },
    {
      "epoch": 0.4832,
      "grad_norm": 0.1535274088382721,
      "learning_rate": 3.792e-05,
      "loss": 0.0026,
      "step": 9060
    },
    {
      "epoch": 0.48373333333333335,
      "grad_norm": 0.5068415403366089,
      "learning_rate": 3.790666666666667e-05,
      "loss": 0.0034,
      "step": 9070
    },
    {
      "epoch": 0.4842666666666667,
      "grad_norm": 0.3163986802101135,
      "learning_rate": 3.789333333333334e-05,
      "loss": 0.0038,
      "step": 9080
    },
    {
      "epoch": 0.4848,
      "grad_norm": 0.28613343834877014,
      "learning_rate": 3.788e-05,
      "loss": 0.0021,
      "step": 9090
    },
    {
      "epoch": 0.48533333333333334,
      "grad_norm": 0.5901463627815247,
      "learning_rate": 3.786666666666667e-05,
      "loss": 0.0031,
      "step": 9100
    },
    {
      "epoch": 0.48586666666666667,
      "grad_norm": 0.2352798879146576,
      "learning_rate": 3.785333333333333e-05,
      "loss": 0.0021,
      "step": 9110
    },
    {
      "epoch": 0.4864,
      "grad_norm": 0.36544692516326904,
      "learning_rate": 3.7840000000000004e-05,
      "loss": 0.0032,
      "step": 9120
    },
    {
      "epoch": 0.48693333333333333,
      "grad_norm": 0.2706757187843323,
      "learning_rate": 3.782666666666667e-05,
      "loss": 0.0043,
      "step": 9130
    },
    {
      "epoch": 0.48746666666666666,
      "grad_norm": 0.07538166642189026,
      "learning_rate": 3.781333333333333e-05,
      "loss": 0.0026,
      "step": 9140
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.44334840774536133,
      "learning_rate": 3.7800000000000004e-05,
      "loss": 0.0021,
      "step": 9150
    },
    {
      "epoch": 0.4885333333333333,
      "grad_norm": 0.4251972734928131,
      "learning_rate": 3.778666666666667e-05,
      "loss": 0.0023,
      "step": 9160
    },
    {
      "epoch": 0.48906666666666665,
      "grad_norm": 0.2589193880558014,
      "learning_rate": 3.777333333333333e-05,
      "loss": 0.0024,
      "step": 9170
    },
    {
      "epoch": 0.4896,
      "grad_norm": 0.25204795598983765,
      "learning_rate": 3.776e-05,
      "loss": 0.0022,
      "step": 9180
    },
    {
      "epoch": 0.4901333333333333,
      "grad_norm": 0.2485068142414093,
      "learning_rate": 3.774666666666667e-05,
      "loss": 0.0023,
      "step": 9190
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 0.0757845789194107,
      "learning_rate": 3.773333333333334e-05,
      "loss": 0.0033,
      "step": 9200
    },
    {
      "epoch": 0.4912,
      "grad_norm": 0.5092377066612244,
      "learning_rate": 3.772e-05,
      "loss": 0.0022,
      "step": 9210
    },
    {
      "epoch": 0.49173333333333336,
      "grad_norm": 0.14860981702804565,
      "learning_rate": 3.770666666666667e-05,
      "loss": 0.0022,
      "step": 9220
    },
    {
      "epoch": 0.4922666666666667,
      "grad_norm": 0.4182594120502472,
      "learning_rate": 3.7693333333333334e-05,
      "loss": 0.0031,
      "step": 9230
    },
    {
      "epoch": 0.4928,
      "grad_norm": 0.38565459847450256,
      "learning_rate": 3.7680000000000005e-05,
      "loss": 0.0029,
      "step": 9240
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.3840005397796631,
      "learning_rate": 3.766666666666667e-05,
      "loss": 0.0023,
      "step": 9250
    },
    {
      "epoch": 0.4938666666666667,
      "grad_norm": 0.8713343143463135,
      "learning_rate": 3.7653333333333334e-05,
      "loss": 0.0025,
      "step": 9260
    },
    {
      "epoch": 0.4944,
      "grad_norm": 0.31777653098106384,
      "learning_rate": 3.7640000000000006e-05,
      "loss": 0.0021,
      "step": 9270
    },
    {
      "epoch": 0.49493333333333334,
      "grad_norm": 0.23527753353118896,
      "learning_rate": 3.762666666666667e-05,
      "loss": 0.0027,
      "step": 9280
    },
    {
      "epoch": 0.49546666666666667,
      "grad_norm": 0.33285146951675415,
      "learning_rate": 3.7613333333333335e-05,
      "loss": 0.0022,
      "step": 9290
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.49599024653434753,
      "learning_rate": 3.76e-05,
      "loss": 0.0029,
      "step": 9300
    },
    {
      "epoch": 0.4965333333333333,
      "grad_norm": 0.1668352335691452,
      "learning_rate": 3.758666666666667e-05,
      "loss": 0.0024,
      "step": 9310
    },
    {
      "epoch": 0.49706666666666666,
      "grad_norm": 0.38931968808174133,
      "learning_rate": 3.7573333333333335e-05,
      "loss": 0.0049,
      "step": 9320
    },
    {
      "epoch": 0.4976,
      "grad_norm": 0.36776837706565857,
      "learning_rate": 3.756e-05,
      "loss": 0.0024,
      "step": 9330
    },
    {
      "epoch": 0.4981333333333333,
      "grad_norm": 0.6038005352020264,
      "learning_rate": 3.754666666666667e-05,
      "loss": 0.0034,
      "step": 9340
    },
    {
      "epoch": 0.49866666666666665,
      "grad_norm": 0.4980277717113495,
      "learning_rate": 3.7533333333333335e-05,
      "loss": 0.002,
      "step": 9350
    },
    {
      "epoch": 0.4992,
      "grad_norm": 0.09550046175718307,
      "learning_rate": 3.752e-05,
      "loss": 0.0025,
      "step": 9360
    },
    {
      "epoch": 0.4997333333333333,
      "grad_norm": 0.4595212936401367,
      "learning_rate": 3.7506666666666664e-05,
      "loss": 0.0036,
      "step": 9370
    },
    {
      "epoch": 0.5002666666666666,
      "grad_norm": 0.5466061234474182,
      "learning_rate": 3.7493333333333336e-05,
      "loss": 0.0033,
      "step": 9380
    },
    {
      "epoch": 0.5008,
      "grad_norm": 0.5433252453804016,
      "learning_rate": 3.748000000000001e-05,
      "loss": 0.0028,
      "step": 9390
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 0.45226341485977173,
      "learning_rate": 3.7466666666666665e-05,
      "loss": 0.0024,
      "step": 9400
    },
    {
      "epoch": 0.5018666666666667,
      "grad_norm": 0.16261199116706848,
      "learning_rate": 3.7453333333333336e-05,
      "loss": 0.0023,
      "step": 9410
    },
    {
      "epoch": 0.5024,
      "grad_norm": 0.514143705368042,
      "learning_rate": 3.744e-05,
      "loss": 0.0027,
      "step": 9420
    },
    {
      "epoch": 0.5029333333333333,
      "grad_norm": 0.10230950266122818,
      "learning_rate": 3.742666666666667e-05,
      "loss": 0.002,
      "step": 9430
    },
    {
      "epoch": 0.5034666666666666,
      "grad_norm": 0.11574068665504456,
      "learning_rate": 3.7413333333333337e-05,
      "loss": 0.0027,
      "step": 9440
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.8262933492660522,
      "learning_rate": 3.74e-05,
      "loss": 0.0022,
      "step": 9450
    },
    {
      "epoch": 0.5045333333333333,
      "grad_norm": 0.32489636540412903,
      "learning_rate": 3.738666666666667e-05,
      "loss": 0.002,
      "step": 9460
    },
    {
      "epoch": 0.5050666666666667,
      "grad_norm": 0.2083706557750702,
      "learning_rate": 3.737333333333333e-05,
      "loss": 0.0032,
      "step": 9470
    },
    {
      "epoch": 0.5056,
      "grad_norm": 0.08017085492610931,
      "learning_rate": 3.736e-05,
      "loss": 0.0019,
      "step": 9480
    },
    {
      "epoch": 0.5061333333333333,
      "grad_norm": 0.09638035297393799,
      "learning_rate": 3.7346666666666666e-05,
      "loss": 0.0018,
      "step": 9490
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.06646483391523361,
      "learning_rate": 3.733333333333334e-05,
      "loss": 0.002,
      "step": 9500
    },
    {
      "epoch": 0.5072,
      "grad_norm": 0.30629414319992065,
      "learning_rate": 3.732e-05,
      "loss": 0.0026,
      "step": 9510
    },
    {
      "epoch": 0.5077333333333334,
      "grad_norm": 0.19065266847610474,
      "learning_rate": 3.7306666666666666e-05,
      "loss": 0.0021,
      "step": 9520
    },
    {
      "epoch": 0.5082666666666666,
      "grad_norm": 0.2981509268283844,
      "learning_rate": 3.729333333333334e-05,
      "loss": 0.003,
      "step": 9530
    },
    {
      "epoch": 0.5088,
      "grad_norm": 0.7120225429534912,
      "learning_rate": 3.728e-05,
      "loss": 0.0023,
      "step": 9540
    },
    {
      "epoch": 0.5093333333333333,
      "grad_norm": 0.05995387211441994,
      "learning_rate": 3.726666666666667e-05,
      "loss": 0.0026,
      "step": 9550
    },
    {
      "epoch": 0.5098666666666667,
      "grad_norm": 0.13163354992866516,
      "learning_rate": 3.725333333333333e-05,
      "loss": 0.0021,
      "step": 9560
    },
    {
      "epoch": 0.5104,
      "grad_norm": 0.05774437636137009,
      "learning_rate": 3.724e-05,
      "loss": 0.0024,
      "step": 9570
    },
    {
      "epoch": 0.5109333333333334,
      "grad_norm": 0.6337917447090149,
      "learning_rate": 3.7226666666666674e-05,
      "loss": 0.0025,
      "step": 9580
    },
    {
      "epoch": 0.5114666666666666,
      "grad_norm": 0.5368757247924805,
      "learning_rate": 3.721333333333333e-05,
      "loss": 0.0025,
      "step": 9590
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.5601949691772461,
      "learning_rate": 3.72e-05,
      "loss": 0.0029,
      "step": 9600
    },
    {
      "epoch": 0.5125333333333333,
      "grad_norm": 0.2923072874546051,
      "learning_rate": 3.718666666666667e-05,
      "loss": 0.0022,
      "step": 9610
    },
    {
      "epoch": 0.5130666666666667,
      "grad_norm": 0.27533137798309326,
      "learning_rate": 3.717333333333334e-05,
      "loss": 0.0026,
      "step": 9620
    },
    {
      "epoch": 0.5136,
      "grad_norm": 0.3436952233314514,
      "learning_rate": 3.716e-05,
      "loss": 0.0029,
      "step": 9630
    },
    {
      "epoch": 0.5141333333333333,
      "grad_norm": 0.15407449007034302,
      "learning_rate": 3.714666666666667e-05,
      "loss": 0.0038,
      "step": 9640
    },
    {
      "epoch": 0.5146666666666667,
      "grad_norm": 0.181808203458786,
      "learning_rate": 3.713333333333334e-05,
      "loss": 0.0033,
      "step": 9650
    },
    {
      "epoch": 0.5152,
      "grad_norm": 0.2860599160194397,
      "learning_rate": 3.712e-05,
      "loss": 0.0034,
      "step": 9660
    },
    {
      "epoch": 0.5157333333333334,
      "grad_norm": 0.14638130366802216,
      "learning_rate": 3.710666666666667e-05,
      "loss": 0.0024,
      "step": 9670
    },
    {
      "epoch": 0.5162666666666667,
      "grad_norm": 0.7533321380615234,
      "learning_rate": 3.709333333333333e-05,
      "loss": 0.0025,
      "step": 9680
    },
    {
      "epoch": 0.5168,
      "grad_norm": 0.7096801400184631,
      "learning_rate": 3.7080000000000004e-05,
      "loss": 0.0021,
      "step": 9690
    },
    {
      "epoch": 0.5173333333333333,
      "grad_norm": 0.36832737922668457,
      "learning_rate": 3.706666666666667e-05,
      "loss": 0.0026,
      "step": 9700
    },
    {
      "epoch": 0.5178666666666667,
      "grad_norm": 0.37424322962760925,
      "learning_rate": 3.705333333333333e-05,
      "loss": 0.0029,
      "step": 9710
    },
    {
      "epoch": 0.5184,
      "grad_norm": 0.23870250582695007,
      "learning_rate": 3.7040000000000005e-05,
      "loss": 0.0047,
      "step": 9720
    },
    {
      "epoch": 0.5189333333333334,
      "grad_norm": 0.4237281382083893,
      "learning_rate": 3.702666666666667e-05,
      "loss": 0.0021,
      "step": 9730
    },
    {
      "epoch": 0.5194666666666666,
      "grad_norm": 0.0912569984793663,
      "learning_rate": 3.7013333333333334e-05,
      "loss": 0.0033,
      "step": 9740
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.1185503676533699,
      "learning_rate": 3.7e-05,
      "loss": 0.0022,
      "step": 9750
    },
    {
      "epoch": 0.5205333333333333,
      "grad_norm": 0.2826104462146759,
      "learning_rate": 3.698666666666667e-05,
      "loss": 0.0021,
      "step": 9760
    },
    {
      "epoch": 0.5210666666666667,
      "grad_norm": 0.6052425503730774,
      "learning_rate": 3.697333333333334e-05,
      "loss": 0.0023,
      "step": 9770
    },
    {
      "epoch": 0.5216,
      "grad_norm": 0.4763772487640381,
      "learning_rate": 3.696e-05,
      "loss": 0.0037,
      "step": 9780
    },
    {
      "epoch": 0.5221333333333333,
      "grad_norm": 0.49371346831321716,
      "learning_rate": 3.694666666666667e-05,
      "loss": 0.0028,
      "step": 9790
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 0.6590660214424133,
      "learning_rate": 3.6933333333333334e-05,
      "loss": 0.0027,
      "step": 9800
    },
    {
      "epoch": 0.5232,
      "grad_norm": 0.2950238883495331,
      "learning_rate": 3.692e-05,
      "loss": 0.0036,
      "step": 9810
    },
    {
      "epoch": 0.5237333333333334,
      "grad_norm": 0.19499506056308746,
      "learning_rate": 3.690666666666667e-05,
      "loss": 0.0024,
      "step": 9820
    },
    {
      "epoch": 0.5242666666666667,
      "grad_norm": 0.1479029804468155,
      "learning_rate": 3.6893333333333335e-05,
      "loss": 0.0026,
      "step": 9830
    },
    {
      "epoch": 0.5248,
      "grad_norm": 0.1874437928199768,
      "learning_rate": 3.6880000000000006e-05,
      "loss": 0.0021,
      "step": 9840
    },
    {
      "epoch": 0.5253333333333333,
      "grad_norm": 0.5463272333145142,
      "learning_rate": 3.6866666666666664e-05,
      "loss": 0.0033,
      "step": 9850
    },
    {
      "epoch": 0.5258666666666667,
      "grad_norm": 0.2799830138683319,
      "learning_rate": 3.6853333333333335e-05,
      "loss": 0.0021,
      "step": 9860
    },
    {
      "epoch": 0.5264,
      "grad_norm": 0.12061382085084915,
      "learning_rate": 3.684e-05,
      "loss": 0.0027,
      "step": 9870
    },
    {
      "epoch": 0.5269333333333334,
      "grad_norm": 0.6228256821632385,
      "learning_rate": 3.682666666666667e-05,
      "loss": 0.0027,
      "step": 9880
    },
    {
      "epoch": 0.5274666666666666,
      "grad_norm": 0.23211950063705444,
      "learning_rate": 3.6813333333333335e-05,
      "loss": 0.0027,
      "step": 9890
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.155632883310318,
      "learning_rate": 3.68e-05,
      "loss": 0.0031,
      "step": 9900
    },
    {
      "epoch": 0.5285333333333333,
      "grad_norm": 0.12775938212871552,
      "learning_rate": 3.678666666666667e-05,
      "loss": 0.0029,
      "step": 9910
    },
    {
      "epoch": 0.5290666666666667,
      "grad_norm": 0.4436101019382477,
      "learning_rate": 3.6773333333333336e-05,
      "loss": 0.0024,
      "step": 9920
    },
    {
      "epoch": 0.5296,
      "grad_norm": 0.18909142911434174,
      "learning_rate": 3.676e-05,
      "loss": 0.0035,
      "step": 9930
    },
    {
      "epoch": 0.5301333333333333,
      "grad_norm": 0.09559868276119232,
      "learning_rate": 3.6746666666666665e-05,
      "loss": 0.0022,
      "step": 9940
    },
    {
      "epoch": 0.5306666666666666,
      "grad_norm": 0.7713942527770996,
      "learning_rate": 3.6733333333333336e-05,
      "loss": 0.0031,
      "step": 9950
    },
    {
      "epoch": 0.5312,
      "grad_norm": 0.15459707379341125,
      "learning_rate": 3.672000000000001e-05,
      "loss": 0.0042,
      "step": 9960
    },
    {
      "epoch": 0.5317333333333333,
      "grad_norm": 0.25883808732032776,
      "learning_rate": 3.6706666666666665e-05,
      "loss": 0.0025,
      "step": 9970
    },
    {
      "epoch": 0.5322666666666667,
      "grad_norm": 0.8605772256851196,
      "learning_rate": 3.669333333333334e-05,
      "loss": 0.0027,
      "step": 9980
    },
    {
      "epoch": 0.5328,
      "grad_norm": 0.06563229858875275,
      "learning_rate": 3.668e-05,
      "loss": 0.0034,
      "step": 9990
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.19490323960781097,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.0021,
      "step": 10000
    },
    {
      "epoch": 0.5338666666666667,
      "grad_norm": 0.18291926383972168,
      "learning_rate": 3.665333333333334e-05,
      "loss": 0.0029,
      "step": 10010
    },
    {
      "epoch": 0.5344,
      "grad_norm": 0.39887478947639465,
      "learning_rate": 3.664e-05,
      "loss": 0.0025,
      "step": 10020
    },
    {
      "epoch": 0.5349333333333334,
      "grad_norm": 0.6121253967285156,
      "learning_rate": 3.662666666666667e-05,
      "loss": 0.0025,
      "step": 10030
    },
    {
      "epoch": 0.5354666666666666,
      "grad_norm": 0.14933346211910248,
      "learning_rate": 3.661333333333333e-05,
      "loss": 0.0031,
      "step": 10040
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.44489115476608276,
      "learning_rate": 3.66e-05,
      "loss": 0.0037,
      "step": 10050
    },
    {
      "epoch": 0.5365333333333333,
      "grad_norm": 0.08612463623285294,
      "learning_rate": 3.6586666666666666e-05,
      "loss": 0.0046,
      "step": 10060
    },
    {
      "epoch": 0.5370666666666667,
      "grad_norm": 0.4770571291446686,
      "learning_rate": 3.657333333333334e-05,
      "loss": 0.0026,
      "step": 10070
    },
    {
      "epoch": 0.5376,
      "grad_norm": 0.3224787414073944,
      "learning_rate": 3.656e-05,
      "loss": 0.0032,
      "step": 10080
    },
    {
      "epoch": 0.5381333333333334,
      "grad_norm": 0.31130707263946533,
      "learning_rate": 3.654666666666667e-05,
      "loss": 0.0027,
      "step": 10090
    },
    {
      "epoch": 0.5386666666666666,
      "grad_norm": 0.7107611298561096,
      "learning_rate": 3.653333333333334e-05,
      "loss": 0.0028,
      "step": 10100
    },
    {
      "epoch": 0.5392,
      "grad_norm": 0.18944552540779114,
      "learning_rate": 3.652e-05,
      "loss": 0.0026,
      "step": 10110
    },
    {
      "epoch": 0.5397333333333333,
      "grad_norm": 0.16775982081890106,
      "learning_rate": 3.650666666666667e-05,
      "loss": 0.0033,
      "step": 10120
    },
    {
      "epoch": 0.5402666666666667,
      "grad_norm": 0.07668745517730713,
      "learning_rate": 3.649333333333333e-05,
      "loss": 0.0024,
      "step": 10130
    },
    {
      "epoch": 0.5408,
      "grad_norm": 0.19045992195606232,
      "learning_rate": 3.648e-05,
      "loss": 0.0027,
      "step": 10140
    },
    {
      "epoch": 0.5413333333333333,
      "grad_norm": 0.1890663057565689,
      "learning_rate": 3.646666666666667e-05,
      "loss": 0.0028,
      "step": 10150
    },
    {
      "epoch": 0.5418666666666667,
      "grad_norm": 0.2964537441730499,
      "learning_rate": 3.645333333333333e-05,
      "loss": 0.0038,
      "step": 10160
    },
    {
      "epoch": 0.5424,
      "grad_norm": 0.6995893120765686,
      "learning_rate": 3.6440000000000003e-05,
      "loss": 0.0028,
      "step": 10170
    },
    {
      "epoch": 0.5429333333333334,
      "grad_norm": 0.6619300246238708,
      "learning_rate": 3.642666666666667e-05,
      "loss": 0.0022,
      "step": 10180
    },
    {
      "epoch": 0.5434666666666667,
      "grad_norm": 0.6986403465270996,
      "learning_rate": 3.641333333333333e-05,
      "loss": 0.0032,
      "step": 10190
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.24958141148090363,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 0.0033,
      "step": 10200
    },
    {
      "epoch": 0.5445333333333333,
      "grad_norm": 0.279490202665329,
      "learning_rate": 3.638666666666667e-05,
      "loss": 0.0025,
      "step": 10210
    },
    {
      "epoch": 0.5450666666666667,
      "grad_norm": 0.41415709257125854,
      "learning_rate": 3.637333333333334e-05,
      "loss": 0.0034,
      "step": 10220
    },
    {
      "epoch": 0.5456,
      "grad_norm": 0.11861523985862732,
      "learning_rate": 3.636e-05,
      "loss": 0.0032,
      "step": 10230
    },
    {
      "epoch": 0.5461333333333334,
      "grad_norm": 0.6348251104354858,
      "learning_rate": 3.634666666666667e-05,
      "loss": 0.0027,
      "step": 10240
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.12070707231760025,
      "learning_rate": 3.633333333333333e-05,
      "loss": 0.0032,
      "step": 10250
    },
    {
      "epoch": 0.5472,
      "grad_norm": 0.3225162625312805,
      "learning_rate": 3.6320000000000005e-05,
      "loss": 0.0021,
      "step": 10260
    },
    {
      "epoch": 0.5477333333333333,
      "grad_norm": 0.21289218962192535,
      "learning_rate": 3.630666666666667e-05,
      "loss": 0.003,
      "step": 10270
    },
    {
      "epoch": 0.5482666666666667,
      "grad_norm": 0.2848413586616516,
      "learning_rate": 3.6293333333333334e-05,
      "loss": 0.0029,
      "step": 10280
    },
    {
      "epoch": 0.5488,
      "grad_norm": 0.4027261435985565,
      "learning_rate": 3.6280000000000005e-05,
      "loss": 0.0037,
      "step": 10290
    },
    {
      "epoch": 0.5493333333333333,
      "grad_norm": 0.300373375415802,
      "learning_rate": 3.626666666666667e-05,
      "loss": 0.0041,
      "step": 10300
    },
    {
      "epoch": 0.5498666666666666,
      "grad_norm": 0.28187620639801025,
      "learning_rate": 3.6253333333333334e-05,
      "loss": 0.002,
      "step": 10310
    },
    {
      "epoch": 0.5504,
      "grad_norm": 0.12138742208480835,
      "learning_rate": 3.624e-05,
      "loss": 0.0031,
      "step": 10320
    },
    {
      "epoch": 0.5509333333333334,
      "grad_norm": 0.43837499618530273,
      "learning_rate": 3.622666666666667e-05,
      "loss": 0.0023,
      "step": 10330
    },
    {
      "epoch": 0.5514666666666667,
      "grad_norm": 0.29198163747787476,
      "learning_rate": 3.6213333333333334e-05,
      "loss": 0.0028,
      "step": 10340
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.5147478580474854,
      "learning_rate": 3.62e-05,
      "loss": 0.0024,
      "step": 10350
    },
    {
      "epoch": 0.5525333333333333,
      "grad_norm": 0.184349924325943,
      "learning_rate": 3.618666666666667e-05,
      "loss": 0.0023,
      "step": 10360
    },
    {
      "epoch": 0.5530666666666667,
      "grad_norm": 0.446888267993927,
      "learning_rate": 3.6173333333333335e-05,
      "loss": 0.0028,
      "step": 10370
    },
    {
      "epoch": 0.5536,
      "grad_norm": 0.5648269653320312,
      "learning_rate": 3.616e-05,
      "loss": 0.0034,
      "step": 10380
    },
    {
      "epoch": 0.5541333333333334,
      "grad_norm": 0.15891502797603607,
      "learning_rate": 3.614666666666667e-05,
      "loss": 0.0042,
      "step": 10390
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 0.13464196026325226,
      "learning_rate": 3.6133333333333335e-05,
      "loss": 0.0029,
      "step": 10400
    },
    {
      "epoch": 0.5552,
      "grad_norm": 0.218303844332695,
      "learning_rate": 3.6120000000000007e-05,
      "loss": 0.0027,
      "step": 10410
    },
    {
      "epoch": 0.5557333333333333,
      "grad_norm": 0.2153962105512619,
      "learning_rate": 3.6106666666666664e-05,
      "loss": 0.004,
      "step": 10420
    },
    {
      "epoch": 0.5562666666666667,
      "grad_norm": 0.18283796310424805,
      "learning_rate": 3.6093333333333336e-05,
      "loss": 0.0028,
      "step": 10430
    },
    {
      "epoch": 0.5568,
      "grad_norm": 0.3080439567565918,
      "learning_rate": 3.608e-05,
      "loss": 0.0024,
      "step": 10440
    },
    {
      "epoch": 0.5573333333333333,
      "grad_norm": 0.251669317483902,
      "learning_rate": 3.606666666666667e-05,
      "loss": 0.0046,
      "step": 10450
    },
    {
      "epoch": 0.5578666666666666,
      "grad_norm": 0.3320143520832062,
      "learning_rate": 3.6053333333333336e-05,
      "loss": 0.0029,
      "step": 10460
    },
    {
      "epoch": 0.5584,
      "grad_norm": 0.17204704880714417,
      "learning_rate": 3.604e-05,
      "loss": 0.0042,
      "step": 10470
    },
    {
      "epoch": 0.5589333333333333,
      "grad_norm": 0.1867375373840332,
      "learning_rate": 3.602666666666667e-05,
      "loss": 0.0027,
      "step": 10480
    },
    {
      "epoch": 0.5594666666666667,
      "grad_norm": 0.37746113538742065,
      "learning_rate": 3.6013333333333336e-05,
      "loss": 0.0029,
      "step": 10490
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.10034573078155518,
      "learning_rate": 3.6e-05,
      "loss": 0.0033,
      "step": 10500
    },
    {
      "epoch": 0.5605333333333333,
      "grad_norm": 0.4813622832298279,
      "learning_rate": 3.5986666666666665e-05,
      "loss": 0.002,
      "step": 10510
    },
    {
      "epoch": 0.5610666666666667,
      "grad_norm": 0.28202974796295166,
      "learning_rate": 3.597333333333334e-05,
      "loss": 0.0022,
      "step": 10520
    },
    {
      "epoch": 0.5616,
      "grad_norm": 0.5272513628005981,
      "learning_rate": 3.596e-05,
      "loss": 0.0033,
      "step": 10530
    },
    {
      "epoch": 0.5621333333333334,
      "grad_norm": 0.5875220894813538,
      "learning_rate": 3.5946666666666666e-05,
      "loss": 0.0031,
      "step": 10540
    },
    {
      "epoch": 0.5626666666666666,
      "grad_norm": 0.6292272806167603,
      "learning_rate": 3.593333333333334e-05,
      "loss": 0.0021,
      "step": 10550
    },
    {
      "epoch": 0.5632,
      "grad_norm": 0.30542445182800293,
      "learning_rate": 3.592e-05,
      "loss": 0.0027,
      "step": 10560
    },
    {
      "epoch": 0.5637333333333333,
      "grad_norm": 0.14754332602024078,
      "learning_rate": 3.5906666666666666e-05,
      "loss": 0.0025,
      "step": 10570
    },
    {
      "epoch": 0.5642666666666667,
      "grad_norm": 0.45758703351020813,
      "learning_rate": 3.589333333333334e-05,
      "loss": 0.0022,
      "step": 10580
    },
    {
      "epoch": 0.5648,
      "grad_norm": 0.20743688941001892,
      "learning_rate": 3.588e-05,
      "loss": 0.0022,
      "step": 10590
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 0.2748556435108185,
      "learning_rate": 3.586666666666667e-05,
      "loss": 0.0036,
      "step": 10600
    },
    {
      "epoch": 0.5658666666666666,
      "grad_norm": 0.2634536325931549,
      "learning_rate": 3.585333333333333e-05,
      "loss": 0.0029,
      "step": 10610
    },
    {
      "epoch": 0.5664,
      "grad_norm": 0.2828727960586548,
      "learning_rate": 3.584e-05,
      "loss": 0.0021,
      "step": 10620
    },
    {
      "epoch": 0.5669333333333333,
      "grad_norm": 0.15936164557933807,
      "learning_rate": 3.582666666666667e-05,
      "loss": 0.003,
      "step": 10630
    },
    {
      "epoch": 0.5674666666666667,
      "grad_norm": 0.6080119013786316,
      "learning_rate": 3.581333333333334e-05,
      "loss": 0.0027,
      "step": 10640
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.11916442215442657,
      "learning_rate": 3.58e-05,
      "loss": 0.0019,
      "step": 10650
    },
    {
      "epoch": 0.5685333333333333,
      "grad_norm": 0.28326934576034546,
      "learning_rate": 3.578666666666667e-05,
      "loss": 0.0025,
      "step": 10660
    },
    {
      "epoch": 0.5690666666666667,
      "grad_norm": 0.1640710085630417,
      "learning_rate": 3.577333333333334e-05,
      "loss": 0.003,
      "step": 10670
    },
    {
      "epoch": 0.5696,
      "grad_norm": 0.0798313245177269,
      "learning_rate": 3.5759999999999996e-05,
      "loss": 0.0035,
      "step": 10680
    },
    {
      "epoch": 0.5701333333333334,
      "grad_norm": 0.15838278830051422,
      "learning_rate": 3.574666666666667e-05,
      "loss": 0.002,
      "step": 10690
    },
    {
      "epoch": 0.5706666666666667,
      "grad_norm": 0.3372447192668915,
      "learning_rate": 3.573333333333333e-05,
      "loss": 0.0035,
      "step": 10700
    },
    {
      "epoch": 0.5712,
      "grad_norm": 0.2552873492240906,
      "learning_rate": 3.5720000000000004e-05,
      "loss": 0.002,
      "step": 10710
    },
    {
      "epoch": 0.5717333333333333,
      "grad_norm": 0.16217714548110962,
      "learning_rate": 3.570666666666667e-05,
      "loss": 0.0027,
      "step": 10720
    },
    {
      "epoch": 0.5722666666666667,
      "grad_norm": 0.8202502131462097,
      "learning_rate": 3.569333333333333e-05,
      "loss": 0.0029,
      "step": 10730
    },
    {
      "epoch": 0.5728,
      "grad_norm": 0.44231271743774414,
      "learning_rate": 3.5680000000000004e-05,
      "loss": 0.0024,
      "step": 10740
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.6958240270614624,
      "learning_rate": 3.566666666666667e-05,
      "loss": 0.0031,
      "step": 10750
    },
    {
      "epoch": 0.5738666666666666,
      "grad_norm": 0.650227963924408,
      "learning_rate": 3.565333333333333e-05,
      "loss": 0.0032,
      "step": 10760
    },
    {
      "epoch": 0.5744,
      "grad_norm": 0.16318705677986145,
      "learning_rate": 3.5640000000000004e-05,
      "loss": 0.0035,
      "step": 10770
    },
    {
      "epoch": 0.5749333333333333,
      "grad_norm": 0.11064918339252472,
      "learning_rate": 3.562666666666667e-05,
      "loss": 0.0022,
      "step": 10780
    },
    {
      "epoch": 0.5754666666666667,
      "grad_norm": 0.2048073261976242,
      "learning_rate": 3.561333333333334e-05,
      "loss": 0.0041,
      "step": 10790
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.15676267445087433,
      "learning_rate": 3.56e-05,
      "loss": 0.0046,
      "step": 10800
    },
    {
      "epoch": 0.5765333333333333,
      "grad_norm": 0.3007989227771759,
      "learning_rate": 3.558666666666667e-05,
      "loss": 0.0023,
      "step": 10810
    },
    {
      "epoch": 0.5770666666666666,
      "grad_norm": 0.794307291507721,
      "learning_rate": 3.5573333333333334e-05,
      "loss": 0.0028,
      "step": 10820
    },
    {
      "epoch": 0.5776,
      "grad_norm": 0.1368127465248108,
      "learning_rate": 3.5560000000000005e-05,
      "loss": 0.0024,
      "step": 10830
    },
    {
      "epoch": 0.5781333333333334,
      "grad_norm": 0.8698424696922302,
      "learning_rate": 3.554666666666667e-05,
      "loss": 0.0028,
      "step": 10840
    },
    {
      "epoch": 0.5786666666666667,
      "grad_norm": 0.07330729812383652,
      "learning_rate": 3.5533333333333334e-05,
      "loss": 0.0025,
      "step": 10850
    },
    {
      "epoch": 0.5792,
      "grad_norm": 0.34432095289230347,
      "learning_rate": 3.5520000000000006e-05,
      "loss": 0.0033,
      "step": 10860
    },
    {
      "epoch": 0.5797333333333333,
      "grad_norm": 0.40057361125946045,
      "learning_rate": 3.550666666666666e-05,
      "loss": 0.002,
      "step": 10870
    },
    {
      "epoch": 0.5802666666666667,
      "grad_norm": 0.4326012432575226,
      "learning_rate": 3.5493333333333335e-05,
      "loss": 0.0023,
      "step": 10880
    },
    {
      "epoch": 0.5808,
      "grad_norm": 0.33056336641311646,
      "learning_rate": 3.548e-05,
      "loss": 0.0023,
      "step": 10890
    },
    {
      "epoch": 0.5813333333333334,
      "grad_norm": 0.2859048545360565,
      "learning_rate": 3.546666666666667e-05,
      "loss": 0.0021,
      "step": 10900
    },
    {
      "epoch": 0.5818666666666666,
      "grad_norm": 0.23189577460289001,
      "learning_rate": 3.5453333333333335e-05,
      "loss": 0.0019,
      "step": 10910
    },
    {
      "epoch": 0.5824,
      "grad_norm": 0.12189707905054092,
      "learning_rate": 3.544e-05,
      "loss": 0.003,
      "step": 10920
    },
    {
      "epoch": 0.5829333333333333,
      "grad_norm": 0.10383681207895279,
      "learning_rate": 3.542666666666667e-05,
      "loss": 0.0028,
      "step": 10930
    },
    {
      "epoch": 0.5834666666666667,
      "grad_norm": 0.21780049800872803,
      "learning_rate": 3.5413333333333335e-05,
      "loss": 0.0026,
      "step": 10940
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.4869660437107086,
      "learning_rate": 3.54e-05,
      "loss": 0.0038,
      "step": 10950
    },
    {
      "epoch": 0.5845333333333333,
      "grad_norm": 0.3758965730667114,
      "learning_rate": 3.538666666666667e-05,
      "loss": 0.0023,
      "step": 10960
    },
    {
      "epoch": 0.5850666666666666,
      "grad_norm": 0.13375088572502136,
      "learning_rate": 3.5373333333333336e-05,
      "loss": 0.0031,
      "step": 10970
    },
    {
      "epoch": 0.5856,
      "grad_norm": 0.5676295757293701,
      "learning_rate": 3.536000000000001e-05,
      "loss": 0.0021,
      "step": 10980
    },
    {
      "epoch": 0.5861333333333333,
      "grad_norm": 0.1970483362674713,
      "learning_rate": 3.5346666666666665e-05,
      "loss": 0.0029,
      "step": 10990
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.43071484565734863,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 0.0029,
      "step": 11000
    },
    {
      "epoch": 0.5872,
      "grad_norm": 0.25609856843948364,
      "learning_rate": 3.532e-05,
      "loss": 0.0018,
      "step": 11010
    },
    {
      "epoch": 0.5877333333333333,
      "grad_norm": 0.17275920510292053,
      "learning_rate": 3.5306666666666665e-05,
      "loss": 0.0034,
      "step": 11020
    },
    {
      "epoch": 0.5882666666666667,
      "grad_norm": 0.22689050436019897,
      "learning_rate": 3.5293333333333336e-05,
      "loss": 0.0025,
      "step": 11030
    },
    {
      "epoch": 0.5888,
      "grad_norm": 0.0754481852054596,
      "learning_rate": 3.528e-05,
      "loss": 0.002,
      "step": 11040
    },
    {
      "epoch": 0.5893333333333334,
      "grad_norm": 0.5896975994110107,
      "learning_rate": 3.526666666666667e-05,
      "loss": 0.0022,
      "step": 11050
    },
    {
      "epoch": 0.5898666666666667,
      "grad_norm": 0.455388605594635,
      "learning_rate": 3.525333333333333e-05,
      "loss": 0.0023,
      "step": 11060
    },
    {
      "epoch": 0.5904,
      "grad_norm": 0.39231380820274353,
      "learning_rate": 3.524e-05,
      "loss": 0.002,
      "step": 11070
    },
    {
      "epoch": 0.5909333333333333,
      "grad_norm": 0.22702088952064514,
      "learning_rate": 3.5226666666666666e-05,
      "loss": 0.0022,
      "step": 11080
    },
    {
      "epoch": 0.5914666666666667,
      "grad_norm": 0.164663165807724,
      "learning_rate": 3.521333333333334e-05,
      "loss": 0.0026,
      "step": 11090
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.15974047780036926,
      "learning_rate": 3.52e-05,
      "loss": 0.002,
      "step": 11100
    },
    {
      "epoch": 0.5925333333333334,
      "grad_norm": 0.10057279467582703,
      "learning_rate": 3.5186666666666666e-05,
      "loss": 0.0024,
      "step": 11110
    },
    {
      "epoch": 0.5930666666666666,
      "grad_norm": 0.26352816820144653,
      "learning_rate": 3.517333333333334e-05,
      "loss": 0.0027,
      "step": 11120
    },
    {
      "epoch": 0.5936,
      "grad_norm": 0.22829045355319977,
      "learning_rate": 3.516e-05,
      "loss": 0.0036,
      "step": 11130
    },
    {
      "epoch": 0.5941333333333333,
      "grad_norm": 0.5845822691917419,
      "learning_rate": 3.514666666666667e-05,
      "loss": 0.003,
      "step": 11140
    },
    {
      "epoch": 0.5946666666666667,
      "grad_norm": 0.12788233160972595,
      "learning_rate": 3.513333333333334e-05,
      "loss": 0.002,
      "step": 11150
    },
    {
      "epoch": 0.5952,
      "grad_norm": 0.22489918768405914,
      "learning_rate": 3.512e-05,
      "loss": 0.0026,
      "step": 11160
    },
    {
      "epoch": 0.5957333333333333,
      "grad_norm": 0.29434698820114136,
      "learning_rate": 3.5106666666666674e-05,
      "loss": 0.0022,
      "step": 11170
    },
    {
      "epoch": 0.5962666666666666,
      "grad_norm": 0.4115380644798279,
      "learning_rate": 3.509333333333333e-05,
      "loss": 0.0027,
      "step": 11180
    },
    {
      "epoch": 0.5968,
      "grad_norm": 0.20655660331249237,
      "learning_rate": 3.508e-05,
      "loss": 0.0029,
      "step": 11190
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.6414287686347961,
      "learning_rate": 3.506666666666667e-05,
      "loss": 0.0025,
      "step": 11200
    },
    {
      "epoch": 0.5978666666666667,
      "grad_norm": 0.7216699123382568,
      "learning_rate": 3.505333333333333e-05,
      "loss": 0.0022,
      "step": 11210
    },
    {
      "epoch": 0.5984,
      "grad_norm": 0.08968307077884674,
      "learning_rate": 3.504e-05,
      "loss": 0.0021,
      "step": 11220
    },
    {
      "epoch": 0.5989333333333333,
      "grad_norm": 0.36668092012405396,
      "learning_rate": 3.502666666666667e-05,
      "loss": 0.0033,
      "step": 11230
    },
    {
      "epoch": 0.5994666666666667,
      "grad_norm": 0.14454714953899384,
      "learning_rate": 3.501333333333334e-05,
      "loss": 0.0024,
      "step": 11240
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.28401243686676025,
      "learning_rate": 3.5e-05,
      "loss": 0.003,
      "step": 11250
    },
    {
      "epoch": 0.6005333333333334,
      "grad_norm": 0.44167447090148926,
      "learning_rate": 3.498666666666667e-05,
      "loss": 0.0024,
      "step": 11260
    },
    {
      "epoch": 0.6010666666666666,
      "grad_norm": 0.08578930050134659,
      "learning_rate": 3.497333333333333e-05,
      "loss": 0.0022,
      "step": 11270
    },
    {
      "epoch": 0.6016,
      "grad_norm": 0.5166487097740173,
      "learning_rate": 3.4960000000000004e-05,
      "loss": 0.0027,
      "step": 11280
    },
    {
      "epoch": 0.6021333333333333,
      "grad_norm": 0.6470375061035156,
      "learning_rate": 3.494666666666667e-05,
      "loss": 0.0041,
      "step": 11290
    },
    {
      "epoch": 0.6026666666666667,
      "grad_norm": 0.07691448926925659,
      "learning_rate": 3.493333333333333e-05,
      "loss": 0.0028,
      "step": 11300
    },
    {
      "epoch": 0.6032,
      "grad_norm": 0.2109806388616562,
      "learning_rate": 3.4920000000000004e-05,
      "loss": 0.0024,
      "step": 11310
    },
    {
      "epoch": 0.6037333333333333,
      "grad_norm": 0.5162118077278137,
      "learning_rate": 3.490666666666667e-05,
      "loss": 0.003,
      "step": 11320
    },
    {
      "epoch": 0.6042666666666666,
      "grad_norm": 0.21966178715229034,
      "learning_rate": 3.4893333333333334e-05,
      "loss": 0.004,
      "step": 11330
    },
    {
      "epoch": 0.6048,
      "grad_norm": 0.6374709010124207,
      "learning_rate": 3.4880000000000005e-05,
      "loss": 0.0019,
      "step": 11340
    },
    {
      "epoch": 0.6053333333333333,
      "grad_norm": 0.14945310354232788,
      "learning_rate": 3.486666666666667e-05,
      "loss": 0.0025,
      "step": 11350
    },
    {
      "epoch": 0.6058666666666667,
      "grad_norm": 0.3187524676322937,
      "learning_rate": 3.4853333333333334e-05,
      "loss": 0.0023,
      "step": 11360
    },
    {
      "epoch": 0.6064,
      "grad_norm": 0.07737234979867935,
      "learning_rate": 3.484e-05,
      "loss": 0.0016,
      "step": 11370
    },
    {
      "epoch": 0.6069333333333333,
      "grad_norm": 0.3948146402835846,
      "learning_rate": 3.482666666666667e-05,
      "loss": 0.0022,
      "step": 11380
    },
    {
      "epoch": 0.6074666666666667,
      "grad_norm": 0.6378915905952454,
      "learning_rate": 3.4813333333333334e-05,
      "loss": 0.0024,
      "step": 11390
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.07712017744779587,
      "learning_rate": 3.48e-05,
      "loss": 0.0024,
      "step": 11400
    },
    {
      "epoch": 0.6085333333333334,
      "grad_norm": 0.153176948428154,
      "learning_rate": 3.478666666666667e-05,
      "loss": 0.0023,
      "step": 11410
    },
    {
      "epoch": 0.6090666666666666,
      "grad_norm": 0.21015985310077667,
      "learning_rate": 3.4773333333333335e-05,
      "loss": 0.0023,
      "step": 11420
    },
    {
      "epoch": 0.6096,
      "grad_norm": 0.12646165490150452,
      "learning_rate": 3.4760000000000006e-05,
      "loss": 0.0021,
      "step": 11430
    },
    {
      "epoch": 0.6101333333333333,
      "grad_norm": 0.340786874294281,
      "learning_rate": 3.4746666666666664e-05,
      "loss": 0.0022,
      "step": 11440
    },
    {
      "epoch": 0.6106666666666667,
      "grad_norm": 0.3724828362464905,
      "learning_rate": 3.4733333333333335e-05,
      "loss": 0.0031,
      "step": 11450
    },
    {
      "epoch": 0.6112,
      "grad_norm": 0.30838483572006226,
      "learning_rate": 3.472e-05,
      "loss": 0.0024,
      "step": 11460
    },
    {
      "epoch": 0.6117333333333334,
      "grad_norm": 0.3582878112792969,
      "learning_rate": 3.470666666666667e-05,
      "loss": 0.0032,
      "step": 11470
    },
    {
      "epoch": 0.6122666666666666,
      "grad_norm": 0.11430951952934265,
      "learning_rate": 3.4693333333333335e-05,
      "loss": 0.0034,
      "step": 11480
    },
    {
      "epoch": 0.6128,
      "grad_norm": 0.2008075714111328,
      "learning_rate": 3.468e-05,
      "loss": 0.0018,
      "step": 11490
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.3486497700214386,
      "learning_rate": 3.466666666666667e-05,
      "loss": 0.0033,
      "step": 11500
    },
    {
      "epoch": 0.6138666666666667,
      "grad_norm": 0.14297842979431152,
      "learning_rate": 3.4653333333333336e-05,
      "loss": 0.0042,
      "step": 11510
    },
    {
      "epoch": 0.6144,
      "grad_norm": 0.5519649386405945,
      "learning_rate": 3.464e-05,
      "loss": 0.0031,
      "step": 11520
    },
    {
      "epoch": 0.6149333333333333,
      "grad_norm": 0.08465343713760376,
      "learning_rate": 3.462666666666667e-05,
      "loss": 0.0025,
      "step": 11530
    },
    {
      "epoch": 0.6154666666666667,
      "grad_norm": 0.3332160413265228,
      "learning_rate": 3.4613333333333336e-05,
      "loss": 0.0033,
      "step": 11540
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.46458402276039124,
      "learning_rate": 3.46e-05,
      "loss": 0.0032,
      "step": 11550
    },
    {
      "epoch": 0.6165333333333334,
      "grad_norm": 0.1427321583032608,
      "learning_rate": 3.4586666666666665e-05,
      "loss": 0.0028,
      "step": 11560
    },
    {
      "epoch": 0.6170666666666667,
      "grad_norm": 0.3179868161678314,
      "learning_rate": 3.4573333333333337e-05,
      "loss": 0.0044,
      "step": 11570
    },
    {
      "epoch": 0.6176,
      "grad_norm": 0.21679331362247467,
      "learning_rate": 3.456e-05,
      "loss": 0.004,
      "step": 11580
    },
    {
      "epoch": 0.6181333333333333,
      "grad_norm": 0.35288766026496887,
      "learning_rate": 3.4546666666666666e-05,
      "loss": 0.0033,
      "step": 11590
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 0.6156855821609497,
      "learning_rate": 3.453333333333334e-05,
      "loss": 0.0027,
      "step": 11600
    },
    {
      "epoch": 0.6192,
      "grad_norm": 0.22906675934791565,
      "learning_rate": 3.452e-05,
      "loss": 0.0032,
      "step": 11610
    },
    {
      "epoch": 0.6197333333333334,
      "grad_norm": 0.5363020300865173,
      "learning_rate": 3.450666666666667e-05,
      "loss": 0.0035,
      "step": 11620
    },
    {
      "epoch": 0.6202666666666666,
      "grad_norm": 0.9204748868942261,
      "learning_rate": 3.449333333333333e-05,
      "loss": 0.0036,
      "step": 11630
    },
    {
      "epoch": 0.6208,
      "grad_norm": 0.4144739508628845,
      "learning_rate": 3.448e-05,
      "loss": 0.0032,
      "step": 11640
    },
    {
      "epoch": 0.6213333333333333,
      "grad_norm": 0.20138005912303925,
      "learning_rate": 3.4466666666666666e-05,
      "loss": 0.0021,
      "step": 11650
    },
    {
      "epoch": 0.6218666666666667,
      "grad_norm": 0.3480508327484131,
      "learning_rate": 3.445333333333334e-05,
      "loss": 0.0029,
      "step": 11660
    },
    {
      "epoch": 0.6224,
      "grad_norm": 0.5217717885971069,
      "learning_rate": 3.444e-05,
      "loss": 0.0035,
      "step": 11670
    },
    {
      "epoch": 0.6229333333333333,
      "grad_norm": 0.32811471819877625,
      "learning_rate": 3.442666666666667e-05,
      "loss": 0.0027,
      "step": 11680
    },
    {
      "epoch": 0.6234666666666666,
      "grad_norm": 0.1971822828054428,
      "learning_rate": 3.441333333333334e-05,
      "loss": 0.0025,
      "step": 11690
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.3784365653991699,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 0.0025,
      "step": 11700
    },
    {
      "epoch": 0.6245333333333334,
      "grad_norm": 0.4085896909236908,
      "learning_rate": 3.438666666666667e-05,
      "loss": 0.0035,
      "step": 11710
    },
    {
      "epoch": 0.6250666666666667,
      "grad_norm": 0.7217289209365845,
      "learning_rate": 3.437333333333334e-05,
      "loss": 0.0029,
      "step": 11720
    },
    {
      "epoch": 0.6256,
      "grad_norm": 0.0923747718334198,
      "learning_rate": 3.436e-05,
      "loss": 0.0025,
      "step": 11730
    },
    {
      "epoch": 0.6261333333333333,
      "grad_norm": 0.38413819670677185,
      "learning_rate": 3.434666666666667e-05,
      "loss": 0.0025,
      "step": 11740
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.5705177783966064,
      "learning_rate": 3.433333333333333e-05,
      "loss": 0.005,
      "step": 11750
    },
    {
      "epoch": 0.6272,
      "grad_norm": 0.49765315651893616,
      "learning_rate": 3.4320000000000003e-05,
      "loss": 0.0035,
      "step": 11760
    },
    {
      "epoch": 0.6277333333333334,
      "grad_norm": 0.2494787722826004,
      "learning_rate": 3.430666666666667e-05,
      "loss": 0.0036,
      "step": 11770
    },
    {
      "epoch": 0.6282666666666666,
      "grad_norm": 0.17195343971252441,
      "learning_rate": 3.429333333333333e-05,
      "loss": 0.0029,
      "step": 11780
    },
    {
      "epoch": 0.6288,
      "grad_norm": 0.14569129049777985,
      "learning_rate": 3.4280000000000004e-05,
      "loss": 0.0031,
      "step": 11790
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 0.6176204085350037,
      "learning_rate": 3.426666666666667e-05,
      "loss": 0.0019,
      "step": 11800
    },
    {
      "epoch": 0.6298666666666667,
      "grad_norm": 0.12691591680049896,
      "learning_rate": 3.425333333333334e-05,
      "loss": 0.0032,
      "step": 11810
    },
    {
      "epoch": 0.6304,
      "grad_norm": 0.2740215063095093,
      "learning_rate": 3.424e-05,
      "loss": 0.0022,
      "step": 11820
    },
    {
      "epoch": 0.6309333333333333,
      "grad_norm": 0.9163009524345398,
      "learning_rate": 3.422666666666667e-05,
      "loss": 0.0021,
      "step": 11830
    },
    {
      "epoch": 0.6314666666666666,
      "grad_norm": 0.4686586856842041,
      "learning_rate": 3.421333333333333e-05,
      "loss": 0.0021,
      "step": 11840
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.16454249620437622,
      "learning_rate": 3.4200000000000005e-05,
      "loss": 0.0019,
      "step": 11850
    },
    {
      "epoch": 0.6325333333333333,
      "grad_norm": 0.2894786596298218,
      "learning_rate": 3.418666666666667e-05,
      "loss": 0.0025,
      "step": 11860
    },
    {
      "epoch": 0.6330666666666667,
      "grad_norm": 0.5515256524085999,
      "learning_rate": 3.4173333333333334e-05,
      "loss": 0.004,
      "step": 11870
    },
    {
      "epoch": 0.6336,
      "grad_norm": 0.7446681261062622,
      "learning_rate": 3.4160000000000005e-05,
      "loss": 0.0025,
      "step": 11880
    },
    {
      "epoch": 0.6341333333333333,
      "grad_norm": 0.28021055459976196,
      "learning_rate": 3.414666666666666e-05,
      "loss": 0.0035,
      "step": 11890
    },
    {
      "epoch": 0.6346666666666667,
      "grad_norm": 0.667262852191925,
      "learning_rate": 3.4133333333333334e-05,
      "loss": 0.0027,
      "step": 11900
    },
    {
      "epoch": 0.6352,
      "grad_norm": 0.22468054294586182,
      "learning_rate": 3.412e-05,
      "loss": 0.0033,
      "step": 11910
    },
    {
      "epoch": 0.6357333333333334,
      "grad_norm": 0.18201270699501038,
      "learning_rate": 3.410666666666667e-05,
      "loss": 0.0032,
      "step": 11920
    },
    {
      "epoch": 0.6362666666666666,
      "grad_norm": 0.4071904122829437,
      "learning_rate": 3.4093333333333334e-05,
      "loss": 0.0025,
      "step": 11930
    },
    {
      "epoch": 0.6368,
      "grad_norm": 0.28058457374572754,
      "learning_rate": 3.408e-05,
      "loss": 0.0021,
      "step": 11940
    },
    {
      "epoch": 0.6373333333333333,
      "grad_norm": 0.3757959008216858,
      "learning_rate": 3.406666666666667e-05,
      "loss": 0.0025,
      "step": 11950
    },
    {
      "epoch": 0.6378666666666667,
      "grad_norm": 0.2791985273361206,
      "learning_rate": 3.4053333333333335e-05,
      "loss": 0.0028,
      "step": 11960
    },
    {
      "epoch": 0.6384,
      "grad_norm": 0.4610050618648529,
      "learning_rate": 3.404e-05,
      "loss": 0.0023,
      "step": 11970
    },
    {
      "epoch": 0.6389333333333334,
      "grad_norm": 0.4934583008289337,
      "learning_rate": 3.402666666666667e-05,
      "loss": 0.0033,
      "step": 11980
    },
    {
      "epoch": 0.6394666666666666,
      "grad_norm": 0.6226707696914673,
      "learning_rate": 3.4013333333333335e-05,
      "loss": 0.0031,
      "step": 11990
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.8092563152313232,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.0025,
      "step": 12000
    },
    {
      "epoch": 0.6405333333333333,
      "grad_norm": 0.09475214779376984,
      "learning_rate": 3.3986666666666664e-05,
      "loss": 0.0027,
      "step": 12010
    },
    {
      "epoch": 0.6410666666666667,
      "grad_norm": 0.8584973216056824,
      "learning_rate": 3.3973333333333336e-05,
      "loss": 0.0021,
      "step": 12020
    },
    {
      "epoch": 0.6416,
      "grad_norm": 0.49268490076065063,
      "learning_rate": 3.396e-05,
      "loss": 0.0031,
      "step": 12030
    },
    {
      "epoch": 0.6421333333333333,
      "grad_norm": 0.6834765076637268,
      "learning_rate": 3.394666666666667e-05,
      "loss": 0.0025,
      "step": 12040
    },
    {
      "epoch": 0.6426666666666667,
      "grad_norm": 0.5029938817024231,
      "learning_rate": 3.3933333333333336e-05,
      "loss": 0.0042,
      "step": 12050
    },
    {
      "epoch": 0.6432,
      "grad_norm": 0.5032097101211548,
      "learning_rate": 3.392e-05,
      "loss": 0.0028,
      "step": 12060
    },
    {
      "epoch": 0.6437333333333334,
      "grad_norm": 0.236868754029274,
      "learning_rate": 3.390666666666667e-05,
      "loss": 0.0038,
      "step": 12070
    },
    {
      "epoch": 0.6442666666666667,
      "grad_norm": 0.22451569139957428,
      "learning_rate": 3.389333333333333e-05,
      "loss": 0.0022,
      "step": 12080
    },
    {
      "epoch": 0.6448,
      "grad_norm": 0.4219108819961548,
      "learning_rate": 3.388e-05,
      "loss": 0.0026,
      "step": 12090
    },
    {
      "epoch": 0.6453333333333333,
      "grad_norm": 0.4882281720638275,
      "learning_rate": 3.3866666666666665e-05,
      "loss": 0.0028,
      "step": 12100
    },
    {
      "epoch": 0.6458666666666667,
      "grad_norm": 0.5889742970466614,
      "learning_rate": 3.385333333333334e-05,
      "loss": 0.0031,
      "step": 12110
    },
    {
      "epoch": 0.6464,
      "grad_norm": 0.16317974030971527,
      "learning_rate": 3.384e-05,
      "loss": 0.0029,
      "step": 12120
    },
    {
      "epoch": 0.6469333333333334,
      "grad_norm": 0.7946674823760986,
      "learning_rate": 3.3826666666666666e-05,
      "loss": 0.003,
      "step": 12130
    },
    {
      "epoch": 0.6474666666666666,
      "grad_norm": 0.2482585459947586,
      "learning_rate": 3.381333333333334e-05,
      "loss": 0.0026,
      "step": 12140
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.6452832818031311,
      "learning_rate": 3.38e-05,
      "loss": 0.0032,
      "step": 12150
    },
    {
      "epoch": 0.6485333333333333,
      "grad_norm": 0.38831859827041626,
      "learning_rate": 3.3786666666666666e-05,
      "loss": 0.0032,
      "step": 12160
    },
    {
      "epoch": 0.6490666666666667,
      "grad_norm": 0.13586437702178955,
      "learning_rate": 3.377333333333334e-05,
      "loss": 0.002,
      "step": 12170
    },
    {
      "epoch": 0.6496,
      "grad_norm": 0.28538063168525696,
      "learning_rate": 3.376e-05,
      "loss": 0.0024,
      "step": 12180
    },
    {
      "epoch": 0.6501333333333333,
      "grad_norm": 0.5045226812362671,
      "learning_rate": 3.374666666666667e-05,
      "loss": 0.0034,
      "step": 12190
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 0.4979356825351715,
      "learning_rate": 3.373333333333333e-05,
      "loss": 0.0026,
      "step": 12200
    },
    {
      "epoch": 0.6512,
      "grad_norm": 0.3267211616039276,
      "learning_rate": 3.372e-05,
      "loss": 0.0022,
      "step": 12210
    },
    {
      "epoch": 0.6517333333333334,
      "grad_norm": 0.505797266960144,
      "learning_rate": 3.370666666666667e-05,
      "loss": 0.0032,
      "step": 12220
    },
    {
      "epoch": 0.6522666666666667,
      "grad_norm": 0.22610872983932495,
      "learning_rate": 3.369333333333333e-05,
      "loss": 0.0033,
      "step": 12230
    },
    {
      "epoch": 0.6528,
      "grad_norm": 0.40756118297576904,
      "learning_rate": 3.368e-05,
      "loss": 0.003,
      "step": 12240
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.1504407525062561,
      "learning_rate": 3.366666666666667e-05,
      "loss": 0.003,
      "step": 12250
    },
    {
      "epoch": 0.6538666666666667,
      "grad_norm": 0.4209945499897003,
      "learning_rate": 3.365333333333334e-05,
      "loss": 0.0024,
      "step": 12260
    },
    {
      "epoch": 0.6544,
      "grad_norm": 0.107184998691082,
      "learning_rate": 3.3639999999999996e-05,
      "loss": 0.0033,
      "step": 12270
    },
    {
      "epoch": 0.6549333333333334,
      "grad_norm": 0.12658481299877167,
      "learning_rate": 3.362666666666667e-05,
      "loss": 0.0019,
      "step": 12280
    },
    {
      "epoch": 0.6554666666666666,
      "grad_norm": 0.4788142144680023,
      "learning_rate": 3.361333333333333e-05,
      "loss": 0.0021,
      "step": 12290
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.20561900734901428,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 0.0031,
      "step": 12300
    },
    {
      "epoch": 0.6565333333333333,
      "grad_norm": 0.29483234882354736,
      "learning_rate": 3.358666666666667e-05,
      "loss": 0.0025,
      "step": 12310
    },
    {
      "epoch": 0.6570666666666667,
      "grad_norm": 0.11074833571910858,
      "learning_rate": 3.357333333333333e-05,
      "loss": 0.0022,
      "step": 12320
    },
    {
      "epoch": 0.6576,
      "grad_norm": 0.36969617009162903,
      "learning_rate": 3.3560000000000004e-05,
      "loss": 0.003,
      "step": 12330
    },
    {
      "epoch": 0.6581333333333333,
      "grad_norm": 0.34481388330459595,
      "learning_rate": 3.354666666666667e-05,
      "loss": 0.0021,
      "step": 12340
    },
    {
      "epoch": 0.6586666666666666,
      "grad_norm": 0.5317859053611755,
      "learning_rate": 3.353333333333333e-05,
      "loss": 0.003,
      "step": 12350
    },
    {
      "epoch": 0.6592,
      "grad_norm": 0.14952312409877777,
      "learning_rate": 3.3520000000000004e-05,
      "loss": 0.0022,
      "step": 12360
    },
    {
      "epoch": 0.6597333333333333,
      "grad_norm": 0.5900107622146606,
      "learning_rate": 3.350666666666667e-05,
      "loss": 0.0026,
      "step": 12370
    },
    {
      "epoch": 0.6602666666666667,
      "grad_norm": 0.2994784712791443,
      "learning_rate": 3.349333333333334e-05,
      "loss": 0.0031,
      "step": 12380
    },
    {
      "epoch": 0.6608,
      "grad_norm": 0.6152486801147461,
      "learning_rate": 3.348e-05,
      "loss": 0.002,
      "step": 12390
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 0.2153375893831253,
      "learning_rate": 3.346666666666667e-05,
      "loss": 0.002,
      "step": 12400
    },
    {
      "epoch": 0.6618666666666667,
      "grad_norm": 0.2743431627750397,
      "learning_rate": 3.3453333333333334e-05,
      "loss": 0.0027,
      "step": 12410
    },
    {
      "epoch": 0.6624,
      "grad_norm": 0.11347631365060806,
      "learning_rate": 3.344e-05,
      "loss": 0.002,
      "step": 12420
    },
    {
      "epoch": 0.6629333333333334,
      "grad_norm": 0.18224909901618958,
      "learning_rate": 3.342666666666667e-05,
      "loss": 0.0021,
      "step": 12430
    },
    {
      "epoch": 0.6634666666666666,
      "grad_norm": 0.5641351938247681,
      "learning_rate": 3.3413333333333334e-05,
      "loss": 0.0026,
      "step": 12440
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.31298166513442993,
      "learning_rate": 3.3400000000000005e-05,
      "loss": 0.0037,
      "step": 12450
    },
    {
      "epoch": 0.6645333333333333,
      "grad_norm": 0.298490971326828,
      "learning_rate": 3.338666666666666e-05,
      "loss": 0.0027,
      "step": 12460
    },
    {
      "epoch": 0.6650666666666667,
      "grad_norm": 0.13004806637763977,
      "learning_rate": 3.3373333333333335e-05,
      "loss": 0.0031,
      "step": 12470
    },
    {
      "epoch": 0.6656,
      "grad_norm": 0.3967592716217041,
      "learning_rate": 3.336e-05,
      "loss": 0.0029,
      "step": 12480
    },
    {
      "epoch": 0.6661333333333334,
      "grad_norm": 0.4933115243911743,
      "learning_rate": 3.334666666666667e-05,
      "loss": 0.003,
      "step": 12490
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.47178351879119873,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.0026,
      "step": 12500
    },
    {
      "epoch": 0.6672,
      "grad_norm": 0.26887819170951843,
      "learning_rate": 3.332e-05,
      "loss": 0.0021,
      "step": 12510
    },
    {
      "epoch": 0.6677333333333333,
      "grad_norm": 0.317390501499176,
      "learning_rate": 3.330666666666667e-05,
      "loss": 0.0036,
      "step": 12520
    },
    {
      "epoch": 0.6682666666666667,
      "grad_norm": 0.16284289956092834,
      "learning_rate": 3.3293333333333335e-05,
      "loss": 0.0027,
      "step": 12530
    },
    {
      "epoch": 0.6688,
      "grad_norm": 0.4043635129928589,
      "learning_rate": 3.328e-05,
      "loss": 0.0038,
      "step": 12540
    },
    {
      "epoch": 0.6693333333333333,
      "grad_norm": 0.11605296283960342,
      "learning_rate": 3.326666666666667e-05,
      "loss": 0.0031,
      "step": 12550
    },
    {
      "epoch": 0.6698666666666667,
      "grad_norm": 0.27517426013946533,
      "learning_rate": 3.3253333333333336e-05,
      "loss": 0.0033,
      "step": 12560
    },
    {
      "epoch": 0.6704,
      "grad_norm": 0.4065697491168976,
      "learning_rate": 3.324e-05,
      "loss": 0.0022,
      "step": 12570
    },
    {
      "epoch": 0.6709333333333334,
      "grad_norm": 0.26154258847236633,
      "learning_rate": 3.3226666666666665e-05,
      "loss": 0.0045,
      "step": 12580
    },
    {
      "epoch": 0.6714666666666667,
      "grad_norm": 0.08203127235174179,
      "learning_rate": 3.3213333333333336e-05,
      "loss": 0.0034,
      "step": 12590
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.14791816473007202,
      "learning_rate": 3.32e-05,
      "loss": 0.0026,
      "step": 12600
    },
    {
      "epoch": 0.6725333333333333,
      "grad_norm": 0.6438758373260498,
      "learning_rate": 3.3186666666666665e-05,
      "loss": 0.0036,
      "step": 12610
    },
    {
      "epoch": 0.6730666666666667,
      "grad_norm": 0.11209123581647873,
      "learning_rate": 3.3173333333333336e-05,
      "loss": 0.0022,
      "step": 12620
    },
    {
      "epoch": 0.6736,
      "grad_norm": 0.6249799132347107,
      "learning_rate": 3.316e-05,
      "loss": 0.0022,
      "step": 12630
    },
    {
      "epoch": 0.6741333333333334,
      "grad_norm": 0.8368920683860779,
      "learning_rate": 3.314666666666667e-05,
      "loss": 0.0017,
      "step": 12640
    },
    {
      "epoch": 0.6746666666666666,
      "grad_norm": 0.4200025498867035,
      "learning_rate": 3.313333333333333e-05,
      "loss": 0.0031,
      "step": 12650
    },
    {
      "epoch": 0.6752,
      "grad_norm": 0.17515721917152405,
      "learning_rate": 3.312e-05,
      "loss": 0.0025,
      "step": 12660
    },
    {
      "epoch": 0.6757333333333333,
      "grad_norm": 0.26658496260643005,
      "learning_rate": 3.3106666666666666e-05,
      "loss": 0.002,
      "step": 12670
    },
    {
      "epoch": 0.6762666666666667,
      "grad_norm": 0.08598557859659195,
      "learning_rate": 3.309333333333334e-05,
      "loss": 0.0019,
      "step": 12680
    },
    {
      "epoch": 0.6768,
      "grad_norm": 0.07203900068998337,
      "learning_rate": 3.308e-05,
      "loss": 0.002,
      "step": 12690
    },
    {
      "epoch": 0.6773333333333333,
      "grad_norm": 0.2117655873298645,
      "learning_rate": 3.3066666666666666e-05,
      "loss": 0.0021,
      "step": 12700
    },
    {
      "epoch": 0.6778666666666666,
      "grad_norm": 0.7027488946914673,
      "learning_rate": 3.305333333333334e-05,
      "loss": 0.0032,
      "step": 12710
    },
    {
      "epoch": 0.6784,
      "grad_norm": 0.7249048948287964,
      "learning_rate": 3.304e-05,
      "loss": 0.0023,
      "step": 12720
    },
    {
      "epoch": 0.6789333333333334,
      "grad_norm": 0.1160081997513771,
      "learning_rate": 3.302666666666667e-05,
      "loss": 0.0038,
      "step": 12730
    },
    {
      "epoch": 0.6794666666666667,
      "grad_norm": 0.5106844902038574,
      "learning_rate": 3.301333333333334e-05,
      "loss": 0.0028,
      "step": 12740
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.31322160363197327,
      "learning_rate": 3.3e-05,
      "loss": 0.0022,
      "step": 12750
    },
    {
      "epoch": 0.6805333333333333,
      "grad_norm": 0.13211019337177277,
      "learning_rate": 3.298666666666667e-05,
      "loss": 0.0023,
      "step": 12760
    },
    {
      "epoch": 0.6810666666666667,
      "grad_norm": 0.3201913833618164,
      "learning_rate": 3.297333333333333e-05,
      "loss": 0.002,
      "step": 12770
    },
    {
      "epoch": 0.6816,
      "grad_norm": 0.17590120434761047,
      "learning_rate": 3.296e-05,
      "loss": 0.0043,
      "step": 12780
    },
    {
      "epoch": 0.6821333333333334,
      "grad_norm": 0.4462512731552124,
      "learning_rate": 3.294666666666667e-05,
      "loss": 0.0042,
      "step": 12790
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 0.5283783674240112,
      "learning_rate": 3.293333333333333e-05,
      "loss": 0.0025,
      "step": 12800
    },
    {
      "epoch": 0.6832,
      "grad_norm": 0.3567841649055481,
      "learning_rate": 3.292e-05,
      "loss": 0.0028,
      "step": 12810
    },
    {
      "epoch": 0.6837333333333333,
      "grad_norm": 0.5027790069580078,
      "learning_rate": 3.290666666666667e-05,
      "loss": 0.0028,
      "step": 12820
    },
    {
      "epoch": 0.6842666666666667,
      "grad_norm": 0.5361237525939941,
      "learning_rate": 3.289333333333334e-05,
      "loss": 0.0033,
      "step": 12830
    },
    {
      "epoch": 0.6848,
      "grad_norm": 0.07731620967388153,
      "learning_rate": 3.288e-05,
      "loss": 0.002,
      "step": 12840
    },
    {
      "epoch": 0.6853333333333333,
      "grad_norm": 0.6536490321159363,
      "learning_rate": 3.286666666666667e-05,
      "loss": 0.0039,
      "step": 12850
    },
    {
      "epoch": 0.6858666666666666,
      "grad_norm": 0.2824441194534302,
      "learning_rate": 3.285333333333333e-05,
      "loss": 0.0028,
      "step": 12860
    },
    {
      "epoch": 0.6864,
      "grad_norm": 0.7972558736801147,
      "learning_rate": 3.2840000000000004e-05,
      "loss": 0.0025,
      "step": 12870
    },
    {
      "epoch": 0.6869333333333333,
      "grad_norm": 0.42515602707862854,
      "learning_rate": 3.282666666666667e-05,
      "loss": 0.0027,
      "step": 12880
    },
    {
      "epoch": 0.6874666666666667,
      "grad_norm": 0.3270631730556488,
      "learning_rate": 3.281333333333333e-05,
      "loss": 0.0037,
      "step": 12890
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.3499748408794403,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 0.0021,
      "step": 12900
    },
    {
      "epoch": 0.6885333333333333,
      "grad_norm": 0.12680959701538086,
      "learning_rate": 3.278666666666666e-05,
      "loss": 0.0037,
      "step": 12910
    },
    {
      "epoch": 0.6890666666666667,
      "grad_norm": 0.1341475397348404,
      "learning_rate": 3.2773333333333334e-05,
      "loss": 0.0021,
      "step": 12920
    },
    {
      "epoch": 0.6896,
      "grad_norm": 0.4640105664730072,
      "learning_rate": 3.2760000000000005e-05,
      "loss": 0.002,
      "step": 12930
    },
    {
      "epoch": 0.6901333333333334,
      "grad_norm": 0.09101049602031708,
      "learning_rate": 3.274666666666667e-05,
      "loss": 0.0028,
      "step": 12940
    },
    {
      "epoch": 0.6906666666666667,
      "grad_norm": 0.07623248547315598,
      "learning_rate": 3.2733333333333334e-05,
      "loss": 0.0018,
      "step": 12950
    },
    {
      "epoch": 0.6912,
      "grad_norm": 0.31771910190582275,
      "learning_rate": 3.272e-05,
      "loss": 0.0027,
      "step": 12960
    },
    {
      "epoch": 0.6917333333333333,
      "grad_norm": 0.3605976998806,
      "learning_rate": 3.270666666666667e-05,
      "loss": 0.0021,
      "step": 12970
    },
    {
      "epoch": 0.6922666666666667,
      "grad_norm": 0.1024056002497673,
      "learning_rate": 3.2693333333333334e-05,
      "loss": 0.0024,
      "step": 12980
    },
    {
      "epoch": 0.6928,
      "grad_norm": 0.20455506443977356,
      "learning_rate": 3.268e-05,
      "loss": 0.0025,
      "step": 12990
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.29425862431526184,
      "learning_rate": 3.266666666666667e-05,
      "loss": 0.0034,
      "step": 13000
    },
    {
      "epoch": 0.6938666666666666,
      "grad_norm": 0.26870104670524597,
      "learning_rate": 3.2653333333333335e-05,
      "loss": 0.003,
      "step": 13010
    },
    {
      "epoch": 0.6944,
      "grad_norm": 0.36492449045181274,
      "learning_rate": 3.2640000000000006e-05,
      "loss": 0.002,
      "step": 13020
    },
    {
      "epoch": 0.6949333333333333,
      "grad_norm": 0.08164140582084656,
      "learning_rate": 3.2626666666666664e-05,
      "loss": 0.0028,
      "step": 13030
    },
    {
      "epoch": 0.6954666666666667,
      "grad_norm": 0.43492886424064636,
      "learning_rate": 3.2613333333333335e-05,
      "loss": 0.002,
      "step": 13040
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.5631804466247559,
      "learning_rate": 3.26e-05,
      "loss": 0.002,
      "step": 13050
    },
    {
      "epoch": 0.6965333333333333,
      "grad_norm": 0.2817094027996063,
      "learning_rate": 3.258666666666667e-05,
      "loss": 0.003,
      "step": 13060
    },
    {
      "epoch": 0.6970666666666666,
      "grad_norm": 0.6356639266014099,
      "learning_rate": 3.2573333333333335e-05,
      "loss": 0.0027,
      "step": 13070
    },
    {
      "epoch": 0.6976,
      "grad_norm": 0.2937021255493164,
      "learning_rate": 3.256e-05,
      "loss": 0.0032,
      "step": 13080
    },
    {
      "epoch": 0.6981333333333334,
      "grad_norm": 0.5230922698974609,
      "learning_rate": 3.254666666666667e-05,
      "loss": 0.0038,
      "step": 13090
    },
    {
      "epoch": 0.6986666666666667,
      "grad_norm": 0.8775936365127563,
      "learning_rate": 3.253333333333333e-05,
      "loss": 0.0027,
      "step": 13100
    },
    {
      "epoch": 0.6992,
      "grad_norm": 0.23777499794960022,
      "learning_rate": 3.252e-05,
      "loss": 0.0023,
      "step": 13110
    },
    {
      "epoch": 0.6997333333333333,
      "grad_norm": 0.32527172565460205,
      "learning_rate": 3.250666666666667e-05,
      "loss": 0.0025,
      "step": 13120
    },
    {
      "epoch": 0.7002666666666667,
      "grad_norm": 0.2309853583574295,
      "learning_rate": 3.2493333333333336e-05,
      "loss": 0.0019,
      "step": 13130
    },
    {
      "epoch": 0.7008,
      "grad_norm": 0.4298241436481476,
      "learning_rate": 3.248e-05,
      "loss": 0.0027,
      "step": 13140
    },
    {
      "epoch": 0.7013333333333334,
      "grad_norm": 0.17724353075027466,
      "learning_rate": 3.2466666666666665e-05,
      "loss": 0.0024,
      "step": 13150
    },
    {
      "epoch": 0.7018666666666666,
      "grad_norm": 0.15918292105197906,
      "learning_rate": 3.2453333333333337e-05,
      "loss": 0.0019,
      "step": 13160
    },
    {
      "epoch": 0.7024,
      "grad_norm": 0.07107781618833542,
      "learning_rate": 3.244e-05,
      "loss": 0.0033,
      "step": 13170
    },
    {
      "epoch": 0.7029333333333333,
      "grad_norm": 0.5000605583190918,
      "learning_rate": 3.2426666666666666e-05,
      "loss": 0.0034,
      "step": 13180
    },
    {
      "epoch": 0.7034666666666667,
      "grad_norm": 0.39050760865211487,
      "learning_rate": 3.241333333333334e-05,
      "loss": 0.0024,
      "step": 13190
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.25475457310676575,
      "learning_rate": 3.24e-05,
      "loss": 0.0027,
      "step": 13200
    },
    {
      "epoch": 0.7045333333333333,
      "grad_norm": 0.12774936854839325,
      "learning_rate": 3.238666666666667e-05,
      "loss": 0.0021,
      "step": 13210
    },
    {
      "epoch": 0.7050666666666666,
      "grad_norm": 0.08432888984680176,
      "learning_rate": 3.237333333333333e-05,
      "loss": 0.0022,
      "step": 13220
    },
    {
      "epoch": 0.7056,
      "grad_norm": 0.288845032453537,
      "learning_rate": 3.236e-05,
      "loss": 0.0025,
      "step": 13230
    },
    {
      "epoch": 0.7061333333333333,
      "grad_norm": 0.2722325325012207,
      "learning_rate": 3.2346666666666666e-05,
      "loss": 0.0018,
      "step": 13240
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.1895437091588974,
      "learning_rate": 3.233333333333333e-05,
      "loss": 0.0019,
      "step": 13250
    },
    {
      "epoch": 0.7072,
      "grad_norm": 0.4388361871242523,
      "learning_rate": 3.232e-05,
      "loss": 0.0021,
      "step": 13260
    },
    {
      "epoch": 0.7077333333333333,
      "grad_norm": 0.21540385484695435,
      "learning_rate": 3.230666666666667e-05,
      "loss": 0.0022,
      "step": 13270
    },
    {
      "epoch": 0.7082666666666667,
      "grad_norm": 0.3653571903705597,
      "learning_rate": 3.229333333333334e-05,
      "loss": 0.003,
      "step": 13280
    },
    {
      "epoch": 0.7088,
      "grad_norm": 0.32820364832878113,
      "learning_rate": 3.2279999999999996e-05,
      "loss": 0.0029,
      "step": 13290
    },
    {
      "epoch": 0.7093333333333334,
      "grad_norm": 0.36038196086883545,
      "learning_rate": 3.226666666666667e-05,
      "loss": 0.0023,
      "step": 13300
    },
    {
      "epoch": 0.7098666666666666,
      "grad_norm": 0.12078973650932312,
      "learning_rate": 3.225333333333334e-05,
      "loss": 0.0025,
      "step": 13310
    },
    {
      "epoch": 0.7104,
      "grad_norm": 0.35380396246910095,
      "learning_rate": 3.224e-05,
      "loss": 0.0027,
      "step": 13320
    },
    {
      "epoch": 0.7109333333333333,
      "grad_norm": 0.12603537738323212,
      "learning_rate": 3.222666666666667e-05,
      "loss": 0.0025,
      "step": 13330
    },
    {
      "epoch": 0.7114666666666667,
      "grad_norm": 0.31276801228523254,
      "learning_rate": 3.221333333333333e-05,
      "loss": 0.0023,
      "step": 13340
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.31975826621055603,
      "learning_rate": 3.2200000000000003e-05,
      "loss": 0.0029,
      "step": 13350
    },
    {
      "epoch": 0.7125333333333334,
      "grad_norm": 0.46845540404319763,
      "learning_rate": 3.218666666666667e-05,
      "loss": 0.0028,
      "step": 13360
    },
    {
      "epoch": 0.7130666666666666,
      "grad_norm": 0.17584657669067383,
      "learning_rate": 3.217333333333333e-05,
      "loss": 0.0039,
      "step": 13370
    },
    {
      "epoch": 0.7136,
      "grad_norm": 0.14594417810440063,
      "learning_rate": 3.2160000000000004e-05,
      "loss": 0.0037,
      "step": 13380
    },
    {
      "epoch": 0.7141333333333333,
      "grad_norm": 0.10284218937158585,
      "learning_rate": 3.214666666666667e-05,
      "loss": 0.0027,
      "step": 13390
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 0.34001097083091736,
      "learning_rate": 3.213333333333334e-05,
      "loss": 0.0022,
      "step": 13400
    },
    {
      "epoch": 0.7152,
      "grad_norm": 0.2413647472858429,
      "learning_rate": 3.212e-05,
      "loss": 0.0042,
      "step": 13410
    },
    {
      "epoch": 0.7157333333333333,
      "grad_norm": 0.061899520456790924,
      "learning_rate": 3.210666666666667e-05,
      "loss": 0.0031,
      "step": 13420
    },
    {
      "epoch": 0.7162666666666667,
      "grad_norm": 0.22131933271884918,
      "learning_rate": 3.209333333333333e-05,
      "loss": 0.0036,
      "step": 13430
    },
    {
      "epoch": 0.7168,
      "grad_norm": 0.49796339869499207,
      "learning_rate": 3.208e-05,
      "loss": 0.0021,
      "step": 13440
    },
    {
      "epoch": 0.7173333333333334,
      "grad_norm": 0.21763074398040771,
      "learning_rate": 3.206666666666667e-05,
      "loss": 0.0028,
      "step": 13450
    },
    {
      "epoch": 0.7178666666666667,
      "grad_norm": 0.13094565272331238,
      "learning_rate": 3.2053333333333334e-05,
      "loss": 0.002,
      "step": 13460
    },
    {
      "epoch": 0.7184,
      "grad_norm": 0.3369266986846924,
      "learning_rate": 3.2040000000000005e-05,
      "loss": 0.002,
      "step": 13470
    },
    {
      "epoch": 0.7189333333333333,
      "grad_norm": 0.46200406551361084,
      "learning_rate": 3.202666666666666e-05,
      "loss": 0.0034,
      "step": 13480
    },
    {
      "epoch": 0.7194666666666667,
      "grad_norm": 0.41598445177078247,
      "learning_rate": 3.2013333333333334e-05,
      "loss": 0.0022,
      "step": 13490
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.1727665662765503,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.002,
      "step": 13500
    },
    {
      "epoch": 0.7205333333333334,
      "grad_norm": 0.5382826328277588,
      "learning_rate": 3.198666666666667e-05,
      "loss": 0.0022,
      "step": 13510
    },
    {
      "epoch": 0.7210666666666666,
      "grad_norm": 0.2041764259338379,
      "learning_rate": 3.1973333333333334e-05,
      "loss": 0.005,
      "step": 13520
    },
    {
      "epoch": 0.7216,
      "grad_norm": 0.13344097137451172,
      "learning_rate": 3.196e-05,
      "loss": 0.0018,
      "step": 13530
    },
    {
      "epoch": 0.7221333333333333,
      "grad_norm": 0.15919151902198792,
      "learning_rate": 3.194666666666667e-05,
      "loss": 0.0023,
      "step": 13540
    },
    {
      "epoch": 0.7226666666666667,
      "grad_norm": 0.08360797166824341,
      "learning_rate": 3.1933333333333335e-05,
      "loss": 0.003,
      "step": 13550
    },
    {
      "epoch": 0.7232,
      "grad_norm": 0.6351733207702637,
      "learning_rate": 3.192e-05,
      "loss": 0.0023,
      "step": 13560
    },
    {
      "epoch": 0.7237333333333333,
      "grad_norm": 0.26619505882263184,
      "learning_rate": 3.190666666666667e-05,
      "loss": 0.0031,
      "step": 13570
    },
    {
      "epoch": 0.7242666666666666,
      "grad_norm": 0.7810855507850647,
      "learning_rate": 3.1893333333333335e-05,
      "loss": 0.0023,
      "step": 13580
    },
    {
      "epoch": 0.7248,
      "grad_norm": 0.09278085082769394,
      "learning_rate": 3.188e-05,
      "loss": 0.0037,
      "step": 13590
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 0.17635035514831543,
      "learning_rate": 3.1866666666666664e-05,
      "loss": 0.0019,
      "step": 13600
    },
    {
      "epoch": 0.7258666666666667,
      "grad_norm": 0.08834393322467804,
      "learning_rate": 3.1853333333333336e-05,
      "loss": 0.0036,
      "step": 13610
    },
    {
      "epoch": 0.7264,
      "grad_norm": 0.3905434012413025,
      "learning_rate": 3.184e-05,
      "loss": 0.003,
      "step": 13620
    },
    {
      "epoch": 0.7269333333333333,
      "grad_norm": 0.4602077603340149,
      "learning_rate": 3.1826666666666665e-05,
      "loss": 0.0019,
      "step": 13630
    },
    {
      "epoch": 0.7274666666666667,
      "grad_norm": 0.6490522623062134,
      "learning_rate": 3.1813333333333336e-05,
      "loss": 0.0021,
      "step": 13640
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.31065037846565247,
      "learning_rate": 3.18e-05,
      "loss": 0.0024,
      "step": 13650
    },
    {
      "epoch": 0.7285333333333334,
      "grad_norm": 0.6686238646507263,
      "learning_rate": 3.178666666666667e-05,
      "loss": 0.0023,
      "step": 13660
    },
    {
      "epoch": 0.7290666666666666,
      "grad_norm": 0.6345345377922058,
      "learning_rate": 3.177333333333333e-05,
      "loss": 0.0021,
      "step": 13670
    },
    {
      "epoch": 0.7296,
      "grad_norm": 0.2536620795726776,
      "learning_rate": 3.176e-05,
      "loss": 0.0045,
      "step": 13680
    },
    {
      "epoch": 0.7301333333333333,
      "grad_norm": 0.5707007646560669,
      "learning_rate": 3.174666666666667e-05,
      "loss": 0.0025,
      "step": 13690
    },
    {
      "epoch": 0.7306666666666667,
      "grad_norm": 0.11972631514072418,
      "learning_rate": 3.173333333333334e-05,
      "loss": 0.0024,
      "step": 13700
    },
    {
      "epoch": 0.7312,
      "grad_norm": 0.2622886896133423,
      "learning_rate": 3.172e-05,
      "loss": 0.0024,
      "step": 13710
    },
    {
      "epoch": 0.7317333333333333,
      "grad_norm": 0.2542083263397217,
      "learning_rate": 3.1706666666666666e-05,
      "loss": 0.0025,
      "step": 13720
    },
    {
      "epoch": 0.7322666666666666,
      "grad_norm": 0.45325589179992676,
      "learning_rate": 3.169333333333334e-05,
      "loss": 0.0019,
      "step": 13730
    },
    {
      "epoch": 0.7328,
      "grad_norm": 0.650531530380249,
      "learning_rate": 3.168e-05,
      "loss": 0.0028,
      "step": 13740
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.4781019687652588,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 0.0026,
      "step": 13750
    },
    {
      "epoch": 0.7338666666666667,
      "grad_norm": 0.34416651725769043,
      "learning_rate": 3.165333333333334e-05,
      "loss": 0.0031,
      "step": 13760
    },
    {
      "epoch": 0.7344,
      "grad_norm": 0.3781043589115143,
      "learning_rate": 3.164e-05,
      "loss": 0.0028,
      "step": 13770
    },
    {
      "epoch": 0.7349333333333333,
      "grad_norm": 0.38036563992500305,
      "learning_rate": 3.1626666666666667e-05,
      "loss": 0.0021,
      "step": 13780
    },
    {
      "epoch": 0.7354666666666667,
      "grad_norm": 0.143802672624588,
      "learning_rate": 3.161333333333333e-05,
      "loss": 0.0027,
      "step": 13790
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.4462778866291046,
      "learning_rate": 3.16e-05,
      "loss": 0.002,
      "step": 13800
    },
    {
      "epoch": 0.7365333333333334,
      "grad_norm": 0.11725667864084244,
      "learning_rate": 3.158666666666667e-05,
      "loss": 0.0021,
      "step": 13810
    },
    {
      "epoch": 0.7370666666666666,
      "grad_norm": 0.08411724120378494,
      "learning_rate": 3.157333333333333e-05,
      "loss": 0.0032,
      "step": 13820
    },
    {
      "epoch": 0.7376,
      "grad_norm": 0.3023473024368286,
      "learning_rate": 3.156e-05,
      "loss": 0.0024,
      "step": 13830
    },
    {
      "epoch": 0.7381333333333333,
      "grad_norm": 0.2896377146244049,
      "learning_rate": 3.154666666666667e-05,
      "loss": 0.0027,
      "step": 13840
    },
    {
      "epoch": 0.7386666666666667,
      "grad_norm": 0.42248404026031494,
      "learning_rate": 3.153333333333334e-05,
      "loss": 0.0027,
      "step": 13850
    },
    {
      "epoch": 0.7392,
      "grad_norm": 0.1989257037639618,
      "learning_rate": 3.1519999999999996e-05,
      "loss": 0.0031,
      "step": 13860
    },
    {
      "epoch": 0.7397333333333334,
      "grad_norm": 0.39460399746894836,
      "learning_rate": 3.150666666666667e-05,
      "loss": 0.0021,
      "step": 13870
    },
    {
      "epoch": 0.7402666666666666,
      "grad_norm": 0.362210750579834,
      "learning_rate": 3.149333333333334e-05,
      "loss": 0.0022,
      "step": 13880
    },
    {
      "epoch": 0.7408,
      "grad_norm": 0.25617754459381104,
      "learning_rate": 3.1480000000000004e-05,
      "loss": 0.0019,
      "step": 13890
    },
    {
      "epoch": 0.7413333333333333,
      "grad_norm": 0.29118046164512634,
      "learning_rate": 3.146666666666667e-05,
      "loss": 0.0038,
      "step": 13900
    },
    {
      "epoch": 0.7418666666666667,
      "grad_norm": 0.40926629304885864,
      "learning_rate": 3.145333333333333e-05,
      "loss": 0.0026,
      "step": 13910
    },
    {
      "epoch": 0.7424,
      "grad_norm": 0.8371043801307678,
      "learning_rate": 3.1440000000000004e-05,
      "loss": 0.0031,
      "step": 13920
    },
    {
      "epoch": 0.7429333333333333,
      "grad_norm": 0.5115819573402405,
      "learning_rate": 3.142666666666667e-05,
      "loss": 0.0026,
      "step": 13930
    },
    {
      "epoch": 0.7434666666666667,
      "grad_norm": 0.5478137135505676,
      "learning_rate": 3.141333333333333e-05,
      "loss": 0.0022,
      "step": 13940
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.213217630982399,
      "learning_rate": 3.1400000000000004e-05,
      "loss": 0.0021,
      "step": 13950
    },
    {
      "epoch": 0.7445333333333334,
      "grad_norm": 0.32610371708869934,
      "learning_rate": 3.138666666666667e-05,
      "loss": 0.0027,
      "step": 13960
    },
    {
      "epoch": 0.7450666666666667,
      "grad_norm": 0.9056483507156372,
      "learning_rate": 3.137333333333333e-05,
      "loss": 0.0022,
      "step": 13970
    },
    {
      "epoch": 0.7456,
      "grad_norm": 0.35107603669166565,
      "learning_rate": 3.136e-05,
      "loss": 0.0044,
      "step": 13980
    },
    {
      "epoch": 0.7461333333333333,
      "grad_norm": 0.48519283533096313,
      "learning_rate": 3.134666666666667e-05,
      "loss": 0.0026,
      "step": 13990
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.4043445587158203,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 0.004,
      "step": 14000
    },
    {
      "epoch": 0.7472,
      "grad_norm": 0.5066653490066528,
      "learning_rate": 3.132e-05,
      "loss": 0.0034,
      "step": 14010
    },
    {
      "epoch": 0.7477333333333334,
      "grad_norm": 0.2139524519443512,
      "learning_rate": 3.130666666666667e-05,
      "loss": 0.0031,
      "step": 14020
    },
    {
      "epoch": 0.7482666666666666,
      "grad_norm": 0.0913902148604393,
      "learning_rate": 3.1293333333333334e-05,
      "loss": 0.0026,
      "step": 14030
    },
    {
      "epoch": 0.7488,
      "grad_norm": 0.14096833765506744,
      "learning_rate": 3.1280000000000005e-05,
      "loss": 0.0023,
      "step": 14040
    },
    {
      "epoch": 0.7493333333333333,
      "grad_norm": 0.31404364109039307,
      "learning_rate": 3.126666666666666e-05,
      "loss": 0.0025,
      "step": 14050
    },
    {
      "epoch": 0.7498666666666667,
      "grad_norm": 0.3489970862865448,
      "learning_rate": 3.1253333333333335e-05,
      "loss": 0.0036,
      "step": 14060
    },
    {
      "epoch": 0.7504,
      "grad_norm": 0.1041528582572937,
      "learning_rate": 3.1240000000000006e-05,
      "loss": 0.0021,
      "step": 14070
    },
    {
      "epoch": 0.7509333333333333,
      "grad_norm": 0.09821684658527374,
      "learning_rate": 3.122666666666667e-05,
      "loss": 0.0017,
      "step": 14080
    },
    {
      "epoch": 0.7514666666666666,
      "grad_norm": 0.31055504083633423,
      "learning_rate": 3.1213333333333335e-05,
      "loss": 0.003,
      "step": 14090
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.16849081218242645,
      "learning_rate": 3.12e-05,
      "loss": 0.0027,
      "step": 14100
    },
    {
      "epoch": 0.7525333333333334,
      "grad_norm": 0.2451627403497696,
      "learning_rate": 3.118666666666667e-05,
      "loss": 0.0037,
      "step": 14110
    },
    {
      "epoch": 0.7530666666666667,
      "grad_norm": 0.39246460795402527,
      "learning_rate": 3.1173333333333335e-05,
      "loss": 0.0033,
      "step": 14120
    },
    {
      "epoch": 0.7536,
      "grad_norm": 0.6757551431655884,
      "learning_rate": 3.116e-05,
      "loss": 0.0031,
      "step": 14130
    },
    {
      "epoch": 0.7541333333333333,
      "grad_norm": 0.1902400553226471,
      "learning_rate": 3.114666666666667e-05,
      "loss": 0.0021,
      "step": 14140
    },
    {
      "epoch": 0.7546666666666667,
      "grad_norm": 0.40749433636665344,
      "learning_rate": 3.1133333333333336e-05,
      "loss": 0.0028,
      "step": 14150
    },
    {
      "epoch": 0.7552,
      "grad_norm": 0.6195001602172852,
      "learning_rate": 3.112e-05,
      "loss": 0.0022,
      "step": 14160
    },
    {
      "epoch": 0.7557333333333334,
      "grad_norm": 0.7810505628585815,
      "learning_rate": 3.1106666666666665e-05,
      "loss": 0.0027,
      "step": 14170
    },
    {
      "epoch": 0.7562666666666666,
      "grad_norm": 0.7658033967018127,
      "learning_rate": 3.1093333333333336e-05,
      "loss": 0.0021,
      "step": 14180
    },
    {
      "epoch": 0.7568,
      "grad_norm": 0.5506821274757385,
      "learning_rate": 3.108e-05,
      "loss": 0.002,
      "step": 14190
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 0.3882026672363281,
      "learning_rate": 3.1066666666666665e-05,
      "loss": 0.0036,
      "step": 14200
    },
    {
      "epoch": 0.7578666666666667,
      "grad_norm": 0.5392599701881409,
      "learning_rate": 3.1053333333333336e-05,
      "loss": 0.0031,
      "step": 14210
    },
    {
      "epoch": 0.7584,
      "grad_norm": 0.19888922572135925,
      "learning_rate": 3.104e-05,
      "loss": 0.0021,
      "step": 14220
    },
    {
      "epoch": 0.7589333333333333,
      "grad_norm": 0.4594407379627228,
      "learning_rate": 3.102666666666667e-05,
      "loss": 0.0023,
      "step": 14230
    },
    {
      "epoch": 0.7594666666666666,
      "grad_norm": 0.35307443141937256,
      "learning_rate": 3.101333333333333e-05,
      "loss": 0.0023,
      "step": 14240
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.21600216627120972,
      "learning_rate": 3.1e-05,
      "loss": 0.0033,
      "step": 14250
    },
    {
      "epoch": 0.7605333333333333,
      "grad_norm": 0.1004384234547615,
      "learning_rate": 3.098666666666667e-05,
      "loss": 0.0023,
      "step": 14260
    },
    {
      "epoch": 0.7610666666666667,
      "grad_norm": 0.2279459834098816,
      "learning_rate": 3.097333333333334e-05,
      "loss": 0.0022,
      "step": 14270
    },
    {
      "epoch": 0.7616,
      "grad_norm": 0.08972684293985367,
      "learning_rate": 3.096e-05,
      "loss": 0.0022,
      "step": 14280
    },
    {
      "epoch": 0.7621333333333333,
      "grad_norm": 0.3760710060596466,
      "learning_rate": 3.0946666666666666e-05,
      "loss": 0.0019,
      "step": 14290
    },
    {
      "epoch": 0.7626666666666667,
      "grad_norm": 0.06832317262887955,
      "learning_rate": 3.093333333333334e-05,
      "loss": 0.0034,
      "step": 14300
    },
    {
      "epoch": 0.7632,
      "grad_norm": 0.659511387348175,
      "learning_rate": 3.092e-05,
      "loss": 0.0022,
      "step": 14310
    },
    {
      "epoch": 0.7637333333333334,
      "grad_norm": 0.3833335340023041,
      "learning_rate": 3.090666666666667e-05,
      "loss": 0.003,
      "step": 14320
    },
    {
      "epoch": 0.7642666666666666,
      "grad_norm": 0.13142402470111847,
      "learning_rate": 3.089333333333334e-05,
      "loss": 0.0018,
      "step": 14330
    },
    {
      "epoch": 0.7648,
      "grad_norm": 0.12777850031852722,
      "learning_rate": 3.088e-05,
      "loss": 0.0033,
      "step": 14340
    },
    {
      "epoch": 0.7653333333333333,
      "grad_norm": 0.1675974577665329,
      "learning_rate": 3.086666666666667e-05,
      "loss": 0.0032,
      "step": 14350
    },
    {
      "epoch": 0.7658666666666667,
      "grad_norm": 0.464405357837677,
      "learning_rate": 3.085333333333333e-05,
      "loss": 0.0033,
      "step": 14360
    },
    {
      "epoch": 0.7664,
      "grad_norm": 0.23984873294830322,
      "learning_rate": 3.084e-05,
      "loss": 0.0035,
      "step": 14370
    },
    {
      "epoch": 0.7669333333333334,
      "grad_norm": 0.1801089495420456,
      "learning_rate": 3.082666666666667e-05,
      "loss": 0.0024,
      "step": 14380
    },
    {
      "epoch": 0.7674666666666666,
      "grad_norm": 0.470312237739563,
      "learning_rate": 3.081333333333333e-05,
      "loss": 0.0023,
      "step": 14390
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.44495993852615356,
      "learning_rate": 3.08e-05,
      "loss": 0.0021,
      "step": 14400
    },
    {
      "epoch": 0.7685333333333333,
      "grad_norm": 0.4539138078689575,
      "learning_rate": 3.078666666666667e-05,
      "loss": 0.0028,
      "step": 14410
    },
    {
      "epoch": 0.7690666666666667,
      "grad_norm": 0.1390683352947235,
      "learning_rate": 3.077333333333334e-05,
      "loss": 0.0028,
      "step": 14420
    },
    {
      "epoch": 0.7696,
      "grad_norm": 0.6808425188064575,
      "learning_rate": 3.076e-05,
      "loss": 0.0037,
      "step": 14430
    },
    {
      "epoch": 0.7701333333333333,
      "grad_norm": 0.3116490840911865,
      "learning_rate": 3.074666666666667e-05,
      "loss": 0.0021,
      "step": 14440
    },
    {
      "epoch": 0.7706666666666667,
      "grad_norm": 0.5548703670501709,
      "learning_rate": 3.073333333333334e-05,
      "loss": 0.0024,
      "step": 14450
    },
    {
      "epoch": 0.7712,
      "grad_norm": 0.6823013424873352,
      "learning_rate": 3.072e-05,
      "loss": 0.0022,
      "step": 14460
    },
    {
      "epoch": 0.7717333333333334,
      "grad_norm": 0.36826348304748535,
      "learning_rate": 3.070666666666667e-05,
      "loss": 0.0032,
      "step": 14470
    },
    {
      "epoch": 0.7722666666666667,
      "grad_norm": 0.25156864523887634,
      "learning_rate": 3.069333333333333e-05,
      "loss": 0.0018,
      "step": 14480
    },
    {
      "epoch": 0.7728,
      "grad_norm": 0.1328207552433014,
      "learning_rate": 3.0680000000000004e-05,
      "loss": 0.0026,
      "step": 14490
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.1172066405415535,
      "learning_rate": 3.066666666666667e-05,
      "loss": 0.0028,
      "step": 14500
    },
    {
      "epoch": 0.7738666666666667,
      "grad_norm": 1.003584861755371,
      "learning_rate": 3.0653333333333333e-05,
      "loss": 0.0036,
      "step": 14510
    },
    {
      "epoch": 0.7744,
      "grad_norm": 0.4390111267566681,
      "learning_rate": 3.0640000000000005e-05,
      "loss": 0.0023,
      "step": 14520
    },
    {
      "epoch": 0.7749333333333334,
      "grad_norm": 0.2076210379600525,
      "learning_rate": 3.062666666666667e-05,
      "loss": 0.0034,
      "step": 14530
    },
    {
      "epoch": 0.7754666666666666,
      "grad_norm": 0.709906280040741,
      "learning_rate": 3.0613333333333334e-05,
      "loss": 0.003,
      "step": 14540
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.1584724485874176,
      "learning_rate": 3.06e-05,
      "loss": 0.0023,
      "step": 14550
    },
    {
      "epoch": 0.7765333333333333,
      "grad_norm": 0.17184355854988098,
      "learning_rate": 3.058666666666667e-05,
      "loss": 0.0031,
      "step": 14560
    },
    {
      "epoch": 0.7770666666666667,
      "grad_norm": 0.13576944172382355,
      "learning_rate": 3.0573333333333334e-05,
      "loss": 0.0026,
      "step": 14570
    },
    {
      "epoch": 0.7776,
      "grad_norm": 0.31470438838005066,
      "learning_rate": 3.056e-05,
      "loss": 0.0022,
      "step": 14580
    },
    {
      "epoch": 0.7781333333333333,
      "grad_norm": 0.5814710259437561,
      "learning_rate": 3.054666666666667e-05,
      "loss": 0.0033,
      "step": 14590
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 0.5488549470901489,
      "learning_rate": 3.0533333333333335e-05,
      "loss": 0.0018,
      "step": 14600
    },
    {
      "epoch": 0.7792,
      "grad_norm": 0.739233672618866,
      "learning_rate": 3.0520000000000006e-05,
      "loss": 0.0026,
      "step": 14610
    },
    {
      "epoch": 0.7797333333333333,
      "grad_norm": 0.4422304332256317,
      "learning_rate": 3.0506666666666667e-05,
      "loss": 0.0025,
      "step": 14620
    },
    {
      "epoch": 0.7802666666666667,
      "grad_norm": 0.5618864893913269,
      "learning_rate": 3.0493333333333335e-05,
      "loss": 0.0044,
      "step": 14630
    },
    {
      "epoch": 0.7808,
      "grad_norm": 0.09634372591972351,
      "learning_rate": 3.0480000000000003e-05,
      "loss": 0.0021,
      "step": 14640
    },
    {
      "epoch": 0.7813333333333333,
      "grad_norm": 0.3326490819454193,
      "learning_rate": 3.0466666666666664e-05,
      "loss": 0.0021,
      "step": 14650
    },
    {
      "epoch": 0.7818666666666667,
      "grad_norm": 0.47564050555229187,
      "learning_rate": 3.0453333333333335e-05,
      "loss": 0.0022,
      "step": 14660
    },
    {
      "epoch": 0.7824,
      "grad_norm": 0.4653759300708771,
      "learning_rate": 3.0440000000000003e-05,
      "loss": 0.0019,
      "step": 14670
    },
    {
      "epoch": 0.7829333333333334,
      "grad_norm": 0.20835091173648834,
      "learning_rate": 3.042666666666667e-05,
      "loss": 0.0037,
      "step": 14680
    },
    {
      "epoch": 0.7834666666666666,
      "grad_norm": 0.4898039400577545,
      "learning_rate": 3.0413333333333332e-05,
      "loss": 0.0027,
      "step": 14690
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.6895619630813599,
      "learning_rate": 3.04e-05,
      "loss": 0.0022,
      "step": 14700
    },
    {
      "epoch": 0.7845333333333333,
      "grad_norm": 0.29385995864868164,
      "learning_rate": 3.0386666666666668e-05,
      "loss": 0.002,
      "step": 14710
    },
    {
      "epoch": 0.7850666666666667,
      "grad_norm": 0.5156472325325012,
      "learning_rate": 3.0373333333333336e-05,
      "loss": 0.0031,
      "step": 14720
    },
    {
      "epoch": 0.7856,
      "grad_norm": 0.4397994577884674,
      "learning_rate": 3.036e-05,
      "loss": 0.0021,
      "step": 14730
    },
    {
      "epoch": 0.7861333333333334,
      "grad_norm": 0.379377156496048,
      "learning_rate": 3.034666666666667e-05,
      "loss": 0.0023,
      "step": 14740
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.17838738858699799,
      "learning_rate": 3.0333333333333337e-05,
      "loss": 0.0027,
      "step": 14750
    },
    {
      "epoch": 0.7872,
      "grad_norm": 0.2557443678379059,
      "learning_rate": 3.0320000000000004e-05,
      "loss": 0.0035,
      "step": 14760
    },
    {
      "epoch": 0.7877333333333333,
      "grad_norm": 0.2820558249950409,
      "learning_rate": 3.0306666666666666e-05,
      "loss": 0.0028,
      "step": 14770
    },
    {
      "epoch": 0.7882666666666667,
      "grad_norm": 0.4609833359718323,
      "learning_rate": 3.0293333333333334e-05,
      "loss": 0.0038,
      "step": 14780
    },
    {
      "epoch": 0.7888,
      "grad_norm": 0.21764087677001953,
      "learning_rate": 3.028e-05,
      "loss": 0.0031,
      "step": 14790
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 0.4250549077987671,
      "learning_rate": 3.0266666666666666e-05,
      "loss": 0.0022,
      "step": 14800
    },
    {
      "epoch": 0.7898666666666667,
      "grad_norm": 0.7626519203186035,
      "learning_rate": 3.0253333333333334e-05,
      "loss": 0.0023,
      "step": 14810
    },
    {
      "epoch": 0.7904,
      "grad_norm": 0.20883087813854218,
      "learning_rate": 3.0240000000000002e-05,
      "loss": 0.0029,
      "step": 14820
    },
    {
      "epoch": 0.7909333333333334,
      "grad_norm": 0.7807728052139282,
      "learning_rate": 3.022666666666667e-05,
      "loss": 0.0036,
      "step": 14830
    },
    {
      "epoch": 0.7914666666666667,
      "grad_norm": 0.28476840257644653,
      "learning_rate": 3.021333333333333e-05,
      "loss": 0.0036,
      "step": 14840
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.4445376694202423,
      "learning_rate": 3.02e-05,
      "loss": 0.0023,
      "step": 14850
    },
    {
      "epoch": 0.7925333333333333,
      "grad_norm": 0.15608561038970947,
      "learning_rate": 3.018666666666667e-05,
      "loss": 0.0025,
      "step": 14860
    },
    {
      "epoch": 0.7930666666666667,
      "grad_norm": 0.28469792008399963,
      "learning_rate": 3.0173333333333338e-05,
      "loss": 0.0028,
      "step": 14870
    },
    {
      "epoch": 0.7936,
      "grad_norm": 0.10318689793348312,
      "learning_rate": 3.016e-05,
      "loss": 0.0024,
      "step": 14880
    },
    {
      "epoch": 0.7941333333333334,
      "grad_norm": 0.2933265268802643,
      "learning_rate": 3.0146666666666667e-05,
      "loss": 0.003,
      "step": 14890
    },
    {
      "epoch": 0.7946666666666666,
      "grad_norm": 0.3471103310585022,
      "learning_rate": 3.0133333333333335e-05,
      "loss": 0.0033,
      "step": 14900
    },
    {
      "epoch": 0.7952,
      "grad_norm": 0.37262362241744995,
      "learning_rate": 3.0120000000000003e-05,
      "loss": 0.0031,
      "step": 14910
    },
    {
      "epoch": 0.7957333333333333,
      "grad_norm": 0.13074371218681335,
      "learning_rate": 3.0106666666666668e-05,
      "loss": 0.0028,
      "step": 14920
    },
    {
      "epoch": 0.7962666666666667,
      "grad_norm": 0.8201713562011719,
      "learning_rate": 3.0093333333333335e-05,
      "loss": 0.0022,
      "step": 14930
    },
    {
      "epoch": 0.7968,
      "grad_norm": 0.45568975806236267,
      "learning_rate": 3.0080000000000003e-05,
      "loss": 0.0031,
      "step": 14940
    },
    {
      "epoch": 0.7973333333333333,
      "grad_norm": 0.8557592034339905,
      "learning_rate": 3.006666666666667e-05,
      "loss": 0.0028,
      "step": 14950
    },
    {
      "epoch": 0.7978666666666666,
      "grad_norm": 0.1542414426803589,
      "learning_rate": 3.0053333333333332e-05,
      "loss": 0.0028,
      "step": 14960
    },
    {
      "epoch": 0.7984,
      "grad_norm": 0.4092092216014862,
      "learning_rate": 3.004e-05,
      "loss": 0.0025,
      "step": 14970
    },
    {
      "epoch": 0.7989333333333334,
      "grad_norm": 0.30250582098960876,
      "learning_rate": 3.0026666666666668e-05,
      "loss": 0.0028,
      "step": 14980
    },
    {
      "epoch": 0.7994666666666667,
      "grad_norm": 0.5132201313972473,
      "learning_rate": 3.0013333333333333e-05,
      "loss": 0.002,
      "step": 14990
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.3657096326351166,
      "learning_rate": 3e-05,
      "loss": 0.0031,
      "step": 15000
    },
    {
      "epoch": 0.8005333333333333,
      "grad_norm": 0.15174078941345215,
      "learning_rate": 2.998666666666667e-05,
      "loss": 0.003,
      "step": 15010
    },
    {
      "epoch": 0.8010666666666667,
      "grad_norm": 0.4496537744998932,
      "learning_rate": 2.9973333333333337e-05,
      "loss": 0.0019,
      "step": 15020
    },
    {
      "epoch": 0.8016,
      "grad_norm": 0.2778288722038269,
      "learning_rate": 2.9959999999999998e-05,
      "loss": 0.0026,
      "step": 15030
    },
    {
      "epoch": 0.8021333333333334,
      "grad_norm": 0.4161335527896881,
      "learning_rate": 2.9946666666666666e-05,
      "loss": 0.0032,
      "step": 15040
    },
    {
      "epoch": 0.8026666666666666,
      "grad_norm": 0.4968676269054413,
      "learning_rate": 2.9933333333333337e-05,
      "loss": 0.0038,
      "step": 15050
    },
    {
      "epoch": 0.8032,
      "grad_norm": 0.5057668685913086,
      "learning_rate": 2.9920000000000005e-05,
      "loss": 0.0021,
      "step": 15060
    },
    {
      "epoch": 0.8037333333333333,
      "grad_norm": 0.37129732966423035,
      "learning_rate": 2.9906666666666666e-05,
      "loss": 0.0026,
      "step": 15070
    },
    {
      "epoch": 0.8042666666666667,
      "grad_norm": 0.5776359438896179,
      "learning_rate": 2.9893333333333334e-05,
      "loss": 0.0023,
      "step": 15080
    },
    {
      "epoch": 0.8048,
      "grad_norm": 0.4371998906135559,
      "learning_rate": 2.9880000000000002e-05,
      "loss": 0.0027,
      "step": 15090
    },
    {
      "epoch": 0.8053333333333333,
      "grad_norm": 0.17833606898784637,
      "learning_rate": 2.986666666666667e-05,
      "loss": 0.0029,
      "step": 15100
    },
    {
      "epoch": 0.8058666666666666,
      "grad_norm": 0.48770657181739807,
      "learning_rate": 2.9853333333333334e-05,
      "loss": 0.0038,
      "step": 15110
    },
    {
      "epoch": 0.8064,
      "grad_norm": 0.38814598321914673,
      "learning_rate": 2.9840000000000002e-05,
      "loss": 0.0026,
      "step": 15120
    },
    {
      "epoch": 0.8069333333333333,
      "grad_norm": 0.2133345603942871,
      "learning_rate": 2.982666666666667e-05,
      "loss": 0.0023,
      "step": 15130
    },
    {
      "epoch": 0.8074666666666667,
      "grad_norm": 0.11640145629644394,
      "learning_rate": 2.981333333333333e-05,
      "loss": 0.0031,
      "step": 15140
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.28319621086120605,
      "learning_rate": 2.98e-05,
      "loss": 0.0024,
      "step": 15150
    },
    {
      "epoch": 0.8085333333333333,
      "grad_norm": 0.2250017374753952,
      "learning_rate": 2.9786666666666667e-05,
      "loss": 0.0024,
      "step": 15160
    },
    {
      "epoch": 0.8090666666666667,
      "grad_norm": 0.356329083442688,
      "learning_rate": 2.9773333333333335e-05,
      "loss": 0.0028,
      "step": 15170
    },
    {
      "epoch": 0.8096,
      "grad_norm": 0.3422424793243408,
      "learning_rate": 2.976e-05,
      "loss": 0.0022,
      "step": 15180
    },
    {
      "epoch": 0.8101333333333334,
      "grad_norm": 0.39749330282211304,
      "learning_rate": 2.9746666666666668e-05,
      "loss": 0.0026,
      "step": 15190
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 0.5172313451766968,
      "learning_rate": 2.9733333333333336e-05,
      "loss": 0.0029,
      "step": 15200
    },
    {
      "epoch": 0.8112,
      "grad_norm": 0.42408740520477295,
      "learning_rate": 2.9720000000000003e-05,
      "loss": 0.0021,
      "step": 15210
    },
    {
      "epoch": 0.8117333333333333,
      "grad_norm": 0.3344794511795044,
      "learning_rate": 2.9706666666666665e-05,
      "loss": 0.0032,
      "step": 15220
    },
    {
      "epoch": 0.8122666666666667,
      "grad_norm": 0.12651538848876953,
      "learning_rate": 2.9693333333333333e-05,
      "loss": 0.0026,
      "step": 15230
    },
    {
      "epoch": 0.8128,
      "grad_norm": 0.7671478390693665,
      "learning_rate": 2.9680000000000004e-05,
      "loss": 0.0031,
      "step": 15240
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.517096221446991,
      "learning_rate": 2.9666666666666672e-05,
      "loss": 0.0031,
      "step": 15250
    },
    {
      "epoch": 0.8138666666666666,
      "grad_norm": 0.4317507743835449,
      "learning_rate": 2.9653333333333333e-05,
      "loss": 0.0026,
      "step": 15260
    },
    {
      "epoch": 0.8144,
      "grad_norm": 0.34250786900520325,
      "learning_rate": 2.964e-05,
      "loss": 0.0029,
      "step": 15270
    },
    {
      "epoch": 0.8149333333333333,
      "grad_norm": 0.322573184967041,
      "learning_rate": 2.962666666666667e-05,
      "loss": 0.0019,
      "step": 15280
    },
    {
      "epoch": 0.8154666666666667,
      "grad_norm": 0.3821013867855072,
      "learning_rate": 2.9613333333333337e-05,
      "loss": 0.0018,
      "step": 15290
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.10361414402723312,
      "learning_rate": 2.96e-05,
      "loss": 0.0025,
      "step": 15300
    },
    {
      "epoch": 0.8165333333333333,
      "grad_norm": 0.30789169669151306,
      "learning_rate": 2.958666666666667e-05,
      "loss": 0.0027,
      "step": 15310
    },
    {
      "epoch": 0.8170666666666667,
      "grad_norm": 0.4257930517196655,
      "learning_rate": 2.9573333333333337e-05,
      "loss": 0.0029,
      "step": 15320
    },
    {
      "epoch": 0.8176,
      "grad_norm": 0.2648215889930725,
      "learning_rate": 2.9559999999999998e-05,
      "loss": 0.0027,
      "step": 15330
    },
    {
      "epoch": 0.8181333333333334,
      "grad_norm": 0.18336139619350433,
      "learning_rate": 2.9546666666666666e-05,
      "loss": 0.0033,
      "step": 15340
    },
    {
      "epoch": 0.8186666666666667,
      "grad_norm": 0.23265747725963593,
      "learning_rate": 2.9533333333333334e-05,
      "loss": 0.0026,
      "step": 15350
    },
    {
      "epoch": 0.8192,
      "grad_norm": 0.6825311183929443,
      "learning_rate": 2.9520000000000002e-05,
      "loss": 0.0031,
      "step": 15360
    },
    {
      "epoch": 0.8197333333333333,
      "grad_norm": 0.15179243683815002,
      "learning_rate": 2.9506666666666667e-05,
      "loss": 0.0025,
      "step": 15370
    },
    {
      "epoch": 0.8202666666666667,
      "grad_norm": 0.19372953474521637,
      "learning_rate": 2.9493333333333334e-05,
      "loss": 0.0023,
      "step": 15380
    },
    {
      "epoch": 0.8208,
      "grad_norm": 0.2583011984825134,
      "learning_rate": 2.9480000000000002e-05,
      "loss": 0.0027,
      "step": 15390
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 0.3083640933036804,
      "learning_rate": 2.946666666666667e-05,
      "loss": 0.0027,
      "step": 15400
    },
    {
      "epoch": 0.8218666666666666,
      "grad_norm": 0.5916658043861389,
      "learning_rate": 2.945333333333333e-05,
      "loss": 0.0028,
      "step": 15410
    },
    {
      "epoch": 0.8224,
      "grad_norm": 0.12878723442554474,
      "learning_rate": 2.944e-05,
      "loss": 0.0021,
      "step": 15420
    },
    {
      "epoch": 0.8229333333333333,
      "grad_norm": 0.5337574481964111,
      "learning_rate": 2.942666666666667e-05,
      "loss": 0.002,
      "step": 15430
    },
    {
      "epoch": 0.8234666666666667,
      "grad_norm": 0.1136789470911026,
      "learning_rate": 2.941333333333334e-05,
      "loss": 0.0032,
      "step": 15440
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.07354532927274704,
      "learning_rate": 2.94e-05,
      "loss": 0.0021,
      "step": 15450
    },
    {
      "epoch": 0.8245333333333333,
      "grad_norm": 0.1349315196275711,
      "learning_rate": 2.9386666666666668e-05,
      "loss": 0.0023,
      "step": 15460
    },
    {
      "epoch": 0.8250666666666666,
      "grad_norm": 0.9294531345367432,
      "learning_rate": 2.9373333333333336e-05,
      "loss": 0.0032,
      "step": 15470
    },
    {
      "epoch": 0.8256,
      "grad_norm": 0.11378206312656403,
      "learning_rate": 2.9360000000000003e-05,
      "loss": 0.0039,
      "step": 15480
    },
    {
      "epoch": 0.8261333333333334,
      "grad_norm": 0.31597891449928284,
      "learning_rate": 2.9346666666666668e-05,
      "loss": 0.004,
      "step": 15490
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.47439780831336975,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 0.0034,
      "step": 15500
    },
    {
      "epoch": 0.8272,
      "grad_norm": 0.1774468719959259,
      "learning_rate": 2.9320000000000004e-05,
      "loss": 0.0021,
      "step": 15510
    },
    {
      "epoch": 0.8277333333333333,
      "grad_norm": 0.30333197116851807,
      "learning_rate": 2.9306666666666665e-05,
      "loss": 0.0032,
      "step": 15520
    },
    {
      "epoch": 0.8282666666666667,
      "grad_norm": 0.2213980108499527,
      "learning_rate": 2.9293333333333333e-05,
      "loss": 0.0023,
      "step": 15530
    },
    {
      "epoch": 0.8288,
      "grad_norm": 0.13729967176914215,
      "learning_rate": 2.928e-05,
      "loss": 0.0019,
      "step": 15540
    },
    {
      "epoch": 0.8293333333333334,
      "grad_norm": 0.9914697408676147,
      "learning_rate": 2.926666666666667e-05,
      "loss": 0.0021,
      "step": 15550
    },
    {
      "epoch": 0.8298666666666666,
      "grad_norm": 0.17131787538528442,
      "learning_rate": 2.9253333333333333e-05,
      "loss": 0.0031,
      "step": 15560
    },
    {
      "epoch": 0.8304,
      "grad_norm": 0.12975271046161652,
      "learning_rate": 2.924e-05,
      "loss": 0.002,
      "step": 15570
    },
    {
      "epoch": 0.8309333333333333,
      "grad_norm": 0.23469747602939606,
      "learning_rate": 2.922666666666667e-05,
      "loss": 0.0031,
      "step": 15580
    },
    {
      "epoch": 0.8314666666666667,
      "grad_norm": 0.4007129669189453,
      "learning_rate": 2.9213333333333337e-05,
      "loss": 0.0021,
      "step": 15590
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.7221958637237549,
      "learning_rate": 2.9199999999999998e-05,
      "loss": 0.003,
      "step": 15600
    },
    {
      "epoch": 0.8325333333333333,
      "grad_norm": 0.26832106709480286,
      "learning_rate": 2.9186666666666666e-05,
      "loss": 0.0027,
      "step": 15610
    },
    {
      "epoch": 0.8330666666666666,
      "grad_norm": 0.11754627525806427,
      "learning_rate": 2.9173333333333337e-05,
      "loss": 0.0024,
      "step": 15620
    },
    {
      "epoch": 0.8336,
      "grad_norm": 0.29502177238464355,
      "learning_rate": 2.9160000000000005e-05,
      "loss": 0.0018,
      "step": 15630
    },
    {
      "epoch": 0.8341333333333333,
      "grad_norm": 0.5576638579368591,
      "learning_rate": 2.9146666666666667e-05,
      "loss": 0.0023,
      "step": 15640
    },
    {
      "epoch": 0.8346666666666667,
      "grad_norm": 0.3010815978050232,
      "learning_rate": 2.9133333333333334e-05,
      "loss": 0.003,
      "step": 15650
    },
    {
      "epoch": 0.8352,
      "grad_norm": 0.13274648785591125,
      "learning_rate": 2.9120000000000002e-05,
      "loss": 0.0017,
      "step": 15660
    },
    {
      "epoch": 0.8357333333333333,
      "grad_norm": 0.4235098659992218,
      "learning_rate": 2.9106666666666667e-05,
      "loss": 0.0029,
      "step": 15670
    },
    {
      "epoch": 0.8362666666666667,
      "grad_norm": 0.17559020221233368,
      "learning_rate": 2.9093333333333335e-05,
      "loss": 0.0021,
      "step": 15680
    },
    {
      "epoch": 0.8368,
      "grad_norm": 0.26402297616004944,
      "learning_rate": 2.9080000000000003e-05,
      "loss": 0.0017,
      "step": 15690
    },
    {
      "epoch": 0.8373333333333334,
      "grad_norm": 0.2111649215221405,
      "learning_rate": 2.906666666666667e-05,
      "loss": 0.0021,
      "step": 15700
    },
    {
      "epoch": 0.8378666666666666,
      "grad_norm": 0.4817759692668915,
      "learning_rate": 2.9053333333333332e-05,
      "loss": 0.0035,
      "step": 15710
    },
    {
      "epoch": 0.8384,
      "grad_norm": 0.6967407464981079,
      "learning_rate": 2.904e-05,
      "loss": 0.0023,
      "step": 15720
    },
    {
      "epoch": 0.8389333333333333,
      "grad_norm": 0.8292334079742432,
      "learning_rate": 2.9026666666666668e-05,
      "loss": 0.0031,
      "step": 15730
    },
    {
      "epoch": 0.8394666666666667,
      "grad_norm": 0.20271919667720795,
      "learning_rate": 2.9013333333333336e-05,
      "loss": 0.002,
      "step": 15740
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.3388337790966034,
      "learning_rate": 2.9e-05,
      "loss": 0.0023,
      "step": 15750
    },
    {
      "epoch": 0.8405333333333334,
      "grad_norm": 0.26619666814804077,
      "learning_rate": 2.8986666666666668e-05,
      "loss": 0.0032,
      "step": 15760
    },
    {
      "epoch": 0.8410666666666666,
      "grad_norm": 0.43686723709106445,
      "learning_rate": 2.8973333333333336e-05,
      "loss": 0.0027,
      "step": 15770
    },
    {
      "epoch": 0.8416,
      "grad_norm": 0.5794420838356018,
      "learning_rate": 2.8960000000000004e-05,
      "loss": 0.003,
      "step": 15780
    },
    {
      "epoch": 0.8421333333333333,
      "grad_norm": 0.05975750833749771,
      "learning_rate": 2.8946666666666665e-05,
      "loss": 0.0034,
      "step": 15790
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 0.20062878727912903,
      "learning_rate": 2.8933333333333333e-05,
      "loss": 0.0026,
      "step": 15800
    },
    {
      "epoch": 0.8432,
      "grad_norm": 0.09090312570333481,
      "learning_rate": 2.8920000000000004e-05,
      "loss": 0.0024,
      "step": 15810
    },
    {
      "epoch": 0.8437333333333333,
      "grad_norm": 0.05553918704390526,
      "learning_rate": 2.8906666666666672e-05,
      "loss": 0.0019,
      "step": 15820
    },
    {
      "epoch": 0.8442666666666667,
      "grad_norm": 0.47605764865875244,
      "learning_rate": 2.8893333333333333e-05,
      "loss": 0.0023,
      "step": 15830
    },
    {
      "epoch": 0.8448,
      "grad_norm": 0.25674256682395935,
      "learning_rate": 2.888e-05,
      "loss": 0.0019,
      "step": 15840
    },
    {
      "epoch": 0.8453333333333334,
      "grad_norm": 0.26203784346580505,
      "learning_rate": 2.886666666666667e-05,
      "loss": 0.0027,
      "step": 15850
    },
    {
      "epoch": 0.8458666666666667,
      "grad_norm": 0.5327005982398987,
      "learning_rate": 2.8853333333333334e-05,
      "loss": 0.0019,
      "step": 15860
    },
    {
      "epoch": 0.8464,
      "grad_norm": 0.253217488527298,
      "learning_rate": 2.8840000000000002e-05,
      "loss": 0.0031,
      "step": 15870
    },
    {
      "epoch": 0.8469333333333333,
      "grad_norm": 0.40384620428085327,
      "learning_rate": 2.882666666666667e-05,
      "loss": 0.0035,
      "step": 15880
    },
    {
      "epoch": 0.8474666666666667,
      "grad_norm": 0.27109646797180176,
      "learning_rate": 2.8813333333333338e-05,
      "loss": 0.0021,
      "step": 15890
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.08066165447235107,
      "learning_rate": 2.88e-05,
      "loss": 0.0021,
      "step": 15900
    },
    {
      "epoch": 0.8485333333333334,
      "grad_norm": 0.24864697456359863,
      "learning_rate": 2.8786666666666667e-05,
      "loss": 0.0027,
      "step": 15910
    },
    {
      "epoch": 0.8490666666666666,
      "grad_norm": 0.5462822318077087,
      "learning_rate": 2.8773333333333335e-05,
      "loss": 0.0038,
      "step": 15920
    },
    {
      "epoch": 0.8496,
      "grad_norm": 0.5016688704490662,
      "learning_rate": 2.8760000000000002e-05,
      "loss": 0.0025,
      "step": 15930
    },
    {
      "epoch": 0.8501333333333333,
      "grad_norm": 0.233836829662323,
      "learning_rate": 2.8746666666666667e-05,
      "loss": 0.0021,
      "step": 15940
    },
    {
      "epoch": 0.8506666666666667,
      "grad_norm": 0.07876185327768326,
      "learning_rate": 2.8733333333333335e-05,
      "loss": 0.0025,
      "step": 15950
    },
    {
      "epoch": 0.8512,
      "grad_norm": 0.31883594393730164,
      "learning_rate": 2.8720000000000003e-05,
      "loss": 0.0028,
      "step": 15960
    },
    {
      "epoch": 0.8517333333333333,
      "grad_norm": 0.3779296576976776,
      "learning_rate": 2.870666666666667e-05,
      "loss": 0.002,
      "step": 15970
    },
    {
      "epoch": 0.8522666666666666,
      "grad_norm": 0.22165144979953766,
      "learning_rate": 2.8693333333333332e-05,
      "loss": 0.0019,
      "step": 15980
    },
    {
      "epoch": 0.8528,
      "grad_norm": 0.42518875002861023,
      "learning_rate": 2.868e-05,
      "loss": 0.0024,
      "step": 15990
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.34073105454444885,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 0.0024,
      "step": 16000
    },
    {
      "epoch": 0.8538666666666667,
      "grad_norm": 0.5638603568077087,
      "learning_rate": 2.8653333333333332e-05,
      "loss": 0.0026,
      "step": 16010
    },
    {
      "epoch": 0.8544,
      "grad_norm": 1.0338104963302612,
      "learning_rate": 2.864e-05,
      "loss": 0.0034,
      "step": 16020
    },
    {
      "epoch": 0.8549333333333333,
      "grad_norm": 0.30234935879707336,
      "learning_rate": 2.8626666666666668e-05,
      "loss": 0.002,
      "step": 16030
    },
    {
      "epoch": 0.8554666666666667,
      "grad_norm": 0.09575657546520233,
      "learning_rate": 2.8613333333333336e-05,
      "loss": 0.002,
      "step": 16040
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.30388882756233215,
      "learning_rate": 2.86e-05,
      "loss": 0.0039,
      "step": 16050
    },
    {
      "epoch": 0.8565333333333334,
      "grad_norm": 0.3843291401863098,
      "learning_rate": 2.858666666666667e-05,
      "loss": 0.0026,
      "step": 16060
    },
    {
      "epoch": 0.8570666666666666,
      "grad_norm": 0.15982088446617126,
      "learning_rate": 2.8573333333333336e-05,
      "loss": 0.0024,
      "step": 16070
    },
    {
      "epoch": 0.8576,
      "grad_norm": 0.10959837585687637,
      "learning_rate": 2.8560000000000004e-05,
      "loss": 0.0022,
      "step": 16080
    },
    {
      "epoch": 0.8581333333333333,
      "grad_norm": 0.6911399960517883,
      "learning_rate": 2.8546666666666666e-05,
      "loss": 0.0027,
      "step": 16090
    },
    {
      "epoch": 0.8586666666666667,
      "grad_norm": 0.2718484103679657,
      "learning_rate": 2.8533333333333333e-05,
      "loss": 0.0036,
      "step": 16100
    },
    {
      "epoch": 0.8592,
      "grad_norm": 0.14682185649871826,
      "learning_rate": 2.852e-05,
      "loss": 0.0022,
      "step": 16110
    },
    {
      "epoch": 0.8597333333333333,
      "grad_norm": 0.5458956360816956,
      "learning_rate": 2.850666666666667e-05,
      "loss": 0.002,
      "step": 16120
    },
    {
      "epoch": 0.8602666666666666,
      "grad_norm": 0.13957475125789642,
      "learning_rate": 2.8493333333333334e-05,
      "loss": 0.0032,
      "step": 16130
    },
    {
      "epoch": 0.8608,
      "grad_norm": 0.5640490055084229,
      "learning_rate": 2.8480000000000002e-05,
      "loss": 0.0018,
      "step": 16140
    },
    {
      "epoch": 0.8613333333333333,
      "grad_norm": 0.40175876021385193,
      "learning_rate": 2.846666666666667e-05,
      "loss": 0.0027,
      "step": 16150
    },
    {
      "epoch": 0.8618666666666667,
      "grad_norm": 0.23535919189453125,
      "learning_rate": 2.8453333333333338e-05,
      "loss": 0.0026,
      "step": 16160
    },
    {
      "epoch": 0.8624,
      "grad_norm": 0.5639955997467041,
      "learning_rate": 2.844e-05,
      "loss": 0.0025,
      "step": 16170
    },
    {
      "epoch": 0.8629333333333333,
      "grad_norm": 0.37678104639053345,
      "learning_rate": 2.8426666666666667e-05,
      "loss": 0.0027,
      "step": 16180
    },
    {
      "epoch": 0.8634666666666667,
      "grad_norm": 0.4760420024394989,
      "learning_rate": 2.8413333333333335e-05,
      "loss": 0.0038,
      "step": 16190
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.34372639656066895,
      "learning_rate": 2.84e-05,
      "loss": 0.002,
      "step": 16200
    },
    {
      "epoch": 0.8645333333333334,
      "grad_norm": 0.10724711418151855,
      "learning_rate": 2.8386666666666667e-05,
      "loss": 0.0029,
      "step": 16210
    },
    {
      "epoch": 0.8650666666666667,
      "grad_norm": 0.17266972362995148,
      "learning_rate": 2.8373333333333335e-05,
      "loss": 0.0023,
      "step": 16220
    },
    {
      "epoch": 0.8656,
      "grad_norm": 0.1487116664648056,
      "learning_rate": 2.8360000000000003e-05,
      "loss": 0.0019,
      "step": 16230
    },
    {
      "epoch": 0.8661333333333333,
      "grad_norm": 0.1487601399421692,
      "learning_rate": 2.8346666666666667e-05,
      "loss": 0.0023,
      "step": 16240
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.634352445602417,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 0.0028,
      "step": 16250
    },
    {
      "epoch": 0.8672,
      "grad_norm": 0.26858705282211304,
      "learning_rate": 2.8320000000000003e-05,
      "loss": 0.0027,
      "step": 16260
    },
    {
      "epoch": 0.8677333333333334,
      "grad_norm": 0.13569271564483643,
      "learning_rate": 2.830666666666667e-05,
      "loss": 0.0033,
      "step": 16270
    },
    {
      "epoch": 0.8682666666666666,
      "grad_norm": 0.38015016913414,
      "learning_rate": 2.8293333333333332e-05,
      "loss": 0.0026,
      "step": 16280
    },
    {
      "epoch": 0.8688,
      "grad_norm": 0.1301502287387848,
      "learning_rate": 2.828e-05,
      "loss": 0.0022,
      "step": 16290
    },
    {
      "epoch": 0.8693333333333333,
      "grad_norm": 0.28270551562309265,
      "learning_rate": 2.8266666666666668e-05,
      "loss": 0.003,
      "step": 16300
    },
    {
      "epoch": 0.8698666666666667,
      "grad_norm": 0.1972135305404663,
      "learning_rate": 2.8253333333333336e-05,
      "loss": 0.0034,
      "step": 16310
    },
    {
      "epoch": 0.8704,
      "grad_norm": 0.1635511964559555,
      "learning_rate": 2.824e-05,
      "loss": 0.003,
      "step": 16320
    },
    {
      "epoch": 0.8709333333333333,
      "grad_norm": 0.5734696984291077,
      "learning_rate": 2.822666666666667e-05,
      "loss": 0.0043,
      "step": 16330
    },
    {
      "epoch": 0.8714666666666666,
      "grad_norm": 0.2282659113407135,
      "learning_rate": 2.8213333333333337e-05,
      "loss": 0.003,
      "step": 16340
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.2736506760120392,
      "learning_rate": 2.8199999999999998e-05,
      "loss": 0.002,
      "step": 16350
    },
    {
      "epoch": 0.8725333333333334,
      "grad_norm": 0.19130973517894745,
      "learning_rate": 2.8186666666666666e-05,
      "loss": 0.0029,
      "step": 16360
    },
    {
      "epoch": 0.8730666666666667,
      "grad_norm": 0.21102584898471832,
      "learning_rate": 2.8173333333333334e-05,
      "loss": 0.0022,
      "step": 16370
    },
    {
      "epoch": 0.8736,
      "grad_norm": 0.4944917857646942,
      "learning_rate": 2.816e-05,
      "loss": 0.0019,
      "step": 16380
    },
    {
      "epoch": 0.8741333333333333,
      "grad_norm": 0.20050978660583496,
      "learning_rate": 2.8146666666666666e-05,
      "loss": 0.0025,
      "step": 16390
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 0.634516179561615,
      "learning_rate": 2.8133333333333334e-05,
      "loss": 0.0021,
      "step": 16400
    },
    {
      "epoch": 0.8752,
      "grad_norm": 0.2802688181400299,
      "learning_rate": 2.8120000000000002e-05,
      "loss": 0.0021,
      "step": 16410
    },
    {
      "epoch": 0.8757333333333334,
      "grad_norm": 0.6268939971923828,
      "learning_rate": 2.810666666666667e-05,
      "loss": 0.0039,
      "step": 16420
    },
    {
      "epoch": 0.8762666666666666,
      "grad_norm": 0.3291098475456238,
      "learning_rate": 2.8093333333333334e-05,
      "loss": 0.0028,
      "step": 16430
    },
    {
      "epoch": 0.8768,
      "grad_norm": 0.22984112799167633,
      "learning_rate": 2.8080000000000002e-05,
      "loss": 0.0022,
      "step": 16440
    },
    {
      "epoch": 0.8773333333333333,
      "grad_norm": 0.6884062886238098,
      "learning_rate": 2.806666666666667e-05,
      "loss": 0.0031,
      "step": 16450
    },
    {
      "epoch": 0.8778666666666667,
      "grad_norm": 0.9256024956703186,
      "learning_rate": 2.8053333333333338e-05,
      "loss": 0.003,
      "step": 16460
    },
    {
      "epoch": 0.8784,
      "grad_norm": 0.5298159718513489,
      "learning_rate": 2.804e-05,
      "loss": 0.0028,
      "step": 16470
    },
    {
      "epoch": 0.8789333333333333,
      "grad_norm": 0.34496429562568665,
      "learning_rate": 2.8026666666666667e-05,
      "loss": 0.0032,
      "step": 16480
    },
    {
      "epoch": 0.8794666666666666,
      "grad_norm": 0.5821647644042969,
      "learning_rate": 2.8013333333333335e-05,
      "loss": 0.0023,
      "step": 16490
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.3584514558315277,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.0023,
      "step": 16500
    },
    {
      "epoch": 0.8805333333333333,
      "grad_norm": 0.31014004349708557,
      "learning_rate": 2.7986666666666668e-05,
      "loss": 0.0027,
      "step": 16510
    },
    {
      "epoch": 0.8810666666666667,
      "grad_norm": 0.4091258645057678,
      "learning_rate": 2.7973333333333335e-05,
      "loss": 0.0033,
      "step": 16520
    },
    {
      "epoch": 0.8816,
      "grad_norm": 0.33002716302871704,
      "learning_rate": 2.7960000000000003e-05,
      "loss": 0.0023,
      "step": 16530
    },
    {
      "epoch": 0.8821333333333333,
      "grad_norm": 0.07817483693361282,
      "learning_rate": 2.7946666666666664e-05,
      "loss": 0.0034,
      "step": 16540
    },
    {
      "epoch": 0.8826666666666667,
      "grad_norm": 0.20220714807510376,
      "learning_rate": 2.7933333333333332e-05,
      "loss": 0.0028,
      "step": 16550
    },
    {
      "epoch": 0.8832,
      "grad_norm": 0.27135106921195984,
      "learning_rate": 2.792e-05,
      "loss": 0.0029,
      "step": 16560
    },
    {
      "epoch": 0.8837333333333334,
      "grad_norm": 0.11390180885791779,
      "learning_rate": 2.7906666666666668e-05,
      "loss": 0.0027,
      "step": 16570
    },
    {
      "epoch": 0.8842666666666666,
      "grad_norm": 0.06005137413740158,
      "learning_rate": 2.7893333333333333e-05,
      "loss": 0.0021,
      "step": 16580
    },
    {
      "epoch": 0.8848,
      "grad_norm": 0.17387589812278748,
      "learning_rate": 2.788e-05,
      "loss": 0.0025,
      "step": 16590
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 0.14574569463729858,
      "learning_rate": 2.786666666666667e-05,
      "loss": 0.002,
      "step": 16600
    },
    {
      "epoch": 0.8858666666666667,
      "grad_norm": 0.1083126962184906,
      "learning_rate": 2.7853333333333337e-05,
      "loss": 0.0028,
      "step": 16610
    },
    {
      "epoch": 0.8864,
      "grad_norm": 0.410922646522522,
      "learning_rate": 2.7839999999999998e-05,
      "loss": 0.0021,
      "step": 16620
    },
    {
      "epoch": 0.8869333333333334,
      "grad_norm": 0.4929693937301636,
      "learning_rate": 2.782666666666667e-05,
      "loss": 0.0029,
      "step": 16630
    },
    {
      "epoch": 0.8874666666666666,
      "grad_norm": 0.48265334963798523,
      "learning_rate": 2.7813333333333337e-05,
      "loss": 0.0023,
      "step": 16640
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.37990424036979675,
      "learning_rate": 2.7800000000000005e-05,
      "loss": 0.0033,
      "step": 16650
    },
    {
      "epoch": 0.8885333333333333,
      "grad_norm": 0.22672490775585175,
      "learning_rate": 2.7786666666666666e-05,
      "loss": 0.0022,
      "step": 16660
    },
    {
      "epoch": 0.8890666666666667,
      "grad_norm": 0.22972983121871948,
      "learning_rate": 2.7773333333333334e-05,
      "loss": 0.002,
      "step": 16670
    },
    {
      "epoch": 0.8896,
      "grad_norm": 0.101588673889637,
      "learning_rate": 2.7760000000000002e-05,
      "loss": 0.0021,
      "step": 16680
    },
    {
      "epoch": 0.8901333333333333,
      "grad_norm": 0.29063698649406433,
      "learning_rate": 2.7746666666666666e-05,
      "loss": 0.0021,
      "step": 16690
    },
    {
      "epoch": 0.8906666666666667,
      "grad_norm": 0.20410944521427155,
      "learning_rate": 2.7733333333333334e-05,
      "loss": 0.0027,
      "step": 16700
    },
    {
      "epoch": 0.8912,
      "grad_norm": 0.4380239248275757,
      "learning_rate": 2.7720000000000002e-05,
      "loss": 0.002,
      "step": 16710
    },
    {
      "epoch": 0.8917333333333334,
      "grad_norm": 0.5269314050674438,
      "learning_rate": 2.770666666666667e-05,
      "loss": 0.0027,
      "step": 16720
    },
    {
      "epoch": 0.8922666666666667,
      "grad_norm": 0.3017457127571106,
      "learning_rate": 2.769333333333333e-05,
      "loss": 0.0018,
      "step": 16730
    },
    {
      "epoch": 0.8928,
      "grad_norm": 0.14868959784507751,
      "learning_rate": 2.768e-05,
      "loss": 0.0039,
      "step": 16740
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.10265173763036728,
      "learning_rate": 2.7666666666666667e-05,
      "loss": 0.0029,
      "step": 16750
    },
    {
      "epoch": 0.8938666666666667,
      "grad_norm": 0.44227010011672974,
      "learning_rate": 2.7653333333333335e-05,
      "loss": 0.0031,
      "step": 16760
    },
    {
      "epoch": 0.8944,
      "grad_norm": 0.23497574031352997,
      "learning_rate": 2.764e-05,
      "loss": 0.002,
      "step": 16770
    },
    {
      "epoch": 0.8949333333333334,
      "grad_norm": 0.1069670245051384,
      "learning_rate": 2.7626666666666668e-05,
      "loss": 0.0019,
      "step": 16780
    },
    {
      "epoch": 0.8954666666666666,
      "grad_norm": 0.09126666933298111,
      "learning_rate": 2.7613333333333335e-05,
      "loss": 0.0022,
      "step": 16790
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.061036404222249985,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 0.002,
      "step": 16800
    },
    {
      "epoch": 0.8965333333333333,
      "grad_norm": 0.28955885767936707,
      "learning_rate": 2.7586666666666665e-05,
      "loss": 0.0028,
      "step": 16810
    },
    {
      "epoch": 0.8970666666666667,
      "grad_norm": 0.5218839645385742,
      "learning_rate": 2.7573333333333336e-05,
      "loss": 0.0029,
      "step": 16820
    },
    {
      "epoch": 0.8976,
      "grad_norm": 0.16778211295604706,
      "learning_rate": 2.7560000000000004e-05,
      "loss": 0.0028,
      "step": 16830
    },
    {
      "epoch": 0.8981333333333333,
      "grad_norm": 0.09074299782514572,
      "learning_rate": 2.7546666666666672e-05,
      "loss": 0.0028,
      "step": 16840
    },
    {
      "epoch": 0.8986666666666666,
      "grad_norm": 0.31432250142097473,
      "learning_rate": 2.7533333333333333e-05,
      "loss": 0.0029,
      "step": 16850
    },
    {
      "epoch": 0.8992,
      "grad_norm": 0.434967041015625,
      "learning_rate": 2.752e-05,
      "loss": 0.0033,
      "step": 16860
    },
    {
      "epoch": 0.8997333333333334,
      "grad_norm": 0.43788254261016846,
      "learning_rate": 2.750666666666667e-05,
      "loss": 0.0031,
      "step": 16870
    },
    {
      "epoch": 0.9002666666666667,
      "grad_norm": 0.12843868136405945,
      "learning_rate": 2.7493333333333333e-05,
      "loss": 0.0022,
      "step": 16880
    },
    {
      "epoch": 0.9008,
      "grad_norm": 0.4594508409500122,
      "learning_rate": 2.748e-05,
      "loss": 0.003,
      "step": 16890
    },
    {
      "epoch": 0.9013333333333333,
      "grad_norm": 0.31643787026405334,
      "learning_rate": 2.746666666666667e-05,
      "loss": 0.0028,
      "step": 16900
    },
    {
      "epoch": 0.9018666666666667,
      "grad_norm": 0.3321859538555145,
      "learning_rate": 2.7453333333333337e-05,
      "loss": 0.003,
      "step": 16910
    },
    {
      "epoch": 0.9024,
      "grad_norm": 0.1765642762184143,
      "learning_rate": 2.7439999999999998e-05,
      "loss": 0.003,
      "step": 16920
    },
    {
      "epoch": 0.9029333333333334,
      "grad_norm": 0.5547363758087158,
      "learning_rate": 2.7426666666666666e-05,
      "loss": 0.0032,
      "step": 16930
    },
    {
      "epoch": 0.9034666666666666,
      "grad_norm": 0.48217543959617615,
      "learning_rate": 2.7413333333333334e-05,
      "loss": 0.0028,
      "step": 16940
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.1069994568824768,
      "learning_rate": 2.7400000000000002e-05,
      "loss": 0.003,
      "step": 16950
    },
    {
      "epoch": 0.9045333333333333,
      "grad_norm": 0.1891525685787201,
      "learning_rate": 2.7386666666666666e-05,
      "loss": 0.0021,
      "step": 16960
    },
    {
      "epoch": 0.9050666666666667,
      "grad_norm": 0.1645781546831131,
      "learning_rate": 2.7373333333333334e-05,
      "loss": 0.0028,
      "step": 16970
    },
    {
      "epoch": 0.9056,
      "grad_norm": 0.5726945400238037,
      "learning_rate": 2.7360000000000002e-05,
      "loss": 0.0025,
      "step": 16980
    },
    {
      "epoch": 0.9061333333333333,
      "grad_norm": 0.3010256588459015,
      "learning_rate": 2.734666666666667e-05,
      "loss": 0.0022,
      "step": 16990
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.11381051689386368,
      "learning_rate": 2.733333333333333e-05,
      "loss": 0.0024,
      "step": 17000
    },
    {
      "epoch": 0.9072,
      "grad_norm": 0.1943005621433258,
      "learning_rate": 2.7320000000000003e-05,
      "loss": 0.0027,
      "step": 17010
    },
    {
      "epoch": 0.9077333333333333,
      "grad_norm": 0.2674741744995117,
      "learning_rate": 2.730666666666667e-05,
      "loss": 0.0023,
      "step": 17020
    },
    {
      "epoch": 0.9082666666666667,
      "grad_norm": 0.5328401923179626,
      "learning_rate": 2.7293333333333332e-05,
      "loss": 0.002,
      "step": 17030
    },
    {
      "epoch": 0.9088,
      "grad_norm": 0.2286682426929474,
      "learning_rate": 2.728e-05,
      "loss": 0.0029,
      "step": 17040
    },
    {
      "epoch": 0.9093333333333333,
      "grad_norm": 0.4813365638256073,
      "learning_rate": 2.7266666666666668e-05,
      "loss": 0.0022,
      "step": 17050
    },
    {
      "epoch": 0.9098666666666667,
      "grad_norm": 0.6166380047798157,
      "learning_rate": 2.7253333333333336e-05,
      "loss": 0.0025,
      "step": 17060
    },
    {
      "epoch": 0.9104,
      "grad_norm": 0.0836087018251419,
      "learning_rate": 2.724e-05,
      "loss": 0.0026,
      "step": 17070
    },
    {
      "epoch": 0.9109333333333334,
      "grad_norm": 0.21207605302333832,
      "learning_rate": 2.7226666666666668e-05,
      "loss": 0.0025,
      "step": 17080
    },
    {
      "epoch": 0.9114666666666666,
      "grad_norm": 0.4217054545879364,
      "learning_rate": 2.7213333333333336e-05,
      "loss": 0.0036,
      "step": 17090
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.5228787064552307,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 0.004,
      "step": 17100
    },
    {
      "epoch": 0.9125333333333333,
      "grad_norm": 0.37753522396087646,
      "learning_rate": 2.7186666666666665e-05,
      "loss": 0.0034,
      "step": 17110
    },
    {
      "epoch": 0.9130666666666667,
      "grad_norm": 0.27350571751594543,
      "learning_rate": 2.7173333333333333e-05,
      "loss": 0.0039,
      "step": 17120
    },
    {
      "epoch": 0.9136,
      "grad_norm": 0.2798725962638855,
      "learning_rate": 2.716e-05,
      "loss": 0.0027,
      "step": 17130
    },
    {
      "epoch": 0.9141333333333334,
      "grad_norm": 0.6067729592323303,
      "learning_rate": 2.714666666666667e-05,
      "loss": 0.0023,
      "step": 17140
    },
    {
      "epoch": 0.9146666666666666,
      "grad_norm": 0.05673973262310028,
      "learning_rate": 2.7133333333333333e-05,
      "loss": 0.0019,
      "step": 17150
    },
    {
      "epoch": 0.9152,
      "grad_norm": 0.5183548331260681,
      "learning_rate": 2.712e-05,
      "loss": 0.0036,
      "step": 17160
    },
    {
      "epoch": 0.9157333333333333,
      "grad_norm": 0.08441738784313202,
      "learning_rate": 2.710666666666667e-05,
      "loss": 0.0027,
      "step": 17170
    },
    {
      "epoch": 0.9162666666666667,
      "grad_norm": 0.2149711400270462,
      "learning_rate": 2.7093333333333337e-05,
      "loss": 0.0023,
      "step": 17180
    },
    {
      "epoch": 0.9168,
      "grad_norm": 0.2525487244129181,
      "learning_rate": 2.7079999999999998e-05,
      "loss": 0.0029,
      "step": 17190
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 0.602008044719696,
      "learning_rate": 2.706666666666667e-05,
      "loss": 0.0032,
      "step": 17200
    },
    {
      "epoch": 0.9178666666666667,
      "grad_norm": 0.5322058796882629,
      "learning_rate": 2.7053333333333337e-05,
      "loss": 0.0018,
      "step": 17210
    },
    {
      "epoch": 0.9184,
      "grad_norm": 0.27673375606536865,
      "learning_rate": 2.704e-05,
      "loss": 0.0019,
      "step": 17220
    },
    {
      "epoch": 0.9189333333333334,
      "grad_norm": 0.13662724196910858,
      "learning_rate": 2.7026666666666667e-05,
      "loss": 0.0045,
      "step": 17230
    },
    {
      "epoch": 0.9194666666666667,
      "grad_norm": 0.21746022999286652,
      "learning_rate": 2.7013333333333334e-05,
      "loss": 0.0025,
      "step": 17240
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.28126344084739685,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.0029,
      "step": 17250
    },
    {
      "epoch": 0.9205333333333333,
      "grad_norm": 0.4471457004547119,
      "learning_rate": 2.6986666666666667e-05,
      "loss": 0.005,
      "step": 17260
    },
    {
      "epoch": 0.9210666666666667,
      "grad_norm": 0.21536093950271606,
      "learning_rate": 2.6973333333333335e-05,
      "loss": 0.0019,
      "step": 17270
    },
    {
      "epoch": 0.9216,
      "grad_norm": 0.47031697630882263,
      "learning_rate": 2.6960000000000003e-05,
      "loss": 0.0033,
      "step": 17280
    },
    {
      "epoch": 0.9221333333333334,
      "grad_norm": 0.29231971502304077,
      "learning_rate": 2.694666666666667e-05,
      "loss": 0.0035,
      "step": 17290
    },
    {
      "epoch": 0.9226666666666666,
      "grad_norm": 0.093253955245018,
      "learning_rate": 2.6933333333333332e-05,
      "loss": 0.002,
      "step": 17300
    },
    {
      "epoch": 0.9232,
      "grad_norm": 0.368738055229187,
      "learning_rate": 2.692e-05,
      "loss": 0.004,
      "step": 17310
    },
    {
      "epoch": 0.9237333333333333,
      "grad_norm": 0.08360602706670761,
      "learning_rate": 2.6906666666666668e-05,
      "loss": 0.0018,
      "step": 17320
    },
    {
      "epoch": 0.9242666666666667,
      "grad_norm": 0.38362982869148254,
      "learning_rate": 2.6893333333333336e-05,
      "loss": 0.0021,
      "step": 17330
    },
    {
      "epoch": 0.9248,
      "grad_norm": 0.14717260003089905,
      "learning_rate": 2.688e-05,
      "loss": 0.0021,
      "step": 17340
    },
    {
      "epoch": 0.9253333333333333,
      "grad_norm": 0.12066910415887833,
      "learning_rate": 2.6866666666666668e-05,
      "loss": 0.0027,
      "step": 17350
    },
    {
      "epoch": 0.9258666666666666,
      "grad_norm": 0.5497219562530518,
      "learning_rate": 2.6853333333333336e-05,
      "loss": 0.0022,
      "step": 17360
    },
    {
      "epoch": 0.9264,
      "grad_norm": 0.4390554130077362,
      "learning_rate": 2.6840000000000004e-05,
      "loss": 0.0019,
      "step": 17370
    },
    {
      "epoch": 0.9269333333333334,
      "grad_norm": 0.3622197210788727,
      "learning_rate": 2.6826666666666665e-05,
      "loss": 0.0026,
      "step": 17380
    },
    {
      "epoch": 0.9274666666666667,
      "grad_norm": 0.1449432373046875,
      "learning_rate": 2.6813333333333336e-05,
      "loss": 0.0026,
      "step": 17390
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.2347296178340912,
      "learning_rate": 2.6800000000000004e-05,
      "loss": 0.0035,
      "step": 17400
    },
    {
      "epoch": 0.9285333333333333,
      "grad_norm": 0.49552860856056213,
      "learning_rate": 2.6786666666666665e-05,
      "loss": 0.0021,
      "step": 17410
    },
    {
      "epoch": 0.9290666666666667,
      "grad_norm": 0.27132636308670044,
      "learning_rate": 2.6773333333333333e-05,
      "loss": 0.0022,
      "step": 17420
    },
    {
      "epoch": 0.9296,
      "grad_norm": 0.29003772139549255,
      "learning_rate": 2.676e-05,
      "loss": 0.0019,
      "step": 17430
    },
    {
      "epoch": 0.9301333333333334,
      "grad_norm": 0.2786577343940735,
      "learning_rate": 2.674666666666667e-05,
      "loss": 0.0021,
      "step": 17440
    },
    {
      "epoch": 0.9306666666666666,
      "grad_norm": 0.43821126222610474,
      "learning_rate": 2.6733333333333334e-05,
      "loss": 0.0018,
      "step": 17450
    },
    {
      "epoch": 0.9312,
      "grad_norm": 0.33800527453422546,
      "learning_rate": 2.672e-05,
      "loss": 0.0017,
      "step": 17460
    },
    {
      "epoch": 0.9317333333333333,
      "grad_norm": 0.45148834586143494,
      "learning_rate": 2.670666666666667e-05,
      "loss": 0.0039,
      "step": 17470
    },
    {
      "epoch": 0.9322666666666667,
      "grad_norm": 0.20881888270378113,
      "learning_rate": 2.6693333333333338e-05,
      "loss": 0.0028,
      "step": 17480
    },
    {
      "epoch": 0.9328,
      "grad_norm": 0.38114917278289795,
      "learning_rate": 2.668e-05,
      "loss": 0.002,
      "step": 17490
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.12310586124658585,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.0019,
      "step": 17500
    },
    {
      "epoch": 0.9338666666666666,
      "grad_norm": 0.1920006424188614,
      "learning_rate": 2.6653333333333335e-05,
      "loss": 0.0028,
      "step": 17510
    },
    {
      "epoch": 0.9344,
      "grad_norm": 0.600916862487793,
      "learning_rate": 2.6640000000000002e-05,
      "loss": 0.0028,
      "step": 17520
    },
    {
      "epoch": 0.9349333333333333,
      "grad_norm": 0.089847132563591,
      "learning_rate": 2.6626666666666667e-05,
      "loss": 0.0026,
      "step": 17530
    },
    {
      "epoch": 0.9354666666666667,
      "grad_norm": 0.09669528901576996,
      "learning_rate": 2.6613333333333335e-05,
      "loss": 0.0034,
      "step": 17540
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.839931309223175,
      "learning_rate": 2.6600000000000003e-05,
      "loss": 0.0022,
      "step": 17550
    },
    {
      "epoch": 0.9365333333333333,
      "grad_norm": 0.20542696118354797,
      "learning_rate": 2.6586666666666664e-05,
      "loss": 0.0021,
      "step": 17560
    },
    {
      "epoch": 0.9370666666666667,
      "grad_norm": 0.36844876408576965,
      "learning_rate": 2.6573333333333332e-05,
      "loss": 0.0028,
      "step": 17570
    },
    {
      "epoch": 0.9376,
      "grad_norm": 0.18230369687080383,
      "learning_rate": 2.6560000000000003e-05,
      "loss": 0.0023,
      "step": 17580
    },
    {
      "epoch": 0.9381333333333334,
      "grad_norm": 0.0792548879981041,
      "learning_rate": 2.654666666666667e-05,
      "loss": 0.0022,
      "step": 17590
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 0.29132020473480225,
      "learning_rate": 2.6533333333333332e-05,
      "loss": 0.0038,
      "step": 17600
    },
    {
      "epoch": 0.9392,
      "grad_norm": 0.5615308880805969,
      "learning_rate": 2.652e-05,
      "loss": 0.0034,
      "step": 17610
    },
    {
      "epoch": 0.9397333333333333,
      "grad_norm": 0.12750139832496643,
      "learning_rate": 2.6506666666666668e-05,
      "loss": 0.0019,
      "step": 17620
    },
    {
      "epoch": 0.9402666666666667,
      "grad_norm": 0.5921674966812134,
      "learning_rate": 2.6493333333333336e-05,
      "loss": 0.0051,
      "step": 17630
    },
    {
      "epoch": 0.9408,
      "grad_norm": 0.2274227887392044,
      "learning_rate": 2.648e-05,
      "loss": 0.002,
      "step": 17640
    },
    {
      "epoch": 0.9413333333333334,
      "grad_norm": 0.16359615325927734,
      "learning_rate": 2.646666666666667e-05,
      "loss": 0.0021,
      "step": 17650
    },
    {
      "epoch": 0.9418666666666666,
      "grad_norm": 0.23122312128543854,
      "learning_rate": 2.6453333333333336e-05,
      "loss": 0.002,
      "step": 17660
    },
    {
      "epoch": 0.9424,
      "grad_norm": 0.2049943506717682,
      "learning_rate": 2.6440000000000004e-05,
      "loss": 0.0028,
      "step": 17670
    },
    {
      "epoch": 0.9429333333333333,
      "grad_norm": 0.1253657042980194,
      "learning_rate": 2.6426666666666665e-05,
      "loss": 0.0023,
      "step": 17680
    },
    {
      "epoch": 0.9434666666666667,
      "grad_norm": 0.2845703065395355,
      "learning_rate": 2.6413333333333333e-05,
      "loss": 0.0033,
      "step": 17690
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.4255727231502533,
      "learning_rate": 2.64e-05,
      "loss": 0.0023,
      "step": 17700
    },
    {
      "epoch": 0.9445333333333333,
      "grad_norm": 0.24450407922267914,
      "learning_rate": 2.638666666666667e-05,
      "loss": 0.0019,
      "step": 17710
    },
    {
      "epoch": 0.9450666666666667,
      "grad_norm": 0.7908580303192139,
      "learning_rate": 2.6373333333333334e-05,
      "loss": 0.0027,
      "step": 17720
    },
    {
      "epoch": 0.9456,
      "grad_norm": 0.08128298074007034,
      "learning_rate": 2.6360000000000002e-05,
      "loss": 0.0021,
      "step": 17730
    },
    {
      "epoch": 0.9461333333333334,
      "grad_norm": 0.13617545366287231,
      "learning_rate": 2.634666666666667e-05,
      "loss": 0.0026,
      "step": 17740
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.5433558821678162,
      "learning_rate": 2.633333333333333e-05,
      "loss": 0.0027,
      "step": 17750
    },
    {
      "epoch": 0.9472,
      "grad_norm": 0.08904250711202621,
      "learning_rate": 2.632e-05,
      "loss": 0.0026,
      "step": 17760
    },
    {
      "epoch": 0.9477333333333333,
      "grad_norm": 0.4963858127593994,
      "learning_rate": 2.630666666666667e-05,
      "loss": 0.0028,
      "step": 17770
    },
    {
      "epoch": 0.9482666666666667,
      "grad_norm": 0.20201615989208221,
      "learning_rate": 2.6293333333333338e-05,
      "loss": 0.0027,
      "step": 17780
    },
    {
      "epoch": 0.9488,
      "grad_norm": 0.42414286732673645,
      "learning_rate": 2.628e-05,
      "loss": 0.0022,
      "step": 17790
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 0.19036488234996796,
      "learning_rate": 2.6266666666666667e-05,
      "loss": 0.0022,
      "step": 17800
    },
    {
      "epoch": 0.9498666666666666,
      "grad_norm": 0.3546346127986908,
      "learning_rate": 2.6253333333333335e-05,
      "loss": 0.0028,
      "step": 17810
    },
    {
      "epoch": 0.9504,
      "grad_norm": 0.26431161165237427,
      "learning_rate": 2.6240000000000003e-05,
      "loss": 0.0025,
      "step": 17820
    },
    {
      "epoch": 0.9509333333333333,
      "grad_norm": 0.20847079157829285,
      "learning_rate": 2.6226666666666667e-05,
      "loss": 0.0017,
      "step": 17830
    },
    {
      "epoch": 0.9514666666666667,
      "grad_norm": 0.08626578003168106,
      "learning_rate": 2.6213333333333335e-05,
      "loss": 0.0026,
      "step": 17840
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.17487962543964386,
      "learning_rate": 2.6200000000000003e-05,
      "loss": 0.0032,
      "step": 17850
    },
    {
      "epoch": 0.9525333333333333,
      "grad_norm": 0.1042163074016571,
      "learning_rate": 2.618666666666667e-05,
      "loss": 0.0021,
      "step": 17860
    },
    {
      "epoch": 0.9530666666666666,
      "grad_norm": 0.33073166012763977,
      "learning_rate": 2.6173333333333332e-05,
      "loss": 0.0029,
      "step": 17870
    },
    {
      "epoch": 0.9536,
      "grad_norm": 0.15968021750450134,
      "learning_rate": 2.616e-05,
      "loss": 0.0023,
      "step": 17880
    },
    {
      "epoch": 0.9541333333333334,
      "grad_norm": 0.7952417135238647,
      "learning_rate": 2.6146666666666668e-05,
      "loss": 0.0042,
      "step": 17890
    },
    {
      "epoch": 0.9546666666666667,
      "grad_norm": 0.4296252727508545,
      "learning_rate": 2.6133333333333333e-05,
      "loss": 0.0033,
      "step": 17900
    },
    {
      "epoch": 0.9552,
      "grad_norm": 0.28051960468292236,
      "learning_rate": 2.612e-05,
      "loss": 0.003,
      "step": 17910
    },
    {
      "epoch": 0.9557333333333333,
      "grad_norm": 0.37617871165275574,
      "learning_rate": 2.610666666666667e-05,
      "loss": 0.0021,
      "step": 17920
    },
    {
      "epoch": 0.9562666666666667,
      "grad_norm": 0.47658005356788635,
      "learning_rate": 2.6093333333333336e-05,
      "loss": 0.0043,
      "step": 17930
    },
    {
      "epoch": 0.9568,
      "grad_norm": 0.27029165625572205,
      "learning_rate": 2.6079999999999998e-05,
      "loss": 0.0036,
      "step": 17940
    },
    {
      "epoch": 0.9573333333333334,
      "grad_norm": 0.11382631957530975,
      "learning_rate": 2.6066666666666666e-05,
      "loss": 0.0021,
      "step": 17950
    },
    {
      "epoch": 0.9578666666666666,
      "grad_norm": 0.3333115577697754,
      "learning_rate": 2.6053333333333333e-05,
      "loss": 0.0028,
      "step": 17960
    },
    {
      "epoch": 0.9584,
      "grad_norm": 0.13577689230442047,
      "learning_rate": 2.6040000000000005e-05,
      "loss": 0.0022,
      "step": 17970
    },
    {
      "epoch": 0.9589333333333333,
      "grad_norm": 0.18095484375953674,
      "learning_rate": 2.6026666666666666e-05,
      "loss": 0.0026,
      "step": 17980
    },
    {
      "epoch": 0.9594666666666667,
      "grad_norm": 0.2749793827533722,
      "learning_rate": 2.6013333333333334e-05,
      "loss": 0.0032,
      "step": 17990
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.4951465427875519,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.0025,
      "step": 18000
    },
    {
      "epoch": 0.9605333333333334,
      "grad_norm": 0.1366136372089386,
      "learning_rate": 2.598666666666667e-05,
      "loss": 0.0022,
      "step": 18010
    },
    {
      "epoch": 0.9610666666666666,
      "grad_norm": 0.19227191805839539,
      "learning_rate": 2.5973333333333334e-05,
      "loss": 0.002,
      "step": 18020
    },
    {
      "epoch": 0.9616,
      "grad_norm": 0.06194518133997917,
      "learning_rate": 2.5960000000000002e-05,
      "loss": 0.0039,
      "step": 18030
    },
    {
      "epoch": 0.9621333333333333,
      "grad_norm": 0.17064829170703888,
      "learning_rate": 2.594666666666667e-05,
      "loss": 0.0023,
      "step": 18040
    },
    {
      "epoch": 0.9626666666666667,
      "grad_norm": 0.17719073593616486,
      "learning_rate": 2.5933333333333338e-05,
      "loss": 0.0022,
      "step": 18050
    },
    {
      "epoch": 0.9632,
      "grad_norm": 0.4399276673793793,
      "learning_rate": 2.592e-05,
      "loss": 0.0034,
      "step": 18060
    },
    {
      "epoch": 0.9637333333333333,
      "grad_norm": 0.35266610980033875,
      "learning_rate": 2.5906666666666667e-05,
      "loss": 0.0032,
      "step": 18070
    },
    {
      "epoch": 0.9642666666666667,
      "grad_norm": 0.409575492143631,
      "learning_rate": 2.5893333333333335e-05,
      "loss": 0.0047,
      "step": 18080
    },
    {
      "epoch": 0.9648,
      "grad_norm": 0.1337500959634781,
      "learning_rate": 2.588e-05,
      "loss": 0.0018,
      "step": 18090
    },
    {
      "epoch": 0.9653333333333334,
      "grad_norm": 0.08032998442649841,
      "learning_rate": 2.5866666666666667e-05,
      "loss": 0.0019,
      "step": 18100
    },
    {
      "epoch": 0.9658666666666667,
      "grad_norm": 0.3596400022506714,
      "learning_rate": 2.5853333333333335e-05,
      "loss": 0.0024,
      "step": 18110
    },
    {
      "epoch": 0.9664,
      "grad_norm": 0.3018319010734558,
      "learning_rate": 2.5840000000000003e-05,
      "loss": 0.0025,
      "step": 18120
    },
    {
      "epoch": 0.9669333333333333,
      "grad_norm": 0.4495653510093689,
      "learning_rate": 2.5826666666666664e-05,
      "loss": 0.0037,
      "step": 18130
    },
    {
      "epoch": 0.9674666666666667,
      "grad_norm": 0.861810564994812,
      "learning_rate": 2.5813333333333332e-05,
      "loss": 0.0031,
      "step": 18140
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.45683562755584717,
      "learning_rate": 2.58e-05,
      "loss": 0.0022,
      "step": 18150
    },
    {
      "epoch": 0.9685333333333334,
      "grad_norm": 0.5976553559303284,
      "learning_rate": 2.578666666666667e-05,
      "loss": 0.0018,
      "step": 18160
    },
    {
      "epoch": 0.9690666666666666,
      "grad_norm": 0.12831149995326996,
      "learning_rate": 2.5773333333333333e-05,
      "loss": 0.0023,
      "step": 18170
    },
    {
      "epoch": 0.9696,
      "grad_norm": 0.37205514311790466,
      "learning_rate": 2.576e-05,
      "loss": 0.0036,
      "step": 18180
    },
    {
      "epoch": 0.9701333333333333,
      "grad_norm": 0.2922293543815613,
      "learning_rate": 2.574666666666667e-05,
      "loss": 0.0025,
      "step": 18190
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 0.22939778864383698,
      "learning_rate": 2.5733333333333337e-05,
      "loss": 0.003,
      "step": 18200
    },
    {
      "epoch": 0.9712,
      "grad_norm": 0.19978180527687073,
      "learning_rate": 2.572e-05,
      "loss": 0.0025,
      "step": 18210
    },
    {
      "epoch": 0.9717333333333333,
      "grad_norm": 0.20789089798927307,
      "learning_rate": 2.570666666666667e-05,
      "loss": 0.003,
      "step": 18220
    },
    {
      "epoch": 0.9722666666666666,
      "grad_norm": 0.22114329040050507,
      "learning_rate": 2.5693333333333337e-05,
      "loss": 0.0021,
      "step": 18230
    },
    {
      "epoch": 0.9728,
      "grad_norm": 0.2157510370016098,
      "learning_rate": 2.5679999999999998e-05,
      "loss": 0.0028,
      "step": 18240
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.13453145325183868,
      "learning_rate": 2.5666666666666666e-05,
      "loss": 0.0019,
      "step": 18250
    },
    {
      "epoch": 0.9738666666666667,
      "grad_norm": 0.0961306244134903,
      "learning_rate": 2.5653333333333334e-05,
      "loss": 0.002,
      "step": 18260
    },
    {
      "epoch": 0.9744,
      "grad_norm": 0.4129820168018341,
      "learning_rate": 2.5640000000000002e-05,
      "loss": 0.0018,
      "step": 18270
    },
    {
      "epoch": 0.9749333333333333,
      "grad_norm": 0.4242302477359772,
      "learning_rate": 2.5626666666666666e-05,
      "loss": 0.0017,
      "step": 18280
    },
    {
      "epoch": 0.9754666666666667,
      "grad_norm": 0.4650990068912506,
      "learning_rate": 2.5613333333333334e-05,
      "loss": 0.0022,
      "step": 18290
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.33961623907089233,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 0.0027,
      "step": 18300
    },
    {
      "epoch": 0.9765333333333334,
      "grad_norm": 0.4771745502948761,
      "learning_rate": 2.558666666666667e-05,
      "loss": 0.003,
      "step": 18310
    },
    {
      "epoch": 0.9770666666666666,
      "grad_norm": 0.2594558596611023,
      "learning_rate": 2.557333333333333e-05,
      "loss": 0.0031,
      "step": 18320
    },
    {
      "epoch": 0.9776,
      "grad_norm": 0.4463152587413788,
      "learning_rate": 2.556e-05,
      "loss": 0.0032,
      "step": 18330
    },
    {
      "epoch": 0.9781333333333333,
      "grad_norm": 0.20430926978588104,
      "learning_rate": 2.5546666666666667e-05,
      "loss": 0.0021,
      "step": 18340
    },
    {
      "epoch": 0.9786666666666667,
      "grad_norm": 0.33195093274116516,
      "learning_rate": 2.553333333333334e-05,
      "loss": 0.0027,
      "step": 18350
    },
    {
      "epoch": 0.9792,
      "grad_norm": 0.33053186535835266,
      "learning_rate": 2.552e-05,
      "loss": 0.0021,
      "step": 18360
    },
    {
      "epoch": 0.9797333333333333,
      "grad_norm": 0.32881641387939453,
      "learning_rate": 2.5506666666666668e-05,
      "loss": 0.0021,
      "step": 18370
    },
    {
      "epoch": 0.9802666666666666,
      "grad_norm": 0.5153064131736755,
      "learning_rate": 2.5493333333333335e-05,
      "loss": 0.0021,
      "step": 18380
    },
    {
      "epoch": 0.9808,
      "grad_norm": 0.1510460525751114,
      "learning_rate": 2.5480000000000003e-05,
      "loss": 0.0025,
      "step": 18390
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 0.28912463784217834,
      "learning_rate": 2.5466666666666668e-05,
      "loss": 0.0028,
      "step": 18400
    },
    {
      "epoch": 0.9818666666666667,
      "grad_norm": 0.3843197524547577,
      "learning_rate": 2.5453333333333336e-05,
      "loss": 0.0021,
      "step": 18410
    },
    {
      "epoch": 0.9824,
      "grad_norm": 0.2678737938404083,
      "learning_rate": 2.5440000000000004e-05,
      "loss": 0.0028,
      "step": 18420
    },
    {
      "epoch": 0.9829333333333333,
      "grad_norm": 0.47733739018440247,
      "learning_rate": 2.5426666666666665e-05,
      "loss": 0.0022,
      "step": 18430
    },
    {
      "epoch": 0.9834666666666667,
      "grad_norm": 0.11546008288860321,
      "learning_rate": 2.5413333333333333e-05,
      "loss": 0.0018,
      "step": 18440
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.23803293704986572,
      "learning_rate": 2.54e-05,
      "loss": 0.0021,
      "step": 18450
    },
    {
      "epoch": 0.9845333333333334,
      "grad_norm": 0.1909930258989334,
      "learning_rate": 2.538666666666667e-05,
      "loss": 0.0019,
      "step": 18460
    },
    {
      "epoch": 0.9850666666666666,
      "grad_norm": 0.3890085816383362,
      "learning_rate": 2.5373333333333333e-05,
      "loss": 0.0022,
      "step": 18470
    },
    {
      "epoch": 0.9856,
      "grad_norm": 0.512566864490509,
      "learning_rate": 2.536e-05,
      "loss": 0.0025,
      "step": 18480
    },
    {
      "epoch": 0.9861333333333333,
      "grad_norm": 0.41925159096717834,
      "learning_rate": 2.534666666666667e-05,
      "loss": 0.0022,
      "step": 18490
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.29874780774116516,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 0.0023,
      "step": 18500
    },
    {
      "epoch": 0.9872,
      "grad_norm": 0.3827809691429138,
      "learning_rate": 2.5319999999999998e-05,
      "loss": 0.0038,
      "step": 18510
    },
    {
      "epoch": 0.9877333333333334,
      "grad_norm": 0.40177419781684875,
      "learning_rate": 2.5306666666666666e-05,
      "loss": 0.0029,
      "step": 18520
    },
    {
      "epoch": 0.9882666666666666,
      "grad_norm": 0.06516805291175842,
      "learning_rate": 2.5293333333333334e-05,
      "loss": 0.0027,
      "step": 18530
    },
    {
      "epoch": 0.9888,
      "grad_norm": 0.31256529688835144,
      "learning_rate": 2.5280000000000005e-05,
      "loss": 0.0021,
      "step": 18540
    },
    {
      "epoch": 0.9893333333333333,
      "grad_norm": 0.1414114087820053,
      "learning_rate": 2.5266666666666666e-05,
      "loss": 0.0024,
      "step": 18550
    },
    {
      "epoch": 0.9898666666666667,
      "grad_norm": 0.1550363153219223,
      "learning_rate": 2.5253333333333334e-05,
      "loss": 0.0019,
      "step": 18560
    },
    {
      "epoch": 0.9904,
      "grad_norm": 0.1588268280029297,
      "learning_rate": 2.5240000000000002e-05,
      "loss": 0.0028,
      "step": 18570
    },
    {
      "epoch": 0.9909333333333333,
      "grad_norm": 1.0775772333145142,
      "learning_rate": 2.5226666666666663e-05,
      "loss": 0.0029,
      "step": 18580
    },
    {
      "epoch": 0.9914666666666667,
      "grad_norm": 0.08459515124559402,
      "learning_rate": 2.5213333333333335e-05,
      "loss": 0.0022,
      "step": 18590
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.265912264585495,
      "learning_rate": 2.5200000000000003e-05,
      "loss": 0.0018,
      "step": 18600
    },
    {
      "epoch": 0.9925333333333334,
      "grad_norm": 0.4818597435951233,
      "learning_rate": 2.518666666666667e-05,
      "loss": 0.0017,
      "step": 18610
    },
    {
      "epoch": 0.9930666666666667,
      "grad_norm": 0.20303946733474731,
      "learning_rate": 2.5173333333333332e-05,
      "loss": 0.0031,
      "step": 18620
    },
    {
      "epoch": 0.9936,
      "grad_norm": 0.09263712912797928,
      "learning_rate": 2.516e-05,
      "loss": 0.002,
      "step": 18630
    },
    {
      "epoch": 0.9941333333333333,
      "grad_norm": 0.663756787776947,
      "learning_rate": 2.5146666666666668e-05,
      "loss": 0.0019,
      "step": 18640
    },
    {
      "epoch": 0.9946666666666667,
      "grad_norm": 0.07554661482572556,
      "learning_rate": 2.5133333333333336e-05,
      "loss": 0.0021,
      "step": 18650
    },
    {
      "epoch": 0.9952,
      "grad_norm": 0.14463622868061066,
      "learning_rate": 2.512e-05,
      "loss": 0.0031,
      "step": 18660
    },
    {
      "epoch": 0.9957333333333334,
      "grad_norm": 0.33675557374954224,
      "learning_rate": 2.5106666666666668e-05,
      "loss": 0.002,
      "step": 18670
    },
    {
      "epoch": 0.9962666666666666,
      "grad_norm": 0.10024376213550568,
      "learning_rate": 2.5093333333333336e-05,
      "loss": 0.0028,
      "step": 18680
    },
    {
      "epoch": 0.9968,
      "grad_norm": 0.2712537348270416,
      "learning_rate": 2.5080000000000004e-05,
      "loss": 0.0029,
      "step": 18690
    },
    {
      "epoch": 0.9973333333333333,
      "grad_norm": 0.22558417916297913,
      "learning_rate": 2.5066666666666665e-05,
      "loss": 0.0016,
      "step": 18700
    },
    {
      "epoch": 0.9978666666666667,
      "grad_norm": 0.31572413444519043,
      "learning_rate": 2.5053333333333333e-05,
      "loss": 0.0028,
      "step": 18710
    },
    {
      "epoch": 0.9984,
      "grad_norm": 0.06725965440273285,
      "learning_rate": 2.504e-05,
      "loss": 0.0017,
      "step": 18720
    },
    {
      "epoch": 0.9989333333333333,
      "grad_norm": 0.08468016237020493,
      "learning_rate": 2.5026666666666672e-05,
      "loss": 0.0026,
      "step": 18730
    },
    {
      "epoch": 0.9994666666666666,
      "grad_norm": 0.23287488520145416,
      "learning_rate": 2.5013333333333333e-05,
      "loss": 0.0021,
      "step": 18740
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.3585725724697113,
      "learning_rate": 2.5e-05,
      "loss": 0.0027,
      "step": 18750
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.0032776042353361845,
      "eval_runtime": 155.0388,
      "eval_samples_per_second": 1612.499,
      "eval_steps_per_second": 40.312,
      "step": 18750
    },
    {
      "epoch": 1.0005333333333333,
      "grad_norm": 0.40558910369873047,
      "learning_rate": 2.4986666666666666e-05,
      "loss": 0.0048,
      "step": 18760
    },
    {
      "epoch": 1.0010666666666668,
      "grad_norm": 0.20789919793605804,
      "learning_rate": 2.4973333333333334e-05,
      "loss": 0.0029,
      "step": 18770
    },
    {
      "epoch": 1.0016,
      "grad_norm": 0.06571450084447861,
      "learning_rate": 2.496e-05,
      "loss": 0.002,
      "step": 18780
    },
    {
      "epoch": 1.0021333333333333,
      "grad_norm": 0.06946002691984177,
      "learning_rate": 2.494666666666667e-05,
      "loss": 0.0023,
      "step": 18790
    },
    {
      "epoch": 1.0026666666666666,
      "grad_norm": 0.5374215245246887,
      "learning_rate": 2.4933333333333334e-05,
      "loss": 0.002,
      "step": 18800
    },
    {
      "epoch": 1.0032,
      "grad_norm": 0.28233444690704346,
      "learning_rate": 2.4920000000000002e-05,
      "loss": 0.0033,
      "step": 18810
    },
    {
      "epoch": 1.0037333333333334,
      "grad_norm": 0.2994210422039032,
      "learning_rate": 2.4906666666666666e-05,
      "loss": 0.0021,
      "step": 18820
    },
    {
      "epoch": 1.0042666666666666,
      "grad_norm": 0.2736091613769531,
      "learning_rate": 2.4893333333333334e-05,
      "loss": 0.0031,
      "step": 18830
    },
    {
      "epoch": 1.0048,
      "grad_norm": 0.39988234639167786,
      "learning_rate": 2.488e-05,
      "loss": 0.0022,
      "step": 18840
    },
    {
      "epoch": 1.0053333333333334,
      "grad_norm": 0.05671257898211479,
      "learning_rate": 2.486666666666667e-05,
      "loss": 0.0021,
      "step": 18850
    },
    {
      "epoch": 1.0058666666666667,
      "grad_norm": 0.6058545112609863,
      "learning_rate": 2.4853333333333335e-05,
      "loss": 0.0029,
      "step": 18860
    },
    {
      "epoch": 1.0064,
      "grad_norm": 0.438841849565506,
      "learning_rate": 2.4840000000000003e-05,
      "loss": 0.0019,
      "step": 18870
    },
    {
      "epoch": 1.0069333333333332,
      "grad_norm": 0.20975345373153687,
      "learning_rate": 2.4826666666666667e-05,
      "loss": 0.0036,
      "step": 18880
    },
    {
      "epoch": 1.0074666666666667,
      "grad_norm": 0.2713107466697693,
      "learning_rate": 2.4813333333333335e-05,
      "loss": 0.0025,
      "step": 18890
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.9174997806549072,
      "learning_rate": 2.48e-05,
      "loss": 0.0016,
      "step": 18900
    },
    {
      "epoch": 1.0085333333333333,
      "grad_norm": 0.1325892060995102,
      "learning_rate": 2.4786666666666668e-05,
      "loss": 0.0018,
      "step": 18910
    },
    {
      "epoch": 1.0090666666666666,
      "grad_norm": 0.08110401034355164,
      "learning_rate": 2.4773333333333336e-05,
      "loss": 0.0029,
      "step": 18920
    },
    {
      "epoch": 1.0096,
      "grad_norm": 0.1284172385931015,
      "learning_rate": 2.476e-05,
      "loss": 0.0023,
      "step": 18930
    },
    {
      "epoch": 1.0101333333333333,
      "grad_norm": 0.44079089164733887,
      "learning_rate": 2.4746666666666668e-05,
      "loss": 0.0018,
      "step": 18940
    },
    {
      "epoch": 1.0106666666666666,
      "grad_norm": 0.5328649878501892,
      "learning_rate": 2.4733333333333333e-05,
      "loss": 0.0037,
      "step": 18950
    },
    {
      "epoch": 1.0112,
      "grad_norm": 0.21185357868671417,
      "learning_rate": 2.472e-05,
      "loss": 0.0029,
      "step": 18960
    },
    {
      "epoch": 1.0117333333333334,
      "grad_norm": 0.26246729493141174,
      "learning_rate": 2.470666666666667e-05,
      "loss": 0.0019,
      "step": 18970
    },
    {
      "epoch": 1.0122666666666666,
      "grad_norm": 0.30942830443382263,
      "learning_rate": 2.4693333333333336e-05,
      "loss": 0.004,
      "step": 18980
    },
    {
      "epoch": 1.0128,
      "grad_norm": 0.0750621035695076,
      "learning_rate": 2.468e-05,
      "loss": 0.0023,
      "step": 18990
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.26054176688194275,
      "learning_rate": 2.466666666666667e-05,
      "loss": 0.0021,
      "step": 19000
    },
    {
      "epoch": 1.0138666666666667,
      "grad_norm": 0.19060364365577698,
      "learning_rate": 2.4653333333333333e-05,
      "loss": 0.0019,
      "step": 19010
    },
    {
      "epoch": 1.0144,
      "grad_norm": 0.632837176322937,
      "learning_rate": 2.464e-05,
      "loss": 0.0024,
      "step": 19020
    },
    {
      "epoch": 1.0149333333333332,
      "grad_norm": 0.13826313614845276,
      "learning_rate": 2.4626666666666666e-05,
      "loss": 0.0026,
      "step": 19030
    },
    {
      "epoch": 1.0154666666666667,
      "grad_norm": 0.18427680432796478,
      "learning_rate": 2.4613333333333337e-05,
      "loss": 0.0029,
      "step": 19040
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.4555579721927643,
      "learning_rate": 2.46e-05,
      "loss": 0.0022,
      "step": 19050
    },
    {
      "epoch": 1.0165333333333333,
      "grad_norm": 0.5142983794212341,
      "learning_rate": 2.458666666666667e-05,
      "loss": 0.0023,
      "step": 19060
    },
    {
      "epoch": 1.0170666666666666,
      "grad_norm": 0.10427062958478928,
      "learning_rate": 2.4573333333333334e-05,
      "loss": 0.0022,
      "step": 19070
    },
    {
      "epoch": 1.0176,
      "grad_norm": 0.5669876337051392,
      "learning_rate": 2.4560000000000002e-05,
      "loss": 0.0023,
      "step": 19080
    },
    {
      "epoch": 1.0181333333333333,
      "grad_norm": 0.2733803391456604,
      "learning_rate": 2.4546666666666667e-05,
      "loss": 0.0023,
      "step": 19090
    },
    {
      "epoch": 1.0186666666666666,
      "grad_norm": 0.34258657693862915,
      "learning_rate": 2.4533333333333334e-05,
      "loss": 0.0026,
      "step": 19100
    },
    {
      "epoch": 1.0192,
      "grad_norm": 0.4851364195346832,
      "learning_rate": 2.4520000000000002e-05,
      "loss": 0.0043,
      "step": 19110
    },
    {
      "epoch": 1.0197333333333334,
      "grad_norm": 0.1739293336868286,
      "learning_rate": 2.4506666666666667e-05,
      "loss": 0.0022,
      "step": 19120
    },
    {
      "epoch": 1.0202666666666667,
      "grad_norm": 0.5180128216743469,
      "learning_rate": 2.4493333333333335e-05,
      "loss": 0.0035,
      "step": 19130
    },
    {
      "epoch": 1.0208,
      "grad_norm": 0.2652399241924286,
      "learning_rate": 2.448e-05,
      "loss": 0.002,
      "step": 19140
    },
    {
      "epoch": 1.0213333333333334,
      "grad_norm": 0.4043453633785248,
      "learning_rate": 2.4466666666666667e-05,
      "loss": 0.003,
      "step": 19150
    },
    {
      "epoch": 1.0218666666666667,
      "grad_norm": 0.29993608593940735,
      "learning_rate": 2.4453333333333335e-05,
      "loss": 0.0018,
      "step": 19160
    },
    {
      "epoch": 1.0224,
      "grad_norm": 0.26930975914001465,
      "learning_rate": 2.4440000000000003e-05,
      "loss": 0.0019,
      "step": 19170
    },
    {
      "epoch": 1.0229333333333333,
      "grad_norm": 0.08498214185237885,
      "learning_rate": 2.4426666666666668e-05,
      "loss": 0.002,
      "step": 19180
    },
    {
      "epoch": 1.0234666666666667,
      "grad_norm": 0.2947746813297272,
      "learning_rate": 2.4413333333333336e-05,
      "loss": 0.0031,
      "step": 19190
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.28376638889312744,
      "learning_rate": 2.44e-05,
      "loss": 0.0031,
      "step": 19200
    },
    {
      "epoch": 1.0245333333333333,
      "grad_norm": 0.12271303683519363,
      "learning_rate": 2.4386666666666668e-05,
      "loss": 0.0026,
      "step": 19210
    },
    {
      "epoch": 1.0250666666666666,
      "grad_norm": 0.7732197642326355,
      "learning_rate": 2.4373333333333333e-05,
      "loss": 0.0035,
      "step": 19220
    },
    {
      "epoch": 1.0256,
      "grad_norm": 0.6836626529693604,
      "learning_rate": 2.4360000000000004e-05,
      "loss": 0.0024,
      "step": 19230
    },
    {
      "epoch": 1.0261333333333333,
      "grad_norm": 0.6085152626037598,
      "learning_rate": 2.434666666666667e-05,
      "loss": 0.0028,
      "step": 19240
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.1035834550857544,
      "learning_rate": 2.4333333333333336e-05,
      "loss": 0.0026,
      "step": 19250
    },
    {
      "epoch": 1.0272,
      "grad_norm": 0.36309880018234253,
      "learning_rate": 2.432e-05,
      "loss": 0.0026,
      "step": 19260
    },
    {
      "epoch": 1.0277333333333334,
      "grad_norm": 0.20856592059135437,
      "learning_rate": 2.4306666666666665e-05,
      "loss": 0.0034,
      "step": 19270
    },
    {
      "epoch": 1.0282666666666667,
      "grad_norm": 0.09420881420373917,
      "learning_rate": 2.4293333333333333e-05,
      "loss": 0.0025,
      "step": 19280
    },
    {
      "epoch": 1.0288,
      "grad_norm": 0.502333402633667,
      "learning_rate": 2.428e-05,
      "loss": 0.0027,
      "step": 19290
    },
    {
      "epoch": 1.0293333333333334,
      "grad_norm": 0.16608192026615143,
      "learning_rate": 2.426666666666667e-05,
      "loss": 0.0021,
      "step": 19300
    },
    {
      "epoch": 1.0298666666666667,
      "grad_norm": 0.2898821532726288,
      "learning_rate": 2.4253333333333334e-05,
      "loss": 0.0028,
      "step": 19310
    },
    {
      "epoch": 1.0304,
      "grad_norm": 0.15121500194072723,
      "learning_rate": 2.4240000000000002e-05,
      "loss": 0.0022,
      "step": 19320
    },
    {
      "epoch": 1.0309333333333333,
      "grad_norm": 0.23670323193073273,
      "learning_rate": 2.4226666666666666e-05,
      "loss": 0.002,
      "step": 19330
    },
    {
      "epoch": 1.0314666666666668,
      "grad_norm": 0.3527144193649292,
      "learning_rate": 2.4213333333333334e-05,
      "loss": 0.0031,
      "step": 19340
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.3596911132335663,
      "learning_rate": 2.4200000000000002e-05,
      "loss": 0.0037,
      "step": 19350
    },
    {
      "epoch": 1.0325333333333333,
      "grad_norm": 0.2276349514722824,
      "learning_rate": 2.418666666666667e-05,
      "loss": 0.0034,
      "step": 19360
    },
    {
      "epoch": 1.0330666666666666,
      "grad_norm": 0.18575331568717957,
      "learning_rate": 2.4173333333333335e-05,
      "loss": 0.0022,
      "step": 19370
    },
    {
      "epoch": 1.0336,
      "grad_norm": 0.32159873843193054,
      "learning_rate": 2.4160000000000002e-05,
      "loss": 0.0039,
      "step": 19380
    },
    {
      "epoch": 1.0341333333333333,
      "grad_norm": 0.25379592180252075,
      "learning_rate": 2.4146666666666667e-05,
      "loss": 0.0023,
      "step": 19390
    },
    {
      "epoch": 1.0346666666666666,
      "grad_norm": 0.18962329626083374,
      "learning_rate": 2.4133333333333335e-05,
      "loss": 0.0031,
      "step": 19400
    },
    {
      "epoch": 1.0352,
      "grad_norm": 0.29726436734199524,
      "learning_rate": 2.412e-05,
      "loss": 0.0027,
      "step": 19410
    },
    {
      "epoch": 1.0357333333333334,
      "grad_norm": 0.2880358397960663,
      "learning_rate": 2.4106666666666667e-05,
      "loss": 0.0029,
      "step": 19420
    },
    {
      "epoch": 1.0362666666666667,
      "grad_norm": 0.6392921209335327,
      "learning_rate": 2.4093333333333335e-05,
      "loss": 0.0021,
      "step": 19430
    },
    {
      "epoch": 1.0368,
      "grad_norm": 0.4145410358905792,
      "learning_rate": 2.408e-05,
      "loss": 0.0024,
      "step": 19440
    },
    {
      "epoch": 1.0373333333333334,
      "grad_norm": 0.28139859437942505,
      "learning_rate": 2.4066666666666668e-05,
      "loss": 0.0024,
      "step": 19450
    },
    {
      "epoch": 1.0378666666666667,
      "grad_norm": 0.5414139628410339,
      "learning_rate": 2.4053333333333332e-05,
      "loss": 0.0029,
      "step": 19460
    },
    {
      "epoch": 1.0384,
      "grad_norm": 0.4398110508918762,
      "learning_rate": 2.404e-05,
      "loss": 0.0021,
      "step": 19470
    },
    {
      "epoch": 1.0389333333333333,
      "grad_norm": 0.3479667901992798,
      "learning_rate": 2.4026666666666668e-05,
      "loss": 0.0028,
      "step": 19480
    },
    {
      "epoch": 1.0394666666666668,
      "grad_norm": 0.20124690234661102,
      "learning_rate": 2.4013333333333336e-05,
      "loss": 0.003,
      "step": 19490
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.31849199533462524,
      "learning_rate": 2.4e-05,
      "loss": 0.0024,
      "step": 19500
    },
    {
      "epoch": 1.0405333333333333,
      "grad_norm": 0.15905995666980743,
      "learning_rate": 2.398666666666667e-05,
      "loss": 0.0035,
      "step": 19510
    },
    {
      "epoch": 1.0410666666666666,
      "grad_norm": 0.12346301972866058,
      "learning_rate": 2.3973333333333333e-05,
      "loss": 0.0018,
      "step": 19520
    },
    {
      "epoch": 1.0416,
      "grad_norm": 0.22953937947750092,
      "learning_rate": 2.396e-05,
      "loss": 0.0028,
      "step": 19530
    },
    {
      "epoch": 1.0421333333333334,
      "grad_norm": 0.6195499300956726,
      "learning_rate": 2.394666666666667e-05,
      "loss": 0.0036,
      "step": 19540
    },
    {
      "epoch": 1.0426666666666666,
      "grad_norm": 0.30819734930992126,
      "learning_rate": 2.3933333333333337e-05,
      "loss": 0.0022,
      "step": 19550
    },
    {
      "epoch": 1.0432,
      "grad_norm": 0.1350894272327423,
      "learning_rate": 2.392e-05,
      "loss": 0.0019,
      "step": 19560
    },
    {
      "epoch": 1.0437333333333334,
      "grad_norm": 0.3916424512863159,
      "learning_rate": 2.390666666666667e-05,
      "loss": 0.0031,
      "step": 19570
    },
    {
      "epoch": 1.0442666666666667,
      "grad_norm": 0.15548762679100037,
      "learning_rate": 2.3893333333333334e-05,
      "loss": 0.0021,
      "step": 19580
    },
    {
      "epoch": 1.0448,
      "grad_norm": 0.12308894097805023,
      "learning_rate": 2.3880000000000002e-05,
      "loss": 0.0032,
      "step": 19590
    },
    {
      "epoch": 1.0453333333333332,
      "grad_norm": 0.14057764410972595,
      "learning_rate": 2.3866666666666666e-05,
      "loss": 0.0023,
      "step": 19600
    },
    {
      "epoch": 1.0458666666666667,
      "grad_norm": 0.06828028708696365,
      "learning_rate": 2.3853333333333334e-05,
      "loss": 0.0023,
      "step": 19610
    },
    {
      "epoch": 1.0464,
      "grad_norm": 0.34592482447624207,
      "learning_rate": 2.3840000000000002e-05,
      "loss": 0.0022,
      "step": 19620
    },
    {
      "epoch": 1.0469333333333333,
      "grad_norm": 0.4116651713848114,
      "learning_rate": 2.3826666666666667e-05,
      "loss": 0.0021,
      "step": 19630
    },
    {
      "epoch": 1.0474666666666668,
      "grad_norm": 0.8609251379966736,
      "learning_rate": 2.3813333333333335e-05,
      "loss": 0.0025,
      "step": 19640
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.3398014307022095,
      "learning_rate": 2.38e-05,
      "loss": 0.0037,
      "step": 19650
    },
    {
      "epoch": 1.0485333333333333,
      "grad_norm": 0.3125927448272705,
      "learning_rate": 2.3786666666666667e-05,
      "loss": 0.0026,
      "step": 19660
    },
    {
      "epoch": 1.0490666666666666,
      "grad_norm": 0.9236876964569092,
      "learning_rate": 2.3773333333333335e-05,
      "loss": 0.0023,
      "step": 19670
    },
    {
      "epoch": 1.0496,
      "grad_norm": 0.23223349452018738,
      "learning_rate": 2.3760000000000003e-05,
      "loss": 0.0029,
      "step": 19680
    },
    {
      "epoch": 1.0501333333333334,
      "grad_norm": 0.316419780254364,
      "learning_rate": 2.3746666666666667e-05,
      "loss": 0.0024,
      "step": 19690
    },
    {
      "epoch": 1.0506666666666666,
      "grad_norm": 0.17559492588043213,
      "learning_rate": 2.3733333333333335e-05,
      "loss": 0.0026,
      "step": 19700
    },
    {
      "epoch": 1.0512,
      "grad_norm": 0.0889296606183052,
      "learning_rate": 2.372e-05,
      "loss": 0.0024,
      "step": 19710
    },
    {
      "epoch": 1.0517333333333334,
      "grad_norm": 0.31927165389060974,
      "learning_rate": 2.3706666666666668e-05,
      "loss": 0.0028,
      "step": 19720
    },
    {
      "epoch": 1.0522666666666667,
      "grad_norm": 0.13690367341041565,
      "learning_rate": 2.3693333333333332e-05,
      "loss": 0.0029,
      "step": 19730
    },
    {
      "epoch": 1.0528,
      "grad_norm": 0.2348928451538086,
      "learning_rate": 2.3680000000000004e-05,
      "loss": 0.0026,
      "step": 19740
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.232914999127388,
      "learning_rate": 2.3666666666666668e-05,
      "loss": 0.0019,
      "step": 19750
    },
    {
      "epoch": 1.0538666666666667,
      "grad_norm": 0.2590656876564026,
      "learning_rate": 2.3653333333333336e-05,
      "loss": 0.002,
      "step": 19760
    },
    {
      "epoch": 1.0544,
      "grad_norm": 0.15190701186656952,
      "learning_rate": 2.364e-05,
      "loss": 0.0035,
      "step": 19770
    },
    {
      "epoch": 1.0549333333333333,
      "grad_norm": 0.17928394675254822,
      "learning_rate": 2.362666666666667e-05,
      "loss": 0.002,
      "step": 19780
    },
    {
      "epoch": 1.0554666666666668,
      "grad_norm": 0.3717859983444214,
      "learning_rate": 2.3613333333333333e-05,
      "loss": 0.0036,
      "step": 19790
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.2841004729270935,
      "learning_rate": 2.36e-05,
      "loss": 0.0029,
      "step": 19800
    },
    {
      "epoch": 1.0565333333333333,
      "grad_norm": 0.12374300509691238,
      "learning_rate": 2.358666666666667e-05,
      "loss": 0.0022,
      "step": 19810
    },
    {
      "epoch": 1.0570666666666666,
      "grad_norm": 0.2719205319881439,
      "learning_rate": 2.3573333333333334e-05,
      "loss": 0.0024,
      "step": 19820
    },
    {
      "epoch": 1.0576,
      "grad_norm": 0.37373316287994385,
      "learning_rate": 2.356e-05,
      "loss": 0.0029,
      "step": 19830
    },
    {
      "epoch": 1.0581333333333334,
      "grad_norm": 0.47013750672340393,
      "learning_rate": 2.3546666666666666e-05,
      "loss": 0.0042,
      "step": 19840
    },
    {
      "epoch": 1.0586666666666666,
      "grad_norm": 0.25861304998397827,
      "learning_rate": 2.3533333333333334e-05,
      "loss": 0.0021,
      "step": 19850
    },
    {
      "epoch": 1.0592,
      "grad_norm": 0.41667506098747253,
      "learning_rate": 2.3520000000000002e-05,
      "loss": 0.0026,
      "step": 19860
    },
    {
      "epoch": 1.0597333333333334,
      "grad_norm": 0.34511062502861023,
      "learning_rate": 2.350666666666667e-05,
      "loss": 0.0033,
      "step": 19870
    },
    {
      "epoch": 1.0602666666666667,
      "grad_norm": 0.21300794184207916,
      "learning_rate": 2.3493333333333334e-05,
      "loss": 0.0029,
      "step": 19880
    },
    {
      "epoch": 1.0608,
      "grad_norm": 0.17187060415744781,
      "learning_rate": 2.3480000000000002e-05,
      "loss": 0.0032,
      "step": 19890
    },
    {
      "epoch": 1.0613333333333332,
      "grad_norm": 0.8082127571105957,
      "learning_rate": 2.3466666666666667e-05,
      "loss": 0.0023,
      "step": 19900
    },
    {
      "epoch": 1.0618666666666667,
      "grad_norm": 0.25781410932540894,
      "learning_rate": 2.3453333333333335e-05,
      "loss": 0.0028,
      "step": 19910
    },
    {
      "epoch": 1.0624,
      "grad_norm": 0.08442242443561554,
      "learning_rate": 2.344e-05,
      "loss": 0.0025,
      "step": 19920
    },
    {
      "epoch": 1.0629333333333333,
      "grad_norm": 0.15041688084602356,
      "learning_rate": 2.342666666666667e-05,
      "loss": 0.002,
      "step": 19930
    },
    {
      "epoch": 1.0634666666666668,
      "grad_norm": 0.5529451966285706,
      "learning_rate": 2.3413333333333335e-05,
      "loss": 0.0026,
      "step": 19940
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.48864230513572693,
      "learning_rate": 2.3400000000000003e-05,
      "loss": 0.0026,
      "step": 19950
    },
    {
      "epoch": 1.0645333333333333,
      "grad_norm": 0.09294899553060532,
      "learning_rate": 2.3386666666666668e-05,
      "loss": 0.0036,
      "step": 19960
    },
    {
      "epoch": 1.0650666666666666,
      "grad_norm": 0.14133623242378235,
      "learning_rate": 2.3373333333333332e-05,
      "loss": 0.002,
      "step": 19970
    },
    {
      "epoch": 1.0656,
      "grad_norm": 0.40936145186424255,
      "learning_rate": 2.336e-05,
      "loss": 0.0029,
      "step": 19980
    },
    {
      "epoch": 1.0661333333333334,
      "grad_norm": 0.1029592752456665,
      "learning_rate": 2.3346666666666668e-05,
      "loss": 0.0026,
      "step": 19990
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.05914485082030296,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.0021,
      "step": 20000
    },
    {
      "epoch": 1.0672,
      "grad_norm": 0.22342999279499054,
      "learning_rate": 2.332e-05,
      "loss": 0.0028,
      "step": 20010
    },
    {
      "epoch": 1.0677333333333334,
      "grad_norm": 0.24734839797019958,
      "learning_rate": 2.3306666666666668e-05,
      "loss": 0.0023,
      "step": 20020
    },
    {
      "epoch": 1.0682666666666667,
      "grad_norm": 0.23393890261650085,
      "learning_rate": 2.3293333333333333e-05,
      "loss": 0.0022,
      "step": 20030
    },
    {
      "epoch": 1.0688,
      "grad_norm": 0.5114985704421997,
      "learning_rate": 2.328e-05,
      "loss": 0.0024,
      "step": 20040
    },
    {
      "epoch": 1.0693333333333332,
      "grad_norm": 0.47463229298591614,
      "learning_rate": 2.326666666666667e-05,
      "loss": 0.002,
      "step": 20050
    },
    {
      "epoch": 1.0698666666666667,
      "grad_norm": 0.14624997973442078,
      "learning_rate": 2.3253333333333337e-05,
      "loss": 0.0022,
      "step": 20060
    },
    {
      "epoch": 1.0704,
      "grad_norm": 0.2707076072692871,
      "learning_rate": 2.324e-05,
      "loss": 0.0018,
      "step": 20070
    },
    {
      "epoch": 1.0709333333333333,
      "grad_norm": 0.15207792818546295,
      "learning_rate": 2.322666666666667e-05,
      "loss": 0.0021,
      "step": 20080
    },
    {
      "epoch": 1.0714666666666666,
      "grad_norm": 0.14793694019317627,
      "learning_rate": 2.3213333333333334e-05,
      "loss": 0.0034,
      "step": 20090
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.0865788385272026,
      "learning_rate": 2.32e-05,
      "loss": 0.0026,
      "step": 20100
    },
    {
      "epoch": 1.0725333333333333,
      "grad_norm": 0.2213759869337082,
      "learning_rate": 2.3186666666666666e-05,
      "loss": 0.0033,
      "step": 20110
    },
    {
      "epoch": 1.0730666666666666,
      "grad_norm": 0.11878960579633713,
      "learning_rate": 2.3173333333333337e-05,
      "loss": 0.0022,
      "step": 20120
    },
    {
      "epoch": 1.0735999999999999,
      "grad_norm": 0.22257453203201294,
      "learning_rate": 2.3160000000000002e-05,
      "loss": 0.0019,
      "step": 20130
    },
    {
      "epoch": 1.0741333333333334,
      "grad_norm": 0.37838026881217957,
      "learning_rate": 2.3146666666666666e-05,
      "loss": 0.0035,
      "step": 20140
    },
    {
      "epoch": 1.0746666666666667,
      "grad_norm": 0.14094975590705872,
      "learning_rate": 2.3133333333333334e-05,
      "loss": 0.0024,
      "step": 20150
    },
    {
      "epoch": 1.0752,
      "grad_norm": 0.1560257077217102,
      "learning_rate": 2.312e-05,
      "loss": 0.0022,
      "step": 20160
    },
    {
      "epoch": 1.0757333333333334,
      "grad_norm": 0.5440845489501953,
      "learning_rate": 2.3106666666666667e-05,
      "loss": 0.0022,
      "step": 20170
    },
    {
      "epoch": 1.0762666666666667,
      "grad_norm": 0.41305941343307495,
      "learning_rate": 2.3093333333333335e-05,
      "loss": 0.0034,
      "step": 20180
    },
    {
      "epoch": 1.0768,
      "grad_norm": 0.1484951674938202,
      "learning_rate": 2.3080000000000003e-05,
      "loss": 0.0023,
      "step": 20190
    },
    {
      "epoch": 1.0773333333333333,
      "grad_norm": 0.49827876687049866,
      "learning_rate": 2.3066666666666667e-05,
      "loss": 0.0022,
      "step": 20200
    },
    {
      "epoch": 1.0778666666666668,
      "grad_norm": 0.4518676996231079,
      "learning_rate": 2.3053333333333335e-05,
      "loss": 0.0024,
      "step": 20210
    },
    {
      "epoch": 1.0784,
      "grad_norm": 0.4866317808628082,
      "learning_rate": 2.304e-05,
      "loss": 0.0027,
      "step": 20220
    },
    {
      "epoch": 1.0789333333333333,
      "grad_norm": 0.24860866367816925,
      "learning_rate": 2.3026666666666668e-05,
      "loss": 0.003,
      "step": 20230
    },
    {
      "epoch": 1.0794666666666666,
      "grad_norm": 0.20451180636882782,
      "learning_rate": 2.3013333333333335e-05,
      "loss": 0.0025,
      "step": 20240
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.1273552030324936,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.0029,
      "step": 20250
    },
    {
      "epoch": 1.0805333333333333,
      "grad_norm": 0.22582824528217316,
      "learning_rate": 2.2986666666666668e-05,
      "loss": 0.002,
      "step": 20260
    },
    {
      "epoch": 1.0810666666666666,
      "grad_norm": 0.35373976826667786,
      "learning_rate": 2.2973333333333336e-05,
      "loss": 0.0022,
      "step": 20270
    },
    {
      "epoch": 1.0816,
      "grad_norm": 0.6598237156867981,
      "learning_rate": 2.296e-05,
      "loss": 0.0018,
      "step": 20280
    },
    {
      "epoch": 1.0821333333333334,
      "grad_norm": 0.26513826847076416,
      "learning_rate": 2.294666666666667e-05,
      "loss": 0.0026,
      "step": 20290
    },
    {
      "epoch": 1.0826666666666667,
      "grad_norm": 0.18613174557685852,
      "learning_rate": 2.2933333333333333e-05,
      "loss": 0.0031,
      "step": 20300
    },
    {
      "epoch": 1.0832,
      "grad_norm": 0.0971963182091713,
      "learning_rate": 2.292e-05,
      "loss": 0.0022,
      "step": 20310
    },
    {
      "epoch": 1.0837333333333334,
      "grad_norm": 0.09026535600423813,
      "learning_rate": 2.290666666666667e-05,
      "loss": 0.0027,
      "step": 20320
    },
    {
      "epoch": 1.0842666666666667,
      "grad_norm": 0.2623598575592041,
      "learning_rate": 2.2893333333333333e-05,
      "loss": 0.0046,
      "step": 20330
    },
    {
      "epoch": 1.0848,
      "grad_norm": 0.537060558795929,
      "learning_rate": 2.288e-05,
      "loss": 0.0027,
      "step": 20340
    },
    {
      "epoch": 1.0853333333333333,
      "grad_norm": 0.21016637980937958,
      "learning_rate": 2.2866666666666666e-05,
      "loss": 0.002,
      "step": 20350
    },
    {
      "epoch": 1.0858666666666668,
      "grad_norm": 0.4095492959022522,
      "learning_rate": 2.2853333333333334e-05,
      "loss": 0.0019,
      "step": 20360
    },
    {
      "epoch": 1.0864,
      "grad_norm": 0.27316027879714966,
      "learning_rate": 2.284e-05,
      "loss": 0.002,
      "step": 20370
    },
    {
      "epoch": 1.0869333333333333,
      "grad_norm": 0.28809642791748047,
      "learning_rate": 2.282666666666667e-05,
      "loss": 0.0034,
      "step": 20380
    },
    {
      "epoch": 1.0874666666666666,
      "grad_norm": 0.83987957239151,
      "learning_rate": 2.2813333333333334e-05,
      "loss": 0.0027,
      "step": 20390
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.11080626398324966,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 0.0039,
      "step": 20400
    },
    {
      "epoch": 1.0885333333333334,
      "grad_norm": 0.1858435869216919,
      "learning_rate": 2.2786666666666666e-05,
      "loss": 0.0033,
      "step": 20410
    },
    {
      "epoch": 1.0890666666666666,
      "grad_norm": 0.3533240556716919,
      "learning_rate": 2.2773333333333334e-05,
      "loss": 0.004,
      "step": 20420
    },
    {
      "epoch": 1.0896,
      "grad_norm": 0.09697060286998749,
      "learning_rate": 2.2760000000000002e-05,
      "loss": 0.0022,
      "step": 20430
    },
    {
      "epoch": 1.0901333333333334,
      "grad_norm": 0.11431137472391129,
      "learning_rate": 2.274666666666667e-05,
      "loss": 0.0024,
      "step": 20440
    },
    {
      "epoch": 1.0906666666666667,
      "grad_norm": 0.10703885555267334,
      "learning_rate": 2.2733333333333335e-05,
      "loss": 0.0022,
      "step": 20450
    },
    {
      "epoch": 1.0912,
      "grad_norm": 0.20095494389533997,
      "learning_rate": 2.2720000000000003e-05,
      "loss": 0.0029,
      "step": 20460
    },
    {
      "epoch": 1.0917333333333334,
      "grad_norm": 0.41330480575561523,
      "learning_rate": 2.2706666666666667e-05,
      "loss": 0.0031,
      "step": 20470
    },
    {
      "epoch": 1.0922666666666667,
      "grad_norm": 0.5500307679176331,
      "learning_rate": 2.2693333333333332e-05,
      "loss": 0.0021,
      "step": 20480
    },
    {
      "epoch": 1.0928,
      "grad_norm": 0.617296576499939,
      "learning_rate": 2.268e-05,
      "loss": 0.0023,
      "step": 20490
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.5324912071228027,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 0.0017,
      "step": 20500
    },
    {
      "epoch": 1.0938666666666668,
      "grad_norm": 0.48874375224113464,
      "learning_rate": 2.2653333333333336e-05,
      "loss": 0.0023,
      "step": 20510
    },
    {
      "epoch": 1.0944,
      "grad_norm": 0.12013372778892517,
      "learning_rate": 2.264e-05,
      "loss": 0.0039,
      "step": 20520
    },
    {
      "epoch": 1.0949333333333333,
      "grad_norm": 0.48658549785614014,
      "learning_rate": 2.2626666666666668e-05,
      "loss": 0.0041,
      "step": 20530
    },
    {
      "epoch": 1.0954666666666666,
      "grad_norm": 0.9210392832756042,
      "learning_rate": 2.2613333333333333e-05,
      "loss": 0.0022,
      "step": 20540
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.14912444353103638,
      "learning_rate": 2.26e-05,
      "loss": 0.0022,
      "step": 20550
    },
    {
      "epoch": 1.0965333333333334,
      "grad_norm": 0.13098293542861938,
      "learning_rate": 2.258666666666667e-05,
      "loss": 0.0027,
      "step": 20560
    },
    {
      "epoch": 1.0970666666666666,
      "grad_norm": 0.2563891112804413,
      "learning_rate": 2.2573333333333336e-05,
      "loss": 0.0023,
      "step": 20570
    },
    {
      "epoch": 1.0976,
      "grad_norm": 0.5838854312896729,
      "learning_rate": 2.256e-05,
      "loss": 0.0024,
      "step": 20580
    },
    {
      "epoch": 1.0981333333333334,
      "grad_norm": 0.30730852484703064,
      "learning_rate": 2.254666666666667e-05,
      "loss": 0.0044,
      "step": 20590
    },
    {
      "epoch": 1.0986666666666667,
      "grad_norm": 0.10065332055091858,
      "learning_rate": 2.2533333333333333e-05,
      "loss": 0.0019,
      "step": 20600
    },
    {
      "epoch": 1.0992,
      "grad_norm": 0.3010159730911255,
      "learning_rate": 2.252e-05,
      "loss": 0.0023,
      "step": 20610
    },
    {
      "epoch": 1.0997333333333332,
      "grad_norm": 0.4372686445713043,
      "learning_rate": 2.250666666666667e-05,
      "loss": 0.0017,
      "step": 20620
    },
    {
      "epoch": 1.1002666666666667,
      "grad_norm": 0.645279049873352,
      "learning_rate": 2.2493333333333337e-05,
      "loss": 0.0021,
      "step": 20630
    },
    {
      "epoch": 1.1008,
      "grad_norm": 0.18676446378231049,
      "learning_rate": 2.248e-05,
      "loss": 0.0031,
      "step": 20640
    },
    {
      "epoch": 1.1013333333333333,
      "grad_norm": 0.34126749634742737,
      "learning_rate": 2.2466666666666666e-05,
      "loss": 0.0022,
      "step": 20650
    },
    {
      "epoch": 1.1018666666666665,
      "grad_norm": 0.3759561777114868,
      "learning_rate": 2.2453333333333334e-05,
      "loss": 0.0019,
      "step": 20660
    },
    {
      "epoch": 1.1024,
      "grad_norm": 0.5441100001335144,
      "learning_rate": 2.244e-05,
      "loss": 0.0029,
      "step": 20670
    },
    {
      "epoch": 1.1029333333333333,
      "grad_norm": 0.4237571954727173,
      "learning_rate": 2.2426666666666667e-05,
      "loss": 0.0033,
      "step": 20680
    },
    {
      "epoch": 1.1034666666666666,
      "grad_norm": 0.25129473209381104,
      "learning_rate": 2.2413333333333334e-05,
      "loss": 0.0024,
      "step": 20690
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.2090366780757904,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 0.0031,
      "step": 20700
    },
    {
      "epoch": 1.1045333333333334,
      "grad_norm": 0.24538525938987732,
      "learning_rate": 2.2386666666666667e-05,
      "loss": 0.0022,
      "step": 20710
    },
    {
      "epoch": 1.1050666666666666,
      "grad_norm": 0.08794457465410233,
      "learning_rate": 2.2373333333333335e-05,
      "loss": 0.002,
      "step": 20720
    },
    {
      "epoch": 1.1056,
      "grad_norm": 0.18094849586486816,
      "learning_rate": 2.236e-05,
      "loss": 0.0036,
      "step": 20730
    },
    {
      "epoch": 1.1061333333333334,
      "grad_norm": 0.11659139394760132,
      "learning_rate": 2.2346666666666667e-05,
      "loss": 0.002,
      "step": 20740
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.2978683114051819,
      "learning_rate": 2.2333333333333335e-05,
      "loss": 0.0029,
      "step": 20750
    },
    {
      "epoch": 1.1072,
      "grad_norm": 0.39018216729164124,
      "learning_rate": 2.2320000000000003e-05,
      "loss": 0.0023,
      "step": 20760
    },
    {
      "epoch": 1.1077333333333332,
      "grad_norm": 0.6202676892280579,
      "learning_rate": 2.2306666666666668e-05,
      "loss": 0.0026,
      "step": 20770
    },
    {
      "epoch": 1.1082666666666667,
      "grad_norm": 0.24557730555534363,
      "learning_rate": 2.2293333333333336e-05,
      "loss": 0.002,
      "step": 20780
    },
    {
      "epoch": 1.1088,
      "grad_norm": 0.0956178680062294,
      "learning_rate": 2.228e-05,
      "loss": 0.0028,
      "step": 20790
    },
    {
      "epoch": 1.1093333333333333,
      "grad_norm": 0.1348586231470108,
      "learning_rate": 2.2266666666666668e-05,
      "loss": 0.0023,
      "step": 20800
    },
    {
      "epoch": 1.1098666666666666,
      "grad_norm": 0.22131159901618958,
      "learning_rate": 2.2253333333333336e-05,
      "loss": 0.0021,
      "step": 20810
    },
    {
      "epoch": 1.1104,
      "grad_norm": 0.22465024888515472,
      "learning_rate": 2.224e-05,
      "loss": 0.0021,
      "step": 20820
    },
    {
      "epoch": 1.1109333333333333,
      "grad_norm": 0.10983375459909439,
      "learning_rate": 2.222666666666667e-05,
      "loss": 0.0027,
      "step": 20830
    },
    {
      "epoch": 1.1114666666666666,
      "grad_norm": 0.6792110204696655,
      "learning_rate": 2.2213333333333333e-05,
      "loss": 0.0018,
      "step": 20840
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.3347577452659607,
      "learning_rate": 2.22e-05,
      "loss": 0.0018,
      "step": 20850
    },
    {
      "epoch": 1.1125333333333334,
      "grad_norm": 0.739601731300354,
      "learning_rate": 2.2186666666666665e-05,
      "loss": 0.0026,
      "step": 20860
    },
    {
      "epoch": 1.1130666666666666,
      "grad_norm": 0.11037961393594742,
      "learning_rate": 2.2173333333333333e-05,
      "loss": 0.0024,
      "step": 20870
    },
    {
      "epoch": 1.1136,
      "grad_norm": 0.4565112292766571,
      "learning_rate": 2.216e-05,
      "loss": 0.0024,
      "step": 20880
    },
    {
      "epoch": 1.1141333333333334,
      "grad_norm": 0.12793616950511932,
      "learning_rate": 2.214666666666667e-05,
      "loss": 0.0025,
      "step": 20890
    },
    {
      "epoch": 1.1146666666666667,
      "grad_norm": 0.586479127407074,
      "learning_rate": 2.2133333333333334e-05,
      "loss": 0.0021,
      "step": 20900
    },
    {
      "epoch": 1.1152,
      "grad_norm": 0.21968135237693787,
      "learning_rate": 2.212e-05,
      "loss": 0.0025,
      "step": 20910
    },
    {
      "epoch": 1.1157333333333332,
      "grad_norm": 0.33655524253845215,
      "learning_rate": 2.2106666666666666e-05,
      "loss": 0.0027,
      "step": 20920
    },
    {
      "epoch": 1.1162666666666667,
      "grad_norm": 0.3117634356021881,
      "learning_rate": 2.2093333333333334e-05,
      "loss": 0.0033,
      "step": 20930
    },
    {
      "epoch": 1.1168,
      "grad_norm": 0.1341010183095932,
      "learning_rate": 2.2080000000000002e-05,
      "loss": 0.0026,
      "step": 20940
    },
    {
      "epoch": 1.1173333333333333,
      "grad_norm": 0.15701553225517273,
      "learning_rate": 2.206666666666667e-05,
      "loss": 0.0025,
      "step": 20950
    },
    {
      "epoch": 1.1178666666666666,
      "grad_norm": 0.5043298006057739,
      "learning_rate": 2.2053333333333335e-05,
      "loss": 0.0026,
      "step": 20960
    },
    {
      "epoch": 1.1184,
      "grad_norm": 0.3154078423976898,
      "learning_rate": 2.2040000000000002e-05,
      "loss": 0.0031,
      "step": 20970
    },
    {
      "epoch": 1.1189333333333333,
      "grad_norm": 0.16198217868804932,
      "learning_rate": 2.2026666666666667e-05,
      "loss": 0.0029,
      "step": 20980
    },
    {
      "epoch": 1.1194666666666666,
      "grad_norm": 0.4805166721343994,
      "learning_rate": 2.201333333333333e-05,
      "loss": 0.0032,
      "step": 20990
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.5149199962615967,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.003,
      "step": 21000
    },
    {
      "epoch": 1.1205333333333334,
      "grad_norm": 0.32267650961875916,
      "learning_rate": 2.1986666666666667e-05,
      "loss": 0.0028,
      "step": 21010
    },
    {
      "epoch": 1.1210666666666667,
      "grad_norm": 0.3732917010784149,
      "learning_rate": 2.1973333333333335e-05,
      "loss": 0.0037,
      "step": 21020
    },
    {
      "epoch": 1.1216,
      "grad_norm": 0.09403282403945923,
      "learning_rate": 2.196e-05,
      "loss": 0.0042,
      "step": 21030
    },
    {
      "epoch": 1.1221333333333334,
      "grad_norm": 0.3447921872138977,
      "learning_rate": 2.1946666666666668e-05,
      "loss": 0.0022,
      "step": 21040
    },
    {
      "epoch": 1.1226666666666667,
      "grad_norm": 0.3881225287914276,
      "learning_rate": 2.1933333333333332e-05,
      "loss": 0.0031,
      "step": 21050
    },
    {
      "epoch": 1.1232,
      "grad_norm": 0.12402106076478958,
      "learning_rate": 2.192e-05,
      "loss": 0.004,
      "step": 21060
    },
    {
      "epoch": 1.1237333333333333,
      "grad_norm": 0.21555903553962708,
      "learning_rate": 2.1906666666666668e-05,
      "loss": 0.0025,
      "step": 21070
    },
    {
      "epoch": 1.1242666666666667,
      "grad_norm": 0.16358275711536407,
      "learning_rate": 2.1893333333333336e-05,
      "loss": 0.0021,
      "step": 21080
    },
    {
      "epoch": 1.1248,
      "grad_norm": 0.3576943576335907,
      "learning_rate": 2.188e-05,
      "loss": 0.0038,
      "step": 21090
    },
    {
      "epoch": 1.1253333333333333,
      "grad_norm": 0.2185232788324356,
      "learning_rate": 2.186666666666667e-05,
      "loss": 0.0029,
      "step": 21100
    },
    {
      "epoch": 1.1258666666666666,
      "grad_norm": 0.15367217361927032,
      "learning_rate": 2.1853333333333333e-05,
      "loss": 0.0023,
      "step": 21110
    },
    {
      "epoch": 1.1264,
      "grad_norm": 0.11350598931312561,
      "learning_rate": 2.184e-05,
      "loss": 0.0026,
      "step": 21120
    },
    {
      "epoch": 1.1269333333333333,
      "grad_norm": 0.31491097807884216,
      "learning_rate": 2.182666666666667e-05,
      "loss": 0.0022,
      "step": 21130
    },
    {
      "epoch": 1.1274666666666666,
      "grad_norm": 0.07857856154441833,
      "learning_rate": 2.1813333333333337e-05,
      "loss": 0.0033,
      "step": 21140
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.15439285337924957,
      "learning_rate": 2.18e-05,
      "loss": 0.0022,
      "step": 21150
    },
    {
      "epoch": 1.1285333333333334,
      "grad_norm": 0.3037622570991516,
      "learning_rate": 2.1786666666666666e-05,
      "loss": 0.0026,
      "step": 21160
    },
    {
      "epoch": 1.1290666666666667,
      "grad_norm": 0.14609667658805847,
      "learning_rate": 2.1773333333333334e-05,
      "loss": 0.0027,
      "step": 21170
    },
    {
      "epoch": 1.1296,
      "grad_norm": 0.4209032654762268,
      "learning_rate": 2.176e-05,
      "loss": 0.0019,
      "step": 21180
    },
    {
      "epoch": 1.1301333333333332,
      "grad_norm": 0.47885262966156006,
      "learning_rate": 2.174666666666667e-05,
      "loss": 0.0022,
      "step": 21190
    },
    {
      "epoch": 1.1306666666666667,
      "grad_norm": 0.10983085632324219,
      "learning_rate": 2.1733333333333334e-05,
      "loss": 0.0025,
      "step": 21200
    },
    {
      "epoch": 1.1312,
      "grad_norm": 0.5036956667900085,
      "learning_rate": 2.1720000000000002e-05,
      "loss": 0.0024,
      "step": 21210
    },
    {
      "epoch": 1.1317333333333333,
      "grad_norm": 0.35926586389541626,
      "learning_rate": 2.1706666666666667e-05,
      "loss": 0.003,
      "step": 21220
    },
    {
      "epoch": 1.1322666666666668,
      "grad_norm": 0.5418834686279297,
      "learning_rate": 2.1693333333333335e-05,
      "loss": 0.0031,
      "step": 21230
    },
    {
      "epoch": 1.1328,
      "grad_norm": 0.2145286202430725,
      "learning_rate": 2.168e-05,
      "loss": 0.0029,
      "step": 21240
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.5002025365829468,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 0.0024,
      "step": 21250
    },
    {
      "epoch": 1.1338666666666666,
      "grad_norm": 0.42379817366600037,
      "learning_rate": 2.1653333333333335e-05,
      "loss": 0.0018,
      "step": 21260
    },
    {
      "epoch": 1.1344,
      "grad_norm": 0.5214234590530396,
      "learning_rate": 2.1640000000000003e-05,
      "loss": 0.0022,
      "step": 21270
    },
    {
      "epoch": 1.1349333333333333,
      "grad_norm": 0.5212952494621277,
      "learning_rate": 2.1626666666666667e-05,
      "loss": 0.0024,
      "step": 21280
    },
    {
      "epoch": 1.1354666666666666,
      "grad_norm": 0.2889593839645386,
      "learning_rate": 2.1613333333333335e-05,
      "loss": 0.0026,
      "step": 21290
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.29124078154563904,
      "learning_rate": 2.16e-05,
      "loss": 0.0026,
      "step": 21300
    },
    {
      "epoch": 1.1365333333333334,
      "grad_norm": 0.0655011311173439,
      "learning_rate": 2.1586666666666668e-05,
      "loss": 0.0024,
      "step": 21310
    },
    {
      "epoch": 1.1370666666666667,
      "grad_norm": 0.22614039480686188,
      "learning_rate": 2.1573333333333336e-05,
      "loss": 0.0028,
      "step": 21320
    },
    {
      "epoch": 1.1376,
      "grad_norm": 0.3547710180282593,
      "learning_rate": 2.1560000000000004e-05,
      "loss": 0.0034,
      "step": 21330
    },
    {
      "epoch": 1.1381333333333332,
      "grad_norm": 0.36674195528030396,
      "learning_rate": 2.1546666666666668e-05,
      "loss": 0.0034,
      "step": 21340
    },
    {
      "epoch": 1.1386666666666667,
      "grad_norm": 0.3504030704498291,
      "learning_rate": 2.1533333333333333e-05,
      "loss": 0.0029,
      "step": 21350
    },
    {
      "epoch": 1.1392,
      "grad_norm": 0.2889641523361206,
      "learning_rate": 2.152e-05,
      "loss": 0.0036,
      "step": 21360
    },
    {
      "epoch": 1.1397333333333333,
      "grad_norm": 0.12956632673740387,
      "learning_rate": 2.1506666666666665e-05,
      "loss": 0.0028,
      "step": 21370
    },
    {
      "epoch": 1.1402666666666668,
      "grad_norm": 0.3415798842906952,
      "learning_rate": 2.1493333333333333e-05,
      "loss": 0.0028,
      "step": 21380
    },
    {
      "epoch": 1.1408,
      "grad_norm": 0.08085568249225616,
      "learning_rate": 2.148e-05,
      "loss": 0.0017,
      "step": 21390
    },
    {
      "epoch": 1.1413333333333333,
      "grad_norm": 0.11351719498634338,
      "learning_rate": 2.146666666666667e-05,
      "loss": 0.002,
      "step": 21400
    },
    {
      "epoch": 1.1418666666666666,
      "grad_norm": 0.12991924583911896,
      "learning_rate": 2.1453333333333333e-05,
      "loss": 0.0017,
      "step": 21410
    },
    {
      "epoch": 1.1424,
      "grad_norm": 0.3861871659755707,
      "learning_rate": 2.144e-05,
      "loss": 0.0027,
      "step": 21420
    },
    {
      "epoch": 1.1429333333333334,
      "grad_norm": 0.6845860481262207,
      "learning_rate": 2.1426666666666666e-05,
      "loss": 0.0031,
      "step": 21430
    },
    {
      "epoch": 1.1434666666666666,
      "grad_norm": 0.4263528883457184,
      "learning_rate": 2.1413333333333334e-05,
      "loss": 0.0037,
      "step": 21440
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.2803431451320648,
      "learning_rate": 2.1400000000000002e-05,
      "loss": 0.0028,
      "step": 21450
    },
    {
      "epoch": 1.1445333333333334,
      "grad_norm": 0.2393125295639038,
      "learning_rate": 2.138666666666667e-05,
      "loss": 0.0028,
      "step": 21460
    },
    {
      "epoch": 1.1450666666666667,
      "grad_norm": 0.4750531017780304,
      "learning_rate": 2.1373333333333334e-05,
      "loss": 0.002,
      "step": 21470
    },
    {
      "epoch": 1.1456,
      "grad_norm": 0.5335237979888916,
      "learning_rate": 2.1360000000000002e-05,
      "loss": 0.0026,
      "step": 21480
    },
    {
      "epoch": 1.1461333333333332,
      "grad_norm": 0.1880994737148285,
      "learning_rate": 2.1346666666666667e-05,
      "loss": 0.0022,
      "step": 21490
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.1563437581062317,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 0.0022,
      "step": 21500
    },
    {
      "epoch": 1.1472,
      "grad_norm": 0.3523237109184265,
      "learning_rate": 2.1320000000000003e-05,
      "loss": 0.0027,
      "step": 21510
    },
    {
      "epoch": 1.1477333333333333,
      "grad_norm": 0.10495872795581818,
      "learning_rate": 2.1306666666666667e-05,
      "loss": 0.0025,
      "step": 21520
    },
    {
      "epoch": 1.1482666666666668,
      "grad_norm": 0.4067528247833252,
      "learning_rate": 2.1293333333333335e-05,
      "loss": 0.0024,
      "step": 21530
    },
    {
      "epoch": 1.1488,
      "grad_norm": 0.10067088901996613,
      "learning_rate": 2.128e-05,
      "loss": 0.0028,
      "step": 21540
    },
    {
      "epoch": 1.1493333333333333,
      "grad_norm": 0.5639714598655701,
      "learning_rate": 2.1266666666666667e-05,
      "loss": 0.0022,
      "step": 21550
    },
    {
      "epoch": 1.1498666666666666,
      "grad_norm": 0.22692400217056274,
      "learning_rate": 2.1253333333333332e-05,
      "loss": 0.0018,
      "step": 21560
    },
    {
      "epoch": 1.1504,
      "grad_norm": 0.09491598606109619,
      "learning_rate": 2.124e-05,
      "loss": 0.003,
      "step": 21570
    },
    {
      "epoch": 1.1509333333333334,
      "grad_norm": 0.26803088188171387,
      "learning_rate": 2.1226666666666668e-05,
      "loss": 0.0035,
      "step": 21580
    },
    {
      "epoch": 1.1514666666666666,
      "grad_norm": 0.13891011476516724,
      "learning_rate": 2.1213333333333336e-05,
      "loss": 0.0022,
      "step": 21590
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.564343273639679,
      "learning_rate": 2.12e-05,
      "loss": 0.0021,
      "step": 21600
    },
    {
      "epoch": 1.1525333333333334,
      "grad_norm": 0.09055277705192566,
      "learning_rate": 2.1186666666666668e-05,
      "loss": 0.002,
      "step": 21610
    },
    {
      "epoch": 1.1530666666666667,
      "grad_norm": 0.12516550719738007,
      "learning_rate": 2.1173333333333333e-05,
      "loss": 0.0032,
      "step": 21620
    },
    {
      "epoch": 1.1536,
      "grad_norm": 0.6538342833518982,
      "learning_rate": 2.116e-05,
      "loss": 0.0034,
      "step": 21630
    },
    {
      "epoch": 1.1541333333333332,
      "grad_norm": 0.08716539293527603,
      "learning_rate": 2.114666666666667e-05,
      "loss": 0.0017,
      "step": 21640
    },
    {
      "epoch": 1.1546666666666667,
      "grad_norm": 0.22237198054790497,
      "learning_rate": 2.1133333333333337e-05,
      "loss": 0.0018,
      "step": 21650
    },
    {
      "epoch": 1.1552,
      "grad_norm": 0.33236396312713623,
      "learning_rate": 2.112e-05,
      "loss": 0.0025,
      "step": 21660
    },
    {
      "epoch": 1.1557333333333333,
      "grad_norm": 0.1906425803899765,
      "learning_rate": 2.110666666666667e-05,
      "loss": 0.002,
      "step": 21670
    },
    {
      "epoch": 1.1562666666666668,
      "grad_norm": 0.5554922223091125,
      "learning_rate": 2.1093333333333334e-05,
      "loss": 0.0018,
      "step": 21680
    },
    {
      "epoch": 1.1568,
      "grad_norm": 0.26195740699768066,
      "learning_rate": 2.1079999999999998e-05,
      "loss": 0.0023,
      "step": 21690
    },
    {
      "epoch": 1.1573333333333333,
      "grad_norm": 0.16586272418498993,
      "learning_rate": 2.106666666666667e-05,
      "loss": 0.0027,
      "step": 21700
    },
    {
      "epoch": 1.1578666666666666,
      "grad_norm": 0.29510393738746643,
      "learning_rate": 2.1053333333333334e-05,
      "loss": 0.0045,
      "step": 21710
    },
    {
      "epoch": 1.1584,
      "grad_norm": 0.17887578904628754,
      "learning_rate": 2.1040000000000002e-05,
      "loss": 0.0025,
      "step": 21720
    },
    {
      "epoch": 1.1589333333333334,
      "grad_norm": 0.1550663411617279,
      "learning_rate": 2.1026666666666666e-05,
      "loss": 0.0032,
      "step": 21730
    },
    {
      "epoch": 1.1594666666666666,
      "grad_norm": 0.538398027420044,
      "learning_rate": 2.1013333333333334e-05,
      "loss": 0.0023,
      "step": 21740
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.4150215685367584,
      "learning_rate": 2.1e-05,
      "loss": 0.0026,
      "step": 21750
    },
    {
      "epoch": 1.1605333333333334,
      "grad_norm": 0.19760286808013916,
      "learning_rate": 2.0986666666666667e-05,
      "loss": 0.002,
      "step": 21760
    },
    {
      "epoch": 1.1610666666666667,
      "grad_norm": 0.15825937688350677,
      "learning_rate": 2.0973333333333335e-05,
      "loss": 0.0031,
      "step": 21770
    },
    {
      "epoch": 1.1616,
      "grad_norm": 0.5174844861030579,
      "learning_rate": 2.0960000000000003e-05,
      "loss": 0.0026,
      "step": 21780
    },
    {
      "epoch": 1.1621333333333332,
      "grad_norm": 0.2874842584133148,
      "learning_rate": 2.0946666666666667e-05,
      "loss": 0.0022,
      "step": 21790
    },
    {
      "epoch": 1.1626666666666667,
      "grad_norm": 0.1509447693824768,
      "learning_rate": 2.0933333333333335e-05,
      "loss": 0.0018,
      "step": 21800
    },
    {
      "epoch": 1.1632,
      "grad_norm": 0.479148268699646,
      "learning_rate": 2.092e-05,
      "loss": 0.0027,
      "step": 21810
    },
    {
      "epoch": 1.1637333333333333,
      "grad_norm": 0.1493615359067917,
      "learning_rate": 2.0906666666666668e-05,
      "loss": 0.0028,
      "step": 21820
    },
    {
      "epoch": 1.1642666666666668,
      "grad_norm": 0.5077409744262695,
      "learning_rate": 2.0893333333333335e-05,
      "loss": 0.0025,
      "step": 21830
    },
    {
      "epoch": 1.1648,
      "grad_norm": 0.13446898758411407,
      "learning_rate": 2.0880000000000003e-05,
      "loss": 0.0027,
      "step": 21840
    },
    {
      "epoch": 1.1653333333333333,
      "grad_norm": 0.12718303501605988,
      "learning_rate": 2.0866666666666668e-05,
      "loss": 0.003,
      "step": 21850
    },
    {
      "epoch": 1.1658666666666666,
      "grad_norm": 0.08509518206119537,
      "learning_rate": 2.0853333333333332e-05,
      "loss": 0.002,
      "step": 21860
    },
    {
      "epoch": 1.1663999999999999,
      "grad_norm": 0.19465839862823486,
      "learning_rate": 2.084e-05,
      "loss": 0.0024,
      "step": 21870
    },
    {
      "epoch": 1.1669333333333334,
      "grad_norm": 0.36835113167762756,
      "learning_rate": 2.0826666666666665e-05,
      "loss": 0.003,
      "step": 21880
    },
    {
      "epoch": 1.1674666666666667,
      "grad_norm": 0.1546810418367386,
      "learning_rate": 2.0813333333333336e-05,
      "loss": 0.002,
      "step": 21890
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.16557741165161133,
      "learning_rate": 2.08e-05,
      "loss": 0.0036,
      "step": 21900
    },
    {
      "epoch": 1.1685333333333334,
      "grad_norm": 0.2964257001876831,
      "learning_rate": 2.078666666666667e-05,
      "loss": 0.0034,
      "step": 21910
    },
    {
      "epoch": 1.1690666666666667,
      "grad_norm": 0.13252949714660645,
      "learning_rate": 2.0773333333333333e-05,
      "loss": 0.0035,
      "step": 21920
    },
    {
      "epoch": 1.1696,
      "grad_norm": 0.3630176782608032,
      "learning_rate": 2.076e-05,
      "loss": 0.0023,
      "step": 21930
    },
    {
      "epoch": 1.1701333333333332,
      "grad_norm": 0.3380429744720459,
      "learning_rate": 2.0746666666666666e-05,
      "loss": 0.0023,
      "step": 21940
    },
    {
      "epoch": 1.1706666666666667,
      "grad_norm": 0.3859151005744934,
      "learning_rate": 2.0733333333333334e-05,
      "loss": 0.0036,
      "step": 21950
    },
    {
      "epoch": 1.1712,
      "grad_norm": 0.38981106877326965,
      "learning_rate": 2.072e-05,
      "loss": 0.0022,
      "step": 21960
    },
    {
      "epoch": 1.1717333333333333,
      "grad_norm": 0.41534700989723206,
      "learning_rate": 2.070666666666667e-05,
      "loss": 0.0018,
      "step": 21970
    },
    {
      "epoch": 1.1722666666666668,
      "grad_norm": 0.22824272513389587,
      "learning_rate": 2.0693333333333334e-05,
      "loss": 0.0034,
      "step": 21980
    },
    {
      "epoch": 1.1728,
      "grad_norm": 0.20820656418800354,
      "learning_rate": 2.0680000000000002e-05,
      "loss": 0.002,
      "step": 21990
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.14138832688331604,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 0.002,
      "step": 22000
    },
    {
      "epoch": 1.1738666666666666,
      "grad_norm": 0.16189610958099365,
      "learning_rate": 2.0653333333333334e-05,
      "loss": 0.0021,
      "step": 22010
    },
    {
      "epoch": 1.1743999999999999,
      "grad_norm": 0.16905485093593597,
      "learning_rate": 2.0640000000000002e-05,
      "loss": 0.0031,
      "step": 22020
    },
    {
      "epoch": 1.1749333333333334,
      "grad_norm": 0.26377367973327637,
      "learning_rate": 2.0626666666666667e-05,
      "loss": 0.0023,
      "step": 22030
    },
    {
      "epoch": 1.1754666666666667,
      "grad_norm": 0.1563272476196289,
      "learning_rate": 2.0613333333333335e-05,
      "loss": 0.004,
      "step": 22040
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.5629945397377014,
      "learning_rate": 2.06e-05,
      "loss": 0.003,
      "step": 22050
    },
    {
      "epoch": 1.1765333333333334,
      "grad_norm": 0.3253888785839081,
      "learning_rate": 2.0586666666666667e-05,
      "loss": 0.0034,
      "step": 22060
    },
    {
      "epoch": 1.1770666666666667,
      "grad_norm": 0.08212018758058548,
      "learning_rate": 2.0573333333333332e-05,
      "loss": 0.0027,
      "step": 22070
    },
    {
      "epoch": 1.1776,
      "grad_norm": 0.23121491074562073,
      "learning_rate": 2.0560000000000003e-05,
      "loss": 0.0021,
      "step": 22080
    },
    {
      "epoch": 1.1781333333333333,
      "grad_norm": 0.3683907985687256,
      "learning_rate": 2.0546666666666668e-05,
      "loss": 0.0023,
      "step": 22090
    },
    {
      "epoch": 1.1786666666666668,
      "grad_norm": 0.46962979435920715,
      "learning_rate": 2.0533333333333336e-05,
      "loss": 0.0022,
      "step": 22100
    },
    {
      "epoch": 1.1792,
      "grad_norm": 0.12725171446800232,
      "learning_rate": 2.052e-05,
      "loss": 0.0024,
      "step": 22110
    },
    {
      "epoch": 1.1797333333333333,
      "grad_norm": 0.4437195956707001,
      "learning_rate": 2.0506666666666668e-05,
      "loss": 0.0026,
      "step": 22120
    },
    {
      "epoch": 1.1802666666666666,
      "grad_norm": 0.4388338029384613,
      "learning_rate": 2.0493333333333333e-05,
      "loss": 0.0032,
      "step": 22130
    },
    {
      "epoch": 1.1808,
      "grad_norm": 0.20518182218074799,
      "learning_rate": 2.048e-05,
      "loss": 0.0028,
      "step": 22140
    },
    {
      "epoch": 1.1813333333333333,
      "grad_norm": 0.19462727010250092,
      "learning_rate": 2.046666666666667e-05,
      "loss": 0.0032,
      "step": 22150
    },
    {
      "epoch": 1.1818666666666666,
      "grad_norm": 0.3992835581302643,
      "learning_rate": 2.0453333333333336e-05,
      "loss": 0.0028,
      "step": 22160
    },
    {
      "epoch": 1.1824,
      "grad_norm": 0.4494633078575134,
      "learning_rate": 2.044e-05,
      "loss": 0.002,
      "step": 22170
    },
    {
      "epoch": 1.1829333333333334,
      "grad_norm": 0.3603053390979767,
      "learning_rate": 2.042666666666667e-05,
      "loss": 0.0019,
      "step": 22180
    },
    {
      "epoch": 1.1834666666666667,
      "grad_norm": 0.1370495855808258,
      "learning_rate": 2.0413333333333333e-05,
      "loss": 0.0018,
      "step": 22190
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.1334352344274521,
      "learning_rate": 2.04e-05,
      "loss": 0.0026,
      "step": 22200
    },
    {
      "epoch": 1.1845333333333334,
      "grad_norm": 0.3190203905105591,
      "learning_rate": 2.038666666666667e-05,
      "loss": 0.0027,
      "step": 22210
    },
    {
      "epoch": 1.1850666666666667,
      "grad_norm": 0.21699495613574982,
      "learning_rate": 2.0373333333333334e-05,
      "loss": 0.0024,
      "step": 22220
    },
    {
      "epoch": 1.1856,
      "grad_norm": 0.20685310661792755,
      "learning_rate": 2.036e-05,
      "loss": 0.0027,
      "step": 22230
    },
    {
      "epoch": 1.1861333333333333,
      "grad_norm": 0.7657751441001892,
      "learning_rate": 2.0346666666666666e-05,
      "loss": 0.0027,
      "step": 22240
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.14745885133743286,
      "learning_rate": 2.0333333333333334e-05,
      "loss": 0.0019,
      "step": 22250
    },
    {
      "epoch": 1.1872,
      "grad_norm": 0.3017004132270813,
      "learning_rate": 2.032e-05,
      "loss": 0.0031,
      "step": 22260
    },
    {
      "epoch": 1.1877333333333333,
      "grad_norm": 0.6080910563468933,
      "learning_rate": 2.030666666666667e-05,
      "loss": 0.0027,
      "step": 22270
    },
    {
      "epoch": 1.1882666666666666,
      "grad_norm": 0.2950526773929596,
      "learning_rate": 2.0293333333333334e-05,
      "loss": 0.0026,
      "step": 22280
    },
    {
      "epoch": 1.1888,
      "grad_norm": 0.11605443805456161,
      "learning_rate": 2.0280000000000002e-05,
      "loss": 0.0027,
      "step": 22290
    },
    {
      "epoch": 1.1893333333333334,
      "grad_norm": 0.2925022542476654,
      "learning_rate": 2.0266666666666667e-05,
      "loss": 0.0021,
      "step": 22300
    },
    {
      "epoch": 1.1898666666666666,
      "grad_norm": 0.19466909766197205,
      "learning_rate": 2.0253333333333335e-05,
      "loss": 0.0019,
      "step": 22310
    },
    {
      "epoch": 1.1904,
      "grad_norm": 0.3209317624568939,
      "learning_rate": 2.024e-05,
      "loss": 0.0025,
      "step": 22320
    },
    {
      "epoch": 1.1909333333333334,
      "grad_norm": 0.06337860971689224,
      "learning_rate": 2.0226666666666667e-05,
      "loss": 0.0021,
      "step": 22330
    },
    {
      "epoch": 1.1914666666666667,
      "grad_norm": 0.2314884215593338,
      "learning_rate": 2.0213333333333335e-05,
      "loss": 0.0032,
      "step": 22340
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.430801123380661,
      "learning_rate": 2.0200000000000003e-05,
      "loss": 0.0019,
      "step": 22350
    },
    {
      "epoch": 1.1925333333333334,
      "grad_norm": 0.530488133430481,
      "learning_rate": 2.0186666666666668e-05,
      "loss": 0.0023,
      "step": 22360
    },
    {
      "epoch": 1.1930666666666667,
      "grad_norm": 0.3909229338169098,
      "learning_rate": 2.0173333333333332e-05,
      "loss": 0.0033,
      "step": 22370
    },
    {
      "epoch": 1.1936,
      "grad_norm": 0.13394059240818024,
      "learning_rate": 2.016e-05,
      "loss": 0.0021,
      "step": 22380
    },
    {
      "epoch": 1.1941333333333333,
      "grad_norm": 0.0617852620780468,
      "learning_rate": 2.0146666666666668e-05,
      "loss": 0.0035,
      "step": 22390
    },
    {
      "epoch": 1.1946666666666665,
      "grad_norm": 0.3391518294811249,
      "learning_rate": 2.0133333333333336e-05,
      "loss": 0.0028,
      "step": 22400
    },
    {
      "epoch": 1.1952,
      "grad_norm": 0.18784527480602264,
      "learning_rate": 2.012e-05,
      "loss": 0.0034,
      "step": 22410
    },
    {
      "epoch": 1.1957333333333333,
      "grad_norm": 0.7695397734642029,
      "learning_rate": 2.010666666666667e-05,
      "loss": 0.003,
      "step": 22420
    },
    {
      "epoch": 1.1962666666666666,
      "grad_norm": 0.2752530574798584,
      "learning_rate": 2.0093333333333333e-05,
      "loss": 0.0033,
      "step": 22430
    },
    {
      "epoch": 1.1968,
      "grad_norm": 0.09264086186885834,
      "learning_rate": 2.008e-05,
      "loss": 0.0021,
      "step": 22440
    },
    {
      "epoch": 1.1973333333333334,
      "grad_norm": 0.3425503075122833,
      "learning_rate": 2.0066666666666665e-05,
      "loss": 0.0022,
      "step": 22450
    },
    {
      "epoch": 1.1978666666666666,
      "grad_norm": 0.1778467744588852,
      "learning_rate": 2.0053333333333337e-05,
      "loss": 0.003,
      "step": 22460
    },
    {
      "epoch": 1.1984,
      "grad_norm": 0.5604854822158813,
      "learning_rate": 2.004e-05,
      "loss": 0.002,
      "step": 22470
    },
    {
      "epoch": 1.1989333333333334,
      "grad_norm": 0.3768366873264313,
      "learning_rate": 2.002666666666667e-05,
      "loss": 0.003,
      "step": 22480
    },
    {
      "epoch": 1.1994666666666667,
      "grad_norm": 0.21723678708076477,
      "learning_rate": 2.0013333333333334e-05,
      "loss": 0.0026,
      "step": 22490
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.38236942887306213,
      "learning_rate": 2e-05,
      "loss": 0.0017,
      "step": 22500
    },
    {
      "epoch": 1.2005333333333335,
      "grad_norm": 0.3418823778629303,
      "learning_rate": 1.9986666666666666e-05,
      "loss": 0.002,
      "step": 22510
    },
    {
      "epoch": 1.2010666666666667,
      "grad_norm": 0.19995400309562683,
      "learning_rate": 1.9973333333333334e-05,
      "loss": 0.0027,
      "step": 22520
    },
    {
      "epoch": 1.2016,
      "grad_norm": 0.15671086311340332,
      "learning_rate": 1.9960000000000002e-05,
      "loss": 0.0031,
      "step": 22530
    },
    {
      "epoch": 1.2021333333333333,
      "grad_norm": 0.25517988204956055,
      "learning_rate": 1.9946666666666667e-05,
      "loss": 0.003,
      "step": 22540
    },
    {
      "epoch": 1.2026666666666666,
      "grad_norm": 0.3723732829093933,
      "learning_rate": 1.9933333333333334e-05,
      "loss": 0.0031,
      "step": 22550
    },
    {
      "epoch": 1.2032,
      "grad_norm": 0.1257992535829544,
      "learning_rate": 1.992e-05,
      "loss": 0.0026,
      "step": 22560
    },
    {
      "epoch": 1.2037333333333333,
      "grad_norm": 0.3065685033798218,
      "learning_rate": 1.9906666666666667e-05,
      "loss": 0.0028,
      "step": 22570
    },
    {
      "epoch": 1.2042666666666666,
      "grad_norm": 0.17628560960292816,
      "learning_rate": 1.9893333333333335e-05,
      "loss": 0.0028,
      "step": 22580
    },
    {
      "epoch": 1.2048,
      "grad_norm": 0.13360846042633057,
      "learning_rate": 1.9880000000000003e-05,
      "loss": 0.002,
      "step": 22590
    },
    {
      "epoch": 1.2053333333333334,
      "grad_norm": 0.20816443860530853,
      "learning_rate": 1.9866666666666667e-05,
      "loss": 0.0018,
      "step": 22600
    },
    {
      "epoch": 1.2058666666666666,
      "grad_norm": 0.5144121646881104,
      "learning_rate": 1.9853333333333335e-05,
      "loss": 0.0018,
      "step": 22610
    },
    {
      "epoch": 1.2064,
      "grad_norm": 0.17199887335300446,
      "learning_rate": 1.984e-05,
      "loss": 0.0024,
      "step": 22620
    },
    {
      "epoch": 1.2069333333333334,
      "grad_norm": 0.2725047767162323,
      "learning_rate": 1.9826666666666668e-05,
      "loss": 0.0041,
      "step": 22630
    },
    {
      "epoch": 1.2074666666666667,
      "grad_norm": 0.14803138375282288,
      "learning_rate": 1.9813333333333332e-05,
      "loss": 0.0027,
      "step": 22640
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.26399311423301697,
      "learning_rate": 1.9800000000000004e-05,
      "loss": 0.0033,
      "step": 22650
    },
    {
      "epoch": 1.2085333333333332,
      "grad_norm": 0.3617774546146393,
      "learning_rate": 1.9786666666666668e-05,
      "loss": 0.0019,
      "step": 22660
    },
    {
      "epoch": 1.2090666666666667,
      "grad_norm": 0.10152462869882584,
      "learning_rate": 1.9773333333333336e-05,
      "loss": 0.0036,
      "step": 22670
    },
    {
      "epoch": 1.2096,
      "grad_norm": 0.2164756804704666,
      "learning_rate": 1.976e-05,
      "loss": 0.0029,
      "step": 22680
    },
    {
      "epoch": 1.2101333333333333,
      "grad_norm": 0.16673512756824493,
      "learning_rate": 1.974666666666667e-05,
      "loss": 0.0024,
      "step": 22690
    },
    {
      "epoch": 1.2106666666666666,
      "grad_norm": 0.3570484519004822,
      "learning_rate": 1.9733333333333333e-05,
      "loss": 0.0028,
      "step": 22700
    },
    {
      "epoch": 1.2112,
      "grad_norm": 0.2117559164762497,
      "learning_rate": 1.972e-05,
      "loss": 0.0018,
      "step": 22710
    },
    {
      "epoch": 1.2117333333333333,
      "grad_norm": 0.20386752486228943,
      "learning_rate": 1.970666666666667e-05,
      "loss": 0.0029,
      "step": 22720
    },
    {
      "epoch": 1.2122666666666666,
      "grad_norm": 0.2973926365375519,
      "learning_rate": 1.9693333333333333e-05,
      "loss": 0.0046,
      "step": 22730
    },
    {
      "epoch": 1.2128,
      "grad_norm": 0.4114908277988434,
      "learning_rate": 1.968e-05,
      "loss": 0.0033,
      "step": 22740
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.09455544501543045,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 0.0021,
      "step": 22750
    },
    {
      "epoch": 1.2138666666666666,
      "grad_norm": 0.23585759103298187,
      "learning_rate": 1.9653333333333334e-05,
      "loss": 0.0022,
      "step": 22760
    },
    {
      "epoch": 1.2144,
      "grad_norm": 0.33523255586624146,
      "learning_rate": 1.9640000000000002e-05,
      "loss": 0.004,
      "step": 22770
    },
    {
      "epoch": 1.2149333333333334,
      "grad_norm": 0.7783383131027222,
      "learning_rate": 1.962666666666667e-05,
      "loss": 0.0031,
      "step": 22780
    },
    {
      "epoch": 1.2154666666666667,
      "grad_norm": 0.11621618270874023,
      "learning_rate": 1.9613333333333334e-05,
      "loss": 0.002,
      "step": 22790
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.1538008451461792,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.0022,
      "step": 22800
    },
    {
      "epoch": 1.2165333333333332,
      "grad_norm": 0.29483145475387573,
      "learning_rate": 1.9586666666666667e-05,
      "loss": 0.0023,
      "step": 22810
    },
    {
      "epoch": 1.2170666666666667,
      "grad_norm": 0.24865394830703735,
      "learning_rate": 1.9573333333333335e-05,
      "loss": 0.0024,
      "step": 22820
    },
    {
      "epoch": 1.2176,
      "grad_norm": 0.1388656049966812,
      "learning_rate": 1.956e-05,
      "loss": 0.0025,
      "step": 22830
    },
    {
      "epoch": 1.2181333333333333,
      "grad_norm": 0.41909804940223694,
      "learning_rate": 1.9546666666666667e-05,
      "loss": 0.002,
      "step": 22840
    },
    {
      "epoch": 1.2186666666666666,
      "grad_norm": 0.2899324297904968,
      "learning_rate": 1.9533333333333335e-05,
      "loss": 0.0028,
      "step": 22850
    },
    {
      "epoch": 1.2192,
      "grad_norm": 0.12481265515089035,
      "learning_rate": 1.9520000000000003e-05,
      "loss": 0.0022,
      "step": 22860
    },
    {
      "epoch": 1.2197333333333333,
      "grad_norm": 0.38974863290786743,
      "learning_rate": 1.9506666666666667e-05,
      "loss": 0.0036,
      "step": 22870
    },
    {
      "epoch": 1.2202666666666666,
      "grad_norm": 0.2813396453857422,
      "learning_rate": 1.9493333333333332e-05,
      "loss": 0.0024,
      "step": 22880
    },
    {
      "epoch": 1.2208,
      "grad_norm": 0.18836478888988495,
      "learning_rate": 1.948e-05,
      "loss": 0.004,
      "step": 22890
    },
    {
      "epoch": 1.2213333333333334,
      "grad_norm": 0.6107548475265503,
      "learning_rate": 1.9466666666666668e-05,
      "loss": 0.0028,
      "step": 22900
    },
    {
      "epoch": 1.2218666666666667,
      "grad_norm": 0.12975090742111206,
      "learning_rate": 1.9453333333333336e-05,
      "loss": 0.0044,
      "step": 22910
    },
    {
      "epoch": 1.2224,
      "grad_norm": 0.5076084136962891,
      "learning_rate": 1.944e-05,
      "loss": 0.0024,
      "step": 22920
    },
    {
      "epoch": 1.2229333333333334,
      "grad_norm": 0.12594977021217346,
      "learning_rate": 1.9426666666666668e-05,
      "loss": 0.002,
      "step": 22930
    },
    {
      "epoch": 1.2234666666666667,
      "grad_norm": 0.39905551075935364,
      "learning_rate": 1.9413333333333333e-05,
      "loss": 0.0023,
      "step": 22940
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.258082777261734,
      "learning_rate": 1.94e-05,
      "loss": 0.0019,
      "step": 22950
    },
    {
      "epoch": 1.2245333333333333,
      "grad_norm": 0.17802311480045319,
      "learning_rate": 1.938666666666667e-05,
      "loss": 0.0024,
      "step": 22960
    },
    {
      "epoch": 1.2250666666666667,
      "grad_norm": 0.5095078349113464,
      "learning_rate": 1.9373333333333336e-05,
      "loss": 0.0023,
      "step": 22970
    },
    {
      "epoch": 1.2256,
      "grad_norm": 0.3863542973995209,
      "learning_rate": 1.936e-05,
      "loss": 0.0028,
      "step": 22980
    },
    {
      "epoch": 1.2261333333333333,
      "grad_norm": 0.22738432884216309,
      "learning_rate": 1.934666666666667e-05,
      "loss": 0.0036,
      "step": 22990
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.35698601603507996,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 0.0022,
      "step": 23000
    },
    {
      "epoch": 1.2272,
      "grad_norm": 0.5551648736000061,
      "learning_rate": 1.932e-05,
      "loss": 0.0019,
      "step": 23010
    },
    {
      "epoch": 1.2277333333333333,
      "grad_norm": 0.37636491656303406,
      "learning_rate": 1.9306666666666666e-05,
      "loss": 0.0029,
      "step": 23020
    },
    {
      "epoch": 1.2282666666666666,
      "grad_norm": 0.11182787269353867,
      "learning_rate": 1.9293333333333334e-05,
      "loss": 0.0029,
      "step": 23030
    },
    {
      "epoch": 1.2288000000000001,
      "grad_norm": 0.6001385450363159,
      "learning_rate": 1.9280000000000002e-05,
      "loss": 0.002,
      "step": 23040
    },
    {
      "epoch": 1.2293333333333334,
      "grad_norm": 0.28944459557533264,
      "learning_rate": 1.926666666666667e-05,
      "loss": 0.0023,
      "step": 23050
    },
    {
      "epoch": 1.2298666666666667,
      "grad_norm": 0.4151388704776764,
      "learning_rate": 1.9253333333333334e-05,
      "loss": 0.002,
      "step": 23060
    },
    {
      "epoch": 1.2304,
      "grad_norm": 0.3353409469127655,
      "learning_rate": 1.924e-05,
      "loss": 0.0023,
      "step": 23070
    },
    {
      "epoch": 1.2309333333333332,
      "grad_norm": 0.36474886536598206,
      "learning_rate": 1.9226666666666667e-05,
      "loss": 0.0019,
      "step": 23080
    },
    {
      "epoch": 1.2314666666666667,
      "grad_norm": 0.3968508243560791,
      "learning_rate": 1.9213333333333335e-05,
      "loss": 0.0026,
      "step": 23090
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.1988200843334198,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.0019,
      "step": 23100
    },
    {
      "epoch": 1.2325333333333333,
      "grad_norm": 0.2721710801124573,
      "learning_rate": 1.9186666666666667e-05,
      "loss": 0.0031,
      "step": 23110
    },
    {
      "epoch": 1.2330666666666668,
      "grad_norm": 0.16182748973369598,
      "learning_rate": 1.9173333333333335e-05,
      "loss": 0.0029,
      "step": 23120
    },
    {
      "epoch": 1.2336,
      "grad_norm": 0.33907416462898254,
      "learning_rate": 1.916e-05,
      "loss": 0.0019,
      "step": 23130
    },
    {
      "epoch": 1.2341333333333333,
      "grad_norm": 0.25685790181159973,
      "learning_rate": 1.9146666666666667e-05,
      "loss": 0.0023,
      "step": 23140
    },
    {
      "epoch": 1.2346666666666666,
      "grad_norm": 0.3733980357646942,
      "learning_rate": 1.9133333333333332e-05,
      "loss": 0.0021,
      "step": 23150
    },
    {
      "epoch": 1.2352,
      "grad_norm": 0.1481935977935791,
      "learning_rate": 1.9120000000000003e-05,
      "loss": 0.0023,
      "step": 23160
    },
    {
      "epoch": 1.2357333333333334,
      "grad_norm": 0.05588546767830849,
      "learning_rate": 1.9106666666666668e-05,
      "loss": 0.0027,
      "step": 23170
    },
    {
      "epoch": 1.2362666666666666,
      "grad_norm": 0.0855197161436081,
      "learning_rate": 1.9093333333333336e-05,
      "loss": 0.0017,
      "step": 23180
    },
    {
      "epoch": 1.2368000000000001,
      "grad_norm": 0.3410522937774658,
      "learning_rate": 1.908e-05,
      "loss": 0.0023,
      "step": 23190
    },
    {
      "epoch": 1.2373333333333334,
      "grad_norm": 0.07433705031871796,
      "learning_rate": 1.9066666666666668e-05,
      "loss": 0.004,
      "step": 23200
    },
    {
      "epoch": 1.2378666666666667,
      "grad_norm": 0.4633939564228058,
      "learning_rate": 1.9053333333333333e-05,
      "loss": 0.0018,
      "step": 23210
    },
    {
      "epoch": 1.2384,
      "grad_norm": 0.1612478494644165,
      "learning_rate": 1.904e-05,
      "loss": 0.003,
      "step": 23220
    },
    {
      "epoch": 1.2389333333333332,
      "grad_norm": 0.30578091740608215,
      "learning_rate": 1.902666666666667e-05,
      "loss": 0.0017,
      "step": 23230
    },
    {
      "epoch": 1.2394666666666667,
      "grad_norm": 0.2774926722049713,
      "learning_rate": 1.9013333333333333e-05,
      "loss": 0.0023,
      "step": 23240
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.37721627950668335,
      "learning_rate": 1.9e-05,
      "loss": 0.0025,
      "step": 23250
    },
    {
      "epoch": 1.2405333333333333,
      "grad_norm": 0.2617783844470978,
      "learning_rate": 1.8986666666666666e-05,
      "loss": 0.0024,
      "step": 23260
    },
    {
      "epoch": 1.2410666666666668,
      "grad_norm": 0.17960761487483978,
      "learning_rate": 1.8973333333333334e-05,
      "loss": 0.002,
      "step": 23270
    },
    {
      "epoch": 1.2416,
      "grad_norm": 0.1801133155822754,
      "learning_rate": 1.896e-05,
      "loss": 0.002,
      "step": 23280
    },
    {
      "epoch": 1.2421333333333333,
      "grad_norm": 0.1966746300458908,
      "learning_rate": 1.894666666666667e-05,
      "loss": 0.0042,
      "step": 23290
    },
    {
      "epoch": 1.2426666666666666,
      "grad_norm": 0.1521628051996231,
      "learning_rate": 1.8933333333333334e-05,
      "loss": 0.0033,
      "step": 23300
    },
    {
      "epoch": 1.2432,
      "grad_norm": 0.2255239337682724,
      "learning_rate": 1.8920000000000002e-05,
      "loss": 0.0028,
      "step": 23310
    },
    {
      "epoch": 1.2437333333333334,
      "grad_norm": 0.22409558296203613,
      "learning_rate": 1.8906666666666666e-05,
      "loss": 0.003,
      "step": 23320
    },
    {
      "epoch": 1.2442666666666666,
      "grad_norm": 0.16855239868164062,
      "learning_rate": 1.8893333333333334e-05,
      "loss": 0.0024,
      "step": 23330
    },
    {
      "epoch": 1.2448,
      "grad_norm": 0.6479185223579407,
      "learning_rate": 1.888e-05,
      "loss": 0.0031,
      "step": 23340
    },
    {
      "epoch": 1.2453333333333334,
      "grad_norm": 0.1171577200293541,
      "learning_rate": 1.886666666666667e-05,
      "loss": 0.002,
      "step": 23350
    },
    {
      "epoch": 1.2458666666666667,
      "grad_norm": 0.11559993773698807,
      "learning_rate": 1.8853333333333335e-05,
      "loss": 0.0022,
      "step": 23360
    },
    {
      "epoch": 1.2464,
      "grad_norm": 0.20068630576133728,
      "learning_rate": 1.8840000000000003e-05,
      "loss": 0.0027,
      "step": 23370
    },
    {
      "epoch": 1.2469333333333332,
      "grad_norm": 0.21564623713493347,
      "learning_rate": 1.8826666666666667e-05,
      "loss": 0.0028,
      "step": 23380
    },
    {
      "epoch": 1.2474666666666667,
      "grad_norm": 0.5275945067405701,
      "learning_rate": 1.8813333333333335e-05,
      "loss": 0.0019,
      "step": 23390
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.4329972565174103,
      "learning_rate": 1.88e-05,
      "loss": 0.003,
      "step": 23400
    },
    {
      "epoch": 1.2485333333333333,
      "grad_norm": 0.4005959928035736,
      "learning_rate": 1.8786666666666667e-05,
      "loss": 0.0034,
      "step": 23410
    },
    {
      "epoch": 1.2490666666666668,
      "grad_norm": 0.23295730352401733,
      "learning_rate": 1.8773333333333335e-05,
      "loss": 0.0023,
      "step": 23420
    },
    {
      "epoch": 1.2496,
      "grad_norm": 0.3816037178039551,
      "learning_rate": 1.876e-05,
      "loss": 0.002,
      "step": 23430
    },
    {
      "epoch": 1.2501333333333333,
      "grad_norm": 0.08125234395265579,
      "learning_rate": 1.8746666666666668e-05,
      "loss": 0.0021,
      "step": 23440
    },
    {
      "epoch": 1.2506666666666666,
      "grad_norm": 0.18364682793617249,
      "learning_rate": 1.8733333333333332e-05,
      "loss": 0.0032,
      "step": 23450
    },
    {
      "epoch": 1.2511999999999999,
      "grad_norm": 0.30925387144088745,
      "learning_rate": 1.872e-05,
      "loss": 0.0031,
      "step": 23460
    },
    {
      "epoch": 1.2517333333333334,
      "grad_norm": 0.2503235638141632,
      "learning_rate": 1.8706666666666668e-05,
      "loss": 0.0037,
      "step": 23470
    },
    {
      "epoch": 1.2522666666666666,
      "grad_norm": 0.3792085647583008,
      "learning_rate": 1.8693333333333336e-05,
      "loss": 0.0027,
      "step": 23480
    },
    {
      "epoch": 1.2528000000000001,
      "grad_norm": 0.32607463002204895,
      "learning_rate": 1.868e-05,
      "loss": 0.0025,
      "step": 23490
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.3448089361190796,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.0017,
      "step": 23500
    },
    {
      "epoch": 1.2538666666666667,
      "grad_norm": 0.3724272549152374,
      "learning_rate": 1.8653333333333333e-05,
      "loss": 0.0028,
      "step": 23510
    },
    {
      "epoch": 1.2544,
      "grad_norm": 0.16875968873500824,
      "learning_rate": 1.864e-05,
      "loss": 0.0022,
      "step": 23520
    },
    {
      "epoch": 1.2549333333333332,
      "grad_norm": 0.22391103208065033,
      "learning_rate": 1.8626666666666666e-05,
      "loss": 0.0028,
      "step": 23530
    },
    {
      "epoch": 1.2554666666666667,
      "grad_norm": 0.07002357393503189,
      "learning_rate": 1.8613333333333337e-05,
      "loss": 0.0026,
      "step": 23540
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.10962488502264023,
      "learning_rate": 1.86e-05,
      "loss": 0.0023,
      "step": 23550
    },
    {
      "epoch": 1.2565333333333333,
      "grad_norm": 0.23588445782661438,
      "learning_rate": 1.858666666666667e-05,
      "loss": 0.0024,
      "step": 23560
    },
    {
      "epoch": 1.2570666666666668,
      "grad_norm": 0.3720674514770508,
      "learning_rate": 1.8573333333333334e-05,
      "loss": 0.0038,
      "step": 23570
    },
    {
      "epoch": 1.2576,
      "grad_norm": 0.3702089190483093,
      "learning_rate": 1.856e-05,
      "loss": 0.0033,
      "step": 23580
    },
    {
      "epoch": 1.2581333333333333,
      "grad_norm": 0.728235125541687,
      "learning_rate": 1.8546666666666666e-05,
      "loss": 0.0034,
      "step": 23590
    },
    {
      "epoch": 1.2586666666666666,
      "grad_norm": 0.37705090641975403,
      "learning_rate": 1.8533333333333334e-05,
      "loss": 0.0036,
      "step": 23600
    },
    {
      "epoch": 1.2591999999999999,
      "grad_norm": 0.2864918112754822,
      "learning_rate": 1.8520000000000002e-05,
      "loss": 0.0025,
      "step": 23610
    },
    {
      "epoch": 1.2597333333333334,
      "grad_norm": 0.24347449839115143,
      "learning_rate": 1.8506666666666667e-05,
      "loss": 0.002,
      "step": 23620
    },
    {
      "epoch": 1.2602666666666666,
      "grad_norm": 0.25702714920043945,
      "learning_rate": 1.8493333333333335e-05,
      "loss": 0.0024,
      "step": 23630
    },
    {
      "epoch": 1.2608,
      "grad_norm": 0.2534584105014801,
      "learning_rate": 1.848e-05,
      "loss": 0.0026,
      "step": 23640
    },
    {
      "epoch": 1.2613333333333334,
      "grad_norm": 0.09743200987577438,
      "learning_rate": 1.8466666666666667e-05,
      "loss": 0.0021,
      "step": 23650
    },
    {
      "epoch": 1.2618666666666667,
      "grad_norm": 0.2864418625831604,
      "learning_rate": 1.8453333333333335e-05,
      "loss": 0.0031,
      "step": 23660
    },
    {
      "epoch": 1.2624,
      "grad_norm": 0.3453332185745239,
      "learning_rate": 1.8440000000000003e-05,
      "loss": 0.0025,
      "step": 23670
    },
    {
      "epoch": 1.2629333333333332,
      "grad_norm": 0.2841699421405792,
      "learning_rate": 1.8426666666666668e-05,
      "loss": 0.002,
      "step": 23680
    },
    {
      "epoch": 1.2634666666666667,
      "grad_norm": 0.3170028328895569,
      "learning_rate": 1.8413333333333335e-05,
      "loss": 0.0035,
      "step": 23690
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.25730401277542114,
      "learning_rate": 1.84e-05,
      "loss": 0.0019,
      "step": 23700
    },
    {
      "epoch": 1.2645333333333333,
      "grad_norm": 0.08818981051445007,
      "learning_rate": 1.8386666666666668e-05,
      "loss": 0.0029,
      "step": 23710
    },
    {
      "epoch": 1.2650666666666668,
      "grad_norm": 0.12045367062091827,
      "learning_rate": 1.8373333333333332e-05,
      "loss": 0.0035,
      "step": 23720
    },
    {
      "epoch": 1.2656,
      "grad_norm": 0.5680591464042664,
      "learning_rate": 1.8360000000000004e-05,
      "loss": 0.0037,
      "step": 23730
    },
    {
      "epoch": 1.2661333333333333,
      "grad_norm": 0.16565176844596863,
      "learning_rate": 1.834666666666667e-05,
      "loss": 0.0018,
      "step": 23740
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.36115890741348267,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.0033,
      "step": 23750
    },
    {
      "epoch": 1.2671999999999999,
      "grad_norm": 0.5807546377182007,
      "learning_rate": 1.832e-05,
      "loss": 0.002,
      "step": 23760
    },
    {
      "epoch": 1.2677333333333334,
      "grad_norm": 0.10940487682819366,
      "learning_rate": 1.8306666666666665e-05,
      "loss": 0.0019,
      "step": 23770
    },
    {
      "epoch": 1.2682666666666667,
      "grad_norm": 0.1847849041223526,
      "learning_rate": 1.8293333333333333e-05,
      "loss": 0.0029,
      "step": 23780
    },
    {
      "epoch": 1.2688,
      "grad_norm": 0.4323791265487671,
      "learning_rate": 1.828e-05,
      "loss": 0.0027,
      "step": 23790
    },
    {
      "epoch": 1.2693333333333334,
      "grad_norm": 0.12587317824363708,
      "learning_rate": 1.826666666666667e-05,
      "loss": 0.002,
      "step": 23800
    },
    {
      "epoch": 1.2698666666666667,
      "grad_norm": 0.5638519525527954,
      "learning_rate": 1.8253333333333334e-05,
      "loss": 0.002,
      "step": 23810
    },
    {
      "epoch": 1.2704,
      "grad_norm": 0.12361027300357819,
      "learning_rate": 1.824e-05,
      "loss": 0.002,
      "step": 23820
    },
    {
      "epoch": 1.2709333333333332,
      "grad_norm": 0.15937933325767517,
      "learning_rate": 1.8226666666666666e-05,
      "loss": 0.0026,
      "step": 23830
    },
    {
      "epoch": 1.2714666666666667,
      "grad_norm": 0.21004559099674225,
      "learning_rate": 1.8213333333333334e-05,
      "loss": 0.0026,
      "step": 23840
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.17950475215911865,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 0.0026,
      "step": 23850
    },
    {
      "epoch": 1.2725333333333333,
      "grad_norm": 0.11252494156360626,
      "learning_rate": 1.818666666666667e-05,
      "loss": 0.0026,
      "step": 23860
    },
    {
      "epoch": 1.2730666666666668,
      "grad_norm": 0.2776396572589874,
      "learning_rate": 1.8173333333333334e-05,
      "loss": 0.0024,
      "step": 23870
    },
    {
      "epoch": 1.2736,
      "grad_norm": 0.09018420428037643,
      "learning_rate": 1.8160000000000002e-05,
      "loss": 0.0018,
      "step": 23880
    },
    {
      "epoch": 1.2741333333333333,
      "grad_norm": 0.6441200375556946,
      "learning_rate": 1.8146666666666667e-05,
      "loss": 0.0019,
      "step": 23890
    },
    {
      "epoch": 1.2746666666666666,
      "grad_norm": 0.5926649570465088,
      "learning_rate": 1.8133333333333335e-05,
      "loss": 0.0027,
      "step": 23900
    },
    {
      "epoch": 1.2752,
      "grad_norm": 0.5937380194664001,
      "learning_rate": 1.812e-05,
      "loss": 0.0024,
      "step": 23910
    },
    {
      "epoch": 1.2757333333333334,
      "grad_norm": 0.32856062054634094,
      "learning_rate": 1.8106666666666667e-05,
      "loss": 0.0021,
      "step": 23920
    },
    {
      "epoch": 1.2762666666666667,
      "grad_norm": 0.16105614602565765,
      "learning_rate": 1.8093333333333335e-05,
      "loss": 0.0036,
      "step": 23930
    },
    {
      "epoch": 1.2768,
      "grad_norm": 0.19670115411281586,
      "learning_rate": 1.808e-05,
      "loss": 0.002,
      "step": 23940
    },
    {
      "epoch": 1.2773333333333334,
      "grad_norm": 0.16958610713481903,
      "learning_rate": 1.8066666666666668e-05,
      "loss": 0.0028,
      "step": 23950
    },
    {
      "epoch": 1.2778666666666667,
      "grad_norm": 0.5434364080429077,
      "learning_rate": 1.8053333333333332e-05,
      "loss": 0.0033,
      "step": 23960
    },
    {
      "epoch": 1.2784,
      "grad_norm": 0.05907929316163063,
      "learning_rate": 1.804e-05,
      "loss": 0.003,
      "step": 23970
    },
    {
      "epoch": 1.2789333333333333,
      "grad_norm": 0.15407302975654602,
      "learning_rate": 1.8026666666666668e-05,
      "loss": 0.0025,
      "step": 23980
    },
    {
      "epoch": 1.2794666666666665,
      "grad_norm": 0.10122130811214447,
      "learning_rate": 1.8013333333333336e-05,
      "loss": 0.0023,
      "step": 23990
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.20135001838207245,
      "learning_rate": 1.8e-05,
      "loss": 0.0033,
      "step": 24000
    },
    {
      "epoch": 1.2805333333333333,
      "grad_norm": 0.368563175201416,
      "learning_rate": 1.798666666666667e-05,
      "loss": 0.002,
      "step": 24010
    },
    {
      "epoch": 1.2810666666666668,
      "grad_norm": 0.22483517229557037,
      "learning_rate": 1.7973333333333333e-05,
      "loss": 0.0027,
      "step": 24020
    },
    {
      "epoch": 1.2816,
      "grad_norm": 0.20555271208286285,
      "learning_rate": 1.796e-05,
      "loss": 0.0027,
      "step": 24030
    },
    {
      "epoch": 1.2821333333333333,
      "grad_norm": 0.4637490510940552,
      "learning_rate": 1.794666666666667e-05,
      "loss": 0.0022,
      "step": 24040
    },
    {
      "epoch": 1.2826666666666666,
      "grad_norm": 0.21179339289665222,
      "learning_rate": 1.7933333333333337e-05,
      "loss": 0.0019,
      "step": 24050
    },
    {
      "epoch": 1.2832,
      "grad_norm": 0.48797285556793213,
      "learning_rate": 1.792e-05,
      "loss": 0.0019,
      "step": 24060
    },
    {
      "epoch": 1.2837333333333334,
      "grad_norm": 0.6264422535896301,
      "learning_rate": 1.790666666666667e-05,
      "loss": 0.0034,
      "step": 24070
    },
    {
      "epoch": 1.2842666666666667,
      "grad_norm": 0.3520108759403229,
      "learning_rate": 1.7893333333333334e-05,
      "loss": 0.0021,
      "step": 24080
    },
    {
      "epoch": 1.2848,
      "grad_norm": 0.3015303313732147,
      "learning_rate": 1.7879999999999998e-05,
      "loss": 0.0021,
      "step": 24090
    },
    {
      "epoch": 1.2853333333333334,
      "grad_norm": 0.24318180978298187,
      "learning_rate": 1.7866666666666666e-05,
      "loss": 0.0019,
      "step": 24100
    },
    {
      "epoch": 1.2858666666666667,
      "grad_norm": 0.27249428629875183,
      "learning_rate": 1.7853333333333334e-05,
      "loss": 0.0023,
      "step": 24110
    },
    {
      "epoch": 1.2864,
      "grad_norm": 0.26526546478271484,
      "learning_rate": 1.7840000000000002e-05,
      "loss": 0.002,
      "step": 24120
    },
    {
      "epoch": 1.2869333333333333,
      "grad_norm": 0.23617586493492126,
      "learning_rate": 1.7826666666666667e-05,
      "loss": 0.0026,
      "step": 24130
    },
    {
      "epoch": 1.2874666666666665,
      "grad_norm": 0.488709419965744,
      "learning_rate": 1.7813333333333334e-05,
      "loss": 0.0017,
      "step": 24140
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.0618399903178215,
      "learning_rate": 1.78e-05,
      "loss": 0.0036,
      "step": 24150
    },
    {
      "epoch": 1.2885333333333333,
      "grad_norm": 0.31621551513671875,
      "learning_rate": 1.7786666666666667e-05,
      "loss": 0.0018,
      "step": 24160
    },
    {
      "epoch": 1.2890666666666668,
      "grad_norm": 0.8341901302337646,
      "learning_rate": 1.7773333333333335e-05,
      "loss": 0.0024,
      "step": 24170
    },
    {
      "epoch": 1.2896,
      "grad_norm": 0.3308362364768982,
      "learning_rate": 1.7760000000000003e-05,
      "loss": 0.0023,
      "step": 24180
    },
    {
      "epoch": 1.2901333333333334,
      "grad_norm": 0.10498706996440887,
      "learning_rate": 1.7746666666666667e-05,
      "loss": 0.0032,
      "step": 24190
    },
    {
      "epoch": 1.2906666666666666,
      "grad_norm": 0.3233436048030853,
      "learning_rate": 1.7733333333333335e-05,
      "loss": 0.0027,
      "step": 24200
    },
    {
      "epoch": 1.2912,
      "grad_norm": 0.14626911282539368,
      "learning_rate": 1.772e-05,
      "loss": 0.002,
      "step": 24210
    },
    {
      "epoch": 1.2917333333333334,
      "grad_norm": 0.0643550455570221,
      "learning_rate": 1.7706666666666668e-05,
      "loss": 0.0018,
      "step": 24220
    },
    {
      "epoch": 1.2922666666666667,
      "grad_norm": 0.29369494318962097,
      "learning_rate": 1.7693333333333336e-05,
      "loss": 0.0024,
      "step": 24230
    },
    {
      "epoch": 1.2928,
      "grad_norm": 0.5429374575614929,
      "learning_rate": 1.7680000000000004e-05,
      "loss": 0.0036,
      "step": 24240
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.2439243197441101,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 0.0039,
      "step": 24250
    },
    {
      "epoch": 1.2938666666666667,
      "grad_norm": 0.21034008264541626,
      "learning_rate": 1.7653333333333333e-05,
      "loss": 0.0028,
      "step": 24260
    },
    {
      "epoch": 1.2944,
      "grad_norm": 0.5229900479316711,
      "learning_rate": 1.764e-05,
      "loss": 0.0026,
      "step": 24270
    },
    {
      "epoch": 1.2949333333333333,
      "grad_norm": 0.32658442854881287,
      "learning_rate": 1.7626666666666665e-05,
      "loss": 0.003,
      "step": 24280
    },
    {
      "epoch": 1.2954666666666665,
      "grad_norm": 0.36506375670433044,
      "learning_rate": 1.7613333333333333e-05,
      "loss": 0.003,
      "step": 24290
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.12103354185819626,
      "learning_rate": 1.76e-05,
      "loss": 0.0025,
      "step": 24300
    },
    {
      "epoch": 1.2965333333333333,
      "grad_norm": 0.31736189126968384,
      "learning_rate": 1.758666666666667e-05,
      "loss": 0.0022,
      "step": 24310
    },
    {
      "epoch": 1.2970666666666666,
      "grad_norm": 0.264835923910141,
      "learning_rate": 1.7573333333333333e-05,
      "loss": 0.0029,
      "step": 24320
    },
    {
      "epoch": 1.2976,
      "grad_norm": 0.43284842371940613,
      "learning_rate": 1.756e-05,
      "loss": 0.0028,
      "step": 24330
    },
    {
      "epoch": 1.2981333333333334,
      "grad_norm": 0.2781111001968384,
      "learning_rate": 1.7546666666666666e-05,
      "loss": 0.0022,
      "step": 24340
    },
    {
      "epoch": 1.2986666666666666,
      "grad_norm": 0.4411018490791321,
      "learning_rate": 1.7533333333333334e-05,
      "loss": 0.0025,
      "step": 24350
    },
    {
      "epoch": 1.2992,
      "grad_norm": 0.36537986993789673,
      "learning_rate": 1.752e-05,
      "loss": 0.0028,
      "step": 24360
    },
    {
      "epoch": 1.2997333333333334,
      "grad_norm": 0.44043150544166565,
      "learning_rate": 1.750666666666667e-05,
      "loss": 0.003,
      "step": 24370
    },
    {
      "epoch": 1.3002666666666667,
      "grad_norm": 0.16046571731567383,
      "learning_rate": 1.7493333333333334e-05,
      "loss": 0.0032,
      "step": 24380
    },
    {
      "epoch": 1.3008,
      "grad_norm": 0.25817230343818665,
      "learning_rate": 1.7480000000000002e-05,
      "loss": 0.0041,
      "step": 24390
    },
    {
      "epoch": 1.3013333333333335,
      "grad_norm": 0.21152380108833313,
      "learning_rate": 1.7466666666666667e-05,
      "loss": 0.0023,
      "step": 24400
    },
    {
      "epoch": 1.3018666666666667,
      "grad_norm": 0.13897022604942322,
      "learning_rate": 1.7453333333333335e-05,
      "loss": 0.0027,
      "step": 24410
    },
    {
      "epoch": 1.3024,
      "grad_norm": 0.0570342056453228,
      "learning_rate": 1.7440000000000002e-05,
      "loss": 0.0026,
      "step": 24420
    },
    {
      "epoch": 1.3029333333333333,
      "grad_norm": 0.1435631364583969,
      "learning_rate": 1.7426666666666667e-05,
      "loss": 0.0024,
      "step": 24430
    },
    {
      "epoch": 1.3034666666666666,
      "grad_norm": 0.19848963618278503,
      "learning_rate": 1.7413333333333335e-05,
      "loss": 0.0038,
      "step": 24440
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.06504439562559128,
      "learning_rate": 1.74e-05,
      "loss": 0.0031,
      "step": 24450
    },
    {
      "epoch": 1.3045333333333333,
      "grad_norm": 0.1637338399887085,
      "learning_rate": 1.7386666666666667e-05,
      "loss": 0.0019,
      "step": 24460
    },
    {
      "epoch": 1.3050666666666666,
      "grad_norm": 0.418623149394989,
      "learning_rate": 1.7373333333333332e-05,
      "loss": 0.0021,
      "step": 24470
    },
    {
      "epoch": 1.3056,
      "grad_norm": 0.2916918992996216,
      "learning_rate": 1.736e-05,
      "loss": 0.0019,
      "step": 24480
    },
    {
      "epoch": 1.3061333333333334,
      "grad_norm": 0.23875238001346588,
      "learning_rate": 1.7346666666666668e-05,
      "loss": 0.0022,
      "step": 24490
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.31411656737327576,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.0024,
      "step": 24500
    },
    {
      "epoch": 1.3072,
      "grad_norm": 0.7132560014724731,
      "learning_rate": 1.732e-05,
      "loss": 0.0024,
      "step": 24510
    },
    {
      "epoch": 1.3077333333333334,
      "grad_norm": 0.16665223240852356,
      "learning_rate": 1.7306666666666668e-05,
      "loss": 0.0018,
      "step": 24520
    },
    {
      "epoch": 1.3082666666666667,
      "grad_norm": 0.2822675108909607,
      "learning_rate": 1.7293333333333333e-05,
      "loss": 0.004,
      "step": 24530
    },
    {
      "epoch": 1.3088,
      "grad_norm": 0.4129811227321625,
      "learning_rate": 1.728e-05,
      "loss": 0.0027,
      "step": 24540
    },
    {
      "epoch": 1.3093333333333335,
      "grad_norm": 0.11431019753217697,
      "learning_rate": 1.726666666666667e-05,
      "loss": 0.0028,
      "step": 24550
    },
    {
      "epoch": 1.3098666666666667,
      "grad_norm": 0.1776590347290039,
      "learning_rate": 1.7253333333333336e-05,
      "loss": 0.0026,
      "step": 24560
    },
    {
      "epoch": 1.3104,
      "grad_norm": 0.34743732213974,
      "learning_rate": 1.724e-05,
      "loss": 0.003,
      "step": 24570
    },
    {
      "epoch": 1.3109333333333333,
      "grad_norm": 0.3620915710926056,
      "learning_rate": 1.722666666666667e-05,
      "loss": 0.0019,
      "step": 24580
    },
    {
      "epoch": 1.3114666666666666,
      "grad_norm": 0.307367205619812,
      "learning_rate": 1.7213333333333333e-05,
      "loss": 0.004,
      "step": 24590
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.1129644513130188,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 0.002,
      "step": 24600
    },
    {
      "epoch": 1.3125333333333333,
      "grad_norm": 0.14631244540214539,
      "learning_rate": 1.718666666666667e-05,
      "loss": 0.0023,
      "step": 24610
    },
    {
      "epoch": 1.3130666666666666,
      "grad_norm": 0.15885919332504272,
      "learning_rate": 1.7173333333333334e-05,
      "loss": 0.0018,
      "step": 24620
    },
    {
      "epoch": 1.3136,
      "grad_norm": 0.24535900354385376,
      "learning_rate": 1.7160000000000002e-05,
      "loss": 0.0028,
      "step": 24630
    },
    {
      "epoch": 1.3141333333333334,
      "grad_norm": 0.11926320195198059,
      "learning_rate": 1.7146666666666666e-05,
      "loss": 0.002,
      "step": 24640
    },
    {
      "epoch": 1.3146666666666667,
      "grad_norm": 0.14940932393074036,
      "learning_rate": 1.7133333333333334e-05,
      "loss": 0.002,
      "step": 24650
    },
    {
      "epoch": 1.3152,
      "grad_norm": 0.11072201281785965,
      "learning_rate": 1.712e-05,
      "loss": 0.0021,
      "step": 24660
    },
    {
      "epoch": 1.3157333333333332,
      "grad_norm": 0.14857588708400726,
      "learning_rate": 1.7106666666666667e-05,
      "loss": 0.0027,
      "step": 24670
    },
    {
      "epoch": 1.3162666666666667,
      "grad_norm": 0.17836213111877441,
      "learning_rate": 1.7093333333333335e-05,
      "loss": 0.0023,
      "step": 24680
    },
    {
      "epoch": 1.3168,
      "grad_norm": 0.492485910654068,
      "learning_rate": 1.7080000000000002e-05,
      "loss": 0.0023,
      "step": 24690
    },
    {
      "epoch": 1.3173333333333335,
      "grad_norm": 0.0914381742477417,
      "learning_rate": 1.7066666666666667e-05,
      "loss": 0.0018,
      "step": 24700
    },
    {
      "epoch": 1.3178666666666667,
      "grad_norm": 0.7461212873458862,
      "learning_rate": 1.7053333333333335e-05,
      "loss": 0.003,
      "step": 24710
    },
    {
      "epoch": 1.3184,
      "grad_norm": 0.14218945801258087,
      "learning_rate": 1.704e-05,
      "loss": 0.0028,
      "step": 24720
    },
    {
      "epoch": 1.3189333333333333,
      "grad_norm": 0.6741598844528198,
      "learning_rate": 1.7026666666666667e-05,
      "loss": 0.0027,
      "step": 24730
    },
    {
      "epoch": 1.3194666666666666,
      "grad_norm": 0.5803037285804749,
      "learning_rate": 1.7013333333333335e-05,
      "loss": 0.0022,
      "step": 24740
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.28608986735343933,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.0019,
      "step": 24750
    },
    {
      "epoch": 1.3205333333333333,
      "grad_norm": 0.5174334049224854,
      "learning_rate": 1.6986666666666668e-05,
      "loss": 0.0019,
      "step": 24760
    },
    {
      "epoch": 1.3210666666666666,
      "grad_norm": 0.15930433571338654,
      "learning_rate": 1.6973333333333336e-05,
      "loss": 0.0032,
      "step": 24770
    },
    {
      "epoch": 1.3216,
      "grad_norm": 0.32523736357688904,
      "learning_rate": 1.696e-05,
      "loss": 0.002,
      "step": 24780
    },
    {
      "epoch": 1.3221333333333334,
      "grad_norm": 0.13776376843452454,
      "learning_rate": 1.6946666666666665e-05,
      "loss": 0.0018,
      "step": 24790
    },
    {
      "epoch": 1.3226666666666667,
      "grad_norm": 0.4846791625022888,
      "learning_rate": 1.6933333333333333e-05,
      "loss": 0.0019,
      "step": 24800
    },
    {
      "epoch": 1.3232,
      "grad_norm": 0.2278231531381607,
      "learning_rate": 1.692e-05,
      "loss": 0.002,
      "step": 24810
    },
    {
      "epoch": 1.3237333333333332,
      "grad_norm": 0.7138704061508179,
      "learning_rate": 1.690666666666667e-05,
      "loss": 0.0037,
      "step": 24820
    },
    {
      "epoch": 1.3242666666666667,
      "grad_norm": 0.07576057314872742,
      "learning_rate": 1.6893333333333333e-05,
      "loss": 0.0026,
      "step": 24830
    },
    {
      "epoch": 1.3248,
      "grad_norm": 0.46687015891075134,
      "learning_rate": 1.688e-05,
      "loss": 0.0027,
      "step": 24840
    },
    {
      "epoch": 1.3253333333333333,
      "grad_norm": 0.15712366998195648,
      "learning_rate": 1.6866666666666666e-05,
      "loss": 0.0025,
      "step": 24850
    },
    {
      "epoch": 1.3258666666666667,
      "grad_norm": 0.17886243760585785,
      "learning_rate": 1.6853333333333333e-05,
      "loss": 0.0024,
      "step": 24860
    },
    {
      "epoch": 1.3264,
      "grad_norm": 0.13178086280822754,
      "learning_rate": 1.684e-05,
      "loss": 0.0022,
      "step": 24870
    },
    {
      "epoch": 1.3269333333333333,
      "grad_norm": 0.2839140295982361,
      "learning_rate": 1.682666666666667e-05,
      "loss": 0.0026,
      "step": 24880
    },
    {
      "epoch": 1.3274666666666666,
      "grad_norm": 0.1088496670126915,
      "learning_rate": 1.6813333333333334e-05,
      "loss": 0.0023,
      "step": 24890
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.20177097618579865,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.0027,
      "step": 24900
    },
    {
      "epoch": 1.3285333333333333,
      "grad_norm": 0.4215748608112335,
      "learning_rate": 1.6786666666666666e-05,
      "loss": 0.0021,
      "step": 24910
    },
    {
      "epoch": 1.3290666666666666,
      "grad_norm": 0.1484639197587967,
      "learning_rate": 1.6773333333333334e-05,
      "loss": 0.0031,
      "step": 24920
    },
    {
      "epoch": 1.3296000000000001,
      "grad_norm": 0.3252430260181427,
      "learning_rate": 1.6760000000000002e-05,
      "loss": 0.0027,
      "step": 24930
    },
    {
      "epoch": 1.3301333333333334,
      "grad_norm": 0.2821258306503296,
      "learning_rate": 1.674666666666667e-05,
      "loss": 0.0028,
      "step": 24940
    },
    {
      "epoch": 1.3306666666666667,
      "grad_norm": 0.08593052625656128,
      "learning_rate": 1.6733333333333335e-05,
      "loss": 0.0021,
      "step": 24950
    },
    {
      "epoch": 1.3312,
      "grad_norm": 0.09008502960205078,
      "learning_rate": 1.672e-05,
      "loss": 0.0019,
      "step": 24960
    },
    {
      "epoch": 1.3317333333333332,
      "grad_norm": 0.36832940578460693,
      "learning_rate": 1.6706666666666667e-05,
      "loss": 0.0022,
      "step": 24970
    },
    {
      "epoch": 1.3322666666666667,
      "grad_norm": 0.23355844616889954,
      "learning_rate": 1.669333333333333e-05,
      "loss": 0.002,
      "step": 24980
    },
    {
      "epoch": 1.3328,
      "grad_norm": 0.26091498136520386,
      "learning_rate": 1.668e-05,
      "loss": 0.0027,
      "step": 24990
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.5863277316093445,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.0023,
      "step": 25000
    },
    {
      "epoch": 1.3338666666666668,
      "grad_norm": 0.6472702026367188,
      "learning_rate": 1.6653333333333335e-05,
      "loss": 0.0027,
      "step": 25010
    },
    {
      "epoch": 1.3344,
      "grad_norm": 0.13542264699935913,
      "learning_rate": 1.664e-05,
      "loss": 0.0032,
      "step": 25020
    },
    {
      "epoch": 1.3349333333333333,
      "grad_norm": 0.3659183084964752,
      "learning_rate": 1.6626666666666668e-05,
      "loss": 0.0024,
      "step": 25030
    },
    {
      "epoch": 1.3354666666666666,
      "grad_norm": 0.4136858880519867,
      "learning_rate": 1.6613333333333332e-05,
      "loss": 0.0048,
      "step": 25040
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.3313504457473755,
      "learning_rate": 1.66e-05,
      "loss": 0.0022,
      "step": 25050
    },
    {
      "epoch": 1.3365333333333334,
      "grad_norm": 0.26685836911201477,
      "learning_rate": 1.6586666666666668e-05,
      "loss": 0.002,
      "step": 25060
    },
    {
      "epoch": 1.3370666666666666,
      "grad_norm": 0.32840433716773987,
      "learning_rate": 1.6573333333333336e-05,
      "loss": 0.0021,
      "step": 25070
    },
    {
      "epoch": 1.3376000000000001,
      "grad_norm": 0.21082288026809692,
      "learning_rate": 1.656e-05,
      "loss": 0.0018,
      "step": 25080
    },
    {
      "epoch": 1.3381333333333334,
      "grad_norm": 0.42569488286972046,
      "learning_rate": 1.654666666666667e-05,
      "loss": 0.0027,
      "step": 25090
    },
    {
      "epoch": 1.3386666666666667,
      "grad_norm": 0.1470663696527481,
      "learning_rate": 1.6533333333333333e-05,
      "loss": 0.0024,
      "step": 25100
    },
    {
      "epoch": 1.3392,
      "grad_norm": 0.5622757077217102,
      "learning_rate": 1.652e-05,
      "loss": 0.0026,
      "step": 25110
    },
    {
      "epoch": 1.3397333333333332,
      "grad_norm": 0.2021639496088028,
      "learning_rate": 1.650666666666667e-05,
      "loss": 0.0025,
      "step": 25120
    },
    {
      "epoch": 1.3402666666666667,
      "grad_norm": 0.42129257321357727,
      "learning_rate": 1.6493333333333334e-05,
      "loss": 0.0029,
      "step": 25130
    },
    {
      "epoch": 1.3408,
      "grad_norm": 0.2998097240924835,
      "learning_rate": 1.648e-05,
      "loss": 0.002,
      "step": 25140
    },
    {
      "epoch": 1.3413333333333333,
      "grad_norm": 0.06573391705751419,
      "learning_rate": 1.6466666666666666e-05,
      "loss": 0.0021,
      "step": 25150
    },
    {
      "epoch": 1.3418666666666668,
      "grad_norm": 0.12323872745037079,
      "learning_rate": 1.6453333333333334e-05,
      "loss": 0.0032,
      "step": 25160
    },
    {
      "epoch": 1.3424,
      "grad_norm": 0.5303763151168823,
      "learning_rate": 1.644e-05,
      "loss": 0.0026,
      "step": 25170
    },
    {
      "epoch": 1.3429333333333333,
      "grad_norm": 0.4835610091686249,
      "learning_rate": 1.6426666666666666e-05,
      "loss": 0.0029,
      "step": 25180
    },
    {
      "epoch": 1.3434666666666666,
      "grad_norm": 0.08240310847759247,
      "learning_rate": 1.6413333333333334e-05,
      "loss": 0.0024,
      "step": 25190
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.26941144466400146,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 0.003,
      "step": 25200
    },
    {
      "epoch": 1.3445333333333334,
      "grad_norm": 0.06766131520271301,
      "learning_rate": 1.6386666666666667e-05,
      "loss": 0.0016,
      "step": 25210
    },
    {
      "epoch": 1.3450666666666666,
      "grad_norm": 0.3511405885219574,
      "learning_rate": 1.6373333333333335e-05,
      "loss": 0.0023,
      "step": 25220
    },
    {
      "epoch": 1.3456000000000001,
      "grad_norm": 0.14799228310585022,
      "learning_rate": 1.636e-05,
      "loss": 0.0019,
      "step": 25230
    },
    {
      "epoch": 1.3461333333333334,
      "grad_norm": 0.2125026285648346,
      "learning_rate": 1.6346666666666667e-05,
      "loss": 0.0025,
      "step": 25240
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.09186962246894836,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 0.0034,
      "step": 25250
    },
    {
      "epoch": 1.3472,
      "grad_norm": 0.13391493260860443,
      "learning_rate": 1.6320000000000003e-05,
      "loss": 0.0035,
      "step": 25260
    },
    {
      "epoch": 1.3477333333333332,
      "grad_norm": 0.4188695549964905,
      "learning_rate": 1.6306666666666668e-05,
      "loss": 0.0029,
      "step": 25270
    },
    {
      "epoch": 1.3482666666666667,
      "grad_norm": 0.4674040675163269,
      "learning_rate": 1.6293333333333335e-05,
      "loss": 0.0032,
      "step": 25280
    },
    {
      "epoch": 1.3488,
      "grad_norm": 0.1281624138355255,
      "learning_rate": 1.628e-05,
      "loss": 0.0022,
      "step": 25290
    },
    {
      "epoch": 1.3493333333333333,
      "grad_norm": 0.30940791964530945,
      "learning_rate": 1.6266666666666665e-05,
      "loss": 0.0024,
      "step": 25300
    },
    {
      "epoch": 1.3498666666666668,
      "grad_norm": 0.18807263672351837,
      "learning_rate": 1.6253333333333336e-05,
      "loss": 0.0023,
      "step": 25310
    },
    {
      "epoch": 1.3504,
      "grad_norm": 0.258790522813797,
      "learning_rate": 1.624e-05,
      "loss": 0.0031,
      "step": 25320
    },
    {
      "epoch": 1.3509333333333333,
      "grad_norm": 0.08275369554758072,
      "learning_rate": 1.6226666666666668e-05,
      "loss": 0.0015,
      "step": 25330
    },
    {
      "epoch": 1.3514666666666666,
      "grad_norm": 0.34747058153152466,
      "learning_rate": 1.6213333333333333e-05,
      "loss": 0.0026,
      "step": 25340
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.5405675768852234,
      "learning_rate": 1.62e-05,
      "loss": 0.002,
      "step": 25350
    },
    {
      "epoch": 1.3525333333333334,
      "grad_norm": 0.23334988951683044,
      "learning_rate": 1.6186666666666665e-05,
      "loss": 0.0017,
      "step": 25360
    },
    {
      "epoch": 1.3530666666666666,
      "grad_norm": 0.28089210391044617,
      "learning_rate": 1.6173333333333333e-05,
      "loss": 0.0034,
      "step": 25370
    },
    {
      "epoch": 1.3536000000000001,
      "grad_norm": 0.15448419749736786,
      "learning_rate": 1.616e-05,
      "loss": 0.0022,
      "step": 25380
    },
    {
      "epoch": 1.3541333333333334,
      "grad_norm": 0.12261845171451569,
      "learning_rate": 1.614666666666667e-05,
      "loss": 0.0017,
      "step": 25390
    },
    {
      "epoch": 1.3546666666666667,
      "grad_norm": 0.18781603872776031,
      "learning_rate": 1.6133333333333334e-05,
      "loss": 0.0022,
      "step": 25400
    },
    {
      "epoch": 1.3552,
      "grad_norm": 0.22720415890216827,
      "learning_rate": 1.612e-05,
      "loss": 0.003,
      "step": 25410
    },
    {
      "epoch": 1.3557333333333332,
      "grad_norm": 0.10790057480335236,
      "learning_rate": 1.6106666666666666e-05,
      "loss": 0.0018,
      "step": 25420
    },
    {
      "epoch": 1.3562666666666667,
      "grad_norm": 0.1574600338935852,
      "learning_rate": 1.6093333333333334e-05,
      "loss": 0.0019,
      "step": 25430
    },
    {
      "epoch": 1.3568,
      "grad_norm": 0.2170408070087433,
      "learning_rate": 1.6080000000000002e-05,
      "loss": 0.0018,
      "step": 25440
    },
    {
      "epoch": 1.3573333333333333,
      "grad_norm": 0.6040113568305969,
      "learning_rate": 1.606666666666667e-05,
      "loss": 0.002,
      "step": 25450
    },
    {
      "epoch": 1.3578666666666668,
      "grad_norm": 0.21339528262615204,
      "learning_rate": 1.6053333333333334e-05,
      "loss": 0.0047,
      "step": 25460
    },
    {
      "epoch": 1.3584,
      "grad_norm": 0.5386449098587036,
      "learning_rate": 1.604e-05,
      "loss": 0.002,
      "step": 25470
    },
    {
      "epoch": 1.3589333333333333,
      "grad_norm": 0.1193665862083435,
      "learning_rate": 1.6026666666666667e-05,
      "loss": 0.003,
      "step": 25480
    },
    {
      "epoch": 1.3594666666666666,
      "grad_norm": 0.33569031953811646,
      "learning_rate": 1.601333333333333e-05,
      "loss": 0.0019,
      "step": 25490
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.2787654399871826,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.0021,
      "step": 25500
    },
    {
      "epoch": 1.3605333333333334,
      "grad_norm": 0.14284317195415497,
      "learning_rate": 1.5986666666666667e-05,
      "loss": 0.0019,
      "step": 25510
    },
    {
      "epoch": 1.3610666666666666,
      "grad_norm": 0.10661862790584564,
      "learning_rate": 1.5973333333333335e-05,
      "loss": 0.0023,
      "step": 25520
    },
    {
      "epoch": 1.3616,
      "grad_norm": 0.3411019444465637,
      "learning_rate": 1.596e-05,
      "loss": 0.0024,
      "step": 25530
    },
    {
      "epoch": 1.3621333333333334,
      "grad_norm": 0.5226393938064575,
      "learning_rate": 1.5946666666666668e-05,
      "loss": 0.0021,
      "step": 25540
    },
    {
      "epoch": 1.3626666666666667,
      "grad_norm": 0.29108089208602905,
      "learning_rate": 1.5933333333333332e-05,
      "loss": 0.0021,
      "step": 25550
    },
    {
      "epoch": 1.3632,
      "grad_norm": 0.28694596886634827,
      "learning_rate": 1.592e-05,
      "loss": 0.0025,
      "step": 25560
    },
    {
      "epoch": 1.3637333333333332,
      "grad_norm": 0.11773201823234558,
      "learning_rate": 1.5906666666666668e-05,
      "loss": 0.0023,
      "step": 25570
    },
    {
      "epoch": 1.3642666666666667,
      "grad_norm": 0.07579968869686127,
      "learning_rate": 1.5893333333333336e-05,
      "loss": 0.0024,
      "step": 25580
    },
    {
      "epoch": 1.3648,
      "grad_norm": 0.2413436770439148,
      "learning_rate": 1.588e-05,
      "loss": 0.002,
      "step": 25590
    },
    {
      "epoch": 1.3653333333333333,
      "grad_norm": 0.28814953565597534,
      "learning_rate": 1.586666666666667e-05,
      "loss": 0.0035,
      "step": 25600
    },
    {
      "epoch": 1.3658666666666668,
      "grad_norm": 0.3571184575557709,
      "learning_rate": 1.5853333333333333e-05,
      "loss": 0.0025,
      "step": 25610
    },
    {
      "epoch": 1.3664,
      "grad_norm": 0.19253739714622498,
      "learning_rate": 1.584e-05,
      "loss": 0.0035,
      "step": 25620
    },
    {
      "epoch": 1.3669333333333333,
      "grad_norm": 0.19892168045043945,
      "learning_rate": 1.582666666666667e-05,
      "loss": 0.0028,
      "step": 25630
    },
    {
      "epoch": 1.3674666666666666,
      "grad_norm": 0.16057483851909637,
      "learning_rate": 1.5813333333333333e-05,
      "loss": 0.0025,
      "step": 25640
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.47141915559768677,
      "learning_rate": 1.58e-05,
      "loss": 0.002,
      "step": 25650
    },
    {
      "epoch": 1.3685333333333334,
      "grad_norm": 0.0934637188911438,
      "learning_rate": 1.5786666666666666e-05,
      "loss": 0.0028,
      "step": 25660
    },
    {
      "epoch": 1.3690666666666667,
      "grad_norm": 0.27837249636650085,
      "learning_rate": 1.5773333333333334e-05,
      "loss": 0.0028,
      "step": 25670
    },
    {
      "epoch": 1.3696,
      "grad_norm": 0.16784584522247314,
      "learning_rate": 1.5759999999999998e-05,
      "loss": 0.0023,
      "step": 25680
    },
    {
      "epoch": 1.3701333333333334,
      "grad_norm": 0.7313963770866394,
      "learning_rate": 1.574666666666667e-05,
      "loss": 0.0021,
      "step": 25690
    },
    {
      "epoch": 1.3706666666666667,
      "grad_norm": 0.6339071989059448,
      "learning_rate": 1.5733333333333334e-05,
      "loss": 0.0034,
      "step": 25700
    },
    {
      "epoch": 1.3712,
      "grad_norm": 0.22729529440402985,
      "learning_rate": 1.5720000000000002e-05,
      "loss": 0.0022,
      "step": 25710
    },
    {
      "epoch": 1.3717333333333332,
      "grad_norm": 0.2580150365829468,
      "learning_rate": 1.5706666666666666e-05,
      "loss": 0.002,
      "step": 25720
    },
    {
      "epoch": 1.3722666666666667,
      "grad_norm": 0.3106047213077545,
      "learning_rate": 1.5693333333333334e-05,
      "loss": 0.0042,
      "step": 25730
    },
    {
      "epoch": 1.3728,
      "grad_norm": 0.41191530227661133,
      "learning_rate": 1.568e-05,
      "loss": 0.0027,
      "step": 25740
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.17238543927669525,
      "learning_rate": 1.5666666666666667e-05,
      "loss": 0.0027,
      "step": 25750
    },
    {
      "epoch": 1.3738666666666668,
      "grad_norm": 0.344256192445755,
      "learning_rate": 1.5653333333333335e-05,
      "loss": 0.002,
      "step": 25760
    },
    {
      "epoch": 1.3744,
      "grad_norm": 0.15274424850940704,
      "learning_rate": 1.5640000000000003e-05,
      "loss": 0.0027,
      "step": 25770
    },
    {
      "epoch": 1.3749333333333333,
      "grad_norm": 0.14901889860630035,
      "learning_rate": 1.5626666666666667e-05,
      "loss": 0.002,
      "step": 25780
    },
    {
      "epoch": 1.3754666666666666,
      "grad_norm": 0.2592565715312958,
      "learning_rate": 1.5613333333333335e-05,
      "loss": 0.002,
      "step": 25790
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.30506569147109985,
      "learning_rate": 1.56e-05,
      "loss": 0.0021,
      "step": 25800
    },
    {
      "epoch": 1.3765333333333334,
      "grad_norm": 0.416341096162796,
      "learning_rate": 1.5586666666666668e-05,
      "loss": 0.0019,
      "step": 25810
    },
    {
      "epoch": 1.3770666666666667,
      "grad_norm": 0.5747963190078735,
      "learning_rate": 1.5573333333333336e-05,
      "loss": 0.0018,
      "step": 25820
    },
    {
      "epoch": 1.3776,
      "grad_norm": 0.5251845121383667,
      "learning_rate": 1.556e-05,
      "loss": 0.0039,
      "step": 25830
    },
    {
      "epoch": 1.3781333333333334,
      "grad_norm": 0.2421416938304901,
      "learning_rate": 1.5546666666666668e-05,
      "loss": 0.0023,
      "step": 25840
    },
    {
      "epoch": 1.3786666666666667,
      "grad_norm": 0.27987074851989746,
      "learning_rate": 1.5533333333333333e-05,
      "loss": 0.0027,
      "step": 25850
    },
    {
      "epoch": 1.3792,
      "grad_norm": 0.5322413444519043,
      "learning_rate": 1.552e-05,
      "loss": 0.0027,
      "step": 25860
    },
    {
      "epoch": 1.3797333333333333,
      "grad_norm": 0.15048307180404663,
      "learning_rate": 1.5506666666666665e-05,
      "loss": 0.0026,
      "step": 25870
    },
    {
      "epoch": 1.3802666666666665,
      "grad_norm": 0.498782217502594,
      "learning_rate": 1.5493333333333336e-05,
      "loss": 0.0029,
      "step": 25880
    },
    {
      "epoch": 1.3808,
      "grad_norm": 0.3420564830303192,
      "learning_rate": 1.548e-05,
      "loss": 0.0017,
      "step": 25890
    },
    {
      "epoch": 1.3813333333333333,
      "grad_norm": 0.13902565836906433,
      "learning_rate": 1.546666666666667e-05,
      "loss": 0.002,
      "step": 25900
    },
    {
      "epoch": 1.3818666666666668,
      "grad_norm": 0.14000006020069122,
      "learning_rate": 1.5453333333333333e-05,
      "loss": 0.003,
      "step": 25910
    },
    {
      "epoch": 1.3824,
      "grad_norm": 0.06667443364858627,
      "learning_rate": 1.544e-05,
      "loss": 0.0023,
      "step": 25920
    },
    {
      "epoch": 1.3829333333333333,
      "grad_norm": 0.14644116163253784,
      "learning_rate": 1.5426666666666666e-05,
      "loss": 0.0022,
      "step": 25930
    },
    {
      "epoch": 1.3834666666666666,
      "grad_norm": 0.32690882682800293,
      "learning_rate": 1.5413333333333334e-05,
      "loss": 0.0029,
      "step": 25940
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.3177344799041748,
      "learning_rate": 1.54e-05,
      "loss": 0.0025,
      "step": 25950
    },
    {
      "epoch": 1.3845333333333334,
      "grad_norm": 0.5078272819519043,
      "learning_rate": 1.538666666666667e-05,
      "loss": 0.0019,
      "step": 25960
    },
    {
      "epoch": 1.3850666666666667,
      "grad_norm": 0.20494653284549713,
      "learning_rate": 1.5373333333333334e-05,
      "loss": 0.0021,
      "step": 25970
    },
    {
      "epoch": 1.3856,
      "grad_norm": 0.1588863879442215,
      "learning_rate": 1.536e-05,
      "loss": 0.0032,
      "step": 25980
    },
    {
      "epoch": 1.3861333333333334,
      "grad_norm": 0.10604291409254074,
      "learning_rate": 1.5346666666666667e-05,
      "loss": 0.0026,
      "step": 25990
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.07502367347478867,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 0.0028,
      "step": 26000
    },
    {
      "epoch": 1.3872,
      "grad_norm": 0.1904187947511673,
      "learning_rate": 1.5320000000000002e-05,
      "loss": 0.0037,
      "step": 26010
    },
    {
      "epoch": 1.3877333333333333,
      "grad_norm": 0.1925230324268341,
      "learning_rate": 1.5306666666666667e-05,
      "loss": 0.0019,
      "step": 26020
    },
    {
      "epoch": 1.3882666666666665,
      "grad_norm": 0.19093015789985657,
      "learning_rate": 1.5293333333333335e-05,
      "loss": 0.0019,
      "step": 26030
    },
    {
      "epoch": 1.3888,
      "grad_norm": 0.7493979930877686,
      "learning_rate": 1.528e-05,
      "loss": 0.003,
      "step": 26040
    },
    {
      "epoch": 1.3893333333333333,
      "grad_norm": 0.65584796667099,
      "learning_rate": 1.5266666666666667e-05,
      "loss": 0.0028,
      "step": 26050
    },
    {
      "epoch": 1.3898666666666666,
      "grad_norm": 0.16909919679164886,
      "learning_rate": 1.5253333333333334e-05,
      "loss": 0.0018,
      "step": 26060
    },
    {
      "epoch": 1.3904,
      "grad_norm": 0.17024865746498108,
      "learning_rate": 1.5240000000000001e-05,
      "loss": 0.0019,
      "step": 26070
    },
    {
      "epoch": 1.3909333333333334,
      "grad_norm": 0.36476317048072815,
      "learning_rate": 1.5226666666666668e-05,
      "loss": 0.0021,
      "step": 26080
    },
    {
      "epoch": 1.3914666666666666,
      "grad_norm": 0.43123865127563477,
      "learning_rate": 1.5213333333333336e-05,
      "loss": 0.0025,
      "step": 26090
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.2757047116756439,
      "learning_rate": 1.52e-05,
      "loss": 0.0025,
      "step": 26100
    },
    {
      "epoch": 1.3925333333333334,
      "grad_norm": 0.07828815281391144,
      "learning_rate": 1.5186666666666668e-05,
      "loss": 0.0037,
      "step": 26110
    },
    {
      "epoch": 1.3930666666666667,
      "grad_norm": 0.1929362565279007,
      "learning_rate": 1.5173333333333334e-05,
      "loss": 0.0036,
      "step": 26120
    },
    {
      "epoch": 1.3936,
      "grad_norm": 0.1544753909111023,
      "learning_rate": 1.5160000000000002e-05,
      "loss": 0.0029,
      "step": 26130
    },
    {
      "epoch": 1.3941333333333334,
      "grad_norm": 0.2445588856935501,
      "learning_rate": 1.5146666666666667e-05,
      "loss": 0.0029,
      "step": 26140
    },
    {
      "epoch": 1.3946666666666667,
      "grad_norm": 0.29540300369262695,
      "learning_rate": 1.5133333333333333e-05,
      "loss": 0.0031,
      "step": 26150
    },
    {
      "epoch": 1.3952,
      "grad_norm": 0.11306549608707428,
      "learning_rate": 1.5120000000000001e-05,
      "loss": 0.0022,
      "step": 26160
    },
    {
      "epoch": 1.3957333333333333,
      "grad_norm": 0.5719038844108582,
      "learning_rate": 1.5106666666666665e-05,
      "loss": 0.0039,
      "step": 26170
    },
    {
      "epoch": 1.3962666666666665,
      "grad_norm": 0.07042574137449265,
      "learning_rate": 1.5093333333333335e-05,
      "loss": 0.0021,
      "step": 26180
    },
    {
      "epoch": 1.3968,
      "grad_norm": 0.32933977246284485,
      "learning_rate": 1.508e-05,
      "loss": 0.0027,
      "step": 26190
    },
    {
      "epoch": 1.3973333333333333,
      "grad_norm": 0.07359535992145538,
      "learning_rate": 1.5066666666666668e-05,
      "loss": 0.0021,
      "step": 26200
    },
    {
      "epoch": 1.3978666666666666,
      "grad_norm": 0.6373725533485413,
      "learning_rate": 1.5053333333333334e-05,
      "loss": 0.0027,
      "step": 26210
    },
    {
      "epoch": 1.3984,
      "grad_norm": 0.10172039270401001,
      "learning_rate": 1.5040000000000002e-05,
      "loss": 0.0027,
      "step": 26220
    },
    {
      "epoch": 1.3989333333333334,
      "grad_norm": 0.2665163278579712,
      "learning_rate": 1.5026666666666666e-05,
      "loss": 0.0022,
      "step": 26230
    },
    {
      "epoch": 1.3994666666666666,
      "grad_norm": 0.6080410480499268,
      "learning_rate": 1.5013333333333334e-05,
      "loss": 0.0021,
      "step": 26240
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.17712177336215973,
      "learning_rate": 1.5e-05,
      "loss": 0.0017,
      "step": 26250
    },
    {
      "epoch": 1.4005333333333334,
      "grad_norm": 0.6635443568229675,
      "learning_rate": 1.4986666666666668e-05,
      "loss": 0.0024,
      "step": 26260
    },
    {
      "epoch": 1.4010666666666667,
      "grad_norm": 0.13922040164470673,
      "learning_rate": 1.4973333333333333e-05,
      "loss": 0.0021,
      "step": 26270
    },
    {
      "epoch": 1.4016,
      "grad_norm": 0.26448506116867065,
      "learning_rate": 1.4960000000000002e-05,
      "loss": 0.0035,
      "step": 26280
    },
    {
      "epoch": 1.4021333333333335,
      "grad_norm": 0.14906495809555054,
      "learning_rate": 1.4946666666666667e-05,
      "loss": 0.0028,
      "step": 26290
    },
    {
      "epoch": 1.4026666666666667,
      "grad_norm": 0.11995856463909149,
      "learning_rate": 1.4933333333333335e-05,
      "loss": 0.0031,
      "step": 26300
    },
    {
      "epoch": 1.4032,
      "grad_norm": 0.3047523498535156,
      "learning_rate": 1.4920000000000001e-05,
      "loss": 0.0029,
      "step": 26310
    },
    {
      "epoch": 1.4037333333333333,
      "grad_norm": 0.14626024663448334,
      "learning_rate": 1.4906666666666666e-05,
      "loss": 0.0019,
      "step": 26320
    },
    {
      "epoch": 1.4042666666666666,
      "grad_norm": 0.1172173023223877,
      "learning_rate": 1.4893333333333334e-05,
      "loss": 0.0022,
      "step": 26330
    },
    {
      "epoch": 1.4048,
      "grad_norm": 0.1034352034330368,
      "learning_rate": 1.488e-05,
      "loss": 0.0019,
      "step": 26340
    },
    {
      "epoch": 1.4053333333333333,
      "grad_norm": 0.41964098811149597,
      "learning_rate": 1.4866666666666668e-05,
      "loss": 0.0021,
      "step": 26350
    },
    {
      "epoch": 1.4058666666666666,
      "grad_norm": 0.2809341847896576,
      "learning_rate": 1.4853333333333332e-05,
      "loss": 0.0026,
      "step": 26360
    },
    {
      "epoch": 1.4064,
      "grad_norm": 0.244008868932724,
      "learning_rate": 1.4840000000000002e-05,
      "loss": 0.0028,
      "step": 26370
    },
    {
      "epoch": 1.4069333333333334,
      "grad_norm": 0.14861875772476196,
      "learning_rate": 1.4826666666666666e-05,
      "loss": 0.0017,
      "step": 26380
    },
    {
      "epoch": 1.4074666666666666,
      "grad_norm": 0.5932595133781433,
      "learning_rate": 1.4813333333333334e-05,
      "loss": 0.0023,
      "step": 26390
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.1359112560749054,
      "learning_rate": 1.48e-05,
      "loss": 0.0037,
      "step": 26400
    },
    {
      "epoch": 1.4085333333333334,
      "grad_norm": 0.19452334940433502,
      "learning_rate": 1.4786666666666669e-05,
      "loss": 0.0023,
      "step": 26410
    },
    {
      "epoch": 1.4090666666666667,
      "grad_norm": 0.2480461448431015,
      "learning_rate": 1.4773333333333333e-05,
      "loss": 0.0018,
      "step": 26420
    },
    {
      "epoch": 1.4096,
      "grad_norm": 0.38768136501312256,
      "learning_rate": 1.4760000000000001e-05,
      "loss": 0.0041,
      "step": 26430
    },
    {
      "epoch": 1.4101333333333335,
      "grad_norm": 0.38311588764190674,
      "learning_rate": 1.4746666666666667e-05,
      "loss": 0.0032,
      "step": 26440
    },
    {
      "epoch": 1.4106666666666667,
      "grad_norm": 0.3429822325706482,
      "learning_rate": 1.4733333333333335e-05,
      "loss": 0.0025,
      "step": 26450
    },
    {
      "epoch": 1.4112,
      "grad_norm": 0.5253860354423523,
      "learning_rate": 1.472e-05,
      "loss": 0.002,
      "step": 26460
    },
    {
      "epoch": 1.4117333333333333,
      "grad_norm": 0.4355347156524658,
      "learning_rate": 1.470666666666667e-05,
      "loss": 0.0018,
      "step": 26470
    },
    {
      "epoch": 1.4122666666666666,
      "grad_norm": 0.08671130239963531,
      "learning_rate": 1.4693333333333334e-05,
      "loss": 0.0026,
      "step": 26480
    },
    {
      "epoch": 1.4128,
      "grad_norm": 0.26998648047447205,
      "learning_rate": 1.4680000000000002e-05,
      "loss": 0.0019,
      "step": 26490
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.14214813709259033,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 0.002,
      "step": 26500
    },
    {
      "epoch": 1.4138666666666666,
      "grad_norm": 0.5638794302940369,
      "learning_rate": 1.4653333333333333e-05,
      "loss": 0.0029,
      "step": 26510
    },
    {
      "epoch": 1.4144,
      "grad_norm": 0.21845991909503937,
      "learning_rate": 1.464e-05,
      "loss": 0.0021,
      "step": 26520
    },
    {
      "epoch": 1.4149333333333334,
      "grad_norm": 0.12131250649690628,
      "learning_rate": 1.4626666666666667e-05,
      "loss": 0.0026,
      "step": 26530
    },
    {
      "epoch": 1.4154666666666667,
      "grad_norm": 0.38683846592903137,
      "learning_rate": 1.4613333333333335e-05,
      "loss": 0.002,
      "step": 26540
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.37154707312583923,
      "learning_rate": 1.4599999999999999e-05,
      "loss": 0.0029,
      "step": 26550
    },
    {
      "epoch": 1.4165333333333332,
      "grad_norm": 0.42277899384498596,
      "learning_rate": 1.4586666666666669e-05,
      "loss": 0.0038,
      "step": 26560
    },
    {
      "epoch": 1.4170666666666667,
      "grad_norm": 0.2415587306022644,
      "learning_rate": 1.4573333333333333e-05,
      "loss": 0.0023,
      "step": 26570
    },
    {
      "epoch": 1.4176,
      "grad_norm": 0.2983984053134918,
      "learning_rate": 1.4560000000000001e-05,
      "loss": 0.0023,
      "step": 26580
    },
    {
      "epoch": 1.4181333333333335,
      "grad_norm": 0.1708887368440628,
      "learning_rate": 1.4546666666666667e-05,
      "loss": 0.002,
      "step": 26590
    },
    {
      "epoch": 1.4186666666666667,
      "grad_norm": 0.16482296586036682,
      "learning_rate": 1.4533333333333335e-05,
      "loss": 0.0022,
      "step": 26600
    },
    {
      "epoch": 1.4192,
      "grad_norm": 0.26987424492836,
      "learning_rate": 1.452e-05,
      "loss": 0.002,
      "step": 26610
    },
    {
      "epoch": 1.4197333333333333,
      "grad_norm": 0.091844841837883,
      "learning_rate": 1.4506666666666668e-05,
      "loss": 0.0019,
      "step": 26620
    },
    {
      "epoch": 1.4202666666666666,
      "grad_norm": 0.4155995845794678,
      "learning_rate": 1.4493333333333334e-05,
      "loss": 0.0031,
      "step": 26630
    },
    {
      "epoch": 1.4208,
      "grad_norm": 0.17171326279640198,
      "learning_rate": 1.4480000000000002e-05,
      "loss": 0.002,
      "step": 26640
    },
    {
      "epoch": 1.4213333333333333,
      "grad_norm": 0.15770429372787476,
      "learning_rate": 1.4466666666666667e-05,
      "loss": 0.0021,
      "step": 26650
    },
    {
      "epoch": 1.4218666666666666,
      "grad_norm": 0.3859306871891022,
      "learning_rate": 1.4453333333333336e-05,
      "loss": 0.0029,
      "step": 26660
    },
    {
      "epoch": 1.4224,
      "grad_norm": 0.12910450994968414,
      "learning_rate": 1.444e-05,
      "loss": 0.0025,
      "step": 26670
    },
    {
      "epoch": 1.4229333333333334,
      "grad_norm": 0.20876577496528625,
      "learning_rate": 1.4426666666666667e-05,
      "loss": 0.0025,
      "step": 26680
    },
    {
      "epoch": 1.4234666666666667,
      "grad_norm": 0.31147778034210205,
      "learning_rate": 1.4413333333333335e-05,
      "loss": 0.0019,
      "step": 26690
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.6453432440757751,
      "learning_rate": 1.44e-05,
      "loss": 0.0018,
      "step": 26700
    },
    {
      "epoch": 1.4245333333333332,
      "grad_norm": 0.10978703945875168,
      "learning_rate": 1.4386666666666667e-05,
      "loss": 0.0022,
      "step": 26710
    },
    {
      "epoch": 1.4250666666666667,
      "grad_norm": 0.31926730275154114,
      "learning_rate": 1.4373333333333334e-05,
      "loss": 0.002,
      "step": 26720
    },
    {
      "epoch": 1.4256,
      "grad_norm": 0.6397647857666016,
      "learning_rate": 1.4360000000000001e-05,
      "loss": 0.0038,
      "step": 26730
    },
    {
      "epoch": 1.4261333333333333,
      "grad_norm": 0.18460515141487122,
      "learning_rate": 1.4346666666666666e-05,
      "loss": 0.0018,
      "step": 26740
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.11752459406852722,
      "learning_rate": 1.4333333333333334e-05,
      "loss": 0.0022,
      "step": 26750
    },
    {
      "epoch": 1.4272,
      "grad_norm": 0.3453224301338196,
      "learning_rate": 1.432e-05,
      "loss": 0.002,
      "step": 26760
    },
    {
      "epoch": 1.4277333333333333,
      "grad_norm": 0.34349092841148376,
      "learning_rate": 1.4306666666666668e-05,
      "loss": 0.0031,
      "step": 26770
    },
    {
      "epoch": 1.4282666666666666,
      "grad_norm": 0.6001555323600769,
      "learning_rate": 1.4293333333333334e-05,
      "loss": 0.0036,
      "step": 26780
    },
    {
      "epoch": 1.4288,
      "grad_norm": 0.18809358775615692,
      "learning_rate": 1.4280000000000002e-05,
      "loss": 0.0031,
      "step": 26790
    },
    {
      "epoch": 1.4293333333333333,
      "grad_norm": 0.5153056979179382,
      "learning_rate": 1.4266666666666667e-05,
      "loss": 0.0019,
      "step": 26800
    },
    {
      "epoch": 1.4298666666666666,
      "grad_norm": 0.20278924703598022,
      "learning_rate": 1.4253333333333335e-05,
      "loss": 0.0037,
      "step": 26810
    },
    {
      "epoch": 1.4304000000000001,
      "grad_norm": 0.08252552896738052,
      "learning_rate": 1.4240000000000001e-05,
      "loss": 0.002,
      "step": 26820
    },
    {
      "epoch": 1.4309333333333334,
      "grad_norm": 0.19874732196331024,
      "learning_rate": 1.4226666666666669e-05,
      "loss": 0.0021,
      "step": 26830
    },
    {
      "epoch": 1.4314666666666667,
      "grad_norm": 0.19720178842544556,
      "learning_rate": 1.4213333333333333e-05,
      "loss": 0.0032,
      "step": 26840
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.36855393648147583,
      "learning_rate": 1.42e-05,
      "loss": 0.0018,
      "step": 26850
    },
    {
      "epoch": 1.4325333333333332,
      "grad_norm": 0.23108135163784027,
      "learning_rate": 1.4186666666666667e-05,
      "loss": 0.0026,
      "step": 26860
    },
    {
      "epoch": 1.4330666666666667,
      "grad_norm": 0.1326335072517395,
      "learning_rate": 1.4173333333333334e-05,
      "loss": 0.0017,
      "step": 26870
    },
    {
      "epoch": 1.4336,
      "grad_norm": 0.22818560898303986,
      "learning_rate": 1.4160000000000002e-05,
      "loss": 0.0032,
      "step": 26880
    },
    {
      "epoch": 1.4341333333333333,
      "grad_norm": 0.14932626485824585,
      "learning_rate": 1.4146666666666666e-05,
      "loss": 0.002,
      "step": 26890
    },
    {
      "epoch": 1.4346666666666668,
      "grad_norm": 0.4039687216281891,
      "learning_rate": 1.4133333333333334e-05,
      "loss": 0.0019,
      "step": 26900
    },
    {
      "epoch": 1.4352,
      "grad_norm": 0.35162073373794556,
      "learning_rate": 1.412e-05,
      "loss": 0.0035,
      "step": 26910
    },
    {
      "epoch": 1.4357333333333333,
      "grad_norm": 0.11626144498586655,
      "learning_rate": 1.4106666666666668e-05,
      "loss": 0.0022,
      "step": 26920
    },
    {
      "epoch": 1.4362666666666666,
      "grad_norm": 0.2574373185634613,
      "learning_rate": 1.4093333333333333e-05,
      "loss": 0.002,
      "step": 26930
    },
    {
      "epoch": 1.4368,
      "grad_norm": 0.5465613603591919,
      "learning_rate": 1.408e-05,
      "loss": 0.0019,
      "step": 26940
    },
    {
      "epoch": 1.4373333333333334,
      "grad_norm": 0.39450663328170776,
      "learning_rate": 1.4066666666666667e-05,
      "loss": 0.0022,
      "step": 26950
    },
    {
      "epoch": 1.4378666666666666,
      "grad_norm": 0.0963519960641861,
      "learning_rate": 1.4053333333333335e-05,
      "loss": 0.0026,
      "step": 26960
    },
    {
      "epoch": 1.4384000000000001,
      "grad_norm": 0.8775702118873596,
      "learning_rate": 1.4040000000000001e-05,
      "loss": 0.0017,
      "step": 26970
    },
    {
      "epoch": 1.4389333333333334,
      "grad_norm": 0.21552245318889618,
      "learning_rate": 1.4026666666666669e-05,
      "loss": 0.0029,
      "step": 26980
    },
    {
      "epoch": 1.4394666666666667,
      "grad_norm": 0.5171452164649963,
      "learning_rate": 1.4013333333333334e-05,
      "loss": 0.0019,
      "step": 26990
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.18115229904651642,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.0018,
      "step": 27000
    },
    {
      "epoch": 1.4405333333333332,
      "grad_norm": 0.829269528388977,
      "learning_rate": 1.3986666666666668e-05,
      "loss": 0.002,
      "step": 27010
    },
    {
      "epoch": 1.4410666666666667,
      "grad_norm": 0.4980814754962921,
      "learning_rate": 1.3973333333333332e-05,
      "loss": 0.0023,
      "step": 27020
    },
    {
      "epoch": 1.4416,
      "grad_norm": 0.14319705963134766,
      "learning_rate": 1.396e-05,
      "loss": 0.0018,
      "step": 27030
    },
    {
      "epoch": 1.4421333333333333,
      "grad_norm": 0.22079476714134216,
      "learning_rate": 1.3946666666666666e-05,
      "loss": 0.0019,
      "step": 27040
    },
    {
      "epoch": 1.4426666666666668,
      "grad_norm": 0.4112611413002014,
      "learning_rate": 1.3933333333333334e-05,
      "loss": 0.0024,
      "step": 27050
    },
    {
      "epoch": 1.4432,
      "grad_norm": 0.43121904134750366,
      "learning_rate": 1.3919999999999999e-05,
      "loss": 0.0026,
      "step": 27060
    },
    {
      "epoch": 1.4437333333333333,
      "grad_norm": 0.19696615636348724,
      "learning_rate": 1.3906666666666668e-05,
      "loss": 0.0027,
      "step": 27070
    },
    {
      "epoch": 1.4442666666666666,
      "grad_norm": 0.09983394294977188,
      "learning_rate": 1.3893333333333333e-05,
      "loss": 0.0028,
      "step": 27080
    },
    {
      "epoch": 1.4447999999999999,
      "grad_norm": 0.16273173689842224,
      "learning_rate": 1.3880000000000001e-05,
      "loss": 0.0028,
      "step": 27090
    },
    {
      "epoch": 1.4453333333333334,
      "grad_norm": 0.25324898958206177,
      "learning_rate": 1.3866666666666667e-05,
      "loss": 0.0021,
      "step": 27100
    },
    {
      "epoch": 1.4458666666666666,
      "grad_norm": 0.22903096675872803,
      "learning_rate": 1.3853333333333335e-05,
      "loss": 0.0018,
      "step": 27110
    },
    {
      "epoch": 1.4464000000000001,
      "grad_norm": 0.40070202946662903,
      "learning_rate": 1.384e-05,
      "loss": 0.003,
      "step": 27120
    },
    {
      "epoch": 1.4469333333333334,
      "grad_norm": 0.5457299947738647,
      "learning_rate": 1.3826666666666668e-05,
      "loss": 0.0024,
      "step": 27130
    },
    {
      "epoch": 1.4474666666666667,
      "grad_norm": 0.33139264583587646,
      "learning_rate": 1.3813333333333334e-05,
      "loss": 0.0019,
      "step": 27140
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.43567708134651184,
      "learning_rate": 1.3800000000000002e-05,
      "loss": 0.002,
      "step": 27150
    },
    {
      "epoch": 1.4485333333333332,
      "grad_norm": 0.302892804145813,
      "learning_rate": 1.3786666666666668e-05,
      "loss": 0.0028,
      "step": 27160
    },
    {
      "epoch": 1.4490666666666667,
      "grad_norm": 0.08065350353717804,
      "learning_rate": 1.3773333333333336e-05,
      "loss": 0.0022,
      "step": 27170
    },
    {
      "epoch": 1.4496,
      "grad_norm": 0.12707066535949707,
      "learning_rate": 1.376e-05,
      "loss": 0.0023,
      "step": 27180
    },
    {
      "epoch": 1.4501333333333333,
      "grad_norm": 0.2421405166387558,
      "learning_rate": 1.3746666666666667e-05,
      "loss": 0.0027,
      "step": 27190
    },
    {
      "epoch": 1.4506666666666668,
      "grad_norm": 0.30992379784584045,
      "learning_rate": 1.3733333333333335e-05,
      "loss": 0.0043,
      "step": 27200
    },
    {
      "epoch": 1.4512,
      "grad_norm": 0.39173486828804016,
      "learning_rate": 1.3719999999999999e-05,
      "loss": 0.0029,
      "step": 27210
    },
    {
      "epoch": 1.4517333333333333,
      "grad_norm": 0.31086012721061707,
      "learning_rate": 1.3706666666666667e-05,
      "loss": 0.0022,
      "step": 27220
    },
    {
      "epoch": 1.4522666666666666,
      "grad_norm": 0.4401336908340454,
      "learning_rate": 1.3693333333333333e-05,
      "loss": 0.0034,
      "step": 27230
    },
    {
      "epoch": 1.4527999999999999,
      "grad_norm": 0.2337213009595871,
      "learning_rate": 1.3680000000000001e-05,
      "loss": 0.0018,
      "step": 27240
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.2286931872367859,
      "learning_rate": 1.3666666666666666e-05,
      "loss": 0.0034,
      "step": 27250
    },
    {
      "epoch": 1.4538666666666666,
      "grad_norm": 0.3540612757205963,
      "learning_rate": 1.3653333333333335e-05,
      "loss": 0.0027,
      "step": 27260
    },
    {
      "epoch": 1.4544000000000001,
      "grad_norm": 0.21000167727470398,
      "learning_rate": 1.364e-05,
      "loss": 0.0018,
      "step": 27270
    },
    {
      "epoch": 1.4549333333333334,
      "grad_norm": 0.8412755131721497,
      "learning_rate": 1.3626666666666668e-05,
      "loss": 0.0044,
      "step": 27280
    },
    {
      "epoch": 1.4554666666666667,
      "grad_norm": 0.36768123507499695,
      "learning_rate": 1.3613333333333334e-05,
      "loss": 0.0021,
      "step": 27290
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.31518983840942383,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.0024,
      "step": 27300
    },
    {
      "epoch": 1.4565333333333332,
      "grad_norm": 0.14924076199531555,
      "learning_rate": 1.3586666666666666e-05,
      "loss": 0.0019,
      "step": 27310
    },
    {
      "epoch": 1.4570666666666667,
      "grad_norm": 0.3178371489048004,
      "learning_rate": 1.3573333333333334e-05,
      "loss": 0.0017,
      "step": 27320
    },
    {
      "epoch": 1.4576,
      "grad_norm": 0.06653723120689392,
      "learning_rate": 1.356e-05,
      "loss": 0.0023,
      "step": 27330
    },
    {
      "epoch": 1.4581333333333333,
      "grad_norm": 0.3003777265548706,
      "learning_rate": 1.3546666666666669e-05,
      "loss": 0.0029,
      "step": 27340
    },
    {
      "epoch": 1.4586666666666668,
      "grad_norm": 0.11270616948604584,
      "learning_rate": 1.3533333333333335e-05,
      "loss": 0.0035,
      "step": 27350
    },
    {
      "epoch": 1.4592,
      "grad_norm": 0.13050709664821625,
      "learning_rate": 1.352e-05,
      "loss": 0.0021,
      "step": 27360
    },
    {
      "epoch": 1.4597333333333333,
      "grad_norm": 0.12404294312000275,
      "learning_rate": 1.3506666666666667e-05,
      "loss": 0.0027,
      "step": 27370
    },
    {
      "epoch": 1.4602666666666666,
      "grad_norm": 0.25188541412353516,
      "learning_rate": 1.3493333333333333e-05,
      "loss": 0.0024,
      "step": 27380
    },
    {
      "epoch": 1.4607999999999999,
      "grad_norm": 0.12510614097118378,
      "learning_rate": 1.3480000000000001e-05,
      "loss": 0.0022,
      "step": 27390
    },
    {
      "epoch": 1.4613333333333334,
      "grad_norm": 0.2144845575094223,
      "learning_rate": 1.3466666666666666e-05,
      "loss": 0.0022,
      "step": 27400
    },
    {
      "epoch": 1.4618666666666666,
      "grad_norm": 0.23617446422576904,
      "learning_rate": 1.3453333333333334e-05,
      "loss": 0.0022,
      "step": 27410
    },
    {
      "epoch": 1.4624,
      "grad_norm": 0.31415319442749023,
      "learning_rate": 1.344e-05,
      "loss": 0.002,
      "step": 27420
    },
    {
      "epoch": 1.4629333333333334,
      "grad_norm": 0.45509979128837585,
      "learning_rate": 1.3426666666666668e-05,
      "loss": 0.0025,
      "step": 27430
    },
    {
      "epoch": 1.4634666666666667,
      "grad_norm": 0.2895503342151642,
      "learning_rate": 1.3413333333333333e-05,
      "loss": 0.0022,
      "step": 27440
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.14513635635375977,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 0.0026,
      "step": 27450
    },
    {
      "epoch": 1.4645333333333332,
      "grad_norm": 0.3902697265148163,
      "learning_rate": 1.3386666666666667e-05,
      "loss": 0.0024,
      "step": 27460
    },
    {
      "epoch": 1.4650666666666667,
      "grad_norm": 0.496780663728714,
      "learning_rate": 1.3373333333333335e-05,
      "loss": 0.0031,
      "step": 27470
    },
    {
      "epoch": 1.4656,
      "grad_norm": 0.1685764491558075,
      "learning_rate": 1.336e-05,
      "loss": 0.0027,
      "step": 27480
    },
    {
      "epoch": 1.4661333333333333,
      "grad_norm": 0.3318101167678833,
      "learning_rate": 1.3346666666666669e-05,
      "loss": 0.0016,
      "step": 27490
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.28228840231895447,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.0024,
      "step": 27500
    },
    {
      "epoch": 1.4672,
      "grad_norm": 0.17738795280456543,
      "learning_rate": 1.3320000000000001e-05,
      "loss": 0.0023,
      "step": 27510
    },
    {
      "epoch": 1.4677333333333333,
      "grad_norm": 0.13462933897972107,
      "learning_rate": 1.3306666666666667e-05,
      "loss": 0.0027,
      "step": 27520
    },
    {
      "epoch": 1.4682666666666666,
      "grad_norm": 0.25403034687042236,
      "learning_rate": 1.3293333333333332e-05,
      "loss": 0.0031,
      "step": 27530
    },
    {
      "epoch": 1.4687999999999999,
      "grad_norm": 0.2887239158153534,
      "learning_rate": 1.3280000000000002e-05,
      "loss": 0.0021,
      "step": 27540
    },
    {
      "epoch": 1.4693333333333334,
      "grad_norm": 0.49538737535476685,
      "learning_rate": 1.3266666666666666e-05,
      "loss": 0.0033,
      "step": 27550
    },
    {
      "epoch": 1.4698666666666667,
      "grad_norm": 0.30839595198631287,
      "learning_rate": 1.3253333333333334e-05,
      "loss": 0.0019,
      "step": 27560
    },
    {
      "epoch": 1.4704,
      "grad_norm": 0.16714274883270264,
      "learning_rate": 1.324e-05,
      "loss": 0.0019,
      "step": 27570
    },
    {
      "epoch": 1.4709333333333334,
      "grad_norm": 0.15926070511341095,
      "learning_rate": 1.3226666666666668e-05,
      "loss": 0.0026,
      "step": 27580
    },
    {
      "epoch": 1.4714666666666667,
      "grad_norm": 0.2629235088825226,
      "learning_rate": 1.3213333333333333e-05,
      "loss": 0.0027,
      "step": 27590
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.16440650820732117,
      "learning_rate": 1.32e-05,
      "loss": 0.0026,
      "step": 27600
    },
    {
      "epoch": 1.4725333333333332,
      "grad_norm": 0.1944313645362854,
      "learning_rate": 1.3186666666666667e-05,
      "loss": 0.0018,
      "step": 27610
    },
    {
      "epoch": 1.4730666666666667,
      "grad_norm": 0.4470418691635132,
      "learning_rate": 1.3173333333333335e-05,
      "loss": 0.0024,
      "step": 27620
    },
    {
      "epoch": 1.4736,
      "grad_norm": 0.5232685804367065,
      "learning_rate": 1.316e-05,
      "loss": 0.0029,
      "step": 27630
    },
    {
      "epoch": 1.4741333333333333,
      "grad_norm": 0.3955995440483093,
      "learning_rate": 1.3146666666666669e-05,
      "loss": 0.0021,
      "step": 27640
    },
    {
      "epoch": 1.4746666666666668,
      "grad_norm": 0.3755117356777191,
      "learning_rate": 1.3133333333333334e-05,
      "loss": 0.0018,
      "step": 27650
    },
    {
      "epoch": 1.4752,
      "grad_norm": 0.2028404027223587,
      "learning_rate": 1.3120000000000001e-05,
      "loss": 0.0023,
      "step": 27660
    },
    {
      "epoch": 1.4757333333333333,
      "grad_norm": 0.19443924725055695,
      "learning_rate": 1.3106666666666668e-05,
      "loss": 0.0022,
      "step": 27670
    },
    {
      "epoch": 1.4762666666666666,
      "grad_norm": 0.1261047124862671,
      "learning_rate": 1.3093333333333336e-05,
      "loss": 0.0031,
      "step": 27680
    },
    {
      "epoch": 1.4768,
      "grad_norm": 0.5203374028205872,
      "learning_rate": 1.308e-05,
      "loss": 0.002,
      "step": 27690
    },
    {
      "epoch": 1.4773333333333334,
      "grad_norm": 0.34338441491127014,
      "learning_rate": 1.3066666666666666e-05,
      "loss": 0.0018,
      "step": 27700
    },
    {
      "epoch": 1.4778666666666667,
      "grad_norm": 0.15523727238178253,
      "learning_rate": 1.3053333333333334e-05,
      "loss": 0.0022,
      "step": 27710
    },
    {
      "epoch": 1.4784,
      "grad_norm": 0.18887513875961304,
      "learning_rate": 1.3039999999999999e-05,
      "loss": 0.0024,
      "step": 27720
    },
    {
      "epoch": 1.4789333333333334,
      "grad_norm": 0.24896034598350525,
      "learning_rate": 1.3026666666666667e-05,
      "loss": 0.0026,
      "step": 27730
    },
    {
      "epoch": 1.4794666666666667,
      "grad_norm": 0.6408787369728088,
      "learning_rate": 1.3013333333333333e-05,
      "loss": 0.0018,
      "step": 27740
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.227303147315979,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.0022,
      "step": 27750
    },
    {
      "epoch": 1.4805333333333333,
      "grad_norm": 0.7439359426498413,
      "learning_rate": 1.2986666666666667e-05,
      "loss": 0.002,
      "step": 27760
    },
    {
      "epoch": 1.4810666666666665,
      "grad_norm": 0.2154274433851242,
      "learning_rate": 1.2973333333333335e-05,
      "loss": 0.0035,
      "step": 27770
    },
    {
      "epoch": 1.4816,
      "grad_norm": 0.17252324521541595,
      "learning_rate": 1.296e-05,
      "loss": 0.0032,
      "step": 27780
    },
    {
      "epoch": 1.4821333333333333,
      "grad_norm": 0.38993188738822937,
      "learning_rate": 1.2946666666666668e-05,
      "loss": 0.002,
      "step": 27790
    },
    {
      "epoch": 1.4826666666666668,
      "grad_norm": 0.09157862514257431,
      "learning_rate": 1.2933333333333334e-05,
      "loss": 0.0025,
      "step": 27800
    },
    {
      "epoch": 1.4832,
      "grad_norm": 0.29256147146224976,
      "learning_rate": 1.2920000000000002e-05,
      "loss": 0.0022,
      "step": 27810
    },
    {
      "epoch": 1.4837333333333333,
      "grad_norm": 0.20310434699058533,
      "learning_rate": 1.2906666666666666e-05,
      "loss": 0.0021,
      "step": 27820
    },
    {
      "epoch": 1.4842666666666666,
      "grad_norm": 0.31501901149749756,
      "learning_rate": 1.2893333333333336e-05,
      "loss": 0.0033,
      "step": 27830
    },
    {
      "epoch": 1.4848,
      "grad_norm": 0.28116875886917114,
      "learning_rate": 1.288e-05,
      "loss": 0.0024,
      "step": 27840
    },
    {
      "epoch": 1.4853333333333334,
      "grad_norm": 0.09781146049499512,
      "learning_rate": 1.2866666666666668e-05,
      "loss": 0.0018,
      "step": 27850
    },
    {
      "epoch": 1.4858666666666667,
      "grad_norm": 0.42384761571884155,
      "learning_rate": 1.2853333333333335e-05,
      "loss": 0.0022,
      "step": 27860
    },
    {
      "epoch": 1.4864,
      "grad_norm": 0.28050222992897034,
      "learning_rate": 1.2839999999999999e-05,
      "loss": 0.0018,
      "step": 27870
    },
    {
      "epoch": 1.4869333333333334,
      "grad_norm": 0.0510006807744503,
      "learning_rate": 1.2826666666666667e-05,
      "loss": 0.0017,
      "step": 27880
    },
    {
      "epoch": 1.4874666666666667,
      "grad_norm": 0.0988163873553276,
      "learning_rate": 1.2813333333333333e-05,
      "loss": 0.0019,
      "step": 27890
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.057267799973487854,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.0023,
      "step": 27900
    },
    {
      "epoch": 1.4885333333333333,
      "grad_norm": 0.11402963101863861,
      "learning_rate": 1.2786666666666666e-05,
      "loss": 0.0027,
      "step": 27910
    },
    {
      "epoch": 1.4890666666666665,
      "grad_norm": 0.5971533060073853,
      "learning_rate": 1.2773333333333334e-05,
      "loss": 0.0023,
      "step": 27920
    },
    {
      "epoch": 1.4896,
      "grad_norm": 0.18675996363162994,
      "learning_rate": 1.276e-05,
      "loss": 0.0033,
      "step": 27930
    },
    {
      "epoch": 1.4901333333333333,
      "grad_norm": 0.3266952931880951,
      "learning_rate": 1.2746666666666668e-05,
      "loss": 0.0019,
      "step": 27940
    },
    {
      "epoch": 1.4906666666666666,
      "grad_norm": 0.26461201906204224,
      "learning_rate": 1.2733333333333334e-05,
      "loss": 0.003,
      "step": 27950
    },
    {
      "epoch": 1.4912,
      "grad_norm": 0.3873661458492279,
      "learning_rate": 1.2720000000000002e-05,
      "loss": 0.0051,
      "step": 27960
    },
    {
      "epoch": 1.4917333333333334,
      "grad_norm": 0.2585485577583313,
      "learning_rate": 1.2706666666666666e-05,
      "loss": 0.0028,
      "step": 27970
    },
    {
      "epoch": 1.4922666666666666,
      "grad_norm": 0.33972954750061035,
      "learning_rate": 1.2693333333333334e-05,
      "loss": 0.0026,
      "step": 27980
    },
    {
      "epoch": 1.4928,
      "grad_norm": 0.19572557508945465,
      "learning_rate": 1.268e-05,
      "loss": 0.0025,
      "step": 27990
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.14396606385707855,
      "learning_rate": 1.2666666666666668e-05,
      "loss": 0.0024,
      "step": 28000
    },
    {
      "epoch": 1.4938666666666667,
      "grad_norm": 0.09642015397548676,
      "learning_rate": 1.2653333333333333e-05,
      "loss": 0.0018,
      "step": 28010
    },
    {
      "epoch": 1.4944,
      "grad_norm": 0.3931656777858734,
      "learning_rate": 1.2640000000000003e-05,
      "loss": 0.0019,
      "step": 28020
    },
    {
      "epoch": 1.4949333333333334,
      "grad_norm": 0.15757548809051514,
      "learning_rate": 1.2626666666666667e-05,
      "loss": 0.0043,
      "step": 28030
    },
    {
      "epoch": 1.4954666666666667,
      "grad_norm": 0.3278343975543976,
      "learning_rate": 1.2613333333333332e-05,
      "loss": 0.0023,
      "step": 28040
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.28282949328422546,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 0.0022,
      "step": 28050
    },
    {
      "epoch": 1.4965333333333333,
      "grad_norm": 0.153445765376091,
      "learning_rate": 1.2586666666666666e-05,
      "loss": 0.0027,
      "step": 28060
    },
    {
      "epoch": 1.4970666666666665,
      "grad_norm": 0.11607006192207336,
      "learning_rate": 1.2573333333333334e-05,
      "loss": 0.0036,
      "step": 28070
    },
    {
      "epoch": 1.4976,
      "grad_norm": 0.4505886137485504,
      "learning_rate": 1.256e-05,
      "loss": 0.0022,
      "step": 28080
    },
    {
      "epoch": 1.4981333333333333,
      "grad_norm": 0.3064069449901581,
      "learning_rate": 1.2546666666666668e-05,
      "loss": 0.002,
      "step": 28090
    },
    {
      "epoch": 1.4986666666666666,
      "grad_norm": 0.2025146335363388,
      "learning_rate": 1.2533333333333332e-05,
      "loss": 0.002,
      "step": 28100
    },
    {
      "epoch": 1.4992,
      "grad_norm": 0.09821207821369171,
      "learning_rate": 1.252e-05,
      "loss": 0.0022,
      "step": 28110
    },
    {
      "epoch": 1.4997333333333334,
      "grad_norm": 0.15660926699638367,
      "learning_rate": 1.2506666666666667e-05,
      "loss": 0.0023,
      "step": 28120
    },
    {
      "epoch": 1.5002666666666666,
      "grad_norm": 0.21612423658370972,
      "learning_rate": 1.2493333333333333e-05,
      "loss": 0.0028,
      "step": 28130
    },
    {
      "epoch": 1.5008,
      "grad_norm": 0.18501929938793182,
      "learning_rate": 1.248e-05,
      "loss": 0.002,
      "step": 28140
    },
    {
      "epoch": 1.5013333333333332,
      "grad_norm": 0.251966655254364,
      "learning_rate": 1.2466666666666667e-05,
      "loss": 0.0019,
      "step": 28150
    },
    {
      "epoch": 1.5018666666666667,
      "grad_norm": 0.17259396612644196,
      "learning_rate": 1.2453333333333333e-05,
      "loss": 0.0022,
      "step": 28160
    },
    {
      "epoch": 1.5024,
      "grad_norm": 0.1231997013092041,
      "learning_rate": 1.244e-05,
      "loss": 0.0022,
      "step": 28170
    },
    {
      "epoch": 1.5029333333333335,
      "grad_norm": 0.09767580032348633,
      "learning_rate": 1.2426666666666667e-05,
      "loss": 0.0023,
      "step": 28180
    },
    {
      "epoch": 1.5034666666666667,
      "grad_norm": 0.2528500556945801,
      "learning_rate": 1.2413333333333334e-05,
      "loss": 0.0029,
      "step": 28190
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.20400437712669373,
      "learning_rate": 1.24e-05,
      "loss": 0.0019,
      "step": 28200
    },
    {
      "epoch": 1.5045333333333333,
      "grad_norm": 0.08808418363332748,
      "learning_rate": 1.2386666666666668e-05,
      "loss": 0.0022,
      "step": 28210
    },
    {
      "epoch": 1.5050666666666666,
      "grad_norm": 0.17616967856884003,
      "learning_rate": 1.2373333333333334e-05,
      "loss": 0.0025,
      "step": 28220
    },
    {
      "epoch": 1.5056,
      "grad_norm": 0.276021271944046,
      "learning_rate": 1.236e-05,
      "loss": 0.0026,
      "step": 28230
    },
    {
      "epoch": 1.5061333333333333,
      "grad_norm": 0.08126424252986908,
      "learning_rate": 1.2346666666666668e-05,
      "loss": 0.004,
      "step": 28240
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.35598674416542053,
      "learning_rate": 1.2333333333333334e-05,
      "loss": 0.0021,
      "step": 28250
    },
    {
      "epoch": 1.5072,
      "grad_norm": 0.38076645135879517,
      "learning_rate": 1.232e-05,
      "loss": 0.002,
      "step": 28260
    },
    {
      "epoch": 1.5077333333333334,
      "grad_norm": 0.7221825122833252,
      "learning_rate": 1.2306666666666669e-05,
      "loss": 0.0022,
      "step": 28270
    },
    {
      "epoch": 1.5082666666666666,
      "grad_norm": 0.06762764602899551,
      "learning_rate": 1.2293333333333335e-05,
      "loss": 0.0022,
      "step": 28280
    },
    {
      "epoch": 1.5088,
      "grad_norm": 0.09627152234315872,
      "learning_rate": 1.2280000000000001e-05,
      "loss": 0.0023,
      "step": 28290
    },
    {
      "epoch": 1.5093333333333332,
      "grad_norm": 0.09119618684053421,
      "learning_rate": 1.2266666666666667e-05,
      "loss": 0.0032,
      "step": 28300
    },
    {
      "epoch": 1.5098666666666667,
      "grad_norm": 0.45184847712516785,
      "learning_rate": 1.2253333333333333e-05,
      "loss": 0.0021,
      "step": 28310
    },
    {
      "epoch": 1.5104,
      "grad_norm": 0.3689975142478943,
      "learning_rate": 1.224e-05,
      "loss": 0.0031,
      "step": 28320
    },
    {
      "epoch": 1.5109333333333335,
      "grad_norm": 0.151190385222435,
      "learning_rate": 1.2226666666666668e-05,
      "loss": 0.003,
      "step": 28330
    },
    {
      "epoch": 1.5114666666666667,
      "grad_norm": 0.14658397436141968,
      "learning_rate": 1.2213333333333334e-05,
      "loss": 0.0024,
      "step": 28340
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.4057634770870209,
      "learning_rate": 1.22e-05,
      "loss": 0.0027,
      "step": 28350
    },
    {
      "epoch": 1.5125333333333333,
      "grad_norm": 0.08897627145051956,
      "learning_rate": 1.2186666666666666e-05,
      "loss": 0.0034,
      "step": 28360
    },
    {
      "epoch": 1.5130666666666666,
      "grad_norm": 0.08378428220748901,
      "learning_rate": 1.2173333333333334e-05,
      "loss": 0.0043,
      "step": 28370
    },
    {
      "epoch": 1.5135999999999998,
      "grad_norm": 0.5281605124473572,
      "learning_rate": 1.216e-05,
      "loss": 0.0021,
      "step": 28380
    },
    {
      "epoch": 1.5141333333333333,
      "grad_norm": 0.3032884895801544,
      "learning_rate": 1.2146666666666667e-05,
      "loss": 0.0037,
      "step": 28390
    },
    {
      "epoch": 1.5146666666666668,
      "grad_norm": 0.06616061925888062,
      "learning_rate": 1.2133333333333335e-05,
      "loss": 0.0053,
      "step": 28400
    },
    {
      "epoch": 1.5152,
      "grad_norm": 0.4734906554222107,
      "learning_rate": 1.2120000000000001e-05,
      "loss": 0.0019,
      "step": 28410
    },
    {
      "epoch": 1.5157333333333334,
      "grad_norm": 0.39913493394851685,
      "learning_rate": 1.2106666666666667e-05,
      "loss": 0.0021,
      "step": 28420
    },
    {
      "epoch": 1.5162666666666667,
      "grad_norm": 0.44395092129707336,
      "learning_rate": 1.2093333333333335e-05,
      "loss": 0.0019,
      "step": 28430
    },
    {
      "epoch": 1.5168,
      "grad_norm": 0.31880271434783936,
      "learning_rate": 1.2080000000000001e-05,
      "loss": 0.0024,
      "step": 28440
    },
    {
      "epoch": 1.5173333333333332,
      "grad_norm": 0.4226250946521759,
      "learning_rate": 1.2066666666666667e-05,
      "loss": 0.0026,
      "step": 28450
    },
    {
      "epoch": 1.5178666666666667,
      "grad_norm": 0.13035428524017334,
      "learning_rate": 1.2053333333333334e-05,
      "loss": 0.0028,
      "step": 28460
    },
    {
      "epoch": 1.5184,
      "grad_norm": 0.4393737316131592,
      "learning_rate": 1.204e-05,
      "loss": 0.0031,
      "step": 28470
    },
    {
      "epoch": 1.5189333333333335,
      "grad_norm": 0.7081037759780884,
      "learning_rate": 1.2026666666666666e-05,
      "loss": 0.0032,
      "step": 28480
    },
    {
      "epoch": 1.5194666666666667,
      "grad_norm": 0.16965283453464508,
      "learning_rate": 1.2013333333333334e-05,
      "loss": 0.0017,
      "step": 28490
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.5983600616455078,
      "learning_rate": 1.2e-05,
      "loss": 0.003,
      "step": 28500
    },
    {
      "epoch": 1.5205333333333333,
      "grad_norm": 0.3417808711528778,
      "learning_rate": 1.1986666666666667e-05,
      "loss": 0.0022,
      "step": 28510
    },
    {
      "epoch": 1.5210666666666666,
      "grad_norm": 0.7202259302139282,
      "learning_rate": 1.1973333333333334e-05,
      "loss": 0.0022,
      "step": 28520
    },
    {
      "epoch": 1.5215999999999998,
      "grad_norm": 0.3323826789855957,
      "learning_rate": 1.196e-05,
      "loss": 0.0026,
      "step": 28530
    },
    {
      "epoch": 1.5221333333333333,
      "grad_norm": 0.5569552779197693,
      "learning_rate": 1.1946666666666667e-05,
      "loss": 0.0021,
      "step": 28540
    },
    {
      "epoch": 1.5226666666666666,
      "grad_norm": 0.826792299747467,
      "learning_rate": 1.1933333333333333e-05,
      "loss": 0.0033,
      "step": 28550
    },
    {
      "epoch": 1.5232,
      "grad_norm": 0.7259182333946228,
      "learning_rate": 1.1920000000000001e-05,
      "loss": 0.0021,
      "step": 28560
    },
    {
      "epoch": 1.5237333333333334,
      "grad_norm": 0.2026384323835373,
      "learning_rate": 1.1906666666666667e-05,
      "loss": 0.0021,
      "step": 28570
    },
    {
      "epoch": 1.5242666666666667,
      "grad_norm": 0.5542849898338318,
      "learning_rate": 1.1893333333333334e-05,
      "loss": 0.0021,
      "step": 28580
    },
    {
      "epoch": 1.5248,
      "grad_norm": 0.3786778450012207,
      "learning_rate": 1.1880000000000001e-05,
      "loss": 0.0019,
      "step": 28590
    },
    {
      "epoch": 1.5253333333333332,
      "grad_norm": 0.04983967915177345,
      "learning_rate": 1.1866666666666668e-05,
      "loss": 0.0019,
      "step": 28600
    },
    {
      "epoch": 1.5258666666666667,
      "grad_norm": 0.19089636206626892,
      "learning_rate": 1.1853333333333334e-05,
      "loss": 0.0029,
      "step": 28610
    },
    {
      "epoch": 1.5264,
      "grad_norm": 0.09330611675977707,
      "learning_rate": 1.1840000000000002e-05,
      "loss": 0.0023,
      "step": 28620
    },
    {
      "epoch": 1.5269333333333335,
      "grad_norm": 0.5851702690124512,
      "learning_rate": 1.1826666666666668e-05,
      "loss": 0.002,
      "step": 28630
    },
    {
      "epoch": 1.5274666666666668,
      "grad_norm": 0.18546950817108154,
      "learning_rate": 1.1813333333333334e-05,
      "loss": 0.0017,
      "step": 28640
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.3792285621166229,
      "learning_rate": 1.18e-05,
      "loss": 0.0017,
      "step": 28650
    },
    {
      "epoch": 1.5285333333333333,
      "grad_norm": 0.3462478220462799,
      "learning_rate": 1.1786666666666667e-05,
      "loss": 0.0035,
      "step": 28660
    },
    {
      "epoch": 1.5290666666666666,
      "grad_norm": 0.21135376393795013,
      "learning_rate": 1.1773333333333333e-05,
      "loss": 0.0026,
      "step": 28670
    },
    {
      "epoch": 1.5295999999999998,
      "grad_norm": 0.20657598972320557,
      "learning_rate": 1.1760000000000001e-05,
      "loss": 0.0024,
      "step": 28680
    },
    {
      "epoch": 1.5301333333333333,
      "grad_norm": 0.09732566773891449,
      "learning_rate": 1.1746666666666667e-05,
      "loss": 0.0031,
      "step": 28690
    },
    {
      "epoch": 1.5306666666666666,
      "grad_norm": 0.0739588513970375,
      "learning_rate": 1.1733333333333333e-05,
      "loss": 0.0023,
      "step": 28700
    },
    {
      "epoch": 1.5312000000000001,
      "grad_norm": 0.24443475902080536,
      "learning_rate": 1.172e-05,
      "loss": 0.0018,
      "step": 28710
    },
    {
      "epoch": 1.5317333333333334,
      "grad_norm": 0.5621258020401001,
      "learning_rate": 1.1706666666666668e-05,
      "loss": 0.0026,
      "step": 28720
    },
    {
      "epoch": 1.5322666666666667,
      "grad_norm": 0.4980614185333252,
      "learning_rate": 1.1693333333333334e-05,
      "loss": 0.0021,
      "step": 28730
    },
    {
      "epoch": 1.5328,
      "grad_norm": 0.352271169424057,
      "learning_rate": 1.168e-05,
      "loss": 0.0025,
      "step": 28740
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.3597889542579651,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 0.0039,
      "step": 28750
    },
    {
      "epoch": 1.5338666666666667,
      "grad_norm": 0.24763919413089752,
      "learning_rate": 1.1653333333333334e-05,
      "loss": 0.0017,
      "step": 28760
    },
    {
      "epoch": 1.5344,
      "grad_norm": 0.14819596707820892,
      "learning_rate": 1.164e-05,
      "loss": 0.0037,
      "step": 28770
    },
    {
      "epoch": 1.5349333333333335,
      "grad_norm": 0.36298659443855286,
      "learning_rate": 1.1626666666666668e-05,
      "loss": 0.0022,
      "step": 28780
    },
    {
      "epoch": 1.5354666666666668,
      "grad_norm": 0.08738239854574203,
      "learning_rate": 1.1613333333333335e-05,
      "loss": 0.0018,
      "step": 28790
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.44671809673309326,
      "learning_rate": 1.16e-05,
      "loss": 0.0026,
      "step": 28800
    },
    {
      "epoch": 1.5365333333333333,
      "grad_norm": 0.704064130783081,
      "learning_rate": 1.1586666666666669e-05,
      "loss": 0.0023,
      "step": 28810
    },
    {
      "epoch": 1.5370666666666666,
      "grad_norm": 0.1026446670293808,
      "learning_rate": 1.1573333333333333e-05,
      "loss": 0.002,
      "step": 28820
    },
    {
      "epoch": 1.5375999999999999,
      "grad_norm": 0.4101354777812958,
      "learning_rate": 1.156e-05,
      "loss": 0.0021,
      "step": 28830
    },
    {
      "epoch": 1.5381333333333334,
      "grad_norm": 0.13413101434707642,
      "learning_rate": 1.1546666666666667e-05,
      "loss": 0.0022,
      "step": 28840
    },
    {
      "epoch": 1.5386666666666666,
      "grad_norm": 0.4078405201435089,
      "learning_rate": 1.1533333333333334e-05,
      "loss": 0.0037,
      "step": 28850
    },
    {
      "epoch": 1.5392000000000001,
      "grad_norm": 0.10411778837442398,
      "learning_rate": 1.152e-05,
      "loss": 0.0019,
      "step": 28860
    },
    {
      "epoch": 1.5397333333333334,
      "grad_norm": 0.7766061425209045,
      "learning_rate": 1.1506666666666668e-05,
      "loss": 0.0022,
      "step": 28870
    },
    {
      "epoch": 1.5402666666666667,
      "grad_norm": 0.283863365650177,
      "learning_rate": 1.1493333333333334e-05,
      "loss": 0.0019,
      "step": 28880
    },
    {
      "epoch": 1.5408,
      "grad_norm": 0.05748671293258667,
      "learning_rate": 1.148e-05,
      "loss": 0.0029,
      "step": 28890
    },
    {
      "epoch": 1.5413333333333332,
      "grad_norm": 0.33435767889022827,
      "learning_rate": 1.1466666666666666e-05,
      "loss": 0.0022,
      "step": 28900
    },
    {
      "epoch": 1.5418666666666667,
      "grad_norm": 0.12192992866039276,
      "learning_rate": 1.1453333333333334e-05,
      "loss": 0.0026,
      "step": 28910
    },
    {
      "epoch": 1.5424,
      "grad_norm": 0.37563636898994446,
      "learning_rate": 1.144e-05,
      "loss": 0.0017,
      "step": 28920
    },
    {
      "epoch": 1.5429333333333335,
      "grad_norm": 0.12487214803695679,
      "learning_rate": 1.1426666666666667e-05,
      "loss": 0.002,
      "step": 28930
    },
    {
      "epoch": 1.5434666666666668,
      "grad_norm": 0.1973395198583603,
      "learning_rate": 1.1413333333333335e-05,
      "loss": 0.0022,
      "step": 28940
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.06894559413194656,
      "learning_rate": 1.1400000000000001e-05,
      "loss": 0.002,
      "step": 28950
    },
    {
      "epoch": 1.5445333333333333,
      "grad_norm": 0.24881868064403534,
      "learning_rate": 1.1386666666666667e-05,
      "loss": 0.002,
      "step": 28960
    },
    {
      "epoch": 1.5450666666666666,
      "grad_norm": 0.1947101354598999,
      "learning_rate": 1.1373333333333335e-05,
      "loss": 0.0017,
      "step": 28970
    },
    {
      "epoch": 1.5455999999999999,
      "grad_norm": 0.18490737676620483,
      "learning_rate": 1.1360000000000001e-05,
      "loss": 0.0031,
      "step": 28980
    },
    {
      "epoch": 1.5461333333333334,
      "grad_norm": 0.3471730351448059,
      "learning_rate": 1.1346666666666666e-05,
      "loss": 0.0027,
      "step": 28990
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.11680829524993896,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 0.0025,
      "step": 29000
    },
    {
      "epoch": 1.5472000000000001,
      "grad_norm": 0.5753852725028992,
      "learning_rate": 1.132e-05,
      "loss": 0.003,
      "step": 29010
    },
    {
      "epoch": 1.5477333333333334,
      "grad_norm": 0.14263217151165009,
      "learning_rate": 1.1306666666666666e-05,
      "loss": 0.0019,
      "step": 29020
    },
    {
      "epoch": 1.5482666666666667,
      "grad_norm": 0.1564837247133255,
      "learning_rate": 1.1293333333333334e-05,
      "loss": 0.0021,
      "step": 29030
    },
    {
      "epoch": 1.5488,
      "grad_norm": 0.2854997217655182,
      "learning_rate": 1.128e-05,
      "loss": 0.0025,
      "step": 29040
    },
    {
      "epoch": 1.5493333333333332,
      "grad_norm": 0.24301962554454803,
      "learning_rate": 1.1266666666666667e-05,
      "loss": 0.0018,
      "step": 29050
    },
    {
      "epoch": 1.5498666666666665,
      "grad_norm": 0.2672537863254547,
      "learning_rate": 1.1253333333333335e-05,
      "loss": 0.002,
      "step": 29060
    },
    {
      "epoch": 1.5504,
      "grad_norm": 0.09031879156827927,
      "learning_rate": 1.124e-05,
      "loss": 0.0027,
      "step": 29070
    },
    {
      "epoch": 1.5509333333333335,
      "grad_norm": 0.4182043671607971,
      "learning_rate": 1.1226666666666667e-05,
      "loss": 0.0033,
      "step": 29080
    },
    {
      "epoch": 1.5514666666666668,
      "grad_norm": 0.3276267349720001,
      "learning_rate": 1.1213333333333333e-05,
      "loss": 0.0029,
      "step": 29090
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.3185054361820221,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.0017,
      "step": 29100
    },
    {
      "epoch": 1.5525333333333333,
      "grad_norm": 0.2652221918106079,
      "learning_rate": 1.1186666666666667e-05,
      "loss": 0.0019,
      "step": 29110
    },
    {
      "epoch": 1.5530666666666666,
      "grad_norm": 0.16804108023643494,
      "learning_rate": 1.1173333333333334e-05,
      "loss": 0.0019,
      "step": 29120
    },
    {
      "epoch": 1.5535999999999999,
      "grad_norm": 0.24621812999248505,
      "learning_rate": 1.1160000000000002e-05,
      "loss": 0.0021,
      "step": 29130
    },
    {
      "epoch": 1.5541333333333334,
      "grad_norm": 0.09893897920846939,
      "learning_rate": 1.1146666666666668e-05,
      "loss": 0.0021,
      "step": 29140
    },
    {
      "epoch": 1.5546666666666666,
      "grad_norm": 0.12091805785894394,
      "learning_rate": 1.1133333333333334e-05,
      "loss": 0.0039,
      "step": 29150
    },
    {
      "epoch": 1.5552000000000001,
      "grad_norm": 0.1981155127286911,
      "learning_rate": 1.112e-05,
      "loss": 0.0026,
      "step": 29160
    },
    {
      "epoch": 1.5557333333333334,
      "grad_norm": 0.10753613710403442,
      "learning_rate": 1.1106666666666666e-05,
      "loss": 0.0038,
      "step": 29170
    },
    {
      "epoch": 1.5562666666666667,
      "grad_norm": 0.629771888256073,
      "learning_rate": 1.1093333333333333e-05,
      "loss": 0.0027,
      "step": 29180
    },
    {
      "epoch": 1.5568,
      "grad_norm": 0.25066983699798584,
      "learning_rate": 1.108e-05,
      "loss": 0.0018,
      "step": 29190
    },
    {
      "epoch": 1.5573333333333332,
      "grad_norm": 0.41571784019470215,
      "learning_rate": 1.1066666666666667e-05,
      "loss": 0.002,
      "step": 29200
    },
    {
      "epoch": 1.5578666666666665,
      "grad_norm": 0.1590849608182907,
      "learning_rate": 1.1053333333333333e-05,
      "loss": 0.0024,
      "step": 29210
    },
    {
      "epoch": 1.5584,
      "grad_norm": 0.11690888553857803,
      "learning_rate": 1.1040000000000001e-05,
      "loss": 0.0018,
      "step": 29220
    },
    {
      "epoch": 1.5589333333333333,
      "grad_norm": 0.4321724474430084,
      "learning_rate": 1.1026666666666667e-05,
      "loss": 0.0041,
      "step": 29230
    },
    {
      "epoch": 1.5594666666666668,
      "grad_norm": 0.0844859629869461,
      "learning_rate": 1.1013333333333333e-05,
      "loss": 0.0024,
      "step": 29240
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.2078217715024948,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.0021,
      "step": 29250
    },
    {
      "epoch": 1.5605333333333333,
      "grad_norm": 0.12392351031303406,
      "learning_rate": 1.0986666666666668e-05,
      "loss": 0.0017,
      "step": 29260
    },
    {
      "epoch": 1.5610666666666666,
      "grad_norm": 0.3470655083656311,
      "learning_rate": 1.0973333333333334e-05,
      "loss": 0.003,
      "step": 29270
    },
    {
      "epoch": 1.5615999999999999,
      "grad_norm": 0.5358819961547852,
      "learning_rate": 1.096e-05,
      "loss": 0.0034,
      "step": 29280
    },
    {
      "epoch": 1.5621333333333334,
      "grad_norm": 0.35147610306739807,
      "learning_rate": 1.0946666666666668e-05,
      "loss": 0.0026,
      "step": 29290
    },
    {
      "epoch": 1.5626666666666666,
      "grad_norm": 0.29368939995765686,
      "learning_rate": 1.0933333333333334e-05,
      "loss": 0.0027,
      "step": 29300
    },
    {
      "epoch": 1.5632000000000001,
      "grad_norm": 0.6529263854026794,
      "learning_rate": 1.092e-05,
      "loss": 0.0028,
      "step": 29310
    },
    {
      "epoch": 1.5637333333333334,
      "grad_norm": 0.3171834945678711,
      "learning_rate": 1.0906666666666668e-05,
      "loss": 0.0019,
      "step": 29320
    },
    {
      "epoch": 1.5642666666666667,
      "grad_norm": 0.568634033203125,
      "learning_rate": 1.0893333333333333e-05,
      "loss": 0.0032,
      "step": 29330
    },
    {
      "epoch": 1.5648,
      "grad_norm": 0.1277363896369934,
      "learning_rate": 1.088e-05,
      "loss": 0.0029,
      "step": 29340
    },
    {
      "epoch": 1.5653333333333332,
      "grad_norm": 0.2390584945678711,
      "learning_rate": 1.0866666666666667e-05,
      "loss": 0.0026,
      "step": 29350
    },
    {
      "epoch": 1.5658666666666665,
      "grad_norm": 0.42837125062942505,
      "learning_rate": 1.0853333333333333e-05,
      "loss": 0.0019,
      "step": 29360
    },
    {
      "epoch": 1.5664,
      "grad_norm": 0.4204425811767578,
      "learning_rate": 1.084e-05,
      "loss": 0.0025,
      "step": 29370
    },
    {
      "epoch": 1.5669333333333333,
      "grad_norm": 0.10894515365362167,
      "learning_rate": 1.0826666666666667e-05,
      "loss": 0.0032,
      "step": 29380
    },
    {
      "epoch": 1.5674666666666668,
      "grad_norm": 0.2488478273153305,
      "learning_rate": 1.0813333333333334e-05,
      "loss": 0.0027,
      "step": 29390
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.4443855881690979,
      "learning_rate": 1.08e-05,
      "loss": 0.0017,
      "step": 29400
    },
    {
      "epoch": 1.5685333333333333,
      "grad_norm": 0.17463062703609467,
      "learning_rate": 1.0786666666666668e-05,
      "loss": 0.002,
      "step": 29410
    },
    {
      "epoch": 1.5690666666666666,
      "grad_norm": 0.1975698322057724,
      "learning_rate": 1.0773333333333334e-05,
      "loss": 0.0022,
      "step": 29420
    },
    {
      "epoch": 1.5695999999999999,
      "grad_norm": 0.07892574369907379,
      "learning_rate": 1.076e-05,
      "loss": 0.002,
      "step": 29430
    },
    {
      "epoch": 1.5701333333333334,
      "grad_norm": 0.44130921363830566,
      "learning_rate": 1.0746666666666667e-05,
      "loss": 0.0021,
      "step": 29440
    },
    {
      "epoch": 1.5706666666666667,
      "grad_norm": 0.24253445863723755,
      "learning_rate": 1.0733333333333334e-05,
      "loss": 0.0024,
      "step": 29450
    },
    {
      "epoch": 1.5712000000000002,
      "grad_norm": 0.37383490800857544,
      "learning_rate": 1.072e-05,
      "loss": 0.003,
      "step": 29460
    },
    {
      "epoch": 1.5717333333333334,
      "grad_norm": 0.5661727786064148,
      "learning_rate": 1.0706666666666667e-05,
      "loss": 0.0027,
      "step": 29470
    },
    {
      "epoch": 1.5722666666666667,
      "grad_norm": 0.20469872653484344,
      "learning_rate": 1.0693333333333335e-05,
      "loss": 0.0026,
      "step": 29480
    },
    {
      "epoch": 1.5728,
      "grad_norm": 0.2811015844345093,
      "learning_rate": 1.0680000000000001e-05,
      "loss": 0.0027,
      "step": 29490
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.17684580385684967,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 0.0017,
      "step": 29500
    },
    {
      "epoch": 1.5738666666666665,
      "grad_norm": 0.35885658860206604,
      "learning_rate": 1.0653333333333334e-05,
      "loss": 0.0021,
      "step": 29510
    },
    {
      "epoch": 1.5744,
      "grad_norm": 0.3498106300830841,
      "learning_rate": 1.064e-05,
      "loss": 0.0026,
      "step": 29520
    },
    {
      "epoch": 1.5749333333333333,
      "grad_norm": 0.35866591334342957,
      "learning_rate": 1.0626666666666666e-05,
      "loss": 0.0031,
      "step": 29530
    },
    {
      "epoch": 1.5754666666666668,
      "grad_norm": 0.11999423056840897,
      "learning_rate": 1.0613333333333334e-05,
      "loss": 0.0025,
      "step": 29540
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.12290006875991821,
      "learning_rate": 1.06e-05,
      "loss": 0.0032,
      "step": 29550
    },
    {
      "epoch": 1.5765333333333333,
      "grad_norm": 0.531471848487854,
      "learning_rate": 1.0586666666666666e-05,
      "loss": 0.0018,
      "step": 29560
    },
    {
      "epoch": 1.5770666666666666,
      "grad_norm": 0.3203607499599457,
      "learning_rate": 1.0573333333333334e-05,
      "loss": 0.0018,
      "step": 29570
    },
    {
      "epoch": 1.5776,
      "grad_norm": 0.09704259037971497,
      "learning_rate": 1.056e-05,
      "loss": 0.0017,
      "step": 29580
    },
    {
      "epoch": 1.5781333333333334,
      "grad_norm": 0.27135443687438965,
      "learning_rate": 1.0546666666666667e-05,
      "loss": 0.0025,
      "step": 29590
    },
    {
      "epoch": 1.5786666666666667,
      "grad_norm": 0.5267398357391357,
      "learning_rate": 1.0533333333333335e-05,
      "loss": 0.0021,
      "step": 29600
    },
    {
      "epoch": 1.5792000000000002,
      "grad_norm": 0.2769182324409485,
      "learning_rate": 1.0520000000000001e-05,
      "loss": 0.0018,
      "step": 29610
    },
    {
      "epoch": 1.5797333333333334,
      "grad_norm": 0.16877394914627075,
      "learning_rate": 1.0506666666666667e-05,
      "loss": 0.003,
      "step": 29620
    },
    {
      "epoch": 1.5802666666666667,
      "grad_norm": 0.545261561870575,
      "learning_rate": 1.0493333333333333e-05,
      "loss": 0.002,
      "step": 29630
    },
    {
      "epoch": 1.5808,
      "grad_norm": 0.17772772908210754,
      "learning_rate": 1.0480000000000001e-05,
      "loss": 0.002,
      "step": 29640
    },
    {
      "epoch": 1.5813333333333333,
      "grad_norm": 0.7112788558006287,
      "learning_rate": 1.0466666666666668e-05,
      "loss": 0.0034,
      "step": 29650
    },
    {
      "epoch": 1.5818666666666665,
      "grad_norm": 0.1802505999803543,
      "learning_rate": 1.0453333333333334e-05,
      "loss": 0.0027,
      "step": 29660
    },
    {
      "epoch": 1.5824,
      "grad_norm": 0.5744975209236145,
      "learning_rate": 1.0440000000000002e-05,
      "loss": 0.0023,
      "step": 29670
    },
    {
      "epoch": 1.5829333333333333,
      "grad_norm": 0.32498061656951904,
      "learning_rate": 1.0426666666666666e-05,
      "loss": 0.0022,
      "step": 29680
    },
    {
      "epoch": 1.5834666666666668,
      "grad_norm": 0.09814412891864777,
      "learning_rate": 1.0413333333333332e-05,
      "loss": 0.0022,
      "step": 29690
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.4458152949810028,
      "learning_rate": 1.04e-05,
      "loss": 0.0026,
      "step": 29700
    },
    {
      "epoch": 1.5845333333333333,
      "grad_norm": 0.10555370151996613,
      "learning_rate": 1.0386666666666667e-05,
      "loss": 0.0031,
      "step": 29710
    },
    {
      "epoch": 1.5850666666666666,
      "grad_norm": 0.6859666109085083,
      "learning_rate": 1.0373333333333333e-05,
      "loss": 0.0018,
      "step": 29720
    },
    {
      "epoch": 1.5856,
      "grad_norm": 0.1396864354610443,
      "learning_rate": 1.036e-05,
      "loss": 0.0018,
      "step": 29730
    },
    {
      "epoch": 1.5861333333333332,
      "grad_norm": 0.2257760912179947,
      "learning_rate": 1.0346666666666667e-05,
      "loss": 0.0034,
      "step": 29740
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.12034468352794647,
      "learning_rate": 1.0333333333333333e-05,
      "loss": 0.0023,
      "step": 29750
    },
    {
      "epoch": 1.5872000000000002,
      "grad_norm": 0.1094234362244606,
      "learning_rate": 1.0320000000000001e-05,
      "loss": 0.0025,
      "step": 29760
    },
    {
      "epoch": 1.5877333333333334,
      "grad_norm": 0.22060644626617432,
      "learning_rate": 1.0306666666666667e-05,
      "loss": 0.0022,
      "step": 29770
    },
    {
      "epoch": 1.5882666666666667,
      "grad_norm": 0.28410232067108154,
      "learning_rate": 1.0293333333333334e-05,
      "loss": 0.0022,
      "step": 29780
    },
    {
      "epoch": 1.5888,
      "grad_norm": 0.6574680805206299,
      "learning_rate": 1.0280000000000002e-05,
      "loss": 0.0018,
      "step": 29790
    },
    {
      "epoch": 1.5893333333333333,
      "grad_norm": 0.19036051630973816,
      "learning_rate": 1.0266666666666668e-05,
      "loss": 0.0031,
      "step": 29800
    },
    {
      "epoch": 1.5898666666666665,
      "grad_norm": 0.29196691513061523,
      "learning_rate": 1.0253333333333334e-05,
      "loss": 0.0042,
      "step": 29810
    },
    {
      "epoch": 1.5904,
      "grad_norm": 0.16961435973644257,
      "learning_rate": 1.024e-05,
      "loss": 0.0018,
      "step": 29820
    },
    {
      "epoch": 1.5909333333333333,
      "grad_norm": 0.17478853464126587,
      "learning_rate": 1.0226666666666668e-05,
      "loss": 0.002,
      "step": 29830
    },
    {
      "epoch": 1.5914666666666668,
      "grad_norm": 0.2339225709438324,
      "learning_rate": 1.0213333333333334e-05,
      "loss": 0.0028,
      "step": 29840
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.100726418197155,
      "learning_rate": 1.02e-05,
      "loss": 0.0023,
      "step": 29850
    },
    {
      "epoch": 1.5925333333333334,
      "grad_norm": 0.09269236028194427,
      "learning_rate": 1.0186666666666667e-05,
      "loss": 0.0021,
      "step": 29860
    },
    {
      "epoch": 1.5930666666666666,
      "grad_norm": 0.5462080836296082,
      "learning_rate": 1.0173333333333333e-05,
      "loss": 0.0041,
      "step": 29870
    },
    {
      "epoch": 1.5936,
      "grad_norm": 0.1533307284116745,
      "learning_rate": 1.016e-05,
      "loss": 0.0032,
      "step": 29880
    },
    {
      "epoch": 1.5941333333333332,
      "grad_norm": 0.09548692405223846,
      "learning_rate": 1.0146666666666667e-05,
      "loss": 0.0025,
      "step": 29890
    },
    {
      "epoch": 1.5946666666666667,
      "grad_norm": 0.2135620266199112,
      "learning_rate": 1.0133333333333333e-05,
      "loss": 0.0019,
      "step": 29900
    },
    {
      "epoch": 1.5952,
      "grad_norm": 0.6894038915634155,
      "learning_rate": 1.012e-05,
      "loss": 0.0018,
      "step": 29910
    },
    {
      "epoch": 1.5957333333333334,
      "grad_norm": 0.49196621775627136,
      "learning_rate": 1.0106666666666668e-05,
      "loss": 0.0022,
      "step": 29920
    },
    {
      "epoch": 1.5962666666666667,
      "grad_norm": 0.11787237972021103,
      "learning_rate": 1.0093333333333334e-05,
      "loss": 0.0034,
      "step": 29930
    },
    {
      "epoch": 1.5968,
      "grad_norm": 0.2626304626464844,
      "learning_rate": 1.008e-05,
      "loss": 0.0027,
      "step": 29940
    },
    {
      "epoch": 1.5973333333333333,
      "grad_norm": 0.07531053572893143,
      "learning_rate": 1.0066666666666668e-05,
      "loss": 0.0031,
      "step": 29950
    },
    {
      "epoch": 1.5978666666666665,
      "grad_norm": 0.17039786279201508,
      "learning_rate": 1.0053333333333334e-05,
      "loss": 0.0028,
      "step": 29960
    },
    {
      "epoch": 1.5984,
      "grad_norm": 0.16201703250408173,
      "learning_rate": 1.004e-05,
      "loss": 0.0044,
      "step": 29970
    },
    {
      "epoch": 1.5989333333333333,
      "grad_norm": 0.21629785001277924,
      "learning_rate": 1.0026666666666668e-05,
      "loss": 0.002,
      "step": 29980
    },
    {
      "epoch": 1.5994666666666668,
      "grad_norm": 0.2100713700056076,
      "learning_rate": 1.0013333333333335e-05,
      "loss": 0.003,
      "step": 29990
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.25759026408195496,
      "learning_rate": 1e-05,
      "loss": 0.0031,
      "step": 30000
    },
    {
      "epoch": 1.6005333333333334,
      "grad_norm": 0.2661733031272888,
      "learning_rate": 9.986666666666667e-06,
      "loss": 0.0021,
      "step": 30010
    },
    {
      "epoch": 1.6010666666666666,
      "grad_norm": 0.14288994669914246,
      "learning_rate": 9.973333333333333e-06,
      "loss": 0.0025,
      "step": 30020
    },
    {
      "epoch": 1.6016,
      "grad_norm": 0.2881503105163574,
      "learning_rate": 9.96e-06,
      "loss": 0.0033,
      "step": 30030
    },
    {
      "epoch": 1.6021333333333332,
      "grad_norm": 0.2912376821041107,
      "learning_rate": 9.946666666666667e-06,
      "loss": 0.0024,
      "step": 30040
    },
    {
      "epoch": 1.6026666666666667,
      "grad_norm": 0.3930363953113556,
      "learning_rate": 9.933333333333334e-06,
      "loss": 0.0023,
      "step": 30050
    },
    {
      "epoch": 1.6032,
      "grad_norm": 0.1999904364347458,
      "learning_rate": 9.92e-06,
      "loss": 0.0027,
      "step": 30060
    },
    {
      "epoch": 1.6037333333333335,
      "grad_norm": 0.2290673851966858,
      "learning_rate": 9.906666666666666e-06,
      "loss": 0.002,
      "step": 30070
    },
    {
      "epoch": 1.6042666666666667,
      "grad_norm": 0.5142995715141296,
      "learning_rate": 9.893333333333334e-06,
      "loss": 0.0028,
      "step": 30080
    },
    {
      "epoch": 1.6048,
      "grad_norm": 0.3188653588294983,
      "learning_rate": 9.88e-06,
      "loss": 0.0023,
      "step": 30090
    },
    {
      "epoch": 1.6053333333333333,
      "grad_norm": 0.18428365886211395,
      "learning_rate": 9.866666666666667e-06,
      "loss": 0.0027,
      "step": 30100
    },
    {
      "epoch": 1.6058666666666666,
      "grad_norm": 0.15911908447742462,
      "learning_rate": 9.853333333333334e-06,
      "loss": 0.0016,
      "step": 30110
    },
    {
      "epoch": 1.6064,
      "grad_norm": 0.4196760952472687,
      "learning_rate": 9.84e-06,
      "loss": 0.0024,
      "step": 30120
    },
    {
      "epoch": 1.6069333333333333,
      "grad_norm": 0.6944382786750793,
      "learning_rate": 9.826666666666667e-06,
      "loss": 0.0018,
      "step": 30130
    },
    {
      "epoch": 1.6074666666666668,
      "grad_norm": 0.3165503740310669,
      "learning_rate": 9.813333333333335e-06,
      "loss": 0.0018,
      "step": 30140
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.5800760388374329,
      "learning_rate": 9.800000000000001e-06,
      "loss": 0.0019,
      "step": 30150
    },
    {
      "epoch": 1.6085333333333334,
      "grad_norm": 0.3426848351955414,
      "learning_rate": 9.786666666666667e-06,
      "loss": 0.0037,
      "step": 30160
    },
    {
      "epoch": 1.6090666666666666,
      "grad_norm": 0.19595731794834137,
      "learning_rate": 9.773333333333333e-06,
      "loss": 0.002,
      "step": 30170
    },
    {
      "epoch": 1.6096,
      "grad_norm": 0.5407149195671082,
      "learning_rate": 9.760000000000001e-06,
      "loss": 0.0022,
      "step": 30180
    },
    {
      "epoch": 1.6101333333333332,
      "grad_norm": 0.5670073628425598,
      "learning_rate": 9.746666666666666e-06,
      "loss": 0.0022,
      "step": 30190
    },
    {
      "epoch": 1.6106666666666667,
      "grad_norm": 0.29625821113586426,
      "learning_rate": 9.733333333333334e-06,
      "loss": 0.0034,
      "step": 30200
    },
    {
      "epoch": 1.6112,
      "grad_norm": 0.40429139137268066,
      "learning_rate": 9.72e-06,
      "loss": 0.0025,
      "step": 30210
    },
    {
      "epoch": 1.6117333333333335,
      "grad_norm": 0.7001497745513916,
      "learning_rate": 9.706666666666666e-06,
      "loss": 0.0018,
      "step": 30220
    },
    {
      "epoch": 1.6122666666666667,
      "grad_norm": 0.15324771404266357,
      "learning_rate": 9.693333333333334e-06,
      "loss": 0.0025,
      "step": 30230
    },
    {
      "epoch": 1.6128,
      "grad_norm": 0.17439842224121094,
      "learning_rate": 9.68e-06,
      "loss": 0.0021,
      "step": 30240
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.18423351645469666,
      "learning_rate": 9.666666666666667e-06,
      "loss": 0.0029,
      "step": 30250
    },
    {
      "epoch": 1.6138666666666666,
      "grad_norm": 0.14287766814231873,
      "learning_rate": 9.653333333333333e-06,
      "loss": 0.002,
      "step": 30260
    },
    {
      "epoch": 1.6143999999999998,
      "grad_norm": 0.35106271505355835,
      "learning_rate": 9.640000000000001e-06,
      "loss": 0.002,
      "step": 30270
    },
    {
      "epoch": 1.6149333333333333,
      "grad_norm": 0.5673983097076416,
      "learning_rate": 9.626666666666667e-06,
      "loss": 0.002,
      "step": 30280
    },
    {
      "epoch": 1.6154666666666668,
      "grad_norm": 0.38972610235214233,
      "learning_rate": 9.613333333333333e-06,
      "loss": 0.0024,
      "step": 30290
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.09593939036130905,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.0043,
      "step": 30300
    },
    {
      "epoch": 1.6165333333333334,
      "grad_norm": 0.30324214696884155,
      "learning_rate": 9.586666666666667e-06,
      "loss": 0.0027,
      "step": 30310
    },
    {
      "epoch": 1.6170666666666667,
      "grad_norm": 0.5245252847671509,
      "learning_rate": 9.573333333333334e-06,
      "loss": 0.0017,
      "step": 30320
    },
    {
      "epoch": 1.6176,
      "grad_norm": 0.22576870024204254,
      "learning_rate": 9.560000000000002e-06,
      "loss": 0.002,
      "step": 30330
    },
    {
      "epoch": 1.6181333333333332,
      "grad_norm": 0.10213889181613922,
      "learning_rate": 9.546666666666668e-06,
      "loss": 0.003,
      "step": 30340
    },
    {
      "epoch": 1.6186666666666667,
      "grad_norm": 0.23599082231521606,
      "learning_rate": 9.533333333333334e-06,
      "loss": 0.0023,
      "step": 30350
    },
    {
      "epoch": 1.6192,
      "grad_norm": 0.39602869749069214,
      "learning_rate": 9.52e-06,
      "loss": 0.0017,
      "step": 30360
    },
    {
      "epoch": 1.6197333333333335,
      "grad_norm": 0.0783935934305191,
      "learning_rate": 9.506666666666667e-06,
      "loss": 0.0028,
      "step": 30370
    },
    {
      "epoch": 1.6202666666666667,
      "grad_norm": 0.35346999764442444,
      "learning_rate": 9.493333333333333e-06,
      "loss": 0.0037,
      "step": 30380
    },
    {
      "epoch": 1.6208,
      "grad_norm": 0.46966642141342163,
      "learning_rate": 9.48e-06,
      "loss": 0.0022,
      "step": 30390
    },
    {
      "epoch": 1.6213333333333333,
      "grad_norm": 0.14992886781692505,
      "learning_rate": 9.466666666666667e-06,
      "loss": 0.0024,
      "step": 30400
    },
    {
      "epoch": 1.6218666666666666,
      "grad_norm": 0.16361576318740845,
      "learning_rate": 9.453333333333333e-06,
      "loss": 0.0019,
      "step": 30410
    },
    {
      "epoch": 1.6223999999999998,
      "grad_norm": 0.18871839344501495,
      "learning_rate": 9.44e-06,
      "loss": 0.0021,
      "step": 30420
    },
    {
      "epoch": 1.6229333333333333,
      "grad_norm": 0.1089886873960495,
      "learning_rate": 9.426666666666667e-06,
      "loss": 0.002,
      "step": 30430
    },
    {
      "epoch": 1.6234666666666666,
      "grad_norm": 0.26918357610702515,
      "learning_rate": 9.413333333333334e-06,
      "loss": 0.0022,
      "step": 30440
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.42477741837501526,
      "learning_rate": 9.4e-06,
      "loss": 0.0021,
      "step": 30450
    },
    {
      "epoch": 1.6245333333333334,
      "grad_norm": 0.3424129784107208,
      "learning_rate": 9.386666666666668e-06,
      "loss": 0.0021,
      "step": 30460
    },
    {
      "epoch": 1.6250666666666667,
      "grad_norm": 0.25657957792282104,
      "learning_rate": 9.373333333333334e-06,
      "loss": 0.0019,
      "step": 30470
    },
    {
      "epoch": 1.6256,
      "grad_norm": 0.32233840227127075,
      "learning_rate": 9.36e-06,
      "loss": 0.0021,
      "step": 30480
    },
    {
      "epoch": 1.6261333333333332,
      "grad_norm": 0.19559557735919952,
      "learning_rate": 9.346666666666668e-06,
      "loss": 0.003,
      "step": 30490
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.27692902088165283,
      "learning_rate": 9.333333333333334e-06,
      "loss": 0.002,
      "step": 30500
    },
    {
      "epoch": 1.6272,
      "grad_norm": 0.09290147572755814,
      "learning_rate": 9.32e-06,
      "loss": 0.0023,
      "step": 30510
    },
    {
      "epoch": 1.6277333333333335,
      "grad_norm": 0.10240741074085236,
      "learning_rate": 9.306666666666668e-06,
      "loss": 0.0029,
      "step": 30520
    },
    {
      "epoch": 1.6282666666666668,
      "grad_norm": 0.0732465460896492,
      "learning_rate": 9.293333333333335e-06,
      "loss": 0.0027,
      "step": 30530
    },
    {
      "epoch": 1.6288,
      "grad_norm": 0.061656057834625244,
      "learning_rate": 9.28e-06,
      "loss": 0.0027,
      "step": 30540
    },
    {
      "epoch": 1.6293333333333333,
      "grad_norm": 0.39962533116340637,
      "learning_rate": 9.266666666666667e-06,
      "loss": 0.0027,
      "step": 30550
    },
    {
      "epoch": 1.6298666666666666,
      "grad_norm": 0.2939487099647522,
      "learning_rate": 9.253333333333333e-06,
      "loss": 0.0021,
      "step": 30560
    },
    {
      "epoch": 1.6303999999999998,
      "grad_norm": 0.1458728313446045,
      "learning_rate": 9.24e-06,
      "loss": 0.0037,
      "step": 30570
    },
    {
      "epoch": 1.6309333333333333,
      "grad_norm": 0.23730085790157318,
      "learning_rate": 9.226666666666668e-06,
      "loss": 0.0038,
      "step": 30580
    },
    {
      "epoch": 1.6314666666666666,
      "grad_norm": 0.1048891618847847,
      "learning_rate": 9.213333333333334e-06,
      "loss": 0.0018,
      "step": 30590
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.17636394500732422,
      "learning_rate": 9.2e-06,
      "loss": 0.0021,
      "step": 30600
    },
    {
      "epoch": 1.6325333333333334,
      "grad_norm": 0.4352802038192749,
      "learning_rate": 9.186666666666666e-06,
      "loss": 0.002,
      "step": 30610
    },
    {
      "epoch": 1.6330666666666667,
      "grad_norm": 0.202855184674263,
      "learning_rate": 9.173333333333334e-06,
      "loss": 0.0024,
      "step": 30620
    },
    {
      "epoch": 1.6336,
      "grad_norm": 0.08243101835250854,
      "learning_rate": 9.16e-06,
      "loss": 0.0022,
      "step": 30630
    },
    {
      "epoch": 1.6341333333333332,
      "grad_norm": 0.522199273109436,
      "learning_rate": 9.146666666666667e-06,
      "loss": 0.0019,
      "step": 30640
    },
    {
      "epoch": 1.6346666666666667,
      "grad_norm": 0.10736512392759323,
      "learning_rate": 9.133333333333335e-06,
      "loss": 0.0018,
      "step": 30650
    },
    {
      "epoch": 1.6352,
      "grad_norm": 0.08094269782304764,
      "learning_rate": 9.12e-06,
      "loss": 0.003,
      "step": 30660
    },
    {
      "epoch": 1.6357333333333335,
      "grad_norm": 0.12112178653478622,
      "learning_rate": 9.106666666666667e-06,
      "loss": 0.0024,
      "step": 30670
    },
    {
      "epoch": 1.6362666666666668,
      "grad_norm": 0.15485988557338715,
      "learning_rate": 9.093333333333335e-06,
      "loss": 0.0025,
      "step": 30680
    },
    {
      "epoch": 1.6368,
      "grad_norm": 0.5309336185455322,
      "learning_rate": 9.080000000000001e-06,
      "loss": 0.0027,
      "step": 30690
    },
    {
      "epoch": 1.6373333333333333,
      "grad_norm": 0.15454907715320587,
      "learning_rate": 9.066666666666667e-06,
      "loss": 0.0024,
      "step": 30700
    },
    {
      "epoch": 1.6378666666666666,
      "grad_norm": 0.3962366282939911,
      "learning_rate": 9.053333333333334e-06,
      "loss": 0.002,
      "step": 30710
    },
    {
      "epoch": 1.6383999999999999,
      "grad_norm": 0.37899258732795715,
      "learning_rate": 9.04e-06,
      "loss": 0.0027,
      "step": 30720
    },
    {
      "epoch": 1.6389333333333334,
      "grad_norm": 0.5406689643859863,
      "learning_rate": 9.026666666666666e-06,
      "loss": 0.0028,
      "step": 30730
    },
    {
      "epoch": 1.6394666666666666,
      "grad_norm": 0.5148783326148987,
      "learning_rate": 9.013333333333334e-06,
      "loss": 0.002,
      "step": 30740
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.3683261275291443,
      "learning_rate": 9e-06,
      "loss": 0.002,
      "step": 30750
    },
    {
      "epoch": 1.6405333333333334,
      "grad_norm": 0.07096459716558456,
      "learning_rate": 8.986666666666666e-06,
      "loss": 0.0044,
      "step": 30760
    },
    {
      "epoch": 1.6410666666666667,
      "grad_norm": 0.0977248027920723,
      "learning_rate": 8.973333333333334e-06,
      "loss": 0.0024,
      "step": 30770
    },
    {
      "epoch": 1.6416,
      "grad_norm": 0.267804890871048,
      "learning_rate": 8.96e-06,
      "loss": 0.0021,
      "step": 30780
    },
    {
      "epoch": 1.6421333333333332,
      "grad_norm": 0.1915500909090042,
      "learning_rate": 8.946666666666667e-06,
      "loss": 0.0023,
      "step": 30790
    },
    {
      "epoch": 1.6426666666666667,
      "grad_norm": 0.17344807088375092,
      "learning_rate": 8.933333333333333e-06,
      "loss": 0.002,
      "step": 30800
    },
    {
      "epoch": 1.6432,
      "grad_norm": 0.14849123358726501,
      "learning_rate": 8.920000000000001e-06,
      "loss": 0.0026,
      "step": 30810
    },
    {
      "epoch": 1.6437333333333335,
      "grad_norm": 0.5265867710113525,
      "learning_rate": 8.906666666666667e-06,
      "loss": 0.003,
      "step": 30820
    },
    {
      "epoch": 1.6442666666666668,
      "grad_norm": 0.10797163099050522,
      "learning_rate": 8.893333333333333e-06,
      "loss": 0.0025,
      "step": 30830
    },
    {
      "epoch": 1.6448,
      "grad_norm": 0.3986932933330536,
      "learning_rate": 8.880000000000001e-06,
      "loss": 0.0017,
      "step": 30840
    },
    {
      "epoch": 1.6453333333333333,
      "grad_norm": 0.10260004550218582,
      "learning_rate": 8.866666666666668e-06,
      "loss": 0.0017,
      "step": 30850
    },
    {
      "epoch": 1.6458666666666666,
      "grad_norm": 0.10444651544094086,
      "learning_rate": 8.853333333333334e-06,
      "loss": 0.002,
      "step": 30860
    },
    {
      "epoch": 1.6463999999999999,
      "grad_norm": 0.10413498431444168,
      "learning_rate": 8.840000000000002e-06,
      "loss": 0.0022,
      "step": 30870
    },
    {
      "epoch": 1.6469333333333334,
      "grad_norm": 0.3466724455356598,
      "learning_rate": 8.826666666666666e-06,
      "loss": 0.0025,
      "step": 30880
    },
    {
      "epoch": 1.6474666666666666,
      "grad_norm": 0.19894251227378845,
      "learning_rate": 8.813333333333333e-06,
      "loss": 0.0016,
      "step": 30890
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.07694897055625916,
      "learning_rate": 8.8e-06,
      "loss": 0.0019,
      "step": 30900
    },
    {
      "epoch": 1.6485333333333334,
      "grad_norm": 0.3244656026363373,
      "learning_rate": 8.786666666666667e-06,
      "loss": 0.0026,
      "step": 30910
    },
    {
      "epoch": 1.6490666666666667,
      "grad_norm": 0.4286087155342102,
      "learning_rate": 8.773333333333333e-06,
      "loss": 0.0018,
      "step": 30920
    },
    {
      "epoch": 1.6496,
      "grad_norm": 0.35627639293670654,
      "learning_rate": 8.76e-06,
      "loss": 0.0019,
      "step": 30930
    },
    {
      "epoch": 1.6501333333333332,
      "grad_norm": 0.4503055512905121,
      "learning_rate": 8.746666666666667e-06,
      "loss": 0.0027,
      "step": 30940
    },
    {
      "epoch": 1.6506666666666665,
      "grad_norm": 0.2936440408229828,
      "learning_rate": 8.733333333333333e-06,
      "loss": 0.0036,
      "step": 30950
    },
    {
      "epoch": 1.6512,
      "grad_norm": 0.4636026918888092,
      "learning_rate": 8.720000000000001e-06,
      "loss": 0.0017,
      "step": 30960
    },
    {
      "epoch": 1.6517333333333335,
      "grad_norm": 0.10850760340690613,
      "learning_rate": 8.706666666666667e-06,
      "loss": 0.0017,
      "step": 30970
    },
    {
      "epoch": 1.6522666666666668,
      "grad_norm": 0.3340049386024475,
      "learning_rate": 8.693333333333334e-06,
      "loss": 0.0019,
      "step": 30980
    },
    {
      "epoch": 1.6528,
      "grad_norm": 0.14072896540164948,
      "learning_rate": 8.68e-06,
      "loss": 0.003,
      "step": 30990
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.18657833337783813,
      "learning_rate": 8.666666666666668e-06,
      "loss": 0.0025,
      "step": 31000
    },
    {
      "epoch": 1.6538666666666666,
      "grad_norm": 0.31660884618759155,
      "learning_rate": 8.653333333333334e-06,
      "loss": 0.0024,
      "step": 31010
    },
    {
      "epoch": 1.6543999999999999,
      "grad_norm": 0.21234716475009918,
      "learning_rate": 8.64e-06,
      "loss": 0.0025,
      "step": 31020
    },
    {
      "epoch": 1.6549333333333334,
      "grad_norm": 0.17320415377616882,
      "learning_rate": 8.626666666666668e-06,
      "loss": 0.0022,
      "step": 31030
    },
    {
      "epoch": 1.6554666666666666,
      "grad_norm": 0.630020260810852,
      "learning_rate": 8.613333333333334e-06,
      "loss": 0.0018,
      "step": 31040
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.3608476519584656,
      "learning_rate": 8.599999999999999e-06,
      "loss": 0.0033,
      "step": 31050
    },
    {
      "epoch": 1.6565333333333334,
      "grad_norm": 0.30737268924713135,
      "learning_rate": 8.586666666666667e-06,
      "loss": 0.0019,
      "step": 31060
    },
    {
      "epoch": 1.6570666666666667,
      "grad_norm": 0.25471583008766174,
      "learning_rate": 8.573333333333333e-06,
      "loss": 0.0021,
      "step": 31070
    },
    {
      "epoch": 1.6576,
      "grad_norm": 0.37060752511024475,
      "learning_rate": 8.56e-06,
      "loss": 0.0026,
      "step": 31080
    },
    {
      "epoch": 1.6581333333333332,
      "grad_norm": 0.3602672219276428,
      "learning_rate": 8.546666666666667e-06,
      "loss": 0.0021,
      "step": 31090
    },
    {
      "epoch": 1.6586666666666665,
      "grad_norm": 0.3281223475933075,
      "learning_rate": 8.533333333333334e-06,
      "loss": 0.0021,
      "step": 31100
    },
    {
      "epoch": 1.6592,
      "grad_norm": 0.3642123341560364,
      "learning_rate": 8.52e-06,
      "loss": 0.0031,
      "step": 31110
    },
    {
      "epoch": 1.6597333333333333,
      "grad_norm": 0.2938975691795349,
      "learning_rate": 8.506666666666668e-06,
      "loss": 0.0021,
      "step": 31120
    },
    {
      "epoch": 1.6602666666666668,
      "grad_norm": 0.44776004552841187,
      "learning_rate": 8.493333333333334e-06,
      "loss": 0.0033,
      "step": 31130
    },
    {
      "epoch": 1.6608,
      "grad_norm": 0.48206883668899536,
      "learning_rate": 8.48e-06,
      "loss": 0.0034,
      "step": 31140
    },
    {
      "epoch": 1.6613333333333333,
      "grad_norm": 0.24788795411586761,
      "learning_rate": 8.466666666666666e-06,
      "loss": 0.0017,
      "step": 31150
    },
    {
      "epoch": 1.6618666666666666,
      "grad_norm": 0.07610571384429932,
      "learning_rate": 8.453333333333334e-06,
      "loss": 0.0017,
      "step": 31160
    },
    {
      "epoch": 1.6623999999999999,
      "grad_norm": 0.15698891878128052,
      "learning_rate": 8.44e-06,
      "loss": 0.002,
      "step": 31170
    },
    {
      "epoch": 1.6629333333333334,
      "grad_norm": 0.1004946380853653,
      "learning_rate": 8.426666666666667e-06,
      "loss": 0.0033,
      "step": 31180
    },
    {
      "epoch": 1.6634666666666666,
      "grad_norm": 0.09004536271095276,
      "learning_rate": 8.413333333333335e-06,
      "loss": 0.0019,
      "step": 31190
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.30491191148757935,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.002,
      "step": 31200
    },
    {
      "epoch": 1.6645333333333334,
      "grad_norm": 0.08532141149044037,
      "learning_rate": 8.386666666666667e-06,
      "loss": 0.002,
      "step": 31210
    },
    {
      "epoch": 1.6650666666666667,
      "grad_norm": 0.24997229874134064,
      "learning_rate": 8.373333333333335e-06,
      "loss": 0.0027,
      "step": 31220
    },
    {
      "epoch": 1.6656,
      "grad_norm": 0.44355309009552,
      "learning_rate": 8.36e-06,
      "loss": 0.0025,
      "step": 31230
    },
    {
      "epoch": 1.6661333333333332,
      "grad_norm": 0.09811358153820038,
      "learning_rate": 8.346666666666666e-06,
      "loss": 0.002,
      "step": 31240
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.1119409054517746,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.0027,
      "step": 31250
    },
    {
      "epoch": 1.6672,
      "grad_norm": 0.5352004170417786,
      "learning_rate": 8.32e-06,
      "loss": 0.0026,
      "step": 31260
    },
    {
      "epoch": 1.6677333333333333,
      "grad_norm": 0.13712482154369354,
      "learning_rate": 8.306666666666666e-06,
      "loss": 0.0027,
      "step": 31270
    },
    {
      "epoch": 1.6682666666666668,
      "grad_norm": 0.2980051636695862,
      "learning_rate": 8.293333333333334e-06,
      "loss": 0.0021,
      "step": 31280
    },
    {
      "epoch": 1.6688,
      "grad_norm": 0.11237814277410507,
      "learning_rate": 8.28e-06,
      "loss": 0.002,
      "step": 31290
    },
    {
      "epoch": 1.6693333333333333,
      "grad_norm": 0.15069285035133362,
      "learning_rate": 8.266666666666667e-06,
      "loss": 0.0034,
      "step": 31300
    },
    {
      "epoch": 1.6698666666666666,
      "grad_norm": 0.39443716406822205,
      "learning_rate": 8.253333333333334e-06,
      "loss": 0.002,
      "step": 31310
    },
    {
      "epoch": 1.6703999999999999,
      "grad_norm": 0.5737326145172119,
      "learning_rate": 8.24e-06,
      "loss": 0.0025,
      "step": 31320
    },
    {
      "epoch": 1.6709333333333334,
      "grad_norm": 0.28693297505378723,
      "learning_rate": 8.226666666666667e-06,
      "loss": 0.0028,
      "step": 31330
    },
    {
      "epoch": 1.6714666666666667,
      "grad_norm": 0.13457491993904114,
      "learning_rate": 8.213333333333333e-06,
      "loss": 0.0019,
      "step": 31340
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.14204078912734985,
      "learning_rate": 8.200000000000001e-06,
      "loss": 0.0026,
      "step": 31350
    },
    {
      "epoch": 1.6725333333333334,
      "grad_norm": 0.08213648200035095,
      "learning_rate": 8.186666666666667e-06,
      "loss": 0.0017,
      "step": 31360
    },
    {
      "epoch": 1.6730666666666667,
      "grad_norm": 0.1964568942785263,
      "learning_rate": 8.173333333333334e-06,
      "loss": 0.003,
      "step": 31370
    },
    {
      "epoch": 1.6736,
      "grad_norm": 0.33803895115852356,
      "learning_rate": 8.160000000000001e-06,
      "loss": 0.0017,
      "step": 31380
    },
    {
      "epoch": 1.6741333333333333,
      "grad_norm": 0.07853101193904877,
      "learning_rate": 8.146666666666668e-06,
      "loss": 0.0025,
      "step": 31390
    },
    {
      "epoch": 1.6746666666666665,
      "grad_norm": 0.41228726506233215,
      "learning_rate": 8.133333333333332e-06,
      "loss": 0.0024,
      "step": 31400
    },
    {
      "epoch": 1.6752,
      "grad_norm": 0.3946788012981415,
      "learning_rate": 8.12e-06,
      "loss": 0.003,
      "step": 31410
    },
    {
      "epoch": 1.6757333333333333,
      "grad_norm": 0.3313595950603485,
      "learning_rate": 8.106666666666666e-06,
      "loss": 0.0029,
      "step": 31420
    },
    {
      "epoch": 1.6762666666666668,
      "grad_norm": 0.1127409040927887,
      "learning_rate": 8.093333333333333e-06,
      "loss": 0.002,
      "step": 31430
    },
    {
      "epoch": 1.6768,
      "grad_norm": 0.38493406772613525,
      "learning_rate": 8.08e-06,
      "loss": 0.0023,
      "step": 31440
    },
    {
      "epoch": 1.6773333333333333,
      "grad_norm": 0.20373503863811493,
      "learning_rate": 8.066666666666667e-06,
      "loss": 0.0023,
      "step": 31450
    },
    {
      "epoch": 1.6778666666666666,
      "grad_norm": 0.2318904995918274,
      "learning_rate": 8.053333333333333e-06,
      "loss": 0.0029,
      "step": 31460
    },
    {
      "epoch": 1.6784,
      "grad_norm": 0.24100665748119354,
      "learning_rate": 8.040000000000001e-06,
      "loss": 0.0024,
      "step": 31470
    },
    {
      "epoch": 1.6789333333333334,
      "grad_norm": 0.3249559998512268,
      "learning_rate": 8.026666666666667e-06,
      "loss": 0.0021,
      "step": 31480
    },
    {
      "epoch": 1.6794666666666667,
      "grad_norm": 0.4765344262123108,
      "learning_rate": 8.013333333333333e-06,
      "loss": 0.002,
      "step": 31490
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.27421286702156067,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.003,
      "step": 31500
    },
    {
      "epoch": 1.6805333333333334,
      "grad_norm": 0.4211598038673401,
      "learning_rate": 7.986666666666668e-06,
      "loss": 0.002,
      "step": 31510
    },
    {
      "epoch": 1.6810666666666667,
      "grad_norm": 0.10095690190792084,
      "learning_rate": 7.973333333333334e-06,
      "loss": 0.0021,
      "step": 31520
    },
    {
      "epoch": 1.6816,
      "grad_norm": 0.0934724509716034,
      "learning_rate": 7.96e-06,
      "loss": 0.002,
      "step": 31530
    },
    {
      "epoch": 1.6821333333333333,
      "grad_norm": 0.06892579048871994,
      "learning_rate": 7.946666666666668e-06,
      "loss": 0.0018,
      "step": 31540
    },
    {
      "epoch": 1.6826666666666665,
      "grad_norm": 0.254281222820282,
      "learning_rate": 7.933333333333334e-06,
      "loss": 0.0027,
      "step": 31550
    },
    {
      "epoch": 1.6832,
      "grad_norm": 0.5825007557868958,
      "learning_rate": 7.92e-06,
      "loss": 0.0021,
      "step": 31560
    },
    {
      "epoch": 1.6837333333333333,
      "grad_norm": 0.5132165551185608,
      "learning_rate": 7.906666666666667e-06,
      "loss": 0.0021,
      "step": 31570
    },
    {
      "epoch": 1.6842666666666668,
      "grad_norm": 0.08119688928127289,
      "learning_rate": 7.893333333333333e-06,
      "loss": 0.0023,
      "step": 31580
    },
    {
      "epoch": 1.6848,
      "grad_norm": 0.3128632605075836,
      "learning_rate": 7.879999999999999e-06,
      "loss": 0.0019,
      "step": 31590
    },
    {
      "epoch": 1.6853333333333333,
      "grad_norm": 0.5477121472358704,
      "learning_rate": 7.866666666666667e-06,
      "loss": 0.0033,
      "step": 31600
    },
    {
      "epoch": 1.6858666666666666,
      "grad_norm": 0.2024187296628952,
      "learning_rate": 7.853333333333333e-06,
      "loss": 0.002,
      "step": 31610
    },
    {
      "epoch": 1.6864,
      "grad_norm": 0.10812947899103165,
      "learning_rate": 7.84e-06,
      "loss": 0.0019,
      "step": 31620
    },
    {
      "epoch": 1.6869333333333332,
      "grad_norm": 0.3809536099433899,
      "learning_rate": 7.826666666666667e-06,
      "loss": 0.0018,
      "step": 31630
    },
    {
      "epoch": 1.6874666666666667,
      "grad_norm": 0.06950755417346954,
      "learning_rate": 7.813333333333334e-06,
      "loss": 0.002,
      "step": 31640
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.13231685757637024,
      "learning_rate": 7.8e-06,
      "loss": 0.0027,
      "step": 31650
    },
    {
      "epoch": 1.6885333333333334,
      "grad_norm": 0.36138051748275757,
      "learning_rate": 7.786666666666668e-06,
      "loss": 0.0021,
      "step": 31660
    },
    {
      "epoch": 1.6890666666666667,
      "grad_norm": 0.14042755961418152,
      "learning_rate": 7.773333333333334e-06,
      "loss": 0.0029,
      "step": 31670
    },
    {
      "epoch": 1.6896,
      "grad_norm": 0.5497264862060547,
      "learning_rate": 7.76e-06,
      "loss": 0.0024,
      "step": 31680
    },
    {
      "epoch": 1.6901333333333333,
      "grad_norm": 0.11514659225940704,
      "learning_rate": 7.746666666666668e-06,
      "loss": 0.0027,
      "step": 31690
    },
    {
      "epoch": 1.6906666666666665,
      "grad_norm": 0.13647355139255524,
      "learning_rate": 7.733333333333334e-06,
      "loss": 0.0019,
      "step": 31700
    },
    {
      "epoch": 1.6912,
      "grad_norm": 0.23454873263835907,
      "learning_rate": 7.72e-06,
      "loss": 0.0017,
      "step": 31710
    },
    {
      "epoch": 1.6917333333333333,
      "grad_norm": 0.18189385533332825,
      "learning_rate": 7.706666666666667e-06,
      "loss": 0.0027,
      "step": 31720
    },
    {
      "epoch": 1.6922666666666668,
      "grad_norm": 0.35007113218307495,
      "learning_rate": 7.693333333333335e-06,
      "loss": 0.0028,
      "step": 31730
    },
    {
      "epoch": 1.6928,
      "grad_norm": 0.3732842206954956,
      "learning_rate": 7.68e-06,
      "loss": 0.0023,
      "step": 31740
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.38516566157341003,
      "learning_rate": 7.666666666666667e-06,
      "loss": 0.002,
      "step": 31750
    },
    {
      "epoch": 1.6938666666666666,
      "grad_norm": 0.24611195921897888,
      "learning_rate": 7.653333333333333e-06,
      "loss": 0.0018,
      "step": 31760
    },
    {
      "epoch": 1.6944,
      "grad_norm": 0.34480464458465576,
      "learning_rate": 7.64e-06,
      "loss": 0.0028,
      "step": 31770
    },
    {
      "epoch": 1.6949333333333332,
      "grad_norm": 0.2770383358001709,
      "learning_rate": 7.626666666666667e-06,
      "loss": 0.0018,
      "step": 31780
    },
    {
      "epoch": 1.6954666666666667,
      "grad_norm": 0.4494335949420929,
      "learning_rate": 7.613333333333334e-06,
      "loss": 0.0025,
      "step": 31790
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.214285209774971,
      "learning_rate": 7.6e-06,
      "loss": 0.002,
      "step": 31800
    },
    {
      "epoch": 1.6965333333333334,
      "grad_norm": 0.2987641990184784,
      "learning_rate": 7.586666666666667e-06,
      "loss": 0.0016,
      "step": 31810
    },
    {
      "epoch": 1.6970666666666667,
      "grad_norm": 0.6171387434005737,
      "learning_rate": 7.573333333333333e-06,
      "loss": 0.0017,
      "step": 31820
    },
    {
      "epoch": 1.6976,
      "grad_norm": 0.21826790273189545,
      "learning_rate": 7.5600000000000005e-06,
      "loss": 0.0023,
      "step": 31830
    },
    {
      "epoch": 1.6981333333333333,
      "grad_norm": 0.2792567014694214,
      "learning_rate": 7.5466666666666675e-06,
      "loss": 0.0018,
      "step": 31840
    },
    {
      "epoch": 1.6986666666666665,
      "grad_norm": 0.15279696881771088,
      "learning_rate": 7.533333333333334e-06,
      "loss": 0.0026,
      "step": 31850
    },
    {
      "epoch": 1.6992,
      "grad_norm": 0.14751064777374268,
      "learning_rate": 7.520000000000001e-06,
      "loss": 0.0038,
      "step": 31860
    },
    {
      "epoch": 1.6997333333333333,
      "grad_norm": 0.35180357098579407,
      "learning_rate": 7.506666666666667e-06,
      "loss": 0.0034,
      "step": 31870
    },
    {
      "epoch": 1.7002666666666668,
      "grad_norm": 0.14242896437644958,
      "learning_rate": 7.493333333333334e-06,
      "loss": 0.0032,
      "step": 31880
    },
    {
      "epoch": 1.7008,
      "grad_norm": 0.10543468594551086,
      "learning_rate": 7.480000000000001e-06,
      "loss": 0.0025,
      "step": 31890
    },
    {
      "epoch": 1.7013333333333334,
      "grad_norm": 0.3242426812648773,
      "learning_rate": 7.4666666666666675e-06,
      "loss": 0.0042,
      "step": 31900
    },
    {
      "epoch": 1.7018666666666666,
      "grad_norm": 0.23389916121959686,
      "learning_rate": 7.453333333333333e-06,
      "loss": 0.0024,
      "step": 31910
    },
    {
      "epoch": 1.7024,
      "grad_norm": 0.122066430747509,
      "learning_rate": 7.44e-06,
      "loss": 0.002,
      "step": 31920
    },
    {
      "epoch": 1.7029333333333332,
      "grad_norm": 0.4137032628059387,
      "learning_rate": 7.426666666666666e-06,
      "loss": 0.0017,
      "step": 31930
    },
    {
      "epoch": 1.7034666666666667,
      "grad_norm": 0.23857130110263824,
      "learning_rate": 7.413333333333333e-06,
      "loss": 0.0016,
      "step": 31940
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.25411850214004517,
      "learning_rate": 7.4e-06,
      "loss": 0.003,
      "step": 31950
    },
    {
      "epoch": 1.7045333333333335,
      "grad_norm": 0.12491119652986526,
      "learning_rate": 7.3866666666666665e-06,
      "loss": 0.0019,
      "step": 31960
    },
    {
      "epoch": 1.7050666666666667,
      "grad_norm": 0.5013899207115173,
      "learning_rate": 7.373333333333334e-06,
      "loss": 0.0021,
      "step": 31970
    },
    {
      "epoch": 1.7056,
      "grad_norm": 0.09288372844457626,
      "learning_rate": 7.36e-06,
      "loss": 0.0036,
      "step": 31980
    },
    {
      "epoch": 1.7061333333333333,
      "grad_norm": 0.04835312440991402,
      "learning_rate": 7.346666666666667e-06,
      "loss": 0.0029,
      "step": 31990
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.10164929926395416,
      "learning_rate": 7.333333333333334e-06,
      "loss": 0.0017,
      "step": 32000
    },
    {
      "epoch": 1.7072,
      "grad_norm": 0.5585207343101501,
      "learning_rate": 7.32e-06,
      "loss": 0.0036,
      "step": 32010
    },
    {
      "epoch": 1.7077333333333333,
      "grad_norm": 0.06266520917415619,
      "learning_rate": 7.306666666666667e-06,
      "loss": 0.0022,
      "step": 32020
    },
    {
      "epoch": 1.7082666666666668,
      "grad_norm": 0.09967627376317978,
      "learning_rate": 7.293333333333334e-06,
      "loss": 0.0041,
      "step": 32030
    },
    {
      "epoch": 1.7088,
      "grad_norm": 0.42291566729545593,
      "learning_rate": 7.280000000000001e-06,
      "loss": 0.0025,
      "step": 32040
    },
    {
      "epoch": 1.7093333333333334,
      "grad_norm": 0.4672313630580902,
      "learning_rate": 7.266666666666668e-06,
      "loss": 0.0027,
      "step": 32050
    },
    {
      "epoch": 1.7098666666666666,
      "grad_norm": 0.24384842813014984,
      "learning_rate": 7.253333333333334e-06,
      "loss": 0.0022,
      "step": 32060
    },
    {
      "epoch": 1.7104,
      "grad_norm": 0.15977352857589722,
      "learning_rate": 7.240000000000001e-06,
      "loss": 0.002,
      "step": 32070
    },
    {
      "epoch": 1.7109333333333332,
      "grad_norm": 0.21666181087493896,
      "learning_rate": 7.226666666666668e-06,
      "loss": 0.0036,
      "step": 32080
    },
    {
      "epoch": 1.7114666666666667,
      "grad_norm": 0.17959511280059814,
      "learning_rate": 7.2133333333333334e-06,
      "loss": 0.0025,
      "step": 32090
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.25547653436660767,
      "learning_rate": 7.2e-06,
      "loss": 0.0025,
      "step": 32100
    },
    {
      "epoch": 1.7125333333333335,
      "grad_norm": 0.36975356936454773,
      "learning_rate": 7.186666666666667e-06,
      "loss": 0.0022,
      "step": 32110
    },
    {
      "epoch": 1.7130666666666667,
      "grad_norm": 0.2162797749042511,
      "learning_rate": 7.173333333333333e-06,
      "loss": 0.0024,
      "step": 32120
    },
    {
      "epoch": 1.7136,
      "grad_norm": 0.1369309276342392,
      "learning_rate": 7.16e-06,
      "loss": 0.0018,
      "step": 32130
    },
    {
      "epoch": 1.7141333333333333,
      "grad_norm": 0.23857535421848297,
      "learning_rate": 7.146666666666667e-06,
      "loss": 0.0018,
      "step": 32140
    },
    {
      "epoch": 1.7146666666666666,
      "grad_norm": 0.344954252243042,
      "learning_rate": 7.133333333333333e-06,
      "loss": 0.0045,
      "step": 32150
    },
    {
      "epoch": 1.7151999999999998,
      "grad_norm": 0.33405739068984985,
      "learning_rate": 7.1200000000000004e-06,
      "loss": 0.0018,
      "step": 32160
    },
    {
      "epoch": 1.7157333333333333,
      "grad_norm": 0.24705670773983002,
      "learning_rate": 7.106666666666667e-06,
      "loss": 0.0021,
      "step": 32170
    },
    {
      "epoch": 1.7162666666666668,
      "grad_norm": 0.3597656786441803,
      "learning_rate": 7.093333333333334e-06,
      "loss": 0.0018,
      "step": 32180
    },
    {
      "epoch": 1.7168,
      "grad_norm": 0.5225053429603577,
      "learning_rate": 7.080000000000001e-06,
      "loss": 0.0019,
      "step": 32190
    },
    {
      "epoch": 1.7173333333333334,
      "grad_norm": 0.6857772469520569,
      "learning_rate": 7.066666666666667e-06,
      "loss": 0.002,
      "step": 32200
    },
    {
      "epoch": 1.7178666666666667,
      "grad_norm": 0.0589052177965641,
      "learning_rate": 7.053333333333334e-06,
      "loss": 0.0018,
      "step": 32210
    },
    {
      "epoch": 1.7184,
      "grad_norm": 0.38015711307525635,
      "learning_rate": 7.04e-06,
      "loss": 0.0018,
      "step": 32220
    },
    {
      "epoch": 1.7189333333333332,
      "grad_norm": 0.16585254669189453,
      "learning_rate": 7.0266666666666674e-06,
      "loss": 0.0029,
      "step": 32230
    },
    {
      "epoch": 1.7194666666666667,
      "grad_norm": 0.36602768301963806,
      "learning_rate": 7.0133333333333345e-06,
      "loss": 0.0018,
      "step": 32240
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.11229036748409271,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.0025,
      "step": 32250
    },
    {
      "epoch": 1.7205333333333335,
      "grad_norm": 0.2731313407421112,
      "learning_rate": 6.986666666666666e-06,
      "loss": 0.0018,
      "step": 32260
    },
    {
      "epoch": 1.7210666666666667,
      "grad_norm": 0.30516746640205383,
      "learning_rate": 6.973333333333333e-06,
      "loss": 0.0026,
      "step": 32270
    },
    {
      "epoch": 1.7216,
      "grad_norm": 0.2834475040435791,
      "learning_rate": 6.9599999999999994e-06,
      "loss": 0.0017,
      "step": 32280
    },
    {
      "epoch": 1.7221333333333333,
      "grad_norm": 0.14753909409046173,
      "learning_rate": 6.9466666666666665e-06,
      "loss": 0.0023,
      "step": 32290
    },
    {
      "epoch": 1.7226666666666666,
      "grad_norm": 0.32460030913352966,
      "learning_rate": 6.933333333333334e-06,
      "loss": 0.0034,
      "step": 32300
    },
    {
      "epoch": 1.7231999999999998,
      "grad_norm": 0.16586050391197205,
      "learning_rate": 6.92e-06,
      "loss": 0.0017,
      "step": 32310
    },
    {
      "epoch": 1.7237333333333333,
      "grad_norm": 0.6983728408813477,
      "learning_rate": 6.906666666666667e-06,
      "loss": 0.0018,
      "step": 32320
    },
    {
      "epoch": 1.7242666666666666,
      "grad_norm": 0.4261271059513092,
      "learning_rate": 6.893333333333334e-06,
      "loss": 0.0023,
      "step": 32330
    },
    {
      "epoch": 1.7248,
      "grad_norm": 0.27040985226631165,
      "learning_rate": 6.88e-06,
      "loss": 0.0027,
      "step": 32340
    },
    {
      "epoch": 1.7253333333333334,
      "grad_norm": 0.11821959167718887,
      "learning_rate": 6.866666666666667e-06,
      "loss": 0.0018,
      "step": 32350
    },
    {
      "epoch": 1.7258666666666667,
      "grad_norm": 0.15766435861587524,
      "learning_rate": 6.8533333333333335e-06,
      "loss": 0.0029,
      "step": 32360
    },
    {
      "epoch": 1.7264,
      "grad_norm": 0.15666469931602478,
      "learning_rate": 6.840000000000001e-06,
      "loss": 0.0027,
      "step": 32370
    },
    {
      "epoch": 1.7269333333333332,
      "grad_norm": 0.19599896669387817,
      "learning_rate": 6.826666666666668e-06,
      "loss": 0.0017,
      "step": 32380
    },
    {
      "epoch": 1.7274666666666667,
      "grad_norm": 0.5250972509384155,
      "learning_rate": 6.813333333333334e-06,
      "loss": 0.0026,
      "step": 32390
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.15950696170330048,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.0019,
      "step": 32400
    },
    {
      "epoch": 1.7285333333333335,
      "grad_norm": 0.1315373182296753,
      "learning_rate": 6.786666666666667e-06,
      "loss": 0.0021,
      "step": 32410
    },
    {
      "epoch": 1.7290666666666668,
      "grad_norm": 0.2830823063850403,
      "learning_rate": 6.773333333333334e-06,
      "loss": 0.0021,
      "step": 32420
    },
    {
      "epoch": 1.7296,
      "grad_norm": 0.09750116616487503,
      "learning_rate": 6.76e-06,
      "loss": 0.004,
      "step": 32430
    },
    {
      "epoch": 1.7301333333333333,
      "grad_norm": 0.13659264147281647,
      "learning_rate": 6.746666666666667e-06,
      "loss": 0.002,
      "step": 32440
    },
    {
      "epoch": 1.7306666666666666,
      "grad_norm": 0.365866482257843,
      "learning_rate": 6.733333333333333e-06,
      "loss": 0.0018,
      "step": 32450
    },
    {
      "epoch": 1.7311999999999999,
      "grad_norm": 0.44478532671928406,
      "learning_rate": 6.72e-06,
      "loss": 0.0036,
      "step": 32460
    },
    {
      "epoch": 1.7317333333333333,
      "grad_norm": 0.11305029690265656,
      "learning_rate": 6.706666666666666e-06,
      "loss": 0.0021,
      "step": 32470
    },
    {
      "epoch": 1.7322666666666666,
      "grad_norm": 0.3771563470363617,
      "learning_rate": 6.693333333333333e-06,
      "loss": 0.0019,
      "step": 32480
    },
    {
      "epoch": 1.7328000000000001,
      "grad_norm": 0.16916923224925995,
      "learning_rate": 6.68e-06,
      "loss": 0.003,
      "step": 32490
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.2976509630680084,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.0036,
      "step": 32500
    },
    {
      "epoch": 1.7338666666666667,
      "grad_norm": 0.19321362674236298,
      "learning_rate": 6.653333333333334e-06,
      "loss": 0.0019,
      "step": 32510
    },
    {
      "epoch": 1.7344,
      "grad_norm": 0.19179590046405792,
      "learning_rate": 6.640000000000001e-06,
      "loss": 0.0025,
      "step": 32520
    },
    {
      "epoch": 1.7349333333333332,
      "grad_norm": 0.21795159578323364,
      "learning_rate": 6.626666666666667e-06,
      "loss": 0.0021,
      "step": 32530
    },
    {
      "epoch": 1.7354666666666667,
      "grad_norm": 0.13690470159053802,
      "learning_rate": 6.613333333333334e-06,
      "loss": 0.0018,
      "step": 32540
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.33172303438186646,
      "learning_rate": 6.6e-06,
      "loss": 0.0019,
      "step": 32550
    },
    {
      "epoch": 1.7365333333333335,
      "grad_norm": 0.3257593512535095,
      "learning_rate": 6.586666666666667e-06,
      "loss": 0.0017,
      "step": 32560
    },
    {
      "epoch": 1.7370666666666668,
      "grad_norm": 0.23713676631450653,
      "learning_rate": 6.5733333333333345e-06,
      "loss": 0.0021,
      "step": 32570
    },
    {
      "epoch": 1.7376,
      "grad_norm": 0.07739085704088211,
      "learning_rate": 6.560000000000001e-06,
      "loss": 0.0024,
      "step": 32580
    },
    {
      "epoch": 1.7381333333333333,
      "grad_norm": 0.07533160597085953,
      "learning_rate": 6.546666666666668e-06,
      "loss": 0.0025,
      "step": 32590
    },
    {
      "epoch": 1.7386666666666666,
      "grad_norm": 0.3301830589771271,
      "learning_rate": 6.533333333333333e-06,
      "loss": 0.0026,
      "step": 32600
    },
    {
      "epoch": 1.7391999999999999,
      "grad_norm": 0.23358891904354095,
      "learning_rate": 6.519999999999999e-06,
      "loss": 0.0033,
      "step": 32610
    },
    {
      "epoch": 1.7397333333333334,
      "grad_norm": 0.07618577033281326,
      "learning_rate": 6.5066666666666665e-06,
      "loss": 0.0017,
      "step": 32620
    },
    {
      "epoch": 1.7402666666666666,
      "grad_norm": 0.12530826032161713,
      "learning_rate": 6.4933333333333336e-06,
      "loss": 0.0021,
      "step": 32630
    },
    {
      "epoch": 1.7408000000000001,
      "grad_norm": 0.2799918055534363,
      "learning_rate": 6.48e-06,
      "loss": 0.0022,
      "step": 32640
    },
    {
      "epoch": 1.7413333333333334,
      "grad_norm": 0.06587624549865723,
      "learning_rate": 6.466666666666667e-06,
      "loss": 0.0037,
      "step": 32650
    },
    {
      "epoch": 1.7418666666666667,
      "grad_norm": 0.11389825493097305,
      "learning_rate": 6.453333333333333e-06,
      "loss": 0.0019,
      "step": 32660
    },
    {
      "epoch": 1.7424,
      "grad_norm": 0.40442129969596863,
      "learning_rate": 6.44e-06,
      "loss": 0.0017,
      "step": 32670
    },
    {
      "epoch": 1.7429333333333332,
      "grad_norm": 0.33340317010879517,
      "learning_rate": 6.426666666666667e-06,
      "loss": 0.0025,
      "step": 32680
    },
    {
      "epoch": 1.7434666666666667,
      "grad_norm": 0.3118811845779419,
      "learning_rate": 6.4133333333333335e-06,
      "loss": 0.0025,
      "step": 32690
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.09047771245241165,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.0022,
      "step": 32700
    },
    {
      "epoch": 1.7445333333333335,
      "grad_norm": 0.5172625184059143,
      "learning_rate": 6.386666666666667e-06,
      "loss": 0.0021,
      "step": 32710
    },
    {
      "epoch": 1.7450666666666668,
      "grad_norm": 0.20407724380493164,
      "learning_rate": 6.373333333333334e-06,
      "loss": 0.0021,
      "step": 32720
    },
    {
      "epoch": 1.7456,
      "grad_norm": 0.31151098012924194,
      "learning_rate": 6.360000000000001e-06,
      "loss": 0.0024,
      "step": 32730
    },
    {
      "epoch": 1.7461333333333333,
      "grad_norm": 0.12287209182977676,
      "learning_rate": 6.346666666666667e-06,
      "loss": 0.0018,
      "step": 32740
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.13586458563804626,
      "learning_rate": 6.333333333333334e-06,
      "loss": 0.0018,
      "step": 32750
    },
    {
      "epoch": 1.7471999999999999,
      "grad_norm": 0.3955945074558258,
      "learning_rate": 6.320000000000001e-06,
      "loss": 0.0019,
      "step": 32760
    },
    {
      "epoch": 1.7477333333333334,
      "grad_norm": 0.1622094362974167,
      "learning_rate": 6.306666666666666e-06,
      "loss": 0.0034,
      "step": 32770
    },
    {
      "epoch": 1.7482666666666666,
      "grad_norm": 0.1513490378856659,
      "learning_rate": 6.293333333333333e-06,
      "loss": 0.003,
      "step": 32780
    },
    {
      "epoch": 1.7488000000000001,
      "grad_norm": 0.475436806678772,
      "learning_rate": 6.28e-06,
      "loss": 0.0028,
      "step": 32790
    },
    {
      "epoch": 1.7493333333333334,
      "grad_norm": 0.14229744672775269,
      "learning_rate": 6.266666666666666e-06,
      "loss": 0.003,
      "step": 32800
    },
    {
      "epoch": 1.7498666666666667,
      "grad_norm": 0.4084285795688629,
      "learning_rate": 6.253333333333333e-06,
      "loss": 0.0026,
      "step": 32810
    },
    {
      "epoch": 1.7504,
      "grad_norm": 0.12267694622278214,
      "learning_rate": 6.24e-06,
      "loss": 0.0032,
      "step": 32820
    },
    {
      "epoch": 1.7509333333333332,
      "grad_norm": 0.48695576190948486,
      "learning_rate": 6.226666666666667e-06,
      "loss": 0.002,
      "step": 32830
    },
    {
      "epoch": 1.7514666666666665,
      "grad_norm": 0.4597373902797699,
      "learning_rate": 6.213333333333334e-06,
      "loss": 0.002,
      "step": 32840
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.4925740659236908,
      "learning_rate": 6.2e-06,
      "loss": 0.0026,
      "step": 32850
    },
    {
      "epoch": 1.7525333333333335,
      "grad_norm": 0.1815003752708435,
      "learning_rate": 6.186666666666667e-06,
      "loss": 0.0044,
      "step": 32860
    },
    {
      "epoch": 1.7530666666666668,
      "grad_norm": 0.17292533814907074,
      "learning_rate": 6.173333333333334e-06,
      "loss": 0.0025,
      "step": 32870
    },
    {
      "epoch": 1.7536,
      "grad_norm": 0.13443012535572052,
      "learning_rate": 6.16e-06,
      "loss": 0.0019,
      "step": 32880
    },
    {
      "epoch": 1.7541333333333333,
      "grad_norm": 0.0938873440027237,
      "learning_rate": 6.146666666666667e-06,
      "loss": 0.0021,
      "step": 32890
    },
    {
      "epoch": 1.7546666666666666,
      "grad_norm": 0.29527902603149414,
      "learning_rate": 6.133333333333334e-06,
      "loss": 0.0025,
      "step": 32900
    },
    {
      "epoch": 1.7551999999999999,
      "grad_norm": 0.3755671977996826,
      "learning_rate": 6.12e-06,
      "loss": 0.0023,
      "step": 32910
    },
    {
      "epoch": 1.7557333333333334,
      "grad_norm": 0.525242030620575,
      "learning_rate": 6.106666666666667e-06,
      "loss": 0.0021,
      "step": 32920
    },
    {
      "epoch": 1.7562666666666666,
      "grad_norm": 0.2638094127178192,
      "learning_rate": 6.093333333333333e-06,
      "loss": 0.0017,
      "step": 32930
    },
    {
      "epoch": 1.7568000000000001,
      "grad_norm": 0.08053046464920044,
      "learning_rate": 6.08e-06,
      "loss": 0.002,
      "step": 32940
    },
    {
      "epoch": 1.7573333333333334,
      "grad_norm": 0.29988566040992737,
      "learning_rate": 6.066666666666667e-06,
      "loss": 0.002,
      "step": 32950
    },
    {
      "epoch": 1.7578666666666667,
      "grad_norm": 0.26881012320518494,
      "learning_rate": 6.0533333333333335e-06,
      "loss": 0.0017,
      "step": 32960
    },
    {
      "epoch": 1.7584,
      "grad_norm": 0.1488390862941742,
      "learning_rate": 6.040000000000001e-06,
      "loss": 0.0016,
      "step": 32970
    },
    {
      "epoch": 1.7589333333333332,
      "grad_norm": 0.15420480072498322,
      "learning_rate": 6.026666666666667e-06,
      "loss": 0.0018,
      "step": 32980
    },
    {
      "epoch": 1.7594666666666665,
      "grad_norm": 0.11769337207078934,
      "learning_rate": 6.013333333333333e-06,
      "loss": 0.0019,
      "step": 32990
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.3078152537345886,
      "learning_rate": 6e-06,
      "loss": 0.0019,
      "step": 33000
    },
    {
      "epoch": 1.7605333333333333,
      "grad_norm": 0.2727518677711487,
      "learning_rate": 5.986666666666667e-06,
      "loss": 0.0028,
      "step": 33010
    },
    {
      "epoch": 1.7610666666666668,
      "grad_norm": 0.3287503719329834,
      "learning_rate": 5.9733333333333335e-06,
      "loss": 0.002,
      "step": 33020
    },
    {
      "epoch": 1.7616,
      "grad_norm": 0.3973841071128845,
      "learning_rate": 5.9600000000000005e-06,
      "loss": 0.0028,
      "step": 33030
    },
    {
      "epoch": 1.7621333333333333,
      "grad_norm": 0.1283925473690033,
      "learning_rate": 5.946666666666667e-06,
      "loss": 0.0019,
      "step": 33040
    },
    {
      "epoch": 1.7626666666666666,
      "grad_norm": 0.16439464688301086,
      "learning_rate": 5.933333333333334e-06,
      "loss": 0.0019,
      "step": 33050
    },
    {
      "epoch": 1.7631999999999999,
      "grad_norm": 0.4773932695388794,
      "learning_rate": 5.920000000000001e-06,
      "loss": 0.0022,
      "step": 33060
    },
    {
      "epoch": 1.7637333333333334,
      "grad_norm": 0.08581561595201492,
      "learning_rate": 5.906666666666667e-06,
      "loss": 0.0033,
      "step": 33070
    },
    {
      "epoch": 1.7642666666666666,
      "grad_norm": 0.1347920447587967,
      "learning_rate": 5.893333333333333e-06,
      "loss": 0.002,
      "step": 33080
    },
    {
      "epoch": 1.7648000000000001,
      "grad_norm": 0.21429672837257385,
      "learning_rate": 5.8800000000000005e-06,
      "loss": 0.0025,
      "step": 33090
    },
    {
      "epoch": 1.7653333333333334,
      "grad_norm": 0.3450777232646942,
      "learning_rate": 5.866666666666667e-06,
      "loss": 0.0021,
      "step": 33100
    },
    {
      "epoch": 1.7658666666666667,
      "grad_norm": 0.12037079781293869,
      "learning_rate": 5.853333333333334e-06,
      "loss": 0.0031,
      "step": 33110
    },
    {
      "epoch": 1.7664,
      "grad_norm": 0.3565661907196045,
      "learning_rate": 5.84e-06,
      "loss": 0.0031,
      "step": 33120
    },
    {
      "epoch": 1.7669333333333332,
      "grad_norm": 0.3269595503807068,
      "learning_rate": 5.826666666666667e-06,
      "loss": 0.0018,
      "step": 33130
    },
    {
      "epoch": 1.7674666666666665,
      "grad_norm": 0.2537887990474701,
      "learning_rate": 5.813333333333334e-06,
      "loss": 0.0021,
      "step": 33140
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.3776901662349701,
      "learning_rate": 5.8e-06,
      "loss": 0.0025,
      "step": 33150
    },
    {
      "epoch": 1.7685333333333333,
      "grad_norm": 0.5533787608146667,
      "learning_rate": 5.786666666666667e-06,
      "loss": 0.0022,
      "step": 33160
    },
    {
      "epoch": 1.7690666666666668,
      "grad_norm": 0.19200769066810608,
      "learning_rate": 5.773333333333334e-06,
      "loss": 0.0033,
      "step": 33170
    },
    {
      "epoch": 1.7696,
      "grad_norm": 0.11301040649414062,
      "learning_rate": 5.76e-06,
      "loss": 0.0019,
      "step": 33180
    },
    {
      "epoch": 1.7701333333333333,
      "grad_norm": 0.2060719132423401,
      "learning_rate": 5.746666666666667e-06,
      "loss": 0.002,
      "step": 33190
    },
    {
      "epoch": 1.7706666666666666,
      "grad_norm": 0.17125627398490906,
      "learning_rate": 5.733333333333333e-06,
      "loss": 0.0019,
      "step": 33200
    },
    {
      "epoch": 1.7711999999999999,
      "grad_norm": 0.24849382042884827,
      "learning_rate": 5.72e-06,
      "loss": 0.0017,
      "step": 33210
    },
    {
      "epoch": 1.7717333333333334,
      "grad_norm": 0.06959948688745499,
      "learning_rate": 5.706666666666667e-06,
      "loss": 0.0025,
      "step": 33220
    },
    {
      "epoch": 1.7722666666666667,
      "grad_norm": 0.12021493911743164,
      "learning_rate": 5.693333333333334e-06,
      "loss": 0.0016,
      "step": 33230
    },
    {
      "epoch": 1.7728000000000002,
      "grad_norm": 0.6020666360855103,
      "learning_rate": 5.680000000000001e-06,
      "loss": 0.0021,
      "step": 33240
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.2106124311685562,
      "learning_rate": 5.666666666666667e-06,
      "loss": 0.0038,
      "step": 33250
    },
    {
      "epoch": 1.7738666666666667,
      "grad_norm": 0.13697931170463562,
      "learning_rate": 5.653333333333333e-06,
      "loss": 0.0025,
      "step": 33260
    },
    {
      "epoch": 1.7744,
      "grad_norm": 0.2144826054573059,
      "learning_rate": 5.64e-06,
      "loss": 0.002,
      "step": 33270
    },
    {
      "epoch": 1.7749333333333333,
      "grad_norm": 0.040371041744947433,
      "learning_rate": 5.626666666666667e-06,
      "loss": 0.0023,
      "step": 33280
    },
    {
      "epoch": 1.7754666666666665,
      "grad_norm": 0.17036864161491394,
      "learning_rate": 5.6133333333333335e-06,
      "loss": 0.0029,
      "step": 33290
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.16338391602039337,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.0019,
      "step": 33300
    },
    {
      "epoch": 1.7765333333333333,
      "grad_norm": 0.10284983366727829,
      "learning_rate": 5.586666666666667e-06,
      "loss": 0.0025,
      "step": 33310
    },
    {
      "epoch": 1.7770666666666668,
      "grad_norm": 0.134890615940094,
      "learning_rate": 5.573333333333334e-06,
      "loss": 0.0031,
      "step": 33320
    },
    {
      "epoch": 1.7776,
      "grad_norm": 0.3719940483570099,
      "learning_rate": 5.56e-06,
      "loss": 0.0018,
      "step": 33330
    },
    {
      "epoch": 1.7781333333333333,
      "grad_norm": 0.2699050307273865,
      "learning_rate": 5.546666666666666e-06,
      "loss": 0.0021,
      "step": 33340
    },
    {
      "epoch": 1.7786666666666666,
      "grad_norm": 0.10188528895378113,
      "learning_rate": 5.5333333333333334e-06,
      "loss": 0.0019,
      "step": 33350
    },
    {
      "epoch": 1.7792,
      "grad_norm": 0.14908427000045776,
      "learning_rate": 5.5200000000000005e-06,
      "loss": 0.0019,
      "step": 33360
    },
    {
      "epoch": 1.7797333333333332,
      "grad_norm": 0.2986244857311249,
      "learning_rate": 5.506666666666667e-06,
      "loss": 0.0019,
      "step": 33370
    },
    {
      "epoch": 1.7802666666666667,
      "grad_norm": 0.39754346013069153,
      "learning_rate": 5.493333333333334e-06,
      "loss": 0.0022,
      "step": 33380
    },
    {
      "epoch": 1.7808000000000002,
      "grad_norm": 0.13636690378189087,
      "learning_rate": 5.48e-06,
      "loss": 0.0025,
      "step": 33390
    },
    {
      "epoch": 1.7813333333333334,
      "grad_norm": 0.23306821286678314,
      "learning_rate": 5.466666666666667e-06,
      "loss": 0.0019,
      "step": 33400
    },
    {
      "epoch": 1.7818666666666667,
      "grad_norm": 0.15262730419635773,
      "learning_rate": 5.453333333333334e-06,
      "loss": 0.002,
      "step": 33410
    },
    {
      "epoch": 1.7824,
      "grad_norm": 0.42236897349357605,
      "learning_rate": 5.44e-06,
      "loss": 0.002,
      "step": 33420
    },
    {
      "epoch": 1.7829333333333333,
      "grad_norm": 0.6398159265518188,
      "learning_rate": 5.426666666666667e-06,
      "loss": 0.0028,
      "step": 33430
    },
    {
      "epoch": 1.7834666666666665,
      "grad_norm": 0.7308099865913391,
      "learning_rate": 5.413333333333334e-06,
      "loss": 0.0019,
      "step": 33440
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.2809763252735138,
      "learning_rate": 5.4e-06,
      "loss": 0.0039,
      "step": 33450
    },
    {
      "epoch": 1.7845333333333333,
      "grad_norm": 0.5035030245780945,
      "learning_rate": 5.386666666666667e-06,
      "loss": 0.0029,
      "step": 33460
    },
    {
      "epoch": 1.7850666666666668,
      "grad_norm": 0.2904186546802521,
      "learning_rate": 5.373333333333333e-06,
      "loss": 0.0018,
      "step": 33470
    },
    {
      "epoch": 1.7856,
      "grad_norm": 0.24982738494873047,
      "learning_rate": 5.36e-06,
      "loss": 0.0019,
      "step": 33480
    },
    {
      "epoch": 1.7861333333333334,
      "grad_norm": 0.08503133803606033,
      "learning_rate": 5.3466666666666674e-06,
      "loss": 0.0032,
      "step": 33490
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.07914498448371887,
      "learning_rate": 5.333333333333334e-06,
      "loss": 0.0018,
      "step": 33500
    },
    {
      "epoch": 1.7872,
      "grad_norm": 0.2932751178741455,
      "learning_rate": 5.32e-06,
      "loss": 0.0031,
      "step": 33510
    },
    {
      "epoch": 1.7877333333333332,
      "grad_norm": 0.09660466760396957,
      "learning_rate": 5.306666666666667e-06,
      "loss": 0.0019,
      "step": 33520
    },
    {
      "epoch": 1.7882666666666667,
      "grad_norm": 0.09151161462068558,
      "learning_rate": 5.293333333333333e-06,
      "loss": 0.0022,
      "step": 33530
    },
    {
      "epoch": 1.7888,
      "grad_norm": 0.09840310364961624,
      "learning_rate": 5.28e-06,
      "loss": 0.0018,
      "step": 33540
    },
    {
      "epoch": 1.7893333333333334,
      "grad_norm": 0.3156423270702362,
      "learning_rate": 5.266666666666667e-06,
      "loss": 0.0017,
      "step": 33550
    },
    {
      "epoch": 1.7898666666666667,
      "grad_norm": 0.27584463357925415,
      "learning_rate": 5.2533333333333336e-06,
      "loss": 0.0022,
      "step": 33560
    },
    {
      "epoch": 1.7904,
      "grad_norm": 0.1296885907649994,
      "learning_rate": 5.240000000000001e-06,
      "loss": 0.0018,
      "step": 33570
    },
    {
      "epoch": 1.7909333333333333,
      "grad_norm": 0.13698212802410126,
      "learning_rate": 5.226666666666667e-06,
      "loss": 0.0023,
      "step": 33580
    },
    {
      "epoch": 1.7914666666666665,
      "grad_norm": 0.2328767478466034,
      "learning_rate": 5.213333333333333e-06,
      "loss": 0.0025,
      "step": 33590
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.10676669329404831,
      "learning_rate": 5.2e-06,
      "loss": 0.0018,
      "step": 33600
    },
    {
      "epoch": 1.7925333333333333,
      "grad_norm": 0.19041885435581207,
      "learning_rate": 5.186666666666666e-06,
      "loss": 0.0017,
      "step": 33610
    },
    {
      "epoch": 1.7930666666666668,
      "grad_norm": 0.19749043881893158,
      "learning_rate": 5.1733333333333335e-06,
      "loss": 0.0018,
      "step": 33620
    },
    {
      "epoch": 1.7936,
      "grad_norm": 0.24089829623699188,
      "learning_rate": 5.1600000000000006e-06,
      "loss": 0.0023,
      "step": 33630
    },
    {
      "epoch": 1.7941333333333334,
      "grad_norm": 0.27407824993133545,
      "learning_rate": 5.146666666666667e-06,
      "loss": 0.0018,
      "step": 33640
    },
    {
      "epoch": 1.7946666666666666,
      "grad_norm": 0.6520236730575562,
      "learning_rate": 5.133333333333334e-06,
      "loss": 0.0021,
      "step": 33650
    },
    {
      "epoch": 1.7952,
      "grad_norm": 0.1668989211320877,
      "learning_rate": 5.12e-06,
      "loss": 0.002,
      "step": 33660
    },
    {
      "epoch": 1.7957333333333332,
      "grad_norm": 0.13049305975437164,
      "learning_rate": 5.106666666666667e-06,
      "loss": 0.002,
      "step": 33670
    },
    {
      "epoch": 1.7962666666666667,
      "grad_norm": 0.10770659148693085,
      "learning_rate": 5.093333333333333e-06,
      "loss": 0.0027,
      "step": 33680
    },
    {
      "epoch": 1.7968,
      "grad_norm": 0.2945597469806671,
      "learning_rate": 5.08e-06,
      "loss": 0.0019,
      "step": 33690
    },
    {
      "epoch": 1.7973333333333334,
      "grad_norm": 0.2881532907485962,
      "learning_rate": 5.066666666666667e-06,
      "loss": 0.0026,
      "step": 33700
    },
    {
      "epoch": 1.7978666666666667,
      "grad_norm": 0.0787293016910553,
      "learning_rate": 5.053333333333334e-06,
      "loss": 0.003,
      "step": 33710
    },
    {
      "epoch": 1.7984,
      "grad_norm": 0.679934024810791,
      "learning_rate": 5.04e-06,
      "loss": 0.0018,
      "step": 33720
    },
    {
      "epoch": 1.7989333333333333,
      "grad_norm": 0.3578941822052002,
      "learning_rate": 5.026666666666667e-06,
      "loss": 0.0038,
      "step": 33730
    },
    {
      "epoch": 1.7994666666666665,
      "grad_norm": 0.23344548046588898,
      "learning_rate": 5.013333333333334e-06,
      "loss": 0.0025,
      "step": 33740
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.23565784096717834,
      "learning_rate": 5e-06,
      "loss": 0.0018,
      "step": 33750
    },
    {
      "epoch": 1.8005333333333333,
      "grad_norm": 0.08689209818840027,
      "learning_rate": 4.986666666666667e-06,
      "loss": 0.0018,
      "step": 33760
    },
    {
      "epoch": 1.8010666666666668,
      "grad_norm": 0.22286972403526306,
      "learning_rate": 4.973333333333334e-06,
      "loss": 0.0026,
      "step": 33770
    },
    {
      "epoch": 1.8016,
      "grad_norm": 0.12525926530361176,
      "learning_rate": 4.96e-06,
      "loss": 0.0034,
      "step": 33780
    },
    {
      "epoch": 1.8021333333333334,
      "grad_norm": 0.3513825833797455,
      "learning_rate": 4.946666666666667e-06,
      "loss": 0.0028,
      "step": 33790
    },
    {
      "epoch": 1.8026666666666666,
      "grad_norm": 0.4530181288719177,
      "learning_rate": 4.933333333333333e-06,
      "loss": 0.0025,
      "step": 33800
    },
    {
      "epoch": 1.8032,
      "grad_norm": 0.24772779643535614,
      "learning_rate": 4.92e-06,
      "loss": 0.0018,
      "step": 33810
    },
    {
      "epoch": 1.8037333333333332,
      "grad_norm": 0.29603132605552673,
      "learning_rate": 4.906666666666667e-06,
      "loss": 0.0026,
      "step": 33820
    },
    {
      "epoch": 1.8042666666666667,
      "grad_norm": 0.11775634437799454,
      "learning_rate": 4.893333333333334e-06,
      "loss": 0.003,
      "step": 33830
    },
    {
      "epoch": 1.8048,
      "grad_norm": 0.11570903658866882,
      "learning_rate": 4.880000000000001e-06,
      "loss": 0.0018,
      "step": 33840
    },
    {
      "epoch": 1.8053333333333335,
      "grad_norm": 0.22342216968536377,
      "learning_rate": 4.866666666666667e-06,
      "loss": 0.0029,
      "step": 33850
    },
    {
      "epoch": 1.8058666666666667,
      "grad_norm": 0.17794521152973175,
      "learning_rate": 4.853333333333333e-06,
      "loss": 0.0027,
      "step": 33860
    },
    {
      "epoch": 1.8064,
      "grad_norm": 0.440500408411026,
      "learning_rate": 4.84e-06,
      "loss": 0.0028,
      "step": 33870
    },
    {
      "epoch": 1.8069333333333333,
      "grad_norm": 0.085294209420681,
      "learning_rate": 4.8266666666666665e-06,
      "loss": 0.0017,
      "step": 33880
    },
    {
      "epoch": 1.8074666666666666,
      "grad_norm": 0.5083491206169128,
      "learning_rate": 4.8133333333333336e-06,
      "loss": 0.002,
      "step": 33890
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.7915025949478149,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.0021,
      "step": 33900
    },
    {
      "epoch": 1.8085333333333333,
      "grad_norm": 0.09800717234611511,
      "learning_rate": 4.786666666666667e-06,
      "loss": 0.0031,
      "step": 33910
    },
    {
      "epoch": 1.8090666666666668,
      "grad_norm": 0.3084098994731903,
      "learning_rate": 4.773333333333334e-06,
      "loss": 0.0017,
      "step": 33920
    },
    {
      "epoch": 1.8096,
      "grad_norm": 0.646999716758728,
      "learning_rate": 4.76e-06,
      "loss": 0.0029,
      "step": 33930
    },
    {
      "epoch": 1.8101333333333334,
      "grad_norm": 0.17913703620433807,
      "learning_rate": 4.746666666666666e-06,
      "loss": 0.002,
      "step": 33940
    },
    {
      "epoch": 1.8106666666666666,
      "grad_norm": 0.14054852724075317,
      "learning_rate": 4.7333333333333335e-06,
      "loss": 0.0024,
      "step": 33950
    },
    {
      "epoch": 1.8112,
      "grad_norm": 0.11417526751756668,
      "learning_rate": 4.72e-06,
      "loss": 0.0021,
      "step": 33960
    },
    {
      "epoch": 1.8117333333333332,
      "grad_norm": 0.17699047923088074,
      "learning_rate": 4.706666666666667e-06,
      "loss": 0.0028,
      "step": 33970
    },
    {
      "epoch": 1.8122666666666667,
      "grad_norm": 0.2719360888004303,
      "learning_rate": 4.693333333333334e-06,
      "loss": 0.0018,
      "step": 33980
    },
    {
      "epoch": 1.8128,
      "grad_norm": 0.07612590491771698,
      "learning_rate": 4.68e-06,
      "loss": 0.0037,
      "step": 33990
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.3894611895084381,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.0018,
      "step": 34000
    },
    {
      "epoch": 1.8138666666666667,
      "grad_norm": 0.199044868350029,
      "learning_rate": 4.653333333333334e-06,
      "loss": 0.0027,
      "step": 34010
    },
    {
      "epoch": 1.8144,
      "grad_norm": 0.16264493763446808,
      "learning_rate": 4.64e-06,
      "loss": 0.003,
      "step": 34020
    },
    {
      "epoch": 1.8149333333333333,
      "grad_norm": 0.1005435436964035,
      "learning_rate": 4.626666666666667e-06,
      "loss": 0.0017,
      "step": 34030
    },
    {
      "epoch": 1.8154666666666666,
      "grad_norm": 0.0717499852180481,
      "learning_rate": 4.613333333333334e-06,
      "loss": 0.004,
      "step": 34040
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.13573768734931946,
      "learning_rate": 4.6e-06,
      "loss": 0.0016,
      "step": 34050
    },
    {
      "epoch": 1.8165333333333333,
      "grad_norm": 0.08183339983224869,
      "learning_rate": 4.586666666666667e-06,
      "loss": 0.0034,
      "step": 34060
    },
    {
      "epoch": 1.8170666666666668,
      "grad_norm": 0.20947881042957306,
      "learning_rate": 4.573333333333333e-06,
      "loss": 0.0018,
      "step": 34070
    },
    {
      "epoch": 1.8176,
      "grad_norm": 0.30194205045700073,
      "learning_rate": 4.56e-06,
      "loss": 0.0025,
      "step": 34080
    },
    {
      "epoch": 1.8181333333333334,
      "grad_norm": 0.06478139758110046,
      "learning_rate": 4.5466666666666675e-06,
      "loss": 0.0025,
      "step": 34090
    },
    {
      "epoch": 1.8186666666666667,
      "grad_norm": 0.10679949074983597,
      "learning_rate": 4.533333333333334e-06,
      "loss": 0.0026,
      "step": 34100
    },
    {
      "epoch": 1.8192,
      "grad_norm": 0.07877235859632492,
      "learning_rate": 4.52e-06,
      "loss": 0.0018,
      "step": 34110
    },
    {
      "epoch": 1.8197333333333332,
      "grad_norm": 0.3626348376274109,
      "learning_rate": 4.506666666666667e-06,
      "loss": 0.0045,
      "step": 34120
    },
    {
      "epoch": 1.8202666666666667,
      "grad_norm": 0.3357327878475189,
      "learning_rate": 4.493333333333333e-06,
      "loss": 0.0021,
      "step": 34130
    },
    {
      "epoch": 1.8208,
      "grad_norm": 0.10640976577997208,
      "learning_rate": 4.48e-06,
      "loss": 0.0035,
      "step": 34140
    },
    {
      "epoch": 1.8213333333333335,
      "grad_norm": 0.07620345801115036,
      "learning_rate": 4.4666666666666665e-06,
      "loss": 0.0023,
      "step": 34150
    },
    {
      "epoch": 1.8218666666666667,
      "grad_norm": 0.15135563910007477,
      "learning_rate": 4.453333333333334e-06,
      "loss": 0.0026,
      "step": 34160
    },
    {
      "epoch": 1.8224,
      "grad_norm": 0.10585186630487442,
      "learning_rate": 4.440000000000001e-06,
      "loss": 0.0018,
      "step": 34170
    },
    {
      "epoch": 1.8229333333333333,
      "grad_norm": 0.4928797483444214,
      "learning_rate": 4.426666666666667e-06,
      "loss": 0.0017,
      "step": 34180
    },
    {
      "epoch": 1.8234666666666666,
      "grad_norm": 0.4748527407646179,
      "learning_rate": 4.413333333333333e-06,
      "loss": 0.0027,
      "step": 34190
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.40652456879615784,
      "learning_rate": 4.4e-06,
      "loss": 0.0017,
      "step": 34200
    },
    {
      "epoch": 1.8245333333333333,
      "grad_norm": 0.19959402084350586,
      "learning_rate": 4.3866666666666665e-06,
      "loss": 0.0024,
      "step": 34210
    },
    {
      "epoch": 1.8250666666666666,
      "grad_norm": 0.1437680721282959,
      "learning_rate": 4.3733333333333335e-06,
      "loss": 0.0023,
      "step": 34220
    },
    {
      "epoch": 1.8256000000000001,
      "grad_norm": 0.1057240292429924,
      "learning_rate": 4.360000000000001e-06,
      "loss": 0.002,
      "step": 34230
    },
    {
      "epoch": 1.8261333333333334,
      "grad_norm": 0.47703444957733154,
      "learning_rate": 4.346666666666667e-06,
      "loss": 0.0027,
      "step": 34240
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.06566590070724487,
      "learning_rate": 4.333333333333334e-06,
      "loss": 0.0016,
      "step": 34250
    },
    {
      "epoch": 1.8272,
      "grad_norm": 0.16342903673648834,
      "learning_rate": 4.32e-06,
      "loss": 0.002,
      "step": 34260
    },
    {
      "epoch": 1.8277333333333332,
      "grad_norm": 0.7766678929328918,
      "learning_rate": 4.306666666666667e-06,
      "loss": 0.0036,
      "step": 34270
    },
    {
      "epoch": 1.8282666666666667,
      "grad_norm": 0.13838276267051697,
      "learning_rate": 4.2933333333333334e-06,
      "loss": 0.0028,
      "step": 34280
    },
    {
      "epoch": 1.8288,
      "grad_norm": 0.1193617507815361,
      "learning_rate": 4.28e-06,
      "loss": 0.002,
      "step": 34290
    },
    {
      "epoch": 1.8293333333333335,
      "grad_norm": 0.28859734535217285,
      "learning_rate": 4.266666666666667e-06,
      "loss": 0.0022,
      "step": 34300
    },
    {
      "epoch": 1.8298666666666668,
      "grad_norm": 0.20975729823112488,
      "learning_rate": 4.253333333333334e-06,
      "loss": 0.0019,
      "step": 34310
    },
    {
      "epoch": 1.8304,
      "grad_norm": 0.4821177124977112,
      "learning_rate": 4.24e-06,
      "loss": 0.0018,
      "step": 34320
    },
    {
      "epoch": 1.8309333333333333,
      "grad_norm": 0.2992396056652069,
      "learning_rate": 4.226666666666667e-06,
      "loss": 0.004,
      "step": 34330
    },
    {
      "epoch": 1.8314666666666666,
      "grad_norm": 0.23414303362369537,
      "learning_rate": 4.213333333333333e-06,
      "loss": 0.0022,
      "step": 34340
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.30721139907836914,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 0.0019,
      "step": 34350
    },
    {
      "epoch": 1.8325333333333333,
      "grad_norm": 0.1216953694820404,
      "learning_rate": 4.1866666666666675e-06,
      "loss": 0.0023,
      "step": 34360
    },
    {
      "epoch": 1.8330666666666666,
      "grad_norm": 0.21377140283584595,
      "learning_rate": 4.173333333333333e-06,
      "loss": 0.0027,
      "step": 34370
    },
    {
      "epoch": 1.8336000000000001,
      "grad_norm": 0.2509213984012604,
      "learning_rate": 4.16e-06,
      "loss": 0.0027,
      "step": 34380
    },
    {
      "epoch": 1.8341333333333334,
      "grad_norm": 0.1420055478811264,
      "learning_rate": 4.146666666666667e-06,
      "loss": 0.0017,
      "step": 34390
    },
    {
      "epoch": 1.8346666666666667,
      "grad_norm": 0.6624472141265869,
      "learning_rate": 4.133333333333333e-06,
      "loss": 0.002,
      "step": 34400
    },
    {
      "epoch": 1.8352,
      "grad_norm": 0.2926219701766968,
      "learning_rate": 4.12e-06,
      "loss": 0.0018,
      "step": 34410
    },
    {
      "epoch": 1.8357333333333332,
      "grad_norm": 0.15820825099945068,
      "learning_rate": 4.106666666666667e-06,
      "loss": 0.0018,
      "step": 34420
    },
    {
      "epoch": 1.8362666666666667,
      "grad_norm": 0.5680309534072876,
      "learning_rate": 4.093333333333334e-06,
      "loss": 0.0018,
      "step": 34430
    },
    {
      "epoch": 1.8368,
      "grad_norm": 0.4828960597515106,
      "learning_rate": 4.080000000000001e-06,
      "loss": 0.0023,
      "step": 34440
    },
    {
      "epoch": 1.8373333333333335,
      "grad_norm": 0.17834614217281342,
      "learning_rate": 4.066666666666666e-06,
      "loss": 0.0018,
      "step": 34450
    },
    {
      "epoch": 1.8378666666666668,
      "grad_norm": 0.04498441517353058,
      "learning_rate": 4.053333333333333e-06,
      "loss": 0.0026,
      "step": 34460
    },
    {
      "epoch": 1.8384,
      "grad_norm": 0.3062799274921417,
      "learning_rate": 4.04e-06,
      "loss": 0.0018,
      "step": 34470
    },
    {
      "epoch": 1.8389333333333333,
      "grad_norm": 0.5766529440879822,
      "learning_rate": 4.0266666666666665e-06,
      "loss": 0.0023,
      "step": 34480
    },
    {
      "epoch": 1.8394666666666666,
      "grad_norm": 0.3538671135902405,
      "learning_rate": 4.013333333333334e-06,
      "loss": 0.0026,
      "step": 34490
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.1167830303311348,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.0022,
      "step": 34500
    },
    {
      "epoch": 1.8405333333333334,
      "grad_norm": 0.24798037111759186,
      "learning_rate": 3.986666666666667e-06,
      "loss": 0.0032,
      "step": 34510
    },
    {
      "epoch": 1.8410666666666666,
      "grad_norm": 0.35071486234664917,
      "learning_rate": 3.973333333333334e-06,
      "loss": 0.0025,
      "step": 34520
    },
    {
      "epoch": 1.8416000000000001,
      "grad_norm": 0.46297189593315125,
      "learning_rate": 3.96e-06,
      "loss": 0.0018,
      "step": 34530
    },
    {
      "epoch": 1.8421333333333334,
      "grad_norm": 0.18375743925571442,
      "learning_rate": 3.9466666666666664e-06,
      "loss": 0.0035,
      "step": 34540
    },
    {
      "epoch": 1.8426666666666667,
      "grad_norm": 0.18404503166675568,
      "learning_rate": 3.9333333333333335e-06,
      "loss": 0.0018,
      "step": 34550
    },
    {
      "epoch": 1.8432,
      "grad_norm": 0.19420106709003448,
      "learning_rate": 3.92e-06,
      "loss": 0.002,
      "step": 34560
    },
    {
      "epoch": 1.8437333333333332,
      "grad_norm": 0.12833857536315918,
      "learning_rate": 3.906666666666667e-06,
      "loss": 0.0021,
      "step": 34570
    },
    {
      "epoch": 1.8442666666666667,
      "grad_norm": 0.10370109230279922,
      "learning_rate": 3.893333333333334e-06,
      "loss": 0.0016,
      "step": 34580
    },
    {
      "epoch": 1.8448,
      "grad_norm": 0.16906794905662537,
      "learning_rate": 3.88e-06,
      "loss": 0.0025,
      "step": 34590
    },
    {
      "epoch": 1.8453333333333335,
      "grad_norm": 0.15681786835193634,
      "learning_rate": 3.866666666666667e-06,
      "loss": 0.0023,
      "step": 34600
    },
    {
      "epoch": 1.8458666666666668,
      "grad_norm": 0.24257412552833557,
      "learning_rate": 3.8533333333333334e-06,
      "loss": 0.0018,
      "step": 34610
    },
    {
      "epoch": 1.8464,
      "grad_norm": 0.10146447271108627,
      "learning_rate": 3.84e-06,
      "loss": 0.0024,
      "step": 34620
    },
    {
      "epoch": 1.8469333333333333,
      "grad_norm": 0.2959255278110504,
      "learning_rate": 3.826666666666667e-06,
      "loss": 0.0022,
      "step": 34630
    },
    {
      "epoch": 1.8474666666666666,
      "grad_norm": 0.1732216775417328,
      "learning_rate": 3.8133333333333334e-06,
      "loss": 0.0024,
      "step": 34640
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.36296674609184265,
      "learning_rate": 3.8e-06,
      "loss": 0.0028,
      "step": 34650
    },
    {
      "epoch": 1.8485333333333334,
      "grad_norm": 0.6375994086265564,
      "learning_rate": 3.7866666666666667e-06,
      "loss": 0.0027,
      "step": 34660
    },
    {
      "epoch": 1.8490666666666666,
      "grad_norm": 0.25747209787368774,
      "learning_rate": 3.7733333333333338e-06,
      "loss": 0.0021,
      "step": 34670
    },
    {
      "epoch": 1.8496000000000001,
      "grad_norm": 0.11106626689434052,
      "learning_rate": 3.7600000000000004e-06,
      "loss": 0.0033,
      "step": 34680
    },
    {
      "epoch": 1.8501333333333334,
      "grad_norm": 0.25556549429893494,
      "learning_rate": 3.746666666666667e-06,
      "loss": 0.0019,
      "step": 34690
    },
    {
      "epoch": 1.8506666666666667,
      "grad_norm": 0.5196773409843445,
      "learning_rate": 3.7333333333333337e-06,
      "loss": 0.003,
      "step": 34700
    },
    {
      "epoch": 1.8512,
      "grad_norm": 0.14472121000289917,
      "learning_rate": 3.72e-06,
      "loss": 0.0038,
      "step": 34710
    },
    {
      "epoch": 1.8517333333333332,
      "grad_norm": 0.3361930549144745,
      "learning_rate": 3.7066666666666666e-06,
      "loss": 0.0017,
      "step": 34720
    },
    {
      "epoch": 1.8522666666666665,
      "grad_norm": 0.3574499785900116,
      "learning_rate": 3.6933333333333333e-06,
      "loss": 0.0024,
      "step": 34730
    },
    {
      "epoch": 1.8528,
      "grad_norm": 0.08954932540655136,
      "learning_rate": 3.68e-06,
      "loss": 0.0027,
      "step": 34740
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.07108034938573837,
      "learning_rate": 3.666666666666667e-06,
      "loss": 0.0026,
      "step": 34750
    },
    {
      "epoch": 1.8538666666666668,
      "grad_norm": 0.22369317710399628,
      "learning_rate": 3.6533333333333336e-06,
      "loss": 0.0019,
      "step": 34760
    },
    {
      "epoch": 1.8544,
      "grad_norm": 0.14144571125507355,
      "learning_rate": 3.6400000000000003e-06,
      "loss": 0.0016,
      "step": 34770
    },
    {
      "epoch": 1.8549333333333333,
      "grad_norm": 0.1308787763118744,
      "learning_rate": 3.626666666666667e-06,
      "loss": 0.0017,
      "step": 34780
    },
    {
      "epoch": 1.8554666666666666,
      "grad_norm": 0.24124829471111298,
      "learning_rate": 3.613333333333334e-06,
      "loss": 0.0019,
      "step": 34790
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.3628876209259033,
      "learning_rate": 3.6e-06,
      "loss": 0.0019,
      "step": 34800
    },
    {
      "epoch": 1.8565333333333334,
      "grad_norm": 0.07908878475427628,
      "learning_rate": 3.5866666666666665e-06,
      "loss": 0.0021,
      "step": 34810
    },
    {
      "epoch": 1.8570666666666666,
      "grad_norm": 0.42558375000953674,
      "learning_rate": 3.5733333333333336e-06,
      "loss": 0.0032,
      "step": 34820
    },
    {
      "epoch": 1.8576000000000001,
      "grad_norm": 0.08062297850847244,
      "learning_rate": 3.5600000000000002e-06,
      "loss": 0.0019,
      "step": 34830
    },
    {
      "epoch": 1.8581333333333334,
      "grad_norm": 0.06962141394615173,
      "learning_rate": 3.546666666666667e-06,
      "loss": 0.0027,
      "step": 34840
    },
    {
      "epoch": 1.8586666666666667,
      "grad_norm": 0.33646076917648315,
      "learning_rate": 3.5333333333333335e-06,
      "loss": 0.0036,
      "step": 34850
    },
    {
      "epoch": 1.8592,
      "grad_norm": 0.37102043628692627,
      "learning_rate": 3.52e-06,
      "loss": 0.0017,
      "step": 34860
    },
    {
      "epoch": 1.8597333333333332,
      "grad_norm": 0.2937411665916443,
      "learning_rate": 3.5066666666666673e-06,
      "loss": 0.0021,
      "step": 34870
    },
    {
      "epoch": 1.8602666666666665,
      "grad_norm": 0.27219250798225403,
      "learning_rate": 3.493333333333333e-06,
      "loss": 0.0028,
      "step": 34880
    },
    {
      "epoch": 1.8608,
      "grad_norm": 0.3806103467941284,
      "learning_rate": 3.4799999999999997e-06,
      "loss": 0.002,
      "step": 34890
    },
    {
      "epoch": 1.8613333333333333,
      "grad_norm": 0.09973639249801636,
      "learning_rate": 3.466666666666667e-06,
      "loss": 0.0025,
      "step": 34900
    },
    {
      "epoch": 1.8618666666666668,
      "grad_norm": 0.5214322209358215,
      "learning_rate": 3.4533333333333334e-06,
      "loss": 0.0023,
      "step": 34910
    },
    {
      "epoch": 1.8624,
      "grad_norm": 0.14270368218421936,
      "learning_rate": 3.44e-06,
      "loss": 0.0018,
      "step": 34920
    },
    {
      "epoch": 1.8629333333333333,
      "grad_norm": 0.09973277151584625,
      "learning_rate": 3.4266666666666668e-06,
      "loss": 0.0024,
      "step": 34930
    },
    {
      "epoch": 1.8634666666666666,
      "grad_norm": 0.083001047372818,
      "learning_rate": 3.413333333333334e-06,
      "loss": 0.0037,
      "step": 34940
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.39021778106689453,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 0.0029,
      "step": 34950
    },
    {
      "epoch": 1.8645333333333334,
      "grad_norm": 0.47916609048843384,
      "learning_rate": 3.386666666666667e-06,
      "loss": 0.0018,
      "step": 34960
    },
    {
      "epoch": 1.8650666666666667,
      "grad_norm": 0.08144642412662506,
      "learning_rate": 3.3733333333333334e-06,
      "loss": 0.0025,
      "step": 34970
    },
    {
      "epoch": 1.8656000000000001,
      "grad_norm": 0.20455807447433472,
      "learning_rate": 3.36e-06,
      "loss": 0.0023,
      "step": 34980
    },
    {
      "epoch": 1.8661333333333334,
      "grad_norm": 0.3195172846317291,
      "learning_rate": 3.3466666666666667e-06,
      "loss": 0.0026,
      "step": 34990
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.3686801493167877,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.0025,
      "step": 35000
    },
    {
      "epoch": 1.8672,
      "grad_norm": 0.22575317323207855,
      "learning_rate": 3.3200000000000004e-06,
      "loss": 0.0025,
      "step": 35010
    },
    {
      "epoch": 1.8677333333333332,
      "grad_norm": 0.3826654553413391,
      "learning_rate": 3.306666666666667e-06,
      "loss": 0.0022,
      "step": 35020
    },
    {
      "epoch": 1.8682666666666665,
      "grad_norm": 0.0800023078918457,
      "learning_rate": 3.2933333333333337e-06,
      "loss": 0.0029,
      "step": 35030
    },
    {
      "epoch": 1.8688,
      "grad_norm": 0.06546986848115921,
      "learning_rate": 3.2800000000000004e-06,
      "loss": 0.0018,
      "step": 35040
    },
    {
      "epoch": 1.8693333333333333,
      "grad_norm": 0.17553983628749847,
      "learning_rate": 3.2666666666666666e-06,
      "loss": 0.0025,
      "step": 35050
    },
    {
      "epoch": 1.8698666666666668,
      "grad_norm": 0.666675865650177,
      "learning_rate": 3.2533333333333332e-06,
      "loss": 0.002,
      "step": 35060
    },
    {
      "epoch": 1.8704,
      "grad_norm": 0.15563324093818665,
      "learning_rate": 3.24e-06,
      "loss": 0.0019,
      "step": 35070
    },
    {
      "epoch": 1.8709333333333333,
      "grad_norm": 0.050980184227228165,
      "learning_rate": 3.2266666666666665e-06,
      "loss": 0.0035,
      "step": 35080
    },
    {
      "epoch": 1.8714666666666666,
      "grad_norm": 0.21233801543712616,
      "learning_rate": 3.2133333333333336e-06,
      "loss": 0.0025,
      "step": 35090
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.38820528984069824,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.0024,
      "step": 35100
    },
    {
      "epoch": 1.8725333333333334,
      "grad_norm": 0.23377551138401031,
      "learning_rate": 3.186666666666667e-06,
      "loss": 0.0018,
      "step": 35110
    },
    {
      "epoch": 1.8730666666666667,
      "grad_norm": 0.2749064862728119,
      "learning_rate": 3.1733333333333336e-06,
      "loss": 0.0027,
      "step": 35120
    },
    {
      "epoch": 1.8736000000000002,
      "grad_norm": 0.15881122648715973,
      "learning_rate": 3.1600000000000007e-06,
      "loss": 0.0033,
      "step": 35130
    },
    {
      "epoch": 1.8741333333333334,
      "grad_norm": 0.35688135027885437,
      "learning_rate": 3.1466666666666665e-06,
      "loss": 0.0024,
      "step": 35140
    },
    {
      "epoch": 1.8746666666666667,
      "grad_norm": 0.5452443957328796,
      "learning_rate": 3.133333333333333e-06,
      "loss": 0.002,
      "step": 35150
    },
    {
      "epoch": 1.8752,
      "grad_norm": 0.20009762048721313,
      "learning_rate": 3.12e-06,
      "loss": 0.0018,
      "step": 35160
    },
    {
      "epoch": 1.8757333333333333,
      "grad_norm": 0.07120555639266968,
      "learning_rate": 3.106666666666667e-06,
      "loss": 0.0026,
      "step": 35170
    },
    {
      "epoch": 1.8762666666666665,
      "grad_norm": 0.34659090638160706,
      "learning_rate": 3.0933333333333335e-06,
      "loss": 0.0021,
      "step": 35180
    },
    {
      "epoch": 1.8768,
      "grad_norm": 0.10473871231079102,
      "learning_rate": 3.08e-06,
      "loss": 0.0023,
      "step": 35190
    },
    {
      "epoch": 1.8773333333333333,
      "grad_norm": 0.3276199400424957,
      "learning_rate": 3.066666666666667e-06,
      "loss": 0.0018,
      "step": 35200
    },
    {
      "epoch": 1.8778666666666668,
      "grad_norm": 0.3204910457134247,
      "learning_rate": 3.0533333333333335e-06,
      "loss": 0.0026,
      "step": 35210
    },
    {
      "epoch": 1.8784,
      "grad_norm": 0.30011895298957825,
      "learning_rate": 3.04e-06,
      "loss": 0.0017,
      "step": 35220
    },
    {
      "epoch": 1.8789333333333333,
      "grad_norm": 0.1473071128129959,
      "learning_rate": 3.0266666666666668e-06,
      "loss": 0.0032,
      "step": 35230
    },
    {
      "epoch": 1.8794666666666666,
      "grad_norm": 0.3794057369232178,
      "learning_rate": 3.0133333333333334e-06,
      "loss": 0.0034,
      "step": 35240
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.4472714066505432,
      "learning_rate": 3e-06,
      "loss": 0.0023,
      "step": 35250
    },
    {
      "epoch": 1.8805333333333332,
      "grad_norm": 0.1359533965587616,
      "learning_rate": 2.9866666666666667e-06,
      "loss": 0.0021,
      "step": 35260
    },
    {
      "epoch": 1.8810666666666667,
      "grad_norm": 0.31586331129074097,
      "learning_rate": 2.9733333333333334e-06,
      "loss": 0.0022,
      "step": 35270
    },
    {
      "epoch": 1.8816000000000002,
      "grad_norm": 0.1899918168783188,
      "learning_rate": 2.9600000000000005e-06,
      "loss": 0.0017,
      "step": 35280
    },
    {
      "epoch": 1.8821333333333334,
      "grad_norm": 0.4200548231601715,
      "learning_rate": 2.9466666666666667e-06,
      "loss": 0.0031,
      "step": 35290
    },
    {
      "epoch": 1.8826666666666667,
      "grad_norm": 0.1868346929550171,
      "learning_rate": 2.9333333333333333e-06,
      "loss": 0.0024,
      "step": 35300
    },
    {
      "epoch": 1.8832,
      "grad_norm": 0.28054529428482056,
      "learning_rate": 2.92e-06,
      "loss": 0.003,
      "step": 35310
    },
    {
      "epoch": 1.8837333333333333,
      "grad_norm": 0.47375214099884033,
      "learning_rate": 2.906666666666667e-06,
      "loss": 0.0021,
      "step": 35320
    },
    {
      "epoch": 1.8842666666666665,
      "grad_norm": 0.33856523036956787,
      "learning_rate": 2.8933333333333333e-06,
      "loss": 0.0033,
      "step": 35330
    },
    {
      "epoch": 1.8848,
      "grad_norm": 0.14125584065914154,
      "learning_rate": 2.88e-06,
      "loss": 0.0023,
      "step": 35340
    },
    {
      "epoch": 1.8853333333333333,
      "grad_norm": 0.40438470244407654,
      "learning_rate": 2.8666666666666666e-06,
      "loss": 0.0032,
      "step": 35350
    },
    {
      "epoch": 1.8858666666666668,
      "grad_norm": 0.18145492672920227,
      "learning_rate": 2.8533333333333337e-06,
      "loss": 0.0016,
      "step": 35360
    },
    {
      "epoch": 1.8864,
      "grad_norm": 0.8345872759819031,
      "learning_rate": 2.8400000000000003e-06,
      "loss": 0.0019,
      "step": 35370
    },
    {
      "epoch": 1.8869333333333334,
      "grad_norm": 0.5290554761886597,
      "learning_rate": 2.8266666666666666e-06,
      "loss": 0.0018,
      "step": 35380
    },
    {
      "epoch": 1.8874666666666666,
      "grad_norm": 0.095596082508564,
      "learning_rate": 2.8133333333333336e-06,
      "loss": 0.0031,
      "step": 35390
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.23013466596603394,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.0026,
      "step": 35400
    },
    {
      "epoch": 1.8885333333333332,
      "grad_norm": 0.11459440737962723,
      "learning_rate": 2.786666666666667e-06,
      "loss": 0.0019,
      "step": 35410
    },
    {
      "epoch": 1.8890666666666667,
      "grad_norm": 0.1843293458223343,
      "learning_rate": 2.773333333333333e-06,
      "loss": 0.0025,
      "step": 35420
    },
    {
      "epoch": 1.8896,
      "grad_norm": 0.08619407564401627,
      "learning_rate": 2.7600000000000003e-06,
      "loss": 0.0017,
      "step": 35430
    },
    {
      "epoch": 1.8901333333333334,
      "grad_norm": 0.3835851550102234,
      "learning_rate": 2.746666666666667e-06,
      "loss": 0.0018,
      "step": 35440
    },
    {
      "epoch": 1.8906666666666667,
      "grad_norm": 0.4500383734703064,
      "learning_rate": 2.7333333333333336e-06,
      "loss": 0.0017,
      "step": 35450
    },
    {
      "epoch": 1.8912,
      "grad_norm": 0.23139308393001556,
      "learning_rate": 2.72e-06,
      "loss": 0.0019,
      "step": 35460
    },
    {
      "epoch": 1.8917333333333333,
      "grad_norm": 0.6168385148048401,
      "learning_rate": 2.706666666666667e-06,
      "loss": 0.002,
      "step": 35470
    },
    {
      "epoch": 1.8922666666666665,
      "grad_norm": 0.09338849782943726,
      "learning_rate": 2.6933333333333335e-06,
      "loss": 0.0025,
      "step": 35480
    },
    {
      "epoch": 1.8928,
      "grad_norm": 0.2582959234714508,
      "learning_rate": 2.68e-06,
      "loss": 0.0028,
      "step": 35490
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.14019638299942017,
      "learning_rate": 2.666666666666667e-06,
      "loss": 0.0017,
      "step": 35500
    },
    {
      "epoch": 1.8938666666666668,
      "grad_norm": 0.6064494848251343,
      "learning_rate": 2.6533333333333335e-06,
      "loss": 0.0019,
      "step": 35510
    },
    {
      "epoch": 1.8944,
      "grad_norm": 0.09837473928928375,
      "learning_rate": 2.64e-06,
      "loss": 0.0018,
      "step": 35520
    },
    {
      "epoch": 1.8949333333333334,
      "grad_norm": 0.25251471996307373,
      "learning_rate": 2.6266666666666668e-06,
      "loss": 0.0019,
      "step": 35530
    },
    {
      "epoch": 1.8954666666666666,
      "grad_norm": 0.6841482520103455,
      "learning_rate": 2.6133333333333334e-06,
      "loss": 0.0022,
      "step": 35540
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.2647112309932709,
      "learning_rate": 2.6e-06,
      "loss": 0.0025,
      "step": 35550
    },
    {
      "epoch": 1.8965333333333332,
      "grad_norm": 0.32922622561454773,
      "learning_rate": 2.5866666666666667e-06,
      "loss": 0.0019,
      "step": 35560
    },
    {
      "epoch": 1.8970666666666667,
      "grad_norm": 0.11662498861551285,
      "learning_rate": 2.5733333333333334e-06,
      "loss": 0.0026,
      "step": 35570
    },
    {
      "epoch": 1.8976,
      "grad_norm": 0.19531145691871643,
      "learning_rate": 2.56e-06,
      "loss": 0.0025,
      "step": 35580
    },
    {
      "epoch": 1.8981333333333335,
      "grad_norm": 0.4300380349159241,
      "learning_rate": 2.5466666666666667e-06,
      "loss": 0.0015,
      "step": 35590
    },
    {
      "epoch": 1.8986666666666667,
      "grad_norm": 0.5719444155693054,
      "learning_rate": 2.5333333333333334e-06,
      "loss": 0.0022,
      "step": 35600
    },
    {
      "epoch": 1.8992,
      "grad_norm": 0.11110111325979233,
      "learning_rate": 2.52e-06,
      "loss": 0.0032,
      "step": 35610
    },
    {
      "epoch": 1.8997333333333333,
      "grad_norm": 0.2751949727535248,
      "learning_rate": 2.506666666666667e-06,
      "loss": 0.0019,
      "step": 35620
    },
    {
      "epoch": 1.9002666666666665,
      "grad_norm": 0.1853465884923935,
      "learning_rate": 2.4933333333333333e-06,
      "loss": 0.0029,
      "step": 35630
    },
    {
      "epoch": 1.9008,
      "grad_norm": 0.18467973172664642,
      "learning_rate": 2.48e-06,
      "loss": 0.0023,
      "step": 35640
    },
    {
      "epoch": 1.9013333333333333,
      "grad_norm": 0.08270474523305893,
      "learning_rate": 2.4666666666666666e-06,
      "loss": 0.0022,
      "step": 35650
    },
    {
      "epoch": 1.9018666666666668,
      "grad_norm": 0.3109685778617859,
      "learning_rate": 2.4533333333333337e-06,
      "loss": 0.0023,
      "step": 35660
    },
    {
      "epoch": 1.9024,
      "grad_norm": 0.07514017820358276,
      "learning_rate": 2.4400000000000004e-06,
      "loss": 0.0017,
      "step": 35670
    },
    {
      "epoch": 1.9029333333333334,
      "grad_norm": 0.40491926670074463,
      "learning_rate": 2.4266666666666666e-06,
      "loss": 0.0024,
      "step": 35680
    },
    {
      "epoch": 1.9034666666666666,
      "grad_norm": 0.29629087448120117,
      "learning_rate": 2.4133333333333332e-06,
      "loss": 0.0018,
      "step": 35690
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.40415507555007935,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.0021,
      "step": 35700
    },
    {
      "epoch": 1.9045333333333332,
      "grad_norm": 0.38740065693855286,
      "learning_rate": 2.386666666666667e-06,
      "loss": 0.0025,
      "step": 35710
    },
    {
      "epoch": 1.9050666666666667,
      "grad_norm": 0.7684979438781738,
      "learning_rate": 2.373333333333333e-06,
      "loss": 0.0019,
      "step": 35720
    },
    {
      "epoch": 1.9056,
      "grad_norm": 0.08075164258480072,
      "learning_rate": 2.36e-06,
      "loss": 0.0024,
      "step": 35730
    },
    {
      "epoch": 1.9061333333333335,
      "grad_norm": 0.07591207325458527,
      "learning_rate": 2.346666666666667e-06,
      "loss": 0.0016,
      "step": 35740
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.15185585618019104,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 0.002,
      "step": 35750
    },
    {
      "epoch": 1.9072,
      "grad_norm": 0.4811722934246063,
      "learning_rate": 2.32e-06,
      "loss": 0.0028,
      "step": 35760
    },
    {
      "epoch": 1.9077333333333333,
      "grad_norm": 0.12263614684343338,
      "learning_rate": 2.306666666666667e-06,
      "loss": 0.002,
      "step": 35770
    },
    {
      "epoch": 1.9082666666666666,
      "grad_norm": 0.13027594983577728,
      "learning_rate": 2.2933333333333335e-06,
      "loss": 0.0024,
      "step": 35780
    },
    {
      "epoch": 1.9088,
      "grad_norm": 0.20386654138565063,
      "learning_rate": 2.28e-06,
      "loss": 0.002,
      "step": 35790
    },
    {
      "epoch": 1.9093333333333333,
      "grad_norm": 0.07170519977807999,
      "learning_rate": 2.266666666666667e-06,
      "loss": 0.004,
      "step": 35800
    },
    {
      "epoch": 1.9098666666666668,
      "grad_norm": 0.18644802272319794,
      "learning_rate": 2.2533333333333335e-06,
      "loss": 0.0027,
      "step": 35810
    },
    {
      "epoch": 1.9104,
      "grad_norm": 0.19206514954566956,
      "learning_rate": 2.24e-06,
      "loss": 0.0028,
      "step": 35820
    },
    {
      "epoch": 1.9109333333333334,
      "grad_norm": 0.1261741816997528,
      "learning_rate": 2.226666666666667e-06,
      "loss": 0.0035,
      "step": 35830
    },
    {
      "epoch": 1.9114666666666666,
      "grad_norm": 0.16789470613002777,
      "learning_rate": 2.2133333333333335e-06,
      "loss": 0.0034,
      "step": 35840
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.43291375041007996,
      "learning_rate": 2.2e-06,
      "loss": 0.0019,
      "step": 35850
    },
    {
      "epoch": 1.9125333333333332,
      "grad_norm": 0.42437124252319336,
      "learning_rate": 2.1866666666666668e-06,
      "loss": 0.0026,
      "step": 35860
    },
    {
      "epoch": 1.9130666666666667,
      "grad_norm": 0.43962711095809937,
      "learning_rate": 2.1733333333333334e-06,
      "loss": 0.0017,
      "step": 35870
    },
    {
      "epoch": 1.9136,
      "grad_norm": 0.242300882935524,
      "learning_rate": 2.16e-06,
      "loss": 0.0025,
      "step": 35880
    },
    {
      "epoch": 1.9141333333333335,
      "grad_norm": 0.37413620948791504,
      "learning_rate": 2.1466666666666667e-06,
      "loss": 0.0025,
      "step": 35890
    },
    {
      "epoch": 1.9146666666666667,
      "grad_norm": 0.10884736478328705,
      "learning_rate": 2.1333333333333334e-06,
      "loss": 0.0023,
      "step": 35900
    },
    {
      "epoch": 1.9152,
      "grad_norm": 0.32741737365722656,
      "learning_rate": 2.12e-06,
      "loss": 0.002,
      "step": 35910
    },
    {
      "epoch": 1.9157333333333333,
      "grad_norm": 0.27871426939964294,
      "learning_rate": 2.1066666666666667e-06,
      "loss": 0.0015,
      "step": 35920
    },
    {
      "epoch": 1.9162666666666666,
      "grad_norm": 0.280651330947876,
      "learning_rate": 2.0933333333333338e-06,
      "loss": 0.0035,
      "step": 35930
    },
    {
      "epoch": 1.9167999999999998,
      "grad_norm": 0.46133488416671753,
      "learning_rate": 2.08e-06,
      "loss": 0.0022,
      "step": 35940
    },
    {
      "epoch": 1.9173333333333333,
      "grad_norm": 0.2011425644159317,
      "learning_rate": 2.0666666666666666e-06,
      "loss": 0.0026,
      "step": 35950
    },
    {
      "epoch": 1.9178666666666668,
      "grad_norm": 0.15643525123596191,
      "learning_rate": 2.0533333333333333e-06,
      "loss": 0.002,
      "step": 35960
    },
    {
      "epoch": 1.9184,
      "grad_norm": 0.39927151799201965,
      "learning_rate": 2.0400000000000004e-06,
      "loss": 0.0027,
      "step": 35970
    },
    {
      "epoch": 1.9189333333333334,
      "grad_norm": 0.5431318283081055,
      "learning_rate": 2.0266666666666666e-06,
      "loss": 0.002,
      "step": 35980
    },
    {
      "epoch": 1.9194666666666667,
      "grad_norm": 0.22127264738082886,
      "learning_rate": 2.0133333333333333e-06,
      "loss": 0.0019,
      "step": 35990
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.3186046779155731,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.0025,
      "step": 36000
    },
    {
      "epoch": 1.9205333333333332,
      "grad_norm": 0.07521756738424301,
      "learning_rate": 1.986666666666667e-06,
      "loss": 0.002,
      "step": 36010
    },
    {
      "epoch": 1.9210666666666667,
      "grad_norm": 0.4447327256202698,
      "learning_rate": 1.9733333333333332e-06,
      "loss": 0.0025,
      "step": 36020
    },
    {
      "epoch": 1.9216,
      "grad_norm": 0.2432294636964798,
      "learning_rate": 1.96e-06,
      "loss": 0.0035,
      "step": 36030
    },
    {
      "epoch": 1.9221333333333335,
      "grad_norm": 0.30326205492019653,
      "learning_rate": 1.946666666666667e-06,
      "loss": 0.0026,
      "step": 36040
    },
    {
      "epoch": 1.9226666666666667,
      "grad_norm": 0.37287968397140503,
      "learning_rate": 1.9333333333333336e-06,
      "loss": 0.0025,
      "step": 36050
    },
    {
      "epoch": 1.9232,
      "grad_norm": 0.11812005937099457,
      "learning_rate": 1.92e-06,
      "loss": 0.0025,
      "step": 36060
    },
    {
      "epoch": 1.9237333333333333,
      "grad_norm": 0.2382621169090271,
      "learning_rate": 1.9066666666666667e-06,
      "loss": 0.0022,
      "step": 36070
    },
    {
      "epoch": 1.9242666666666666,
      "grad_norm": 0.27741530537605286,
      "learning_rate": 1.8933333333333333e-06,
      "loss": 0.0023,
      "step": 36080
    },
    {
      "epoch": 1.9247999999999998,
      "grad_norm": 0.25585946440696716,
      "learning_rate": 1.8800000000000002e-06,
      "loss": 0.0027,
      "step": 36090
    },
    {
      "epoch": 1.9253333333333333,
      "grad_norm": 0.3552415668964386,
      "learning_rate": 1.8666666666666669e-06,
      "loss": 0.0023,
      "step": 36100
    },
    {
      "epoch": 1.9258666666666666,
      "grad_norm": 0.0821097269654274,
      "learning_rate": 1.8533333333333333e-06,
      "loss": 0.002,
      "step": 36110
    },
    {
      "epoch": 1.9264000000000001,
      "grad_norm": 0.16573046147823334,
      "learning_rate": 1.84e-06,
      "loss": 0.002,
      "step": 36120
    },
    {
      "epoch": 1.9269333333333334,
      "grad_norm": 0.12663868069648743,
      "learning_rate": 1.8266666666666668e-06,
      "loss": 0.0031,
      "step": 36130
    },
    {
      "epoch": 1.9274666666666667,
      "grad_norm": 0.31996431946754456,
      "learning_rate": 1.8133333333333335e-06,
      "loss": 0.002,
      "step": 36140
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.19195635616779327,
      "learning_rate": 1.8e-06,
      "loss": 0.0032,
      "step": 36150
    },
    {
      "epoch": 1.9285333333333332,
      "grad_norm": 0.30338361859321594,
      "learning_rate": 1.7866666666666668e-06,
      "loss": 0.0022,
      "step": 36160
    },
    {
      "epoch": 1.9290666666666667,
      "grad_norm": 0.08730130642652512,
      "learning_rate": 1.7733333333333334e-06,
      "loss": 0.0025,
      "step": 36170
    },
    {
      "epoch": 1.9296,
      "grad_norm": 0.2528924345970154,
      "learning_rate": 1.76e-06,
      "loss": 0.0018,
      "step": 36180
    },
    {
      "epoch": 1.9301333333333335,
      "grad_norm": 0.5624719262123108,
      "learning_rate": 1.7466666666666665e-06,
      "loss": 0.0031,
      "step": 36190
    },
    {
      "epoch": 1.9306666666666668,
      "grad_norm": 0.25789982080459595,
      "learning_rate": 1.7333333333333334e-06,
      "loss": 0.0031,
      "step": 36200
    },
    {
      "epoch": 1.9312,
      "grad_norm": 0.06211743876338005,
      "learning_rate": 1.72e-06,
      "loss": 0.0035,
      "step": 36210
    },
    {
      "epoch": 1.9317333333333333,
      "grad_norm": 0.37645143270492554,
      "learning_rate": 1.706666666666667e-06,
      "loss": 0.0017,
      "step": 36220
    },
    {
      "epoch": 1.9322666666666666,
      "grad_norm": 0.134136363863945,
      "learning_rate": 1.6933333333333336e-06,
      "loss": 0.0029,
      "step": 36230
    },
    {
      "epoch": 1.9327999999999999,
      "grad_norm": 0.2799672484397888,
      "learning_rate": 1.68e-06,
      "loss": 0.0032,
      "step": 36240
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.1284097284078598,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 0.0027,
      "step": 36250
    },
    {
      "epoch": 1.9338666666666666,
      "grad_norm": 0.16827495396137238,
      "learning_rate": 1.6533333333333335e-06,
      "loss": 0.002,
      "step": 36260
    },
    {
      "epoch": 1.9344000000000001,
      "grad_norm": 0.14007389545440674,
      "learning_rate": 1.6400000000000002e-06,
      "loss": 0.0024,
      "step": 36270
    },
    {
      "epoch": 1.9349333333333334,
      "grad_norm": 0.4132096469402313,
      "learning_rate": 1.6266666666666666e-06,
      "loss": 0.003,
      "step": 36280
    },
    {
      "epoch": 1.9354666666666667,
      "grad_norm": 0.4963878095149994,
      "learning_rate": 1.6133333333333333e-06,
      "loss": 0.0027,
      "step": 36290
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.16947293281555176,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.0023,
      "step": 36300
    },
    {
      "epoch": 1.9365333333333332,
      "grad_norm": 0.16799527406692505,
      "learning_rate": 1.5866666666666668e-06,
      "loss": 0.0019,
      "step": 36310
    },
    {
      "epoch": 1.9370666666666667,
      "grad_norm": 0.13143722712993622,
      "learning_rate": 1.5733333333333332e-06,
      "loss": 0.0017,
      "step": 36320
    },
    {
      "epoch": 1.9376,
      "grad_norm": 0.2002233862876892,
      "learning_rate": 1.56e-06,
      "loss": 0.0018,
      "step": 36330
    },
    {
      "epoch": 1.9381333333333335,
      "grad_norm": 0.3071092665195465,
      "learning_rate": 1.5466666666666668e-06,
      "loss": 0.0018,
      "step": 36340
    },
    {
      "epoch": 1.9386666666666668,
      "grad_norm": 0.1966920793056488,
      "learning_rate": 1.5333333333333334e-06,
      "loss": 0.002,
      "step": 36350
    },
    {
      "epoch": 1.9392,
      "grad_norm": 0.398660272359848,
      "learning_rate": 1.52e-06,
      "loss": 0.0021,
      "step": 36360
    },
    {
      "epoch": 1.9397333333333333,
      "grad_norm": 0.38835853338241577,
      "learning_rate": 1.5066666666666667e-06,
      "loss": 0.0019,
      "step": 36370
    },
    {
      "epoch": 1.9402666666666666,
      "grad_norm": 0.43262532353401184,
      "learning_rate": 1.4933333333333334e-06,
      "loss": 0.0022,
      "step": 36380
    },
    {
      "epoch": 1.9407999999999999,
      "grad_norm": 0.43009549379348755,
      "learning_rate": 1.4800000000000002e-06,
      "loss": 0.0021,
      "step": 36390
    },
    {
      "epoch": 1.9413333333333334,
      "grad_norm": 0.10817908495664597,
      "learning_rate": 1.4666666666666667e-06,
      "loss": 0.0027,
      "step": 36400
    },
    {
      "epoch": 1.9418666666666666,
      "grad_norm": 0.09270195662975311,
      "learning_rate": 1.4533333333333335e-06,
      "loss": 0.0018,
      "step": 36410
    },
    {
      "epoch": 1.9424000000000001,
      "grad_norm": 0.4533579349517822,
      "learning_rate": 1.44e-06,
      "loss": 0.0021,
      "step": 36420
    },
    {
      "epoch": 1.9429333333333334,
      "grad_norm": 0.08559221029281616,
      "learning_rate": 1.4266666666666668e-06,
      "loss": 0.0017,
      "step": 36430
    },
    {
      "epoch": 1.9434666666666667,
      "grad_norm": 0.5457730293273926,
      "learning_rate": 1.4133333333333333e-06,
      "loss": 0.0026,
      "step": 36440
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.3009941875934601,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 0.0041,
      "step": 36450
    },
    {
      "epoch": 1.9445333333333332,
      "grad_norm": 0.1018245667219162,
      "learning_rate": 1.3866666666666666e-06,
      "loss": 0.0017,
      "step": 36460
    },
    {
      "epoch": 1.9450666666666667,
      "grad_norm": 0.20753644406795502,
      "learning_rate": 1.3733333333333335e-06,
      "loss": 0.0017,
      "step": 36470
    },
    {
      "epoch": 1.9456,
      "grad_norm": 0.3276165723800659,
      "learning_rate": 1.36e-06,
      "loss": 0.0018,
      "step": 36480
    },
    {
      "epoch": 1.9461333333333335,
      "grad_norm": 0.21385522186756134,
      "learning_rate": 1.3466666666666668e-06,
      "loss": 0.0026,
      "step": 36490
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.1688264161348343,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.0025,
      "step": 36500
    },
    {
      "epoch": 1.9472,
      "grad_norm": 0.11756943166255951,
      "learning_rate": 1.32e-06,
      "loss": 0.0024,
      "step": 36510
    },
    {
      "epoch": 1.9477333333333333,
      "grad_norm": 0.05301747843623161,
      "learning_rate": 1.3066666666666667e-06,
      "loss": 0.0019,
      "step": 36520
    },
    {
      "epoch": 1.9482666666666666,
      "grad_norm": 0.13638466596603394,
      "learning_rate": 1.2933333333333334e-06,
      "loss": 0.0017,
      "step": 36530
    },
    {
      "epoch": 1.9487999999999999,
      "grad_norm": 0.3778843283653259,
      "learning_rate": 1.28e-06,
      "loss": 0.003,
      "step": 36540
    },
    {
      "epoch": 1.9493333333333334,
      "grad_norm": 0.08225943893194199,
      "learning_rate": 1.2666666666666667e-06,
      "loss": 0.0016,
      "step": 36550
    },
    {
      "epoch": 1.9498666666666666,
      "grad_norm": 0.21955184638500214,
      "learning_rate": 1.2533333333333335e-06,
      "loss": 0.0017,
      "step": 36560
    },
    {
      "epoch": 1.9504000000000001,
      "grad_norm": 0.15877673029899597,
      "learning_rate": 1.24e-06,
      "loss": 0.0018,
      "step": 36570
    },
    {
      "epoch": 1.9509333333333334,
      "grad_norm": 0.37601539492607117,
      "learning_rate": 1.2266666666666669e-06,
      "loss": 0.0024,
      "step": 36580
    },
    {
      "epoch": 1.9514666666666667,
      "grad_norm": 0.18734712898731232,
      "learning_rate": 1.2133333333333333e-06,
      "loss": 0.003,
      "step": 36590
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.11813561618328094,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 0.0028,
      "step": 36600
    },
    {
      "epoch": 1.9525333333333332,
      "grad_norm": 0.2931457459926605,
      "learning_rate": 1.1866666666666666e-06,
      "loss": 0.0027,
      "step": 36610
    },
    {
      "epoch": 1.9530666666666665,
      "grad_norm": 0.4148954749107361,
      "learning_rate": 1.1733333333333335e-06,
      "loss": 0.0023,
      "step": 36620
    },
    {
      "epoch": 1.9536,
      "grad_norm": 0.23846745491027832,
      "learning_rate": 1.16e-06,
      "loss": 0.0017,
      "step": 36630
    },
    {
      "epoch": 1.9541333333333335,
      "grad_norm": 0.09249766916036606,
      "learning_rate": 1.1466666666666668e-06,
      "loss": 0.003,
      "step": 36640
    },
    {
      "epoch": 1.9546666666666668,
      "grad_norm": 0.1745591163635254,
      "learning_rate": 1.1333333333333334e-06,
      "loss": 0.0025,
      "step": 36650
    },
    {
      "epoch": 1.9552,
      "grad_norm": 0.13622567057609558,
      "learning_rate": 1.12e-06,
      "loss": 0.0017,
      "step": 36660
    },
    {
      "epoch": 1.9557333333333333,
      "grad_norm": 0.11831498891115189,
      "learning_rate": 1.1066666666666667e-06,
      "loss": 0.002,
      "step": 36670
    },
    {
      "epoch": 1.9562666666666666,
      "grad_norm": 0.14277783036231995,
      "learning_rate": 1.0933333333333334e-06,
      "loss": 0.0024,
      "step": 36680
    },
    {
      "epoch": 1.9567999999999999,
      "grad_norm": 0.08651027828454971,
      "learning_rate": 1.08e-06,
      "loss": 0.0026,
      "step": 36690
    },
    {
      "epoch": 1.9573333333333334,
      "grad_norm": 0.1442374885082245,
      "learning_rate": 1.0666666666666667e-06,
      "loss": 0.0019,
      "step": 36700
    },
    {
      "epoch": 1.9578666666666666,
      "grad_norm": 0.11363013833761215,
      "learning_rate": 1.0533333333333333e-06,
      "loss": 0.0023,
      "step": 36710
    },
    {
      "epoch": 1.9584000000000001,
      "grad_norm": 0.09976915270090103,
      "learning_rate": 1.04e-06,
      "loss": 0.0021,
      "step": 36720
    },
    {
      "epoch": 1.9589333333333334,
      "grad_norm": 0.15293531119823456,
      "learning_rate": 1.0266666666666666e-06,
      "loss": 0.0034,
      "step": 36730
    },
    {
      "epoch": 1.9594666666666667,
      "grad_norm": 0.23334573209285736,
      "learning_rate": 1.0133333333333333e-06,
      "loss": 0.0015,
      "step": 36740
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.7826735973358154,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.0041,
      "step": 36750
    },
    {
      "epoch": 1.9605333333333332,
      "grad_norm": 0.13960397243499756,
      "learning_rate": 9.866666666666666e-07,
      "loss": 0.0043,
      "step": 36760
    },
    {
      "epoch": 1.9610666666666665,
      "grad_norm": 0.15406587719917297,
      "learning_rate": 9.733333333333335e-07,
      "loss": 0.0021,
      "step": 36770
    },
    {
      "epoch": 1.9616,
      "grad_norm": 0.095702163875103,
      "learning_rate": 9.6e-07,
      "loss": 0.0017,
      "step": 36780
    },
    {
      "epoch": 1.9621333333333333,
      "grad_norm": 0.2925557792186737,
      "learning_rate": 9.466666666666667e-07,
      "loss": 0.0026,
      "step": 36790
    },
    {
      "epoch": 1.9626666666666668,
      "grad_norm": 0.07865554094314575,
      "learning_rate": 9.333333333333334e-07,
      "loss": 0.0016,
      "step": 36800
    },
    {
      "epoch": 1.9632,
      "grad_norm": 0.17782267928123474,
      "learning_rate": 9.2e-07,
      "loss": 0.0024,
      "step": 36810
    },
    {
      "epoch": 1.9637333333333333,
      "grad_norm": 0.5439122319221497,
      "learning_rate": 9.066666666666667e-07,
      "loss": 0.0018,
      "step": 36820
    },
    {
      "epoch": 1.9642666666666666,
      "grad_norm": 0.10741595923900604,
      "learning_rate": 8.933333333333334e-07,
      "loss": 0.0026,
      "step": 36830
    },
    {
      "epoch": 1.9647999999999999,
      "grad_norm": 0.2604387402534485,
      "learning_rate": 8.8e-07,
      "loss": 0.0022,
      "step": 36840
    },
    {
      "epoch": 1.9653333333333334,
      "grad_norm": 0.07873410731554031,
      "learning_rate": 8.666666666666667e-07,
      "loss": 0.0019,
      "step": 36850
    },
    {
      "epoch": 1.9658666666666667,
      "grad_norm": 0.22003760933876038,
      "learning_rate": 8.533333333333335e-07,
      "loss": 0.0021,
      "step": 36860
    },
    {
      "epoch": 1.9664000000000001,
      "grad_norm": 0.12327150255441666,
      "learning_rate": 8.4e-07,
      "loss": 0.0018,
      "step": 36870
    },
    {
      "epoch": 1.9669333333333334,
      "grad_norm": 0.19588513672351837,
      "learning_rate": 8.266666666666668e-07,
      "loss": 0.0017,
      "step": 36880
    },
    {
      "epoch": 1.9674666666666667,
      "grad_norm": 0.35261934995651245,
      "learning_rate": 8.133333333333333e-07,
      "loss": 0.0019,
      "step": 36890
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.30954769253730774,
      "learning_rate": 8.000000000000001e-07,
      "loss": 0.002,
      "step": 36900
    },
    {
      "epoch": 1.9685333333333332,
      "grad_norm": 0.7761536240577698,
      "learning_rate": 7.866666666666666e-07,
      "loss": 0.0017,
      "step": 36910
    },
    {
      "epoch": 1.9690666666666665,
      "grad_norm": 0.09302379190921783,
      "learning_rate": 7.733333333333334e-07,
      "loss": 0.0025,
      "step": 36920
    },
    {
      "epoch": 1.9696,
      "grad_norm": 0.3800521194934845,
      "learning_rate": 7.6e-07,
      "loss": 0.0019,
      "step": 36930
    },
    {
      "epoch": 1.9701333333333333,
      "grad_norm": 0.28223514556884766,
      "learning_rate": 7.466666666666667e-07,
      "loss": 0.0027,
      "step": 36940
    },
    {
      "epoch": 1.9706666666666668,
      "grad_norm": 0.14411966502666473,
      "learning_rate": 7.333333333333333e-07,
      "loss": 0.0018,
      "step": 36950
    },
    {
      "epoch": 1.9712,
      "grad_norm": 0.22856739163398743,
      "learning_rate": 7.2e-07,
      "loss": 0.0017,
      "step": 36960
    },
    {
      "epoch": 1.9717333333333333,
      "grad_norm": 0.09259363263845444,
      "learning_rate": 7.066666666666666e-07,
      "loss": 0.0028,
      "step": 36970
    },
    {
      "epoch": 1.9722666666666666,
      "grad_norm": 0.36118417978286743,
      "learning_rate": 6.933333333333333e-07,
      "loss": 0.0026,
      "step": 36980
    },
    {
      "epoch": 1.9727999999999999,
      "grad_norm": 0.4771713316440582,
      "learning_rate": 6.8e-07,
      "loss": 0.0021,
      "step": 36990
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.23092231154441833,
      "learning_rate": 6.666666666666667e-07,
      "loss": 0.0032,
      "step": 37000
    }
  ],
  "logging_steps": 10,
  "max_steps": 37500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 40,
  "trial_name": null,
  "trial_params": null
}

{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.9733333333333336,
  "eval_steps": 500,
  "global_step": 74500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0005333333333333334,
      "grad_norm": 0.6908137798309326,
      "learning_rate": 4.9993333333333335e-05,
      "loss": 0.0027,
      "step": 10
    },
    {
      "epoch": 0.0010666666666666667,
      "grad_norm": 0.6026488542556763,
      "learning_rate": 4.9986666666666674e-05,
      "loss": 0.0041,
      "step": 20
    },
    {
      "epoch": 0.0016,
      "grad_norm": 0.2995379567146301,
      "learning_rate": 4.9980000000000006e-05,
      "loss": 0.0045,
      "step": 30
    },
    {
      "epoch": 0.0021333333333333334,
      "grad_norm": 0.3347100615501404,
      "learning_rate": 4.997333333333333e-05,
      "loss": 0.0049,
      "step": 40
    },
    {
      "epoch": 0.0026666666666666666,
      "grad_norm": 0.2978513240814209,
      "learning_rate": 4.996666666666667e-05,
      "loss": 0.0043,
      "step": 50
    },
    {
      "epoch": 0.0032,
      "grad_norm": 0.3630513548851013,
      "learning_rate": 4.996e-05,
      "loss": 0.0051,
      "step": 60
    },
    {
      "epoch": 0.0037333333333333333,
      "grad_norm": 0.3230796158313751,
      "learning_rate": 4.9953333333333335e-05,
      "loss": 0.0044,
      "step": 70
    },
    {
      "epoch": 0.004266666666666667,
      "grad_norm": 0.5056726932525635,
      "learning_rate": 4.994666666666667e-05,
      "loss": 0.0034,
      "step": 80
    },
    {
      "epoch": 0.0048,
      "grad_norm": 0.3384779393672943,
      "learning_rate": 4.9940000000000006e-05,
      "loss": 0.0041,
      "step": 90
    },
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 0.24424296617507935,
      "learning_rate": 4.993333333333334e-05,
      "loss": 0.0044,
      "step": 100
    },
    {
      "epoch": 0.005866666666666667,
      "grad_norm": 0.2572211027145386,
      "learning_rate": 4.992666666666667e-05,
      "loss": 0.0045,
      "step": 110
    },
    {
      "epoch": 0.0064,
      "grad_norm": 0.6537948846817017,
      "learning_rate": 4.992e-05,
      "loss": 0.0044,
      "step": 120
    },
    {
      "epoch": 0.006933333333333333,
      "grad_norm": 0.632026731967926,
      "learning_rate": 4.9913333333333335e-05,
      "loss": 0.0048,
      "step": 130
    },
    {
      "epoch": 0.007466666666666667,
      "grad_norm": 0.3497622013092041,
      "learning_rate": 4.990666666666667e-05,
      "loss": 0.0032,
      "step": 140
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.5543965101242065,
      "learning_rate": 4.99e-05,
      "loss": 0.003,
      "step": 150
    },
    {
      "epoch": 0.008533333333333334,
      "grad_norm": 0.4155106544494629,
      "learning_rate": 4.989333333333334e-05,
      "loss": 0.0035,
      "step": 160
    },
    {
      "epoch": 0.009066666666666667,
      "grad_norm": 0.4926118850708008,
      "learning_rate": 4.988666666666667e-05,
      "loss": 0.0032,
      "step": 170
    },
    {
      "epoch": 0.0096,
      "grad_norm": 0.5908145904541016,
      "learning_rate": 4.9880000000000004e-05,
      "loss": 0.0045,
      "step": 180
    },
    {
      "epoch": 0.010133333333333333,
      "grad_norm": 0.5135098695755005,
      "learning_rate": 4.9873333333333336e-05,
      "loss": 0.0035,
      "step": 190
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 0.6706601977348328,
      "learning_rate": 4.986666666666667e-05,
      "loss": 0.0036,
      "step": 200
    },
    {
      "epoch": 0.0112,
      "grad_norm": 0.6976452469825745,
      "learning_rate": 4.986e-05,
      "loss": 0.003,
      "step": 210
    },
    {
      "epoch": 0.011733333333333333,
      "grad_norm": 0.46379783749580383,
      "learning_rate": 4.985333333333333e-05,
      "loss": 0.0034,
      "step": 220
    },
    {
      "epoch": 0.012266666666666667,
      "grad_norm": 0.17075732350349426,
      "learning_rate": 4.984666666666667e-05,
      "loss": 0.0047,
      "step": 230
    },
    {
      "epoch": 0.0128,
      "grad_norm": 0.6962354183197021,
      "learning_rate": 4.9840000000000004e-05,
      "loss": 0.004,
      "step": 240
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.33678552508354187,
      "learning_rate": 4.9833333333333336e-05,
      "loss": 0.0032,
      "step": 250
    },
    {
      "epoch": 0.013866666666666666,
      "grad_norm": 0.8293354511260986,
      "learning_rate": 4.982666666666667e-05,
      "loss": 0.0038,
      "step": 260
    },
    {
      "epoch": 0.0144,
      "grad_norm": 0.3188028931617737,
      "learning_rate": 4.982e-05,
      "loss": 0.0033,
      "step": 270
    },
    {
      "epoch": 0.014933333333333333,
      "grad_norm": 0.8215818405151367,
      "learning_rate": 4.981333333333333e-05,
      "loss": 0.0027,
      "step": 280
    },
    {
      "epoch": 0.015466666666666667,
      "grad_norm": 0.6390659213066101,
      "learning_rate": 4.9806666666666665e-05,
      "loss": 0.0032,
      "step": 290
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.35627079010009766,
      "learning_rate": 4.9800000000000004e-05,
      "loss": 0.0024,
      "step": 300
    },
    {
      "epoch": 0.016533333333333334,
      "grad_norm": 0.2399812787771225,
      "learning_rate": 4.9793333333333337e-05,
      "loss": 0.003,
      "step": 310
    },
    {
      "epoch": 0.017066666666666667,
      "grad_norm": 0.24113783240318298,
      "learning_rate": 4.978666666666667e-05,
      "loss": 0.0032,
      "step": 320
    },
    {
      "epoch": 0.0176,
      "grad_norm": 0.6551643013954163,
      "learning_rate": 4.978e-05,
      "loss": 0.003,
      "step": 330
    },
    {
      "epoch": 0.018133333333333335,
      "grad_norm": 0.21069009602069855,
      "learning_rate": 4.977333333333334e-05,
      "loss": 0.0036,
      "step": 340
    },
    {
      "epoch": 0.018666666666666668,
      "grad_norm": 0.21163438260555267,
      "learning_rate": 4.9766666666666666e-05,
      "loss": 0.0028,
      "step": 350
    },
    {
      "epoch": 0.0192,
      "grad_norm": 0.4889642000198364,
      "learning_rate": 4.976e-05,
      "loss": 0.0041,
      "step": 360
    },
    {
      "epoch": 0.019733333333333332,
      "grad_norm": 0.39538708329200745,
      "learning_rate": 4.975333333333334e-05,
      "loss": 0.0028,
      "step": 370
    },
    {
      "epoch": 0.020266666666666665,
      "grad_norm": 0.3661622703075409,
      "learning_rate": 4.974666666666667e-05,
      "loss": 0.0029,
      "step": 380
    },
    {
      "epoch": 0.0208,
      "grad_norm": 0.1663849651813507,
      "learning_rate": 4.974e-05,
      "loss": 0.0044,
      "step": 390
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 0.751911997795105,
      "learning_rate": 4.973333333333334e-05,
      "loss": 0.0026,
      "step": 400
    },
    {
      "epoch": 0.021866666666666666,
      "grad_norm": 0.3986719846725464,
      "learning_rate": 4.972666666666667e-05,
      "loss": 0.0029,
      "step": 410
    },
    {
      "epoch": 0.0224,
      "grad_norm": 0.20153416693210602,
      "learning_rate": 4.972e-05,
      "loss": 0.0033,
      "step": 420
    },
    {
      "epoch": 0.022933333333333333,
      "grad_norm": 0.3842357397079468,
      "learning_rate": 4.971333333333334e-05,
      "loss": 0.0033,
      "step": 430
    },
    {
      "epoch": 0.023466666666666667,
      "grad_norm": 0.6367905139923096,
      "learning_rate": 4.970666666666667e-05,
      "loss": 0.0032,
      "step": 440
    },
    {
      "epoch": 0.024,
      "grad_norm": 0.30844953656196594,
      "learning_rate": 4.97e-05,
      "loss": 0.0026,
      "step": 450
    },
    {
      "epoch": 0.024533333333333334,
      "grad_norm": 0.419351190328598,
      "learning_rate": 4.9693333333333334e-05,
      "loss": 0.003,
      "step": 460
    },
    {
      "epoch": 0.025066666666666668,
      "grad_norm": 0.8639441132545471,
      "learning_rate": 4.968666666666667e-05,
      "loss": 0.0029,
      "step": 470
    },
    {
      "epoch": 0.0256,
      "grad_norm": 0.6615460515022278,
      "learning_rate": 4.9680000000000005e-05,
      "loss": 0.0031,
      "step": 480
    },
    {
      "epoch": 0.026133333333333335,
      "grad_norm": 0.2932450473308563,
      "learning_rate": 4.967333333333334e-05,
      "loss": 0.0033,
      "step": 490
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.3454500436782837,
      "learning_rate": 4.966666666666667e-05,
      "loss": 0.0024,
      "step": 500
    },
    {
      "epoch": 0.0272,
      "grad_norm": 0.24611781537532806,
      "learning_rate": 4.966e-05,
      "loss": 0.0042,
      "step": 510
    },
    {
      "epoch": 0.027733333333333332,
      "grad_norm": 0.8653075695037842,
      "learning_rate": 4.9653333333333335e-05,
      "loss": 0.0025,
      "step": 520
    },
    {
      "epoch": 0.028266666666666666,
      "grad_norm": 0.21464039385318756,
      "learning_rate": 4.964666666666667e-05,
      "loss": 0.0026,
      "step": 530
    },
    {
      "epoch": 0.0288,
      "grad_norm": 0.7412664294242859,
      "learning_rate": 4.9640000000000006e-05,
      "loss": 0.0046,
      "step": 540
    },
    {
      "epoch": 0.029333333333333333,
      "grad_norm": 0.9622895121574402,
      "learning_rate": 4.963333333333334e-05,
      "loss": 0.0038,
      "step": 550
    },
    {
      "epoch": 0.029866666666666666,
      "grad_norm": 0.7880212664604187,
      "learning_rate": 4.962666666666667e-05,
      "loss": 0.0028,
      "step": 560
    },
    {
      "epoch": 0.0304,
      "grad_norm": 0.6453324556350708,
      "learning_rate": 4.962e-05,
      "loss": 0.0039,
      "step": 570
    },
    {
      "epoch": 0.030933333333333334,
      "grad_norm": 0.286281555891037,
      "learning_rate": 4.9613333333333335e-05,
      "loss": 0.0033,
      "step": 580
    },
    {
      "epoch": 0.031466666666666664,
      "grad_norm": 0.7326911687850952,
      "learning_rate": 4.960666666666667e-05,
      "loss": 0.0027,
      "step": 590
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.15093502402305603,
      "learning_rate": 4.96e-05,
      "loss": 0.0031,
      "step": 600
    },
    {
      "epoch": 0.03253333333333333,
      "grad_norm": 0.5348328948020935,
      "learning_rate": 4.959333333333334e-05,
      "loss": 0.0039,
      "step": 610
    },
    {
      "epoch": 0.03306666666666667,
      "grad_norm": 0.48072224855422974,
      "learning_rate": 4.958666666666667e-05,
      "loss": 0.0038,
      "step": 620
    },
    {
      "epoch": 0.0336,
      "grad_norm": 0.3155007064342499,
      "learning_rate": 4.958e-05,
      "loss": 0.003,
      "step": 630
    },
    {
      "epoch": 0.034133333333333335,
      "grad_norm": 0.6224960088729858,
      "learning_rate": 4.9573333333333335e-05,
      "loss": 0.0036,
      "step": 640
    },
    {
      "epoch": 0.034666666666666665,
      "grad_norm": 0.6405271291732788,
      "learning_rate": 4.956666666666667e-05,
      "loss": 0.0054,
      "step": 650
    },
    {
      "epoch": 0.0352,
      "grad_norm": 0.46752721071243286,
      "learning_rate": 4.956e-05,
      "loss": 0.0041,
      "step": 660
    },
    {
      "epoch": 0.03573333333333333,
      "grad_norm": 0.6641970276832581,
      "learning_rate": 4.955333333333333e-05,
      "loss": 0.0029,
      "step": 670
    },
    {
      "epoch": 0.03626666666666667,
      "grad_norm": 0.10912565141916275,
      "learning_rate": 4.954666666666667e-05,
      "loss": 0.0029,
      "step": 680
    },
    {
      "epoch": 0.0368,
      "grad_norm": 1.0019599199295044,
      "learning_rate": 4.9540000000000003e-05,
      "loss": 0.006,
      "step": 690
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 1.0155121088027954,
      "learning_rate": 4.9533333333333336e-05,
      "loss": 0.0044,
      "step": 700
    },
    {
      "epoch": 0.037866666666666667,
      "grad_norm": 0.27186423540115356,
      "learning_rate": 4.952666666666667e-05,
      "loss": 0.0035,
      "step": 710
    },
    {
      "epoch": 0.0384,
      "grad_norm": 0.32357585430145264,
      "learning_rate": 4.952e-05,
      "loss": 0.0031,
      "step": 720
    },
    {
      "epoch": 0.038933333333333334,
      "grad_norm": 0.17599672079086304,
      "learning_rate": 4.951333333333333e-05,
      "loss": 0.0031,
      "step": 730
    },
    {
      "epoch": 0.039466666666666664,
      "grad_norm": 0.430574893951416,
      "learning_rate": 4.9506666666666665e-05,
      "loss": 0.0035,
      "step": 740
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.43517014384269714,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 0.0026,
      "step": 750
    },
    {
      "epoch": 0.04053333333333333,
      "grad_norm": 0.496162474155426,
      "learning_rate": 4.9493333333333336e-05,
      "loss": 0.0024,
      "step": 760
    },
    {
      "epoch": 0.04106666666666667,
      "grad_norm": 0.5434445738792419,
      "learning_rate": 4.948666666666667e-05,
      "loss": 0.0022,
      "step": 770
    },
    {
      "epoch": 0.0416,
      "grad_norm": 0.20254111289978027,
      "learning_rate": 4.948000000000001e-05,
      "loss": 0.0028,
      "step": 780
    },
    {
      "epoch": 0.042133333333333335,
      "grad_norm": 0.45046719908714294,
      "learning_rate": 4.947333333333334e-05,
      "loss": 0.0031,
      "step": 790
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 0.44899290800094604,
      "learning_rate": 4.9466666666666665e-05,
      "loss": 0.0025,
      "step": 800
    },
    {
      "epoch": 0.0432,
      "grad_norm": 0.4810263514518738,
      "learning_rate": 4.946e-05,
      "loss": 0.0038,
      "step": 810
    },
    {
      "epoch": 0.04373333333333333,
      "grad_norm": 0.48361000418663025,
      "learning_rate": 4.9453333333333336e-05,
      "loss": 0.0048,
      "step": 820
    },
    {
      "epoch": 0.04426666666666667,
      "grad_norm": 0.4057503342628479,
      "learning_rate": 4.944666666666667e-05,
      "loss": 0.0025,
      "step": 830
    },
    {
      "epoch": 0.0448,
      "grad_norm": 0.9181573987007141,
      "learning_rate": 4.944e-05,
      "loss": 0.0037,
      "step": 840
    },
    {
      "epoch": 0.04533333333333334,
      "grad_norm": 1.0838991403579712,
      "learning_rate": 4.943333333333334e-05,
      "loss": 0.0031,
      "step": 850
    },
    {
      "epoch": 0.04586666666666667,
      "grad_norm": 0.15900260210037231,
      "learning_rate": 4.942666666666667e-05,
      "loss": 0.0025,
      "step": 860
    },
    {
      "epoch": 0.0464,
      "grad_norm": 0.5431777834892273,
      "learning_rate": 4.942e-05,
      "loss": 0.0028,
      "step": 870
    },
    {
      "epoch": 0.046933333333333334,
      "grad_norm": 0.22223612666130066,
      "learning_rate": 4.941333333333334e-05,
      "loss": 0.0023,
      "step": 880
    },
    {
      "epoch": 0.047466666666666664,
      "grad_norm": 0.5498281121253967,
      "learning_rate": 4.940666666666667e-05,
      "loss": 0.0048,
      "step": 890
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.16812454164028168,
      "learning_rate": 4.94e-05,
      "loss": 0.0036,
      "step": 900
    },
    {
      "epoch": 0.04853333333333333,
      "grad_norm": 0.36686649918556213,
      "learning_rate": 4.9393333333333334e-05,
      "loss": 0.0024,
      "step": 910
    },
    {
      "epoch": 0.04906666666666667,
      "grad_norm": 0.8192600607872009,
      "learning_rate": 4.938666666666667e-05,
      "loss": 0.0032,
      "step": 920
    },
    {
      "epoch": 0.0496,
      "grad_norm": 0.35299620032310486,
      "learning_rate": 4.9380000000000005e-05,
      "loss": 0.0027,
      "step": 930
    },
    {
      "epoch": 0.050133333333333335,
      "grad_norm": 0.3533863425254822,
      "learning_rate": 4.937333333333334e-05,
      "loss": 0.0032,
      "step": 940
    },
    {
      "epoch": 0.050666666666666665,
      "grad_norm": 0.15511511266231537,
      "learning_rate": 4.936666666666667e-05,
      "loss": 0.0022,
      "step": 950
    },
    {
      "epoch": 0.0512,
      "grad_norm": 0.2283962219953537,
      "learning_rate": 4.936e-05,
      "loss": 0.0044,
      "step": 960
    },
    {
      "epoch": 0.05173333333333333,
      "grad_norm": 0.17462404072284698,
      "learning_rate": 4.9353333333333334e-05,
      "loss": 0.0036,
      "step": 970
    },
    {
      "epoch": 0.05226666666666667,
      "grad_norm": 0.24438650906085968,
      "learning_rate": 4.9346666666666666e-05,
      "loss": 0.0041,
      "step": 980
    },
    {
      "epoch": 0.0528,
      "grad_norm": 0.12954658269882202,
      "learning_rate": 4.9340000000000005e-05,
      "loss": 0.0027,
      "step": 990
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.4287966787815094,
      "learning_rate": 4.933333333333334e-05,
      "loss": 0.0036,
      "step": 1000
    },
    {
      "epoch": 0.05386666666666667,
      "grad_norm": 0.18894343078136444,
      "learning_rate": 4.932666666666667e-05,
      "loss": 0.0031,
      "step": 1010
    },
    {
      "epoch": 0.0544,
      "grad_norm": 0.18495452404022217,
      "learning_rate": 4.932e-05,
      "loss": 0.0037,
      "step": 1020
    },
    {
      "epoch": 0.054933333333333334,
      "grad_norm": 0.43049827218055725,
      "learning_rate": 4.9313333333333334e-05,
      "loss": 0.0028,
      "step": 1030
    },
    {
      "epoch": 0.055466666666666664,
      "grad_norm": 0.716175377368927,
      "learning_rate": 4.930666666666667e-05,
      "loss": 0.0031,
      "step": 1040
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.6954548954963684,
      "learning_rate": 4.93e-05,
      "loss": 0.0024,
      "step": 1050
    },
    {
      "epoch": 0.05653333333333333,
      "grad_norm": 0.25487449765205383,
      "learning_rate": 4.929333333333334e-05,
      "loss": 0.0032,
      "step": 1060
    },
    {
      "epoch": 0.05706666666666667,
      "grad_norm": 0.13902728259563446,
      "learning_rate": 4.928666666666667e-05,
      "loss": 0.0022,
      "step": 1070
    },
    {
      "epoch": 0.0576,
      "grad_norm": 0.5909250974655151,
      "learning_rate": 4.928e-05,
      "loss": 0.003,
      "step": 1080
    },
    {
      "epoch": 0.058133333333333335,
      "grad_norm": 0.3086582124233246,
      "learning_rate": 4.9273333333333335e-05,
      "loss": 0.0035,
      "step": 1090
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 0.3228115439414978,
      "learning_rate": 4.926666666666667e-05,
      "loss": 0.0036,
      "step": 1100
    },
    {
      "epoch": 0.0592,
      "grad_norm": 0.13484124839305878,
      "learning_rate": 4.926e-05,
      "loss": 0.0031,
      "step": 1110
    },
    {
      "epoch": 0.05973333333333333,
      "grad_norm": 0.09703236818313599,
      "learning_rate": 4.925333333333333e-05,
      "loss": 0.0026,
      "step": 1120
    },
    {
      "epoch": 0.06026666666666667,
      "grad_norm": 0.44571760296821594,
      "learning_rate": 4.924666666666667e-05,
      "loss": 0.0025,
      "step": 1130
    },
    {
      "epoch": 0.0608,
      "grad_norm": 0.5164414644241333,
      "learning_rate": 4.924e-05,
      "loss": 0.0021,
      "step": 1140
    },
    {
      "epoch": 0.06133333333333333,
      "grad_norm": 0.5577749609947205,
      "learning_rate": 4.9233333333333335e-05,
      "loss": 0.0021,
      "step": 1150
    },
    {
      "epoch": 0.06186666666666667,
      "grad_norm": 0.2984468936920166,
      "learning_rate": 4.9226666666666674e-05,
      "loss": 0.0025,
      "step": 1160
    },
    {
      "epoch": 0.0624,
      "grad_norm": 0.390328586101532,
      "learning_rate": 4.9220000000000006e-05,
      "loss": 0.0026,
      "step": 1170
    },
    {
      "epoch": 0.06293333333333333,
      "grad_norm": 0.5768676400184631,
      "learning_rate": 4.921333333333333e-05,
      "loss": 0.0023,
      "step": 1180
    },
    {
      "epoch": 0.06346666666666667,
      "grad_norm": 0.43872717022895813,
      "learning_rate": 4.9206666666666664e-05,
      "loss": 0.0028,
      "step": 1190
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.19695183634757996,
      "learning_rate": 4.92e-05,
      "loss": 0.0039,
      "step": 1200
    },
    {
      "epoch": 0.06453333333333333,
      "grad_norm": 0.19623211026191711,
      "learning_rate": 4.9193333333333336e-05,
      "loss": 0.0023,
      "step": 1210
    },
    {
      "epoch": 0.06506666666666666,
      "grad_norm": 0.2098476141691208,
      "learning_rate": 4.918666666666667e-05,
      "loss": 0.0039,
      "step": 1220
    },
    {
      "epoch": 0.0656,
      "grad_norm": 0.277245432138443,
      "learning_rate": 4.918000000000001e-05,
      "loss": 0.0042,
      "step": 1230
    },
    {
      "epoch": 0.06613333333333334,
      "grad_norm": 0.6626431941986084,
      "learning_rate": 4.917333333333334e-05,
      "loss": 0.003,
      "step": 1240
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.21307794749736786,
      "learning_rate": 4.9166666666666665e-05,
      "loss": 0.0033,
      "step": 1250
    },
    {
      "epoch": 0.0672,
      "grad_norm": 0.23177997767925262,
      "learning_rate": 4.9160000000000004e-05,
      "loss": 0.0028,
      "step": 1260
    },
    {
      "epoch": 0.06773333333333334,
      "grad_norm": 0.33486926555633545,
      "learning_rate": 4.9153333333333336e-05,
      "loss": 0.0047,
      "step": 1270
    },
    {
      "epoch": 0.06826666666666667,
      "grad_norm": 0.15891771018505096,
      "learning_rate": 4.914666666666667e-05,
      "loss": 0.0042,
      "step": 1280
    },
    {
      "epoch": 0.0688,
      "grad_norm": 0.25056302547454834,
      "learning_rate": 4.914e-05,
      "loss": 0.004,
      "step": 1290
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 0.33007338643074036,
      "learning_rate": 4.913333333333334e-05,
      "loss": 0.0024,
      "step": 1300
    },
    {
      "epoch": 0.06986666666666666,
      "grad_norm": 0.23983973264694214,
      "learning_rate": 4.912666666666667e-05,
      "loss": 0.0029,
      "step": 1310
    },
    {
      "epoch": 0.0704,
      "grad_norm": 0.43846753239631653,
      "learning_rate": 4.9120000000000004e-05,
      "loss": 0.0039,
      "step": 1320
    },
    {
      "epoch": 0.07093333333333333,
      "grad_norm": 0.4106719493865967,
      "learning_rate": 4.9113333333333336e-05,
      "loss": 0.0031,
      "step": 1330
    },
    {
      "epoch": 0.07146666666666666,
      "grad_norm": 0.3610917627811432,
      "learning_rate": 4.910666666666667e-05,
      "loss": 0.0031,
      "step": 1340
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.2585710883140564,
      "learning_rate": 4.91e-05,
      "loss": 0.0022,
      "step": 1350
    },
    {
      "epoch": 0.07253333333333334,
      "grad_norm": 0.6462547183036804,
      "learning_rate": 4.909333333333333e-05,
      "loss": 0.003,
      "step": 1360
    },
    {
      "epoch": 0.07306666666666667,
      "grad_norm": 0.5511786341667175,
      "learning_rate": 4.908666666666667e-05,
      "loss": 0.0026,
      "step": 1370
    },
    {
      "epoch": 0.0736,
      "grad_norm": 0.6630218029022217,
      "learning_rate": 4.9080000000000004e-05,
      "loss": 0.0025,
      "step": 1380
    },
    {
      "epoch": 0.07413333333333333,
      "grad_norm": 0.27795398235321045,
      "learning_rate": 4.907333333333334e-05,
      "loss": 0.0022,
      "step": 1390
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 0.34374889731407166,
      "learning_rate": 4.906666666666667e-05,
      "loss": 0.0043,
      "step": 1400
    },
    {
      "epoch": 0.0752,
      "grad_norm": 0.5389921069145203,
      "learning_rate": 4.906e-05,
      "loss": 0.0031,
      "step": 1410
    },
    {
      "epoch": 0.07573333333333333,
      "grad_norm": 0.9063681960105896,
      "learning_rate": 4.9053333333333333e-05,
      "loss": 0.0037,
      "step": 1420
    },
    {
      "epoch": 0.07626666666666666,
      "grad_norm": 0.3176506459712982,
      "learning_rate": 4.9046666666666666e-05,
      "loss": 0.0042,
      "step": 1430
    },
    {
      "epoch": 0.0768,
      "grad_norm": 0.23215679824352264,
      "learning_rate": 4.9040000000000005e-05,
      "loss": 0.0035,
      "step": 1440
    },
    {
      "epoch": 0.07733333333333334,
      "grad_norm": 0.347090482711792,
      "learning_rate": 4.903333333333334e-05,
      "loss": 0.0027,
      "step": 1450
    },
    {
      "epoch": 0.07786666666666667,
      "grad_norm": 0.1710996925830841,
      "learning_rate": 4.902666666666667e-05,
      "loss": 0.0027,
      "step": 1460
    },
    {
      "epoch": 0.0784,
      "grad_norm": 0.5213346481323242,
      "learning_rate": 4.902e-05,
      "loss": 0.0033,
      "step": 1470
    },
    {
      "epoch": 0.07893333333333333,
      "grad_norm": 0.5329596996307373,
      "learning_rate": 4.9013333333333334e-05,
      "loss": 0.0039,
      "step": 1480
    },
    {
      "epoch": 0.07946666666666667,
      "grad_norm": 0.262925386428833,
      "learning_rate": 4.9006666666666666e-05,
      "loss": 0.0033,
      "step": 1490
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.26970288157463074,
      "learning_rate": 4.9e-05,
      "loss": 0.0039,
      "step": 1500
    },
    {
      "epoch": 0.08053333333333333,
      "grad_norm": 0.6190519332885742,
      "learning_rate": 4.899333333333334e-05,
      "loss": 0.0024,
      "step": 1510
    },
    {
      "epoch": 0.08106666666666666,
      "grad_norm": 0.6892129778862,
      "learning_rate": 4.898666666666667e-05,
      "loss": 0.0031,
      "step": 1520
    },
    {
      "epoch": 0.0816,
      "grad_norm": 0.4891677796840668,
      "learning_rate": 4.898e-05,
      "loss": 0.0035,
      "step": 1530
    },
    {
      "epoch": 0.08213333333333334,
      "grad_norm": 0.4798769950866699,
      "learning_rate": 4.897333333333334e-05,
      "loss": 0.0033,
      "step": 1540
    },
    {
      "epoch": 0.08266666666666667,
      "grad_norm": 0.45802539587020874,
      "learning_rate": 4.8966666666666667e-05,
      "loss": 0.0033,
      "step": 1550
    },
    {
      "epoch": 0.0832,
      "grad_norm": 0.8281010389328003,
      "learning_rate": 4.896e-05,
      "loss": 0.0039,
      "step": 1560
    },
    {
      "epoch": 0.08373333333333334,
      "grad_norm": 1.1078095436096191,
      "learning_rate": 4.895333333333333e-05,
      "loss": 0.0035,
      "step": 1570
    },
    {
      "epoch": 0.08426666666666667,
      "grad_norm": 1.129035234451294,
      "learning_rate": 4.894666666666667e-05,
      "loss": 0.0033,
      "step": 1580
    },
    {
      "epoch": 0.0848,
      "grad_norm": 0.1949368119239807,
      "learning_rate": 4.894e-05,
      "loss": 0.003,
      "step": 1590
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 0.23646919429302216,
      "learning_rate": 4.8933333333333335e-05,
      "loss": 0.0028,
      "step": 1600
    },
    {
      "epoch": 0.08586666666666666,
      "grad_norm": 0.26629161834716797,
      "learning_rate": 4.8926666666666674e-05,
      "loss": 0.0022,
      "step": 1610
    },
    {
      "epoch": 0.0864,
      "grad_norm": 0.3085705637931824,
      "learning_rate": 4.8920000000000006e-05,
      "loss": 0.0037,
      "step": 1620
    },
    {
      "epoch": 0.08693333333333333,
      "grad_norm": 0.24770833551883698,
      "learning_rate": 4.891333333333333e-05,
      "loss": 0.0035,
      "step": 1630
    },
    {
      "epoch": 0.08746666666666666,
      "grad_norm": 0.2832576036453247,
      "learning_rate": 4.890666666666667e-05,
      "loss": 0.0022,
      "step": 1640
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.3704847991466522,
      "learning_rate": 4.89e-05,
      "loss": 0.0034,
      "step": 1650
    },
    {
      "epoch": 0.08853333333333334,
      "grad_norm": 0.19836287200450897,
      "learning_rate": 4.8893333333333335e-05,
      "loss": 0.0025,
      "step": 1660
    },
    {
      "epoch": 0.08906666666666667,
      "grad_norm": 0.7088045477867126,
      "learning_rate": 4.888666666666667e-05,
      "loss": 0.003,
      "step": 1670
    },
    {
      "epoch": 0.0896,
      "grad_norm": 0.4408802092075348,
      "learning_rate": 4.8880000000000006e-05,
      "loss": 0.0026,
      "step": 1680
    },
    {
      "epoch": 0.09013333333333333,
      "grad_norm": 0.10854864120483398,
      "learning_rate": 4.887333333333334e-05,
      "loss": 0.0032,
      "step": 1690
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 0.800971269607544,
      "learning_rate": 4.886666666666667e-05,
      "loss": 0.0029,
      "step": 1700
    },
    {
      "epoch": 0.0912,
      "grad_norm": 0.42659011483192444,
      "learning_rate": 4.886e-05,
      "loss": 0.0045,
      "step": 1710
    },
    {
      "epoch": 0.09173333333333333,
      "grad_norm": 0.36800283193588257,
      "learning_rate": 4.8853333333333335e-05,
      "loss": 0.0028,
      "step": 1720
    },
    {
      "epoch": 0.09226666666666666,
      "grad_norm": 0.2959628403186798,
      "learning_rate": 4.884666666666667e-05,
      "loss": 0.0026,
      "step": 1730
    },
    {
      "epoch": 0.0928,
      "grad_norm": 0.3637312948703766,
      "learning_rate": 4.884e-05,
      "loss": 0.0026,
      "step": 1740
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.19197984039783478,
      "learning_rate": 4.883333333333334e-05,
      "loss": 0.0034,
      "step": 1750
    },
    {
      "epoch": 0.09386666666666667,
      "grad_norm": 0.1738472878932953,
      "learning_rate": 4.882666666666667e-05,
      "loss": 0.0034,
      "step": 1760
    },
    {
      "epoch": 0.0944,
      "grad_norm": 0.2502989172935486,
      "learning_rate": 4.8820000000000004e-05,
      "loss": 0.0028,
      "step": 1770
    },
    {
      "epoch": 0.09493333333333333,
      "grad_norm": 0.22265277802944183,
      "learning_rate": 4.8813333333333336e-05,
      "loss": 0.0024,
      "step": 1780
    },
    {
      "epoch": 0.09546666666666667,
      "grad_norm": 0.15893857181072235,
      "learning_rate": 4.880666666666667e-05,
      "loss": 0.003,
      "step": 1790
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.570586621761322,
      "learning_rate": 4.88e-05,
      "loss": 0.0036,
      "step": 1800
    },
    {
      "epoch": 0.09653333333333333,
      "grad_norm": 0.37424802780151367,
      "learning_rate": 4.879333333333333e-05,
      "loss": 0.0028,
      "step": 1810
    },
    {
      "epoch": 0.09706666666666666,
      "grad_norm": 0.3725544810295105,
      "learning_rate": 4.878666666666667e-05,
      "loss": 0.0025,
      "step": 1820
    },
    {
      "epoch": 0.0976,
      "grad_norm": 0.3808002173900604,
      "learning_rate": 4.8780000000000004e-05,
      "loss": 0.0035,
      "step": 1830
    },
    {
      "epoch": 0.09813333333333334,
      "grad_norm": 0.8292204737663269,
      "learning_rate": 4.8773333333333336e-05,
      "loss": 0.0028,
      "step": 1840
    },
    {
      "epoch": 0.09866666666666667,
      "grad_norm": 0.22059687972068787,
      "learning_rate": 4.876666666666667e-05,
      "loss": 0.0023,
      "step": 1850
    },
    {
      "epoch": 0.0992,
      "grad_norm": 0.420265257358551,
      "learning_rate": 4.876e-05,
      "loss": 0.0024,
      "step": 1860
    },
    {
      "epoch": 0.09973333333333333,
      "grad_norm": 0.9178813099861145,
      "learning_rate": 4.875333333333333e-05,
      "loss": 0.0033,
      "step": 1870
    },
    {
      "epoch": 0.10026666666666667,
      "grad_norm": 0.1835985630750656,
      "learning_rate": 4.8746666666666665e-05,
      "loss": 0.0027,
      "step": 1880
    },
    {
      "epoch": 0.1008,
      "grad_norm": 0.7501837015151978,
      "learning_rate": 4.8740000000000004e-05,
      "loss": 0.0025,
      "step": 1890
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 0.6023700833320618,
      "learning_rate": 4.8733333333333337e-05,
      "loss": 0.0037,
      "step": 1900
    },
    {
      "epoch": 0.10186666666666666,
      "grad_norm": 0.11912760138511658,
      "learning_rate": 4.872666666666667e-05,
      "loss": 0.0031,
      "step": 1910
    },
    {
      "epoch": 0.1024,
      "grad_norm": 0.28362494707107544,
      "learning_rate": 4.872000000000001e-05,
      "loss": 0.0042,
      "step": 1920
    },
    {
      "epoch": 0.10293333333333334,
      "grad_norm": 0.3117889165878296,
      "learning_rate": 4.871333333333333e-05,
      "loss": 0.0039,
      "step": 1930
    },
    {
      "epoch": 0.10346666666666667,
      "grad_norm": 0.9477078318595886,
      "learning_rate": 4.8706666666666666e-05,
      "loss": 0.0029,
      "step": 1940
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.5847869515419006,
      "learning_rate": 4.87e-05,
      "loss": 0.0026,
      "step": 1950
    },
    {
      "epoch": 0.10453333333333334,
      "grad_norm": 0.2650566101074219,
      "learning_rate": 4.869333333333334e-05,
      "loss": 0.0021,
      "step": 1960
    },
    {
      "epoch": 0.10506666666666667,
      "grad_norm": 0.24197255074977875,
      "learning_rate": 4.868666666666667e-05,
      "loss": 0.0028,
      "step": 1970
    },
    {
      "epoch": 0.1056,
      "grad_norm": 0.33823564648628235,
      "learning_rate": 4.868e-05,
      "loss": 0.0023,
      "step": 1980
    },
    {
      "epoch": 0.10613333333333333,
      "grad_norm": 0.7414970993995667,
      "learning_rate": 4.867333333333334e-05,
      "loss": 0.0034,
      "step": 1990
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.20036658644676208,
      "learning_rate": 4.866666666666667e-05,
      "loss": 0.0031,
      "step": 2000
    },
    {
      "epoch": 0.1072,
      "grad_norm": 0.2373487651348114,
      "learning_rate": 4.866e-05,
      "loss": 0.0035,
      "step": 2010
    },
    {
      "epoch": 0.10773333333333333,
      "grad_norm": 0.2211100310087204,
      "learning_rate": 4.865333333333334e-05,
      "loss": 0.003,
      "step": 2020
    },
    {
      "epoch": 0.10826666666666666,
      "grad_norm": 0.16220462322235107,
      "learning_rate": 4.864666666666667e-05,
      "loss": 0.0028,
      "step": 2030
    },
    {
      "epoch": 0.1088,
      "grad_norm": 0.11679273098707199,
      "learning_rate": 4.864e-05,
      "loss": 0.0037,
      "step": 2040
    },
    {
      "epoch": 0.10933333333333334,
      "grad_norm": 0.611859142780304,
      "learning_rate": 4.8633333333333334e-05,
      "loss": 0.0031,
      "step": 2050
    },
    {
      "epoch": 0.10986666666666667,
      "grad_norm": 0.582345724105835,
      "learning_rate": 4.862666666666667e-05,
      "loss": 0.0026,
      "step": 2060
    },
    {
      "epoch": 0.1104,
      "grad_norm": 0.15417005121707916,
      "learning_rate": 4.8620000000000005e-05,
      "loss": 0.0033,
      "step": 2070
    },
    {
      "epoch": 0.11093333333333333,
      "grad_norm": 0.9601085782051086,
      "learning_rate": 4.861333333333333e-05,
      "loss": 0.0023,
      "step": 2080
    },
    {
      "epoch": 0.11146666666666667,
      "grad_norm": 0.5744803547859192,
      "learning_rate": 4.860666666666667e-05,
      "loss": 0.0025,
      "step": 2090
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.32883480191230774,
      "learning_rate": 4.86e-05,
      "loss": 0.0034,
      "step": 2100
    },
    {
      "epoch": 0.11253333333333333,
      "grad_norm": 0.5859524607658386,
      "learning_rate": 4.8593333333333335e-05,
      "loss": 0.0029,
      "step": 2110
    },
    {
      "epoch": 0.11306666666666666,
      "grad_norm": 0.594915509223938,
      "learning_rate": 4.858666666666667e-05,
      "loss": 0.0037,
      "step": 2120
    },
    {
      "epoch": 0.1136,
      "grad_norm": 1.034444808959961,
      "learning_rate": 4.8580000000000006e-05,
      "loss": 0.0035,
      "step": 2130
    },
    {
      "epoch": 0.11413333333333334,
      "grad_norm": 0.6875618696212769,
      "learning_rate": 4.857333333333334e-05,
      "loss": 0.0038,
      "step": 2140
    },
    {
      "epoch": 0.11466666666666667,
      "grad_norm": 0.7397558689117432,
      "learning_rate": 4.856666666666667e-05,
      "loss": 0.0031,
      "step": 2150
    },
    {
      "epoch": 0.1152,
      "grad_norm": 0.2930509150028229,
      "learning_rate": 4.856e-05,
      "loss": 0.0039,
      "step": 2160
    },
    {
      "epoch": 0.11573333333333333,
      "grad_norm": 0.48045632243156433,
      "learning_rate": 4.8553333333333335e-05,
      "loss": 0.0034,
      "step": 2170
    },
    {
      "epoch": 0.11626666666666667,
      "grad_norm": 0.10955721884965897,
      "learning_rate": 4.854666666666667e-05,
      "loss": 0.0037,
      "step": 2180
    },
    {
      "epoch": 0.1168,
      "grad_norm": 0.3962500989437103,
      "learning_rate": 4.854e-05,
      "loss": 0.003,
      "step": 2190
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 0.48582491278648376,
      "learning_rate": 4.853333333333334e-05,
      "loss": 0.0029,
      "step": 2200
    },
    {
      "epoch": 0.11786666666666666,
      "grad_norm": 0.28244253993034363,
      "learning_rate": 4.852666666666667e-05,
      "loss": 0.0026,
      "step": 2210
    },
    {
      "epoch": 0.1184,
      "grad_norm": 0.17839214205741882,
      "learning_rate": 4.852e-05,
      "loss": 0.0027,
      "step": 2220
    },
    {
      "epoch": 0.11893333333333334,
      "grad_norm": 0.30853429436683655,
      "learning_rate": 4.8513333333333335e-05,
      "loss": 0.0039,
      "step": 2230
    },
    {
      "epoch": 0.11946666666666667,
      "grad_norm": 0.6621583700180054,
      "learning_rate": 4.850666666666667e-05,
      "loss": 0.0028,
      "step": 2240
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.3529681861400604,
      "learning_rate": 4.85e-05,
      "loss": 0.0023,
      "step": 2250
    },
    {
      "epoch": 0.12053333333333334,
      "grad_norm": 0.20816102623939514,
      "learning_rate": 4.849333333333333e-05,
      "loss": 0.0022,
      "step": 2260
    },
    {
      "epoch": 0.12106666666666667,
      "grad_norm": 0.45448529720306396,
      "learning_rate": 4.848666666666667e-05,
      "loss": 0.0033,
      "step": 2270
    },
    {
      "epoch": 0.1216,
      "grad_norm": 0.6203455924987793,
      "learning_rate": 4.8480000000000003e-05,
      "loss": 0.0021,
      "step": 2280
    },
    {
      "epoch": 0.12213333333333333,
      "grad_norm": 0.7811646461486816,
      "learning_rate": 4.8473333333333336e-05,
      "loss": 0.0043,
      "step": 2290
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 0.19494257867336273,
      "learning_rate": 4.8466666666666675e-05,
      "loss": 0.0028,
      "step": 2300
    },
    {
      "epoch": 0.1232,
      "grad_norm": 0.5974649786949158,
      "learning_rate": 4.846e-05,
      "loss": 0.0031,
      "step": 2310
    },
    {
      "epoch": 0.12373333333333333,
      "grad_norm": 0.7239753007888794,
      "learning_rate": 4.845333333333333e-05,
      "loss": 0.004,
      "step": 2320
    },
    {
      "epoch": 0.12426666666666666,
      "grad_norm": 0.20314238965511322,
      "learning_rate": 4.8446666666666665e-05,
      "loss": 0.0028,
      "step": 2330
    },
    {
      "epoch": 0.1248,
      "grad_norm": 0.21430090069770813,
      "learning_rate": 4.8440000000000004e-05,
      "loss": 0.0033,
      "step": 2340
    },
    {
      "epoch": 0.12533333333333332,
      "grad_norm": 0.14830760657787323,
      "learning_rate": 4.8433333333333336e-05,
      "loss": 0.0026,
      "step": 2350
    },
    {
      "epoch": 0.12586666666666665,
      "grad_norm": 0.22686821222305298,
      "learning_rate": 4.842666666666667e-05,
      "loss": 0.0028,
      "step": 2360
    },
    {
      "epoch": 0.1264,
      "grad_norm": 0.37799930572509766,
      "learning_rate": 4.842000000000001e-05,
      "loss": 0.0039,
      "step": 2370
    },
    {
      "epoch": 0.12693333333333334,
      "grad_norm": 0.28103548288345337,
      "learning_rate": 4.841333333333334e-05,
      "loss": 0.0037,
      "step": 2380
    },
    {
      "epoch": 0.12746666666666667,
      "grad_norm": 0.41032713651657104,
      "learning_rate": 4.8406666666666665e-05,
      "loss": 0.0023,
      "step": 2390
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.33487892150878906,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 0.0025,
      "step": 2400
    },
    {
      "epoch": 0.12853333333333333,
      "grad_norm": 0.33025917410850525,
      "learning_rate": 4.8393333333333336e-05,
      "loss": 0.0027,
      "step": 2410
    },
    {
      "epoch": 0.12906666666666666,
      "grad_norm": 0.3875495195388794,
      "learning_rate": 4.838666666666667e-05,
      "loss": 0.0023,
      "step": 2420
    },
    {
      "epoch": 0.1296,
      "grad_norm": 0.39635488390922546,
      "learning_rate": 4.838e-05,
      "loss": 0.003,
      "step": 2430
    },
    {
      "epoch": 0.13013333333333332,
      "grad_norm": 0.5559028387069702,
      "learning_rate": 4.837333333333334e-05,
      "loss": 0.0028,
      "step": 2440
    },
    {
      "epoch": 0.13066666666666665,
      "grad_norm": 0.24389785528182983,
      "learning_rate": 4.836666666666667e-05,
      "loss": 0.003,
      "step": 2450
    },
    {
      "epoch": 0.1312,
      "grad_norm": 0.37160179018974304,
      "learning_rate": 4.836e-05,
      "loss": 0.0029,
      "step": 2460
    },
    {
      "epoch": 0.13173333333333334,
      "grad_norm": 0.26751595735549927,
      "learning_rate": 4.835333333333334e-05,
      "loss": 0.0037,
      "step": 2470
    },
    {
      "epoch": 0.13226666666666667,
      "grad_norm": 0.3339851200580597,
      "learning_rate": 4.834666666666667e-05,
      "loss": 0.0027,
      "step": 2480
    },
    {
      "epoch": 0.1328,
      "grad_norm": 0.4716613292694092,
      "learning_rate": 4.834e-05,
      "loss": 0.0033,
      "step": 2490
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.3461732268333435,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 0.0033,
      "step": 2500
    },
    {
      "epoch": 0.13386666666666666,
      "grad_norm": 0.47230181097984314,
      "learning_rate": 4.832666666666667e-05,
      "loss": 0.0037,
      "step": 2510
    },
    {
      "epoch": 0.1344,
      "grad_norm": 0.2361648827791214,
      "learning_rate": 4.8320000000000005e-05,
      "loss": 0.0036,
      "step": 2520
    },
    {
      "epoch": 0.13493333333333332,
      "grad_norm": 0.3709558844566345,
      "learning_rate": 4.831333333333334e-05,
      "loss": 0.0044,
      "step": 2530
    },
    {
      "epoch": 0.13546666666666668,
      "grad_norm": 0.5890878438949585,
      "learning_rate": 4.830666666666667e-05,
      "loss": 0.0026,
      "step": 2540
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.32905343174934387,
      "learning_rate": 4.83e-05,
      "loss": 0.003,
      "step": 2550
    },
    {
      "epoch": 0.13653333333333334,
      "grad_norm": 0.15806430578231812,
      "learning_rate": 4.8293333333333334e-05,
      "loss": 0.0025,
      "step": 2560
    },
    {
      "epoch": 0.13706666666666667,
      "grad_norm": 0.13721205294132233,
      "learning_rate": 4.8286666666666666e-05,
      "loss": 0.0035,
      "step": 2570
    },
    {
      "epoch": 0.1376,
      "grad_norm": 0.5323536396026611,
      "learning_rate": 4.8280000000000005e-05,
      "loss": 0.0026,
      "step": 2580
    },
    {
      "epoch": 0.13813333333333333,
      "grad_norm": 0.40986138582229614,
      "learning_rate": 4.827333333333334e-05,
      "loss": 0.0023,
      "step": 2590
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 0.437791645526886,
      "learning_rate": 4.826666666666667e-05,
      "loss": 0.0023,
      "step": 2600
    },
    {
      "epoch": 0.1392,
      "grad_norm": 0.42005276679992676,
      "learning_rate": 4.826e-05,
      "loss": 0.0031,
      "step": 2610
    },
    {
      "epoch": 0.13973333333333332,
      "grad_norm": 0.5963097810745239,
      "learning_rate": 4.8253333333333334e-05,
      "loss": 0.0026,
      "step": 2620
    },
    {
      "epoch": 0.14026666666666668,
      "grad_norm": 0.41570815443992615,
      "learning_rate": 4.824666666666667e-05,
      "loss": 0.0038,
      "step": 2630
    },
    {
      "epoch": 0.1408,
      "grad_norm": 0.731195330619812,
      "learning_rate": 4.824e-05,
      "loss": 0.0028,
      "step": 2640
    },
    {
      "epoch": 0.14133333333333334,
      "grad_norm": 0.195075124502182,
      "learning_rate": 4.823333333333334e-05,
      "loss": 0.0037,
      "step": 2650
    },
    {
      "epoch": 0.14186666666666667,
      "grad_norm": 0.6162320971488953,
      "learning_rate": 4.822666666666667e-05,
      "loss": 0.0042,
      "step": 2660
    },
    {
      "epoch": 0.1424,
      "grad_norm": 0.48461905121803284,
      "learning_rate": 4.822e-05,
      "loss": 0.0036,
      "step": 2670
    },
    {
      "epoch": 0.14293333333333333,
      "grad_norm": 0.281964510679245,
      "learning_rate": 4.8213333333333335e-05,
      "loss": 0.0023,
      "step": 2680
    },
    {
      "epoch": 0.14346666666666666,
      "grad_norm": 0.4155087471008301,
      "learning_rate": 4.820666666666667e-05,
      "loss": 0.0045,
      "step": 2690
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.0780610591173172,
      "learning_rate": 4.82e-05,
      "loss": 0.0023,
      "step": 2700
    },
    {
      "epoch": 0.14453333333333335,
      "grad_norm": 0.41383084654808044,
      "learning_rate": 4.819333333333333e-05,
      "loss": 0.0026,
      "step": 2710
    },
    {
      "epoch": 0.14506666666666668,
      "grad_norm": 0.6120644211769104,
      "learning_rate": 4.818666666666667e-05,
      "loss": 0.0025,
      "step": 2720
    },
    {
      "epoch": 0.1456,
      "grad_norm": 0.4961821436882019,
      "learning_rate": 4.818e-05,
      "loss": 0.0047,
      "step": 2730
    },
    {
      "epoch": 0.14613333333333334,
      "grad_norm": 0.320987731218338,
      "learning_rate": 4.8173333333333335e-05,
      "loss": 0.0035,
      "step": 2740
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.19533006846904755,
      "learning_rate": 4.8166666666666674e-05,
      "loss": 0.003,
      "step": 2750
    },
    {
      "epoch": 0.1472,
      "grad_norm": 0.3000967800617218,
      "learning_rate": 4.816e-05,
      "loss": 0.0033,
      "step": 2760
    },
    {
      "epoch": 0.14773333333333333,
      "grad_norm": 0.5385287404060364,
      "learning_rate": 4.815333333333333e-05,
      "loss": 0.004,
      "step": 2770
    },
    {
      "epoch": 0.14826666666666666,
      "grad_norm": 0.4098113179206848,
      "learning_rate": 4.814666666666667e-05,
      "loss": 0.0042,
      "step": 2780
    },
    {
      "epoch": 0.1488,
      "grad_norm": 0.3650752305984497,
      "learning_rate": 4.814e-05,
      "loss": 0.0027,
      "step": 2790
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 0.2977967858314514,
      "learning_rate": 4.8133333333333336e-05,
      "loss": 0.0032,
      "step": 2800
    },
    {
      "epoch": 0.14986666666666668,
      "grad_norm": 0.4256817102432251,
      "learning_rate": 4.812666666666667e-05,
      "loss": 0.0048,
      "step": 2810
    },
    {
      "epoch": 0.1504,
      "grad_norm": 0.5257108807563782,
      "learning_rate": 4.812000000000001e-05,
      "loss": 0.0029,
      "step": 2820
    },
    {
      "epoch": 0.15093333333333334,
      "grad_norm": 0.3750307559967041,
      "learning_rate": 4.811333333333334e-05,
      "loss": 0.0031,
      "step": 2830
    },
    {
      "epoch": 0.15146666666666667,
      "grad_norm": 0.14514650404453278,
      "learning_rate": 4.8106666666666665e-05,
      "loss": 0.0031,
      "step": 2840
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.47894206643104553,
      "learning_rate": 4.8100000000000004e-05,
      "loss": 0.0035,
      "step": 2850
    },
    {
      "epoch": 0.15253333333333333,
      "grad_norm": 0.4724922180175781,
      "learning_rate": 4.8093333333333336e-05,
      "loss": 0.0054,
      "step": 2860
    },
    {
      "epoch": 0.15306666666666666,
      "grad_norm": 0.38062548637390137,
      "learning_rate": 4.808666666666667e-05,
      "loss": 0.0028,
      "step": 2870
    },
    {
      "epoch": 0.1536,
      "grad_norm": 0.40041661262512207,
      "learning_rate": 4.808e-05,
      "loss": 0.0031,
      "step": 2880
    },
    {
      "epoch": 0.15413333333333334,
      "grad_norm": 0.391505628824234,
      "learning_rate": 4.807333333333334e-05,
      "loss": 0.0039,
      "step": 2890
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 0.2973228394985199,
      "learning_rate": 4.806666666666667e-05,
      "loss": 0.0041,
      "step": 2900
    },
    {
      "epoch": 0.1552,
      "grad_norm": 0.29491618275642395,
      "learning_rate": 4.8060000000000004e-05,
      "loss": 0.0032,
      "step": 2910
    },
    {
      "epoch": 0.15573333333333333,
      "grad_norm": 0.6860978007316589,
      "learning_rate": 4.8053333333333336e-05,
      "loss": 0.0045,
      "step": 2920
    },
    {
      "epoch": 0.15626666666666666,
      "grad_norm": 0.19510075449943542,
      "learning_rate": 4.804666666666667e-05,
      "loss": 0.0025,
      "step": 2930
    },
    {
      "epoch": 0.1568,
      "grad_norm": 0.49909260869026184,
      "learning_rate": 4.804e-05,
      "loss": 0.0023,
      "step": 2940
    },
    {
      "epoch": 0.15733333333333333,
      "grad_norm": 0.4151288866996765,
      "learning_rate": 4.803333333333333e-05,
      "loss": 0.0036,
      "step": 2950
    },
    {
      "epoch": 0.15786666666666666,
      "grad_norm": 0.30875563621520996,
      "learning_rate": 4.802666666666667e-05,
      "loss": 0.0025,
      "step": 2960
    },
    {
      "epoch": 0.1584,
      "grad_norm": 0.5267183184623718,
      "learning_rate": 4.8020000000000004e-05,
      "loss": 0.003,
      "step": 2970
    },
    {
      "epoch": 0.15893333333333334,
      "grad_norm": 0.6288435459136963,
      "learning_rate": 4.801333333333334e-05,
      "loss": 0.0029,
      "step": 2980
    },
    {
      "epoch": 0.15946666666666667,
      "grad_norm": 1.0031499862670898,
      "learning_rate": 4.800666666666667e-05,
      "loss": 0.0027,
      "step": 2990
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.7590799331665039,
      "learning_rate": 4.8e-05,
      "loss": 0.0046,
      "step": 3000
    },
    {
      "epoch": 0.16053333333333333,
      "grad_norm": 0.12940488755702972,
      "learning_rate": 4.7993333333333333e-05,
      "loss": 0.0027,
      "step": 3010
    },
    {
      "epoch": 0.16106666666666666,
      "grad_norm": 0.21097983419895172,
      "learning_rate": 4.7986666666666666e-05,
      "loss": 0.003,
      "step": 3020
    },
    {
      "epoch": 0.1616,
      "grad_norm": 0.257891982793808,
      "learning_rate": 4.7980000000000005e-05,
      "loss": 0.0023,
      "step": 3030
    },
    {
      "epoch": 0.16213333333333332,
      "grad_norm": 0.319674015045166,
      "learning_rate": 4.797333333333334e-05,
      "loss": 0.0035,
      "step": 3040
    },
    {
      "epoch": 0.16266666666666665,
      "grad_norm": 0.11011334508657455,
      "learning_rate": 4.796666666666667e-05,
      "loss": 0.0025,
      "step": 3050
    },
    {
      "epoch": 0.1632,
      "grad_norm": 0.6384733319282532,
      "learning_rate": 4.796e-05,
      "loss": 0.0039,
      "step": 3060
    },
    {
      "epoch": 0.16373333333333334,
      "grad_norm": 0.4184318482875824,
      "learning_rate": 4.7953333333333334e-05,
      "loss": 0.0025,
      "step": 3070
    },
    {
      "epoch": 0.16426666666666667,
      "grad_norm": 0.6912758946418762,
      "learning_rate": 4.7946666666666666e-05,
      "loss": 0.0036,
      "step": 3080
    },
    {
      "epoch": 0.1648,
      "grad_norm": 0.5039706230163574,
      "learning_rate": 4.794e-05,
      "loss": 0.0026,
      "step": 3090
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 0.6121901273727417,
      "learning_rate": 4.793333333333334e-05,
      "loss": 0.0031,
      "step": 3100
    },
    {
      "epoch": 0.16586666666666666,
      "grad_norm": 0.48349466919898987,
      "learning_rate": 4.792666666666667e-05,
      "loss": 0.0049,
      "step": 3110
    },
    {
      "epoch": 0.1664,
      "grad_norm": 0.5499138236045837,
      "learning_rate": 4.792e-05,
      "loss": 0.0034,
      "step": 3120
    },
    {
      "epoch": 0.16693333333333332,
      "grad_norm": 0.2768326997756958,
      "learning_rate": 4.791333333333334e-05,
      "loss": 0.0027,
      "step": 3130
    },
    {
      "epoch": 0.16746666666666668,
      "grad_norm": 0.4153347909450531,
      "learning_rate": 4.7906666666666667e-05,
      "loss": 0.0042,
      "step": 3140
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.2729402780532837,
      "learning_rate": 4.79e-05,
      "loss": 0.0024,
      "step": 3150
    },
    {
      "epoch": 0.16853333333333334,
      "grad_norm": 0.3688238859176636,
      "learning_rate": 4.789333333333334e-05,
      "loss": 0.0027,
      "step": 3160
    },
    {
      "epoch": 0.16906666666666667,
      "grad_norm": 0.27727970480918884,
      "learning_rate": 4.788666666666667e-05,
      "loss": 0.0036,
      "step": 3170
    },
    {
      "epoch": 0.1696,
      "grad_norm": 0.2506515383720398,
      "learning_rate": 4.788e-05,
      "loss": 0.0041,
      "step": 3180
    },
    {
      "epoch": 0.17013333333333333,
      "grad_norm": 0.31059080362319946,
      "learning_rate": 4.7873333333333335e-05,
      "loss": 0.0029,
      "step": 3190
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 0.30045774579048157,
      "learning_rate": 4.7866666666666674e-05,
      "loss": 0.0026,
      "step": 3200
    },
    {
      "epoch": 0.1712,
      "grad_norm": 0.1872415393590927,
      "learning_rate": 4.7860000000000006e-05,
      "loss": 0.0033,
      "step": 3210
    },
    {
      "epoch": 0.17173333333333332,
      "grad_norm": 0.2425980269908905,
      "learning_rate": 4.785333333333333e-05,
      "loss": 0.0046,
      "step": 3220
    },
    {
      "epoch": 0.17226666666666668,
      "grad_norm": 0.4134896695613861,
      "learning_rate": 4.784666666666667e-05,
      "loss": 0.003,
      "step": 3230
    },
    {
      "epoch": 0.1728,
      "grad_norm": 0.39836859703063965,
      "learning_rate": 4.784e-05,
      "loss": 0.0029,
      "step": 3240
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.5415756702423096,
      "learning_rate": 4.7833333333333335e-05,
      "loss": 0.0021,
      "step": 3250
    },
    {
      "epoch": 0.17386666666666667,
      "grad_norm": 0.6388959288597107,
      "learning_rate": 4.782666666666667e-05,
      "loss": 0.0021,
      "step": 3260
    },
    {
      "epoch": 0.1744,
      "grad_norm": 0.5678301453590393,
      "learning_rate": 4.7820000000000006e-05,
      "loss": 0.0033,
      "step": 3270
    },
    {
      "epoch": 0.17493333333333333,
      "grad_norm": 0.3648126721382141,
      "learning_rate": 4.781333333333334e-05,
      "loss": 0.0032,
      "step": 3280
    },
    {
      "epoch": 0.17546666666666666,
      "grad_norm": 0.6208207011222839,
      "learning_rate": 4.7806666666666664e-05,
      "loss": 0.0044,
      "step": 3290
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.5253379344940186,
      "learning_rate": 4.78e-05,
      "loss": 0.0024,
      "step": 3300
    },
    {
      "epoch": 0.17653333333333332,
      "grad_norm": 0.6105543971061707,
      "learning_rate": 4.7793333333333335e-05,
      "loss": 0.0037,
      "step": 3310
    },
    {
      "epoch": 0.17706666666666668,
      "grad_norm": 0.3989449143409729,
      "learning_rate": 4.778666666666667e-05,
      "loss": 0.0023,
      "step": 3320
    },
    {
      "epoch": 0.1776,
      "grad_norm": 0.4784407615661621,
      "learning_rate": 4.778e-05,
      "loss": 0.0028,
      "step": 3330
    },
    {
      "epoch": 0.17813333333333334,
      "grad_norm": 0.3647109866142273,
      "learning_rate": 4.777333333333334e-05,
      "loss": 0.0039,
      "step": 3340
    },
    {
      "epoch": 0.17866666666666667,
      "grad_norm": 0.2877965569496155,
      "learning_rate": 4.776666666666667e-05,
      "loss": 0.0027,
      "step": 3350
    },
    {
      "epoch": 0.1792,
      "grad_norm": 0.5765588283538818,
      "learning_rate": 4.7760000000000004e-05,
      "loss": 0.0031,
      "step": 3360
    },
    {
      "epoch": 0.17973333333333333,
      "grad_norm": 0.29756590723991394,
      "learning_rate": 4.7753333333333336e-05,
      "loss": 0.0027,
      "step": 3370
    },
    {
      "epoch": 0.18026666666666666,
      "grad_norm": 0.22525654733181,
      "learning_rate": 4.774666666666667e-05,
      "loss": 0.0023,
      "step": 3380
    },
    {
      "epoch": 0.1808,
      "grad_norm": 0.3089345693588257,
      "learning_rate": 4.774e-05,
      "loss": 0.0024,
      "step": 3390
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 0.20806460082530975,
      "learning_rate": 4.773333333333333e-05,
      "loss": 0.0021,
      "step": 3400
    },
    {
      "epoch": 0.18186666666666668,
      "grad_norm": 0.1779526323080063,
      "learning_rate": 4.772666666666667e-05,
      "loss": 0.0027,
      "step": 3410
    },
    {
      "epoch": 0.1824,
      "grad_norm": 0.39932751655578613,
      "learning_rate": 4.7720000000000004e-05,
      "loss": 0.0037,
      "step": 3420
    },
    {
      "epoch": 0.18293333333333334,
      "grad_norm": 0.6947712898254395,
      "learning_rate": 4.7713333333333336e-05,
      "loss": 0.0036,
      "step": 3430
    },
    {
      "epoch": 0.18346666666666667,
      "grad_norm": 0.5125948190689087,
      "learning_rate": 4.770666666666667e-05,
      "loss": 0.0025,
      "step": 3440
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.11528827995061874,
      "learning_rate": 4.77e-05,
      "loss": 0.004,
      "step": 3450
    },
    {
      "epoch": 0.18453333333333333,
      "grad_norm": 0.13884149491786957,
      "learning_rate": 4.769333333333333e-05,
      "loss": 0.0022,
      "step": 3460
    },
    {
      "epoch": 0.18506666666666666,
      "grad_norm": 0.6777562499046326,
      "learning_rate": 4.7686666666666665e-05,
      "loss": 0.0029,
      "step": 3470
    },
    {
      "epoch": 0.1856,
      "grad_norm": 0.22552692890167236,
      "learning_rate": 4.7680000000000004e-05,
      "loss": 0.0031,
      "step": 3480
    },
    {
      "epoch": 0.18613333333333335,
      "grad_norm": 0.35101330280303955,
      "learning_rate": 4.7673333333333337e-05,
      "loss": 0.0028,
      "step": 3490
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.44058337807655334,
      "learning_rate": 4.766666666666667e-05,
      "loss": 0.0025,
      "step": 3500
    },
    {
      "epoch": 0.1872,
      "grad_norm": 0.4811692237854004,
      "learning_rate": 4.766000000000001e-05,
      "loss": 0.0033,
      "step": 3510
    },
    {
      "epoch": 0.18773333333333334,
      "grad_norm": 0.4486238956451416,
      "learning_rate": 4.765333333333333e-05,
      "loss": 0.0042,
      "step": 3520
    },
    {
      "epoch": 0.18826666666666667,
      "grad_norm": 0.23275627195835114,
      "learning_rate": 4.7646666666666666e-05,
      "loss": 0.0039,
      "step": 3530
    },
    {
      "epoch": 0.1888,
      "grad_norm": 0.2989046573638916,
      "learning_rate": 4.7640000000000005e-05,
      "loss": 0.0026,
      "step": 3540
    },
    {
      "epoch": 0.18933333333333333,
      "grad_norm": 0.3678875267505646,
      "learning_rate": 4.763333333333334e-05,
      "loss": 0.004,
      "step": 3550
    },
    {
      "epoch": 0.18986666666666666,
      "grad_norm": 0.4546627998352051,
      "learning_rate": 4.762666666666667e-05,
      "loss": 0.0029,
      "step": 3560
    },
    {
      "epoch": 0.1904,
      "grad_norm": 0.17569778859615326,
      "learning_rate": 4.762e-05,
      "loss": 0.0022,
      "step": 3570
    },
    {
      "epoch": 0.19093333333333334,
      "grad_norm": 0.10546398907899857,
      "learning_rate": 4.761333333333334e-05,
      "loss": 0.0029,
      "step": 3580
    },
    {
      "epoch": 0.19146666666666667,
      "grad_norm": 0.20439811050891876,
      "learning_rate": 4.760666666666667e-05,
      "loss": 0.0038,
      "step": 3590
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.2953263223171234,
      "learning_rate": 4.76e-05,
      "loss": 0.004,
      "step": 3600
    },
    {
      "epoch": 0.19253333333333333,
      "grad_norm": 0.34453874826431274,
      "learning_rate": 4.759333333333334e-05,
      "loss": 0.0029,
      "step": 3610
    },
    {
      "epoch": 0.19306666666666666,
      "grad_norm": 0.5209170579910278,
      "learning_rate": 4.758666666666667e-05,
      "loss": 0.0027,
      "step": 3620
    },
    {
      "epoch": 0.1936,
      "grad_norm": 0.2825407087802887,
      "learning_rate": 4.758e-05,
      "loss": 0.0031,
      "step": 3630
    },
    {
      "epoch": 0.19413333333333332,
      "grad_norm": 0.22122804820537567,
      "learning_rate": 4.7573333333333334e-05,
      "loss": 0.0038,
      "step": 3640
    },
    {
      "epoch": 0.19466666666666665,
      "grad_norm": 0.21200031042099,
      "learning_rate": 4.756666666666667e-05,
      "loss": 0.0024,
      "step": 3650
    },
    {
      "epoch": 0.1952,
      "grad_norm": 0.4197395145893097,
      "learning_rate": 4.7560000000000005e-05,
      "loss": 0.0037,
      "step": 3660
    },
    {
      "epoch": 0.19573333333333334,
      "grad_norm": 0.3815772533416748,
      "learning_rate": 4.755333333333333e-05,
      "loss": 0.0031,
      "step": 3670
    },
    {
      "epoch": 0.19626666666666667,
      "grad_norm": 0.2577569782733917,
      "learning_rate": 4.754666666666667e-05,
      "loss": 0.002,
      "step": 3680
    },
    {
      "epoch": 0.1968,
      "grad_norm": 0.39731529355049133,
      "learning_rate": 4.754e-05,
      "loss": 0.004,
      "step": 3690
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 0.19125041365623474,
      "learning_rate": 4.7533333333333334e-05,
      "loss": 0.0027,
      "step": 3700
    },
    {
      "epoch": 0.19786666666666666,
      "grad_norm": 0.5251250863075256,
      "learning_rate": 4.752666666666667e-05,
      "loss": 0.0036,
      "step": 3710
    },
    {
      "epoch": 0.1984,
      "grad_norm": 0.3704773783683777,
      "learning_rate": 4.7520000000000006e-05,
      "loss": 0.0027,
      "step": 3720
    },
    {
      "epoch": 0.19893333333333332,
      "grad_norm": 0.4376111626625061,
      "learning_rate": 4.751333333333334e-05,
      "loss": 0.0026,
      "step": 3730
    },
    {
      "epoch": 0.19946666666666665,
      "grad_norm": 0.35032469034194946,
      "learning_rate": 4.750666666666667e-05,
      "loss": 0.0026,
      "step": 3740
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.2590775191783905,
      "learning_rate": 4.75e-05,
      "loss": 0.0027,
      "step": 3750
    },
    {
      "epoch": 0.20053333333333334,
      "grad_norm": 0.5203791856765747,
      "learning_rate": 4.7493333333333335e-05,
      "loss": 0.0028,
      "step": 3760
    },
    {
      "epoch": 0.20106666666666667,
      "grad_norm": 0.6369812488555908,
      "learning_rate": 4.748666666666667e-05,
      "loss": 0.0028,
      "step": 3770
    },
    {
      "epoch": 0.2016,
      "grad_norm": 0.5438604950904846,
      "learning_rate": 4.748e-05,
      "loss": 0.0024,
      "step": 3780
    },
    {
      "epoch": 0.20213333333333333,
      "grad_norm": 0.5208097696304321,
      "learning_rate": 4.747333333333334e-05,
      "loss": 0.0034,
      "step": 3790
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 0.801034152507782,
      "learning_rate": 4.746666666666667e-05,
      "loss": 0.0028,
      "step": 3800
    },
    {
      "epoch": 0.2032,
      "grad_norm": 0.9287384748458862,
      "learning_rate": 4.746e-05,
      "loss": 0.0041,
      "step": 3810
    },
    {
      "epoch": 0.20373333333333332,
      "grad_norm": 0.15169250965118408,
      "learning_rate": 4.7453333333333335e-05,
      "loss": 0.0036,
      "step": 3820
    },
    {
      "epoch": 0.20426666666666668,
      "grad_norm": 0.2613357901573181,
      "learning_rate": 4.744666666666667e-05,
      "loss": 0.0029,
      "step": 3830
    },
    {
      "epoch": 0.2048,
      "grad_norm": 0.33263444900512695,
      "learning_rate": 4.744e-05,
      "loss": 0.0034,
      "step": 3840
    },
    {
      "epoch": 0.20533333333333334,
      "grad_norm": 0.24011214077472687,
      "learning_rate": 4.743333333333333e-05,
      "loss": 0.0034,
      "step": 3850
    },
    {
      "epoch": 0.20586666666666667,
      "grad_norm": 0.5536648035049438,
      "learning_rate": 4.742666666666667e-05,
      "loss": 0.0032,
      "step": 3860
    },
    {
      "epoch": 0.2064,
      "grad_norm": 0.18272894620895386,
      "learning_rate": 4.742e-05,
      "loss": 0.0026,
      "step": 3870
    },
    {
      "epoch": 0.20693333333333333,
      "grad_norm": 0.5214344263076782,
      "learning_rate": 4.7413333333333336e-05,
      "loss": 0.003,
      "step": 3880
    },
    {
      "epoch": 0.20746666666666666,
      "grad_norm": 0.21188919246196747,
      "learning_rate": 4.7406666666666675e-05,
      "loss": 0.0033,
      "step": 3890
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.23890471458435059,
      "learning_rate": 4.74e-05,
      "loss": 0.0025,
      "step": 3900
    },
    {
      "epoch": 0.20853333333333332,
      "grad_norm": 0.15195170044898987,
      "learning_rate": 4.739333333333333e-05,
      "loss": 0.0021,
      "step": 3910
    },
    {
      "epoch": 0.20906666666666668,
      "grad_norm": 0.2730677127838135,
      "learning_rate": 4.7386666666666665e-05,
      "loss": 0.003,
      "step": 3920
    },
    {
      "epoch": 0.2096,
      "grad_norm": 0.47866109013557434,
      "learning_rate": 4.7380000000000004e-05,
      "loss": 0.0026,
      "step": 3930
    },
    {
      "epoch": 0.21013333333333334,
      "grad_norm": 0.16815830767154694,
      "learning_rate": 4.7373333333333336e-05,
      "loss": 0.0033,
      "step": 3940
    },
    {
      "epoch": 0.21066666666666667,
      "grad_norm": 0.7211558818817139,
      "learning_rate": 4.736666666666667e-05,
      "loss": 0.004,
      "step": 3950
    },
    {
      "epoch": 0.2112,
      "grad_norm": 0.424699991941452,
      "learning_rate": 4.736000000000001e-05,
      "loss": 0.0035,
      "step": 3960
    },
    {
      "epoch": 0.21173333333333333,
      "grad_norm": 0.17732346057891846,
      "learning_rate": 4.735333333333333e-05,
      "loss": 0.0024,
      "step": 3970
    },
    {
      "epoch": 0.21226666666666666,
      "grad_norm": 0.4138984680175781,
      "learning_rate": 4.7346666666666665e-05,
      "loss": 0.0026,
      "step": 3980
    },
    {
      "epoch": 0.2128,
      "grad_norm": 0.2037304788827896,
      "learning_rate": 4.7340000000000004e-05,
      "loss": 0.0043,
      "step": 3990
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.8859982490539551,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 0.0026,
      "step": 4000
    },
    {
      "epoch": 0.21386666666666668,
      "grad_norm": 0.21911601722240448,
      "learning_rate": 4.732666666666667e-05,
      "loss": 0.0032,
      "step": 4010
    },
    {
      "epoch": 0.2144,
      "grad_norm": 0.12193365395069122,
      "learning_rate": 4.732e-05,
      "loss": 0.0031,
      "step": 4020
    },
    {
      "epoch": 0.21493333333333334,
      "grad_norm": 0.29031407833099365,
      "learning_rate": 4.731333333333334e-05,
      "loss": 0.0023,
      "step": 4030
    },
    {
      "epoch": 0.21546666666666667,
      "grad_norm": 0.3852815330028534,
      "learning_rate": 4.730666666666667e-05,
      "loss": 0.0028,
      "step": 4040
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.1735517382621765,
      "learning_rate": 4.73e-05,
      "loss": 0.0039,
      "step": 4050
    },
    {
      "epoch": 0.21653333333333333,
      "grad_norm": 0.6157423853874207,
      "learning_rate": 4.729333333333334e-05,
      "loss": 0.0039,
      "step": 4060
    },
    {
      "epoch": 0.21706666666666666,
      "grad_norm": 0.38768067955970764,
      "learning_rate": 4.728666666666667e-05,
      "loss": 0.0035,
      "step": 4070
    },
    {
      "epoch": 0.2176,
      "grad_norm": 1.0548956394195557,
      "learning_rate": 4.728e-05,
      "loss": 0.0039,
      "step": 4080
    },
    {
      "epoch": 0.21813333333333335,
      "grad_norm": 0.8835408687591553,
      "learning_rate": 4.7273333333333334e-05,
      "loss": 0.003,
      "step": 4090
    },
    {
      "epoch": 0.21866666666666668,
      "grad_norm": 0.7818339467048645,
      "learning_rate": 4.726666666666667e-05,
      "loss": 0.0032,
      "step": 4100
    },
    {
      "epoch": 0.2192,
      "grad_norm": 0.3063831627368927,
      "learning_rate": 4.7260000000000005e-05,
      "loss": 0.0038,
      "step": 4110
    },
    {
      "epoch": 0.21973333333333334,
      "grad_norm": 0.2948009669780731,
      "learning_rate": 4.725333333333334e-05,
      "loss": 0.0025,
      "step": 4120
    },
    {
      "epoch": 0.22026666666666667,
      "grad_norm": 0.18631383776664734,
      "learning_rate": 4.724666666666667e-05,
      "loss": 0.0033,
      "step": 4130
    },
    {
      "epoch": 0.2208,
      "grad_norm": 0.23534870147705078,
      "learning_rate": 4.724e-05,
      "loss": 0.0037,
      "step": 4140
    },
    {
      "epoch": 0.22133333333333333,
      "grad_norm": 0.14105476438999176,
      "learning_rate": 4.7233333333333334e-05,
      "loss": 0.0027,
      "step": 4150
    },
    {
      "epoch": 0.22186666666666666,
      "grad_norm": 0.540485143661499,
      "learning_rate": 4.7226666666666666e-05,
      "loss": 0.0031,
      "step": 4160
    },
    {
      "epoch": 0.2224,
      "grad_norm": 0.30490177869796753,
      "learning_rate": 4.7220000000000005e-05,
      "loss": 0.0025,
      "step": 4170
    },
    {
      "epoch": 0.22293333333333334,
      "grad_norm": 0.5169258713722229,
      "learning_rate": 4.721333333333334e-05,
      "loss": 0.0021,
      "step": 4180
    },
    {
      "epoch": 0.22346666666666667,
      "grad_norm": 0.10222150385379791,
      "learning_rate": 4.720666666666667e-05,
      "loss": 0.0035,
      "step": 4190
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.5720906853675842,
      "learning_rate": 4.72e-05,
      "loss": 0.003,
      "step": 4200
    },
    {
      "epoch": 0.22453333333333333,
      "grad_norm": 0.13333384692668915,
      "learning_rate": 4.7193333333333334e-05,
      "loss": 0.003,
      "step": 4210
    },
    {
      "epoch": 0.22506666666666666,
      "grad_norm": 0.41022366285324097,
      "learning_rate": 4.718666666666667e-05,
      "loss": 0.0023,
      "step": 4220
    },
    {
      "epoch": 0.2256,
      "grad_norm": 0.16930516064167023,
      "learning_rate": 4.718e-05,
      "loss": 0.0026,
      "step": 4230
    },
    {
      "epoch": 0.22613333333333333,
      "grad_norm": 1.0925676822662354,
      "learning_rate": 4.717333333333334e-05,
      "loss": 0.0025,
      "step": 4240
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.3142196238040924,
      "learning_rate": 4.716666666666667e-05,
      "loss": 0.0031,
      "step": 4250
    },
    {
      "epoch": 0.2272,
      "grad_norm": 0.8303325772285461,
      "learning_rate": 4.716e-05,
      "loss": 0.0031,
      "step": 4260
    },
    {
      "epoch": 0.22773333333333334,
      "grad_norm": 0.20221975445747375,
      "learning_rate": 4.715333333333334e-05,
      "loss": 0.0028,
      "step": 4270
    },
    {
      "epoch": 0.22826666666666667,
      "grad_norm": 0.8250420689582825,
      "learning_rate": 4.714666666666667e-05,
      "loss": 0.0035,
      "step": 4280
    },
    {
      "epoch": 0.2288,
      "grad_norm": 0.7548585534095764,
      "learning_rate": 4.714e-05,
      "loss": 0.0024,
      "step": 4290
    },
    {
      "epoch": 0.22933333333333333,
      "grad_norm": 0.33859002590179443,
      "learning_rate": 4.713333333333333e-05,
      "loss": 0.0021,
      "step": 4300
    },
    {
      "epoch": 0.22986666666666666,
      "grad_norm": 0.22901874780654907,
      "learning_rate": 4.712666666666667e-05,
      "loss": 0.0026,
      "step": 4310
    },
    {
      "epoch": 0.2304,
      "grad_norm": 0.3099289536476135,
      "learning_rate": 4.712e-05,
      "loss": 0.0027,
      "step": 4320
    },
    {
      "epoch": 0.23093333333333332,
      "grad_norm": 0.6085391044616699,
      "learning_rate": 4.7113333333333335e-05,
      "loss": 0.0043,
      "step": 4330
    },
    {
      "epoch": 0.23146666666666665,
      "grad_norm": 0.24712447822093964,
      "learning_rate": 4.7106666666666674e-05,
      "loss": 0.0036,
      "step": 4340
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.10306964814662933,
      "learning_rate": 4.71e-05,
      "loss": 0.0019,
      "step": 4350
    },
    {
      "epoch": 0.23253333333333334,
      "grad_norm": 0.2954567074775696,
      "learning_rate": 4.709333333333333e-05,
      "loss": 0.0042,
      "step": 4360
    },
    {
      "epoch": 0.23306666666666667,
      "grad_norm": 0.33077001571655273,
      "learning_rate": 4.708666666666667e-05,
      "loss": 0.0027,
      "step": 4370
    },
    {
      "epoch": 0.2336,
      "grad_norm": 0.2708573341369629,
      "learning_rate": 4.708e-05,
      "loss": 0.0018,
      "step": 4380
    },
    {
      "epoch": 0.23413333333333333,
      "grad_norm": 0.5556225180625916,
      "learning_rate": 4.7073333333333336e-05,
      "loss": 0.0027,
      "step": 4390
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 0.2537716031074524,
      "learning_rate": 4.706666666666667e-05,
      "loss": 0.0025,
      "step": 4400
    },
    {
      "epoch": 0.2352,
      "grad_norm": 0.6230668425559998,
      "learning_rate": 4.706000000000001e-05,
      "loss": 0.0032,
      "step": 4410
    },
    {
      "epoch": 0.23573333333333332,
      "grad_norm": 0.16829347610473633,
      "learning_rate": 4.705333333333334e-05,
      "loss": 0.0023,
      "step": 4420
    },
    {
      "epoch": 0.23626666666666668,
      "grad_norm": 0.31688833236694336,
      "learning_rate": 4.7046666666666665e-05,
      "loss": 0.0023,
      "step": 4430
    },
    {
      "epoch": 0.2368,
      "grad_norm": 0.48150092363357544,
      "learning_rate": 4.7040000000000004e-05,
      "loss": 0.0024,
      "step": 4440
    },
    {
      "epoch": 0.23733333333333334,
      "grad_norm": 0.4721076488494873,
      "learning_rate": 4.7033333333333336e-05,
      "loss": 0.0023,
      "step": 4450
    },
    {
      "epoch": 0.23786666666666667,
      "grad_norm": 0.24041800200939178,
      "learning_rate": 4.702666666666667e-05,
      "loss": 0.0038,
      "step": 4460
    },
    {
      "epoch": 0.2384,
      "grad_norm": 0.14341507852077484,
      "learning_rate": 4.702e-05,
      "loss": 0.0022,
      "step": 4470
    },
    {
      "epoch": 0.23893333333333333,
      "grad_norm": 0.16902698576450348,
      "learning_rate": 4.701333333333334e-05,
      "loss": 0.0032,
      "step": 4480
    },
    {
      "epoch": 0.23946666666666666,
      "grad_norm": 0.1447974145412445,
      "learning_rate": 4.700666666666667e-05,
      "loss": 0.0022,
      "step": 4490
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.3067938983440399,
      "learning_rate": 4.7e-05,
      "loss": 0.0027,
      "step": 4500
    },
    {
      "epoch": 0.24053333333333332,
      "grad_norm": 0.42376211285591125,
      "learning_rate": 4.6993333333333336e-05,
      "loss": 0.0027,
      "step": 4510
    },
    {
      "epoch": 0.24106666666666668,
      "grad_norm": 0.7417770028114319,
      "learning_rate": 4.698666666666667e-05,
      "loss": 0.0035,
      "step": 4520
    },
    {
      "epoch": 0.2416,
      "grad_norm": 0.31874382495880127,
      "learning_rate": 4.698e-05,
      "loss": 0.0024,
      "step": 4530
    },
    {
      "epoch": 0.24213333333333334,
      "grad_norm": 0.5039326548576355,
      "learning_rate": 4.697333333333333e-05,
      "loss": 0.0026,
      "step": 4540
    },
    {
      "epoch": 0.24266666666666667,
      "grad_norm": 0.684794545173645,
      "learning_rate": 4.696666666666667e-05,
      "loss": 0.0031,
      "step": 4550
    },
    {
      "epoch": 0.2432,
      "grad_norm": 0.28463301062583923,
      "learning_rate": 4.6960000000000004e-05,
      "loss": 0.0023,
      "step": 4560
    },
    {
      "epoch": 0.24373333333333333,
      "grad_norm": 0.6492963433265686,
      "learning_rate": 4.695333333333334e-05,
      "loss": 0.0024,
      "step": 4570
    },
    {
      "epoch": 0.24426666666666666,
      "grad_norm": 0.3106297254562378,
      "learning_rate": 4.694666666666667e-05,
      "loss": 0.0029,
      "step": 4580
    },
    {
      "epoch": 0.2448,
      "grad_norm": 0.1659156233072281,
      "learning_rate": 4.694e-05,
      "loss": 0.0022,
      "step": 4590
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 0.40203967690467834,
      "learning_rate": 4.6933333333333333e-05,
      "loss": 0.0024,
      "step": 4600
    },
    {
      "epoch": 0.24586666666666668,
      "grad_norm": 0.15287216007709503,
      "learning_rate": 4.6926666666666666e-05,
      "loss": 0.0043,
      "step": 4610
    },
    {
      "epoch": 0.2464,
      "grad_norm": 0.555717408657074,
      "learning_rate": 4.6920000000000005e-05,
      "loss": 0.0024,
      "step": 4620
    },
    {
      "epoch": 0.24693333333333334,
      "grad_norm": 0.2445833683013916,
      "learning_rate": 4.691333333333334e-05,
      "loss": 0.0024,
      "step": 4630
    },
    {
      "epoch": 0.24746666666666667,
      "grad_norm": 0.43778666853904724,
      "learning_rate": 4.690666666666667e-05,
      "loss": 0.0032,
      "step": 4640
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.6145407557487488,
      "learning_rate": 4.69e-05,
      "loss": 0.003,
      "step": 4650
    },
    {
      "epoch": 0.24853333333333333,
      "grad_norm": 0.42767295241355896,
      "learning_rate": 4.6893333333333334e-05,
      "loss": 0.0028,
      "step": 4660
    },
    {
      "epoch": 0.24906666666666666,
      "grad_norm": 0.4259949326515198,
      "learning_rate": 4.6886666666666666e-05,
      "loss": 0.0024,
      "step": 4670
    },
    {
      "epoch": 0.2496,
      "grad_norm": 0.27978500723838806,
      "learning_rate": 4.688e-05,
      "loss": 0.0033,
      "step": 4680
    },
    {
      "epoch": 0.2501333333333333,
      "grad_norm": 0.3470403850078583,
      "learning_rate": 4.687333333333334e-05,
      "loss": 0.0031,
      "step": 4690
    },
    {
      "epoch": 0.25066666666666665,
      "grad_norm": 0.41269350051879883,
      "learning_rate": 4.686666666666667e-05,
      "loss": 0.0031,
      "step": 4700
    },
    {
      "epoch": 0.2512,
      "grad_norm": 0.2544626295566559,
      "learning_rate": 4.686e-05,
      "loss": 0.0027,
      "step": 4710
    },
    {
      "epoch": 0.2517333333333333,
      "grad_norm": 0.29681137204170227,
      "learning_rate": 4.685333333333334e-05,
      "loss": 0.0025,
      "step": 4720
    },
    {
      "epoch": 0.25226666666666664,
      "grad_norm": 0.5787114500999451,
      "learning_rate": 4.6846666666666667e-05,
      "loss": 0.0024,
      "step": 4730
    },
    {
      "epoch": 0.2528,
      "grad_norm": 0.4512456953525543,
      "learning_rate": 4.684e-05,
      "loss": 0.0035,
      "step": 4740
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.45159444212913513,
      "learning_rate": 4.683333333333334e-05,
      "loss": 0.0045,
      "step": 4750
    },
    {
      "epoch": 0.2538666666666667,
      "grad_norm": 0.3528304100036621,
      "learning_rate": 4.682666666666667e-05,
      "loss": 0.0024,
      "step": 4760
    },
    {
      "epoch": 0.2544,
      "grad_norm": 0.6447713375091553,
      "learning_rate": 4.682e-05,
      "loss": 0.0028,
      "step": 4770
    },
    {
      "epoch": 0.25493333333333335,
      "grad_norm": 0.30666589736938477,
      "learning_rate": 4.6813333333333335e-05,
      "loss": 0.003,
      "step": 4780
    },
    {
      "epoch": 0.2554666666666667,
      "grad_norm": 0.4263785183429718,
      "learning_rate": 4.6806666666666674e-05,
      "loss": 0.003,
      "step": 4790
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.6857650876045227,
      "learning_rate": 4.6800000000000006e-05,
      "loss": 0.0034,
      "step": 4800
    },
    {
      "epoch": 0.25653333333333334,
      "grad_norm": 0.40837645530700684,
      "learning_rate": 4.679333333333333e-05,
      "loss": 0.0033,
      "step": 4810
    },
    {
      "epoch": 0.25706666666666667,
      "grad_norm": 0.23088113963603973,
      "learning_rate": 4.678666666666667e-05,
      "loss": 0.0043,
      "step": 4820
    },
    {
      "epoch": 0.2576,
      "grad_norm": 0.287646621465683,
      "learning_rate": 4.678e-05,
      "loss": 0.0036,
      "step": 4830
    },
    {
      "epoch": 0.2581333333333333,
      "grad_norm": 0.1472252905368805,
      "learning_rate": 4.6773333333333335e-05,
      "loss": 0.0021,
      "step": 4840
    },
    {
      "epoch": 0.25866666666666666,
      "grad_norm": 0.5069343447685242,
      "learning_rate": 4.676666666666667e-05,
      "loss": 0.0028,
      "step": 4850
    },
    {
      "epoch": 0.2592,
      "grad_norm": 0.6601565480232239,
      "learning_rate": 4.6760000000000006e-05,
      "loss": 0.0037,
      "step": 4860
    },
    {
      "epoch": 0.2597333333333333,
      "grad_norm": 0.39844173192977905,
      "learning_rate": 4.675333333333334e-05,
      "loss": 0.0023,
      "step": 4870
    },
    {
      "epoch": 0.26026666666666665,
      "grad_norm": 0.4785657823085785,
      "learning_rate": 4.6746666666666664e-05,
      "loss": 0.0035,
      "step": 4880
    },
    {
      "epoch": 0.2608,
      "grad_norm": 1.0016093254089355,
      "learning_rate": 4.674e-05,
      "loss": 0.0042,
      "step": 4890
    },
    {
      "epoch": 0.2613333333333333,
      "grad_norm": 0.5544456243515015,
      "learning_rate": 4.6733333333333335e-05,
      "loss": 0.0027,
      "step": 4900
    },
    {
      "epoch": 0.2618666666666667,
      "grad_norm": 0.42174533009529114,
      "learning_rate": 4.672666666666667e-05,
      "loss": 0.0041,
      "step": 4910
    },
    {
      "epoch": 0.2624,
      "grad_norm": 0.9338240623474121,
      "learning_rate": 4.672e-05,
      "loss": 0.0031,
      "step": 4920
    },
    {
      "epoch": 0.26293333333333335,
      "grad_norm": 0.2815571129322052,
      "learning_rate": 4.671333333333334e-05,
      "loss": 0.0021,
      "step": 4930
    },
    {
      "epoch": 0.2634666666666667,
      "grad_norm": 0.2142074555158615,
      "learning_rate": 4.670666666666667e-05,
      "loss": 0.002,
      "step": 4940
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.23494853079319,
      "learning_rate": 4.6700000000000003e-05,
      "loss": 0.0038,
      "step": 4950
    },
    {
      "epoch": 0.26453333333333334,
      "grad_norm": 0.43258941173553467,
      "learning_rate": 4.6693333333333336e-05,
      "loss": 0.0027,
      "step": 4960
    },
    {
      "epoch": 0.2650666666666667,
      "grad_norm": 0.16781829297542572,
      "learning_rate": 4.668666666666667e-05,
      "loss": 0.0031,
      "step": 4970
    },
    {
      "epoch": 0.2656,
      "grad_norm": 0.2981184720993042,
      "learning_rate": 4.668e-05,
      "loss": 0.0024,
      "step": 4980
    },
    {
      "epoch": 0.26613333333333333,
      "grad_norm": 0.7329245805740356,
      "learning_rate": 4.667333333333333e-05,
      "loss": 0.0022,
      "step": 4990
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.5696213245391846,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.0034,
      "step": 5000
    },
    {
      "epoch": 0.2672,
      "grad_norm": 0.238735169172287,
      "learning_rate": 4.6660000000000004e-05,
      "loss": 0.0038,
      "step": 5010
    },
    {
      "epoch": 0.2677333333333333,
      "grad_norm": 0.4107729494571686,
      "learning_rate": 4.6653333333333336e-05,
      "loss": 0.0049,
      "step": 5020
    },
    {
      "epoch": 0.26826666666666665,
      "grad_norm": 0.4123401343822479,
      "learning_rate": 4.664666666666667e-05,
      "loss": 0.0021,
      "step": 5030
    },
    {
      "epoch": 0.2688,
      "grad_norm": 0.3781704008579254,
      "learning_rate": 4.664e-05,
      "loss": 0.0021,
      "step": 5040
    },
    {
      "epoch": 0.2693333333333333,
      "grad_norm": 0.5258826613426208,
      "learning_rate": 4.663333333333333e-05,
      "loss": 0.0024,
      "step": 5050
    },
    {
      "epoch": 0.26986666666666664,
      "grad_norm": 0.281949907541275,
      "learning_rate": 4.6626666666666665e-05,
      "loss": 0.0027,
      "step": 5060
    },
    {
      "epoch": 0.2704,
      "grad_norm": 0.7366169095039368,
      "learning_rate": 4.6620000000000004e-05,
      "loss": 0.0025,
      "step": 5070
    },
    {
      "epoch": 0.27093333333333336,
      "grad_norm": 0.22816161811351776,
      "learning_rate": 4.6613333333333337e-05,
      "loss": 0.0025,
      "step": 5080
    },
    {
      "epoch": 0.2714666666666667,
      "grad_norm": 0.6117860674858093,
      "learning_rate": 4.660666666666667e-05,
      "loss": 0.0029,
      "step": 5090
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.18177567422389984,
      "learning_rate": 4.660000000000001e-05,
      "loss": 0.0022,
      "step": 5100
    },
    {
      "epoch": 0.27253333333333335,
      "grad_norm": 0.37745723128318787,
      "learning_rate": 4.659333333333333e-05,
      "loss": 0.0027,
      "step": 5110
    },
    {
      "epoch": 0.2730666666666667,
      "grad_norm": 0.44054916501045227,
      "learning_rate": 4.6586666666666666e-05,
      "loss": 0.0029,
      "step": 5120
    },
    {
      "epoch": 0.2736,
      "grad_norm": 0.1583373248577118,
      "learning_rate": 4.6580000000000005e-05,
      "loss": 0.0027,
      "step": 5130
    },
    {
      "epoch": 0.27413333333333334,
      "grad_norm": 0.46865323185920715,
      "learning_rate": 4.657333333333334e-05,
      "loss": 0.0027,
      "step": 5140
    },
    {
      "epoch": 0.27466666666666667,
      "grad_norm": 0.3472897708415985,
      "learning_rate": 4.656666666666667e-05,
      "loss": 0.0028,
      "step": 5150
    },
    {
      "epoch": 0.2752,
      "grad_norm": 0.5331024527549744,
      "learning_rate": 4.656e-05,
      "loss": 0.0023,
      "step": 5160
    },
    {
      "epoch": 0.27573333333333333,
      "grad_norm": 0.55333012342453,
      "learning_rate": 4.655333333333334e-05,
      "loss": 0.0022,
      "step": 5170
    },
    {
      "epoch": 0.27626666666666666,
      "grad_norm": 0.434289813041687,
      "learning_rate": 4.6546666666666666e-05,
      "loss": 0.0021,
      "step": 5180
    },
    {
      "epoch": 0.2768,
      "grad_norm": 0.32559967041015625,
      "learning_rate": 4.654e-05,
      "loss": 0.0028,
      "step": 5190
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 0.7965193390846252,
      "learning_rate": 4.653333333333334e-05,
      "loss": 0.0026,
      "step": 5200
    },
    {
      "epoch": 0.27786666666666665,
      "grad_norm": 0.5420857667922974,
      "learning_rate": 4.652666666666667e-05,
      "loss": 0.0021,
      "step": 5210
    },
    {
      "epoch": 0.2784,
      "grad_norm": 0.35152116417884827,
      "learning_rate": 4.652e-05,
      "loss": 0.0038,
      "step": 5220
    },
    {
      "epoch": 0.2789333333333333,
      "grad_norm": 0.17360475659370422,
      "learning_rate": 4.6513333333333334e-05,
      "loss": 0.0026,
      "step": 5230
    },
    {
      "epoch": 0.27946666666666664,
      "grad_norm": 0.2927291989326477,
      "learning_rate": 4.650666666666667e-05,
      "loss": 0.0045,
      "step": 5240
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.5760789513587952,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 0.0021,
      "step": 5250
    },
    {
      "epoch": 0.28053333333333336,
      "grad_norm": 0.39937421679496765,
      "learning_rate": 4.649333333333333e-05,
      "loss": 0.0032,
      "step": 5260
    },
    {
      "epoch": 0.2810666666666667,
      "grad_norm": 0.9651690125465393,
      "learning_rate": 4.648666666666667e-05,
      "loss": 0.0025,
      "step": 5270
    },
    {
      "epoch": 0.2816,
      "grad_norm": 0.11113063991069794,
      "learning_rate": 4.648e-05,
      "loss": 0.0034,
      "step": 5280
    },
    {
      "epoch": 0.28213333333333335,
      "grad_norm": 0.3671668767929077,
      "learning_rate": 4.6473333333333334e-05,
      "loss": 0.0023,
      "step": 5290
    },
    {
      "epoch": 0.2826666666666667,
      "grad_norm": 0.8890907168388367,
      "learning_rate": 4.646666666666667e-05,
      "loss": 0.0034,
      "step": 5300
    },
    {
      "epoch": 0.2832,
      "grad_norm": 0.4563392102718353,
      "learning_rate": 4.6460000000000006e-05,
      "loss": 0.0042,
      "step": 5310
    },
    {
      "epoch": 0.28373333333333334,
      "grad_norm": 0.8349430561065674,
      "learning_rate": 4.645333333333334e-05,
      "loss": 0.0028,
      "step": 5320
    },
    {
      "epoch": 0.28426666666666667,
      "grad_norm": 0.2123790830373764,
      "learning_rate": 4.644666666666667e-05,
      "loss": 0.0027,
      "step": 5330
    },
    {
      "epoch": 0.2848,
      "grad_norm": 0.3281066119670868,
      "learning_rate": 4.644e-05,
      "loss": 0.0033,
      "step": 5340
    },
    {
      "epoch": 0.2853333333333333,
      "grad_norm": 0.39603710174560547,
      "learning_rate": 4.6433333333333335e-05,
      "loss": 0.0026,
      "step": 5350
    },
    {
      "epoch": 0.28586666666666666,
      "grad_norm": 0.5774742960929871,
      "learning_rate": 4.642666666666667e-05,
      "loss": 0.0032,
      "step": 5360
    },
    {
      "epoch": 0.2864,
      "grad_norm": 0.15657870471477509,
      "learning_rate": 4.642e-05,
      "loss": 0.0031,
      "step": 5370
    },
    {
      "epoch": 0.2869333333333333,
      "grad_norm": 0.14737720787525177,
      "learning_rate": 4.641333333333334e-05,
      "loss": 0.003,
      "step": 5380
    },
    {
      "epoch": 0.28746666666666665,
      "grad_norm": 0.9896837472915649,
      "learning_rate": 4.640666666666667e-05,
      "loss": 0.0024,
      "step": 5390
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.2883104979991913,
      "learning_rate": 4.64e-05,
      "loss": 0.0032,
      "step": 5400
    },
    {
      "epoch": 0.2885333333333333,
      "grad_norm": 0.6155561208724976,
      "learning_rate": 4.6393333333333335e-05,
      "loss": 0.0021,
      "step": 5410
    },
    {
      "epoch": 0.2890666666666667,
      "grad_norm": 0.12068665772676468,
      "learning_rate": 4.638666666666667e-05,
      "loss": 0.0042,
      "step": 5420
    },
    {
      "epoch": 0.2896,
      "grad_norm": 0.8173726797103882,
      "learning_rate": 4.638e-05,
      "loss": 0.0027,
      "step": 5430
    },
    {
      "epoch": 0.29013333333333335,
      "grad_norm": 0.44468390941619873,
      "learning_rate": 4.637333333333333e-05,
      "loss": 0.0028,
      "step": 5440
    },
    {
      "epoch": 0.2906666666666667,
      "grad_norm": 0.2211223840713501,
      "learning_rate": 4.636666666666667e-05,
      "loss": 0.0026,
      "step": 5450
    },
    {
      "epoch": 0.2912,
      "grad_norm": 0.29922157526016235,
      "learning_rate": 4.636e-05,
      "loss": 0.0022,
      "step": 5460
    },
    {
      "epoch": 0.29173333333333334,
      "grad_norm": 0.2827645540237427,
      "learning_rate": 4.6353333333333336e-05,
      "loss": 0.0024,
      "step": 5470
    },
    {
      "epoch": 0.2922666666666667,
      "grad_norm": 0.1312853991985321,
      "learning_rate": 4.6346666666666675e-05,
      "loss": 0.0024,
      "step": 5480
    },
    {
      "epoch": 0.2928,
      "grad_norm": 0.6047903895378113,
      "learning_rate": 4.634e-05,
      "loss": 0.0035,
      "step": 5490
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.1811807006597519,
      "learning_rate": 4.633333333333333e-05,
      "loss": 0.0034,
      "step": 5500
    },
    {
      "epoch": 0.29386666666666666,
      "grad_norm": 0.25907954573631287,
      "learning_rate": 4.632666666666667e-05,
      "loss": 0.0032,
      "step": 5510
    },
    {
      "epoch": 0.2944,
      "grad_norm": 0.20848099887371063,
      "learning_rate": 4.6320000000000004e-05,
      "loss": 0.0032,
      "step": 5520
    },
    {
      "epoch": 0.2949333333333333,
      "grad_norm": 0.1753378063440323,
      "learning_rate": 4.6313333333333336e-05,
      "loss": 0.0026,
      "step": 5530
    },
    {
      "epoch": 0.29546666666666666,
      "grad_norm": 0.2665535509586334,
      "learning_rate": 4.630666666666667e-05,
      "loss": 0.0023,
      "step": 5540
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.6285258531570435,
      "learning_rate": 4.630000000000001e-05,
      "loss": 0.0033,
      "step": 5550
    },
    {
      "epoch": 0.2965333333333333,
      "grad_norm": 0.22724983096122742,
      "learning_rate": 4.629333333333333e-05,
      "loss": 0.0026,
      "step": 5560
    },
    {
      "epoch": 0.29706666666666665,
      "grad_norm": 0.3252686858177185,
      "learning_rate": 4.6286666666666665e-05,
      "loss": 0.0039,
      "step": 5570
    },
    {
      "epoch": 0.2976,
      "grad_norm": 0.5023481249809265,
      "learning_rate": 4.6280000000000004e-05,
      "loss": 0.0052,
      "step": 5580
    },
    {
      "epoch": 0.2981333333333333,
      "grad_norm": 0.37756019830703735,
      "learning_rate": 4.6273333333333336e-05,
      "loss": 0.004,
      "step": 5590
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 0.3191165626049042,
      "learning_rate": 4.626666666666667e-05,
      "loss": 0.0036,
      "step": 5600
    },
    {
      "epoch": 0.2992,
      "grad_norm": 0.24185629189014435,
      "learning_rate": 4.626e-05,
      "loss": 0.0051,
      "step": 5610
    },
    {
      "epoch": 0.29973333333333335,
      "grad_norm": 0.8190969824790955,
      "learning_rate": 4.625333333333334e-05,
      "loss": 0.0042,
      "step": 5620
    },
    {
      "epoch": 0.3002666666666667,
      "grad_norm": 0.48133784532546997,
      "learning_rate": 4.624666666666667e-05,
      "loss": 0.0029,
      "step": 5630
    },
    {
      "epoch": 0.3008,
      "grad_norm": 0.14763852953910828,
      "learning_rate": 4.624e-05,
      "loss": 0.0032,
      "step": 5640
    },
    {
      "epoch": 0.30133333333333334,
      "grad_norm": 0.788478434085846,
      "learning_rate": 4.623333333333334e-05,
      "loss": 0.0033,
      "step": 5650
    },
    {
      "epoch": 0.30186666666666667,
      "grad_norm": 0.41973596811294556,
      "learning_rate": 4.622666666666667e-05,
      "loss": 0.004,
      "step": 5660
    },
    {
      "epoch": 0.3024,
      "grad_norm": 0.5250651240348816,
      "learning_rate": 4.622e-05,
      "loss": 0.0033,
      "step": 5670
    },
    {
      "epoch": 0.30293333333333333,
      "grad_norm": 0.17645180225372314,
      "learning_rate": 4.6213333333333334e-05,
      "loss": 0.0025,
      "step": 5680
    },
    {
      "epoch": 0.30346666666666666,
      "grad_norm": 0.3704414367675781,
      "learning_rate": 4.620666666666667e-05,
      "loss": 0.0034,
      "step": 5690
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.12114166468381882,
      "learning_rate": 4.6200000000000005e-05,
      "loss": 0.0037,
      "step": 5700
    },
    {
      "epoch": 0.3045333333333333,
      "grad_norm": 0.6856337785720825,
      "learning_rate": 4.619333333333333e-05,
      "loss": 0.0023,
      "step": 5710
    },
    {
      "epoch": 0.30506666666666665,
      "grad_norm": 1.1187604665756226,
      "learning_rate": 4.618666666666667e-05,
      "loss": 0.0019,
      "step": 5720
    },
    {
      "epoch": 0.3056,
      "grad_norm": 0.851509153842926,
      "learning_rate": 4.618e-05,
      "loss": 0.0019,
      "step": 5730
    },
    {
      "epoch": 0.3061333333333333,
      "grad_norm": 0.2320144772529602,
      "learning_rate": 4.6173333333333334e-05,
      "loss": 0.0032,
      "step": 5740
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.26059451699256897,
      "learning_rate": 4.6166666666666666e-05,
      "loss": 0.0026,
      "step": 5750
    },
    {
      "epoch": 0.3072,
      "grad_norm": 0.16153834760189056,
      "learning_rate": 4.6160000000000005e-05,
      "loss": 0.0033,
      "step": 5760
    },
    {
      "epoch": 0.30773333333333336,
      "grad_norm": 0.20976679027080536,
      "learning_rate": 4.615333333333334e-05,
      "loss": 0.0022,
      "step": 5770
    },
    {
      "epoch": 0.3082666666666667,
      "grad_norm": 0.5948770046234131,
      "learning_rate": 4.614666666666667e-05,
      "loss": 0.0029,
      "step": 5780
    },
    {
      "epoch": 0.3088,
      "grad_norm": 0.6968031525611877,
      "learning_rate": 4.614e-05,
      "loss": 0.0033,
      "step": 5790
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 0.24055913090705872,
      "learning_rate": 4.6133333333333334e-05,
      "loss": 0.002,
      "step": 5800
    },
    {
      "epoch": 0.3098666666666667,
      "grad_norm": 0.4588007628917694,
      "learning_rate": 4.612666666666667e-05,
      "loss": 0.0025,
      "step": 5810
    },
    {
      "epoch": 0.3104,
      "grad_norm": 0.6248383522033691,
      "learning_rate": 4.612e-05,
      "loss": 0.0026,
      "step": 5820
    },
    {
      "epoch": 0.31093333333333334,
      "grad_norm": 0.2307928502559662,
      "learning_rate": 4.611333333333334e-05,
      "loss": 0.0025,
      "step": 5830
    },
    {
      "epoch": 0.31146666666666667,
      "grad_norm": 0.4166848361492157,
      "learning_rate": 4.610666666666667e-05,
      "loss": 0.0026,
      "step": 5840
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.680240273475647,
      "learning_rate": 4.61e-05,
      "loss": 0.0035,
      "step": 5850
    },
    {
      "epoch": 0.31253333333333333,
      "grad_norm": 0.29215651750564575,
      "learning_rate": 4.6093333333333335e-05,
      "loss": 0.0025,
      "step": 5860
    },
    {
      "epoch": 0.31306666666666666,
      "grad_norm": 0.2111167162656784,
      "learning_rate": 4.608666666666667e-05,
      "loss": 0.0026,
      "step": 5870
    },
    {
      "epoch": 0.3136,
      "grad_norm": 0.36517491936683655,
      "learning_rate": 4.608e-05,
      "loss": 0.0025,
      "step": 5880
    },
    {
      "epoch": 0.3141333333333333,
      "grad_norm": 0.23406478762626648,
      "learning_rate": 4.607333333333334e-05,
      "loss": 0.0023,
      "step": 5890
    },
    {
      "epoch": 0.31466666666666665,
      "grad_norm": 0.3684067130088806,
      "learning_rate": 4.606666666666667e-05,
      "loss": 0.0036,
      "step": 5900
    },
    {
      "epoch": 0.3152,
      "grad_norm": 0.4382248818874359,
      "learning_rate": 4.606e-05,
      "loss": 0.0022,
      "step": 5910
    },
    {
      "epoch": 0.3157333333333333,
      "grad_norm": 0.18496806919574738,
      "learning_rate": 4.6053333333333335e-05,
      "loss": 0.0023,
      "step": 5920
    },
    {
      "epoch": 0.31626666666666664,
      "grad_norm": 0.3721272647380829,
      "learning_rate": 4.6046666666666674e-05,
      "loss": 0.0035,
      "step": 5930
    },
    {
      "epoch": 0.3168,
      "grad_norm": 0.8033830523490906,
      "learning_rate": 4.604e-05,
      "loss": 0.0047,
      "step": 5940
    },
    {
      "epoch": 0.31733333333333336,
      "grad_norm": 0.1642746776342392,
      "learning_rate": 4.603333333333333e-05,
      "loss": 0.0022,
      "step": 5950
    },
    {
      "epoch": 0.3178666666666667,
      "grad_norm": 0.21734926104545593,
      "learning_rate": 4.602666666666667e-05,
      "loss": 0.0024,
      "step": 5960
    },
    {
      "epoch": 0.3184,
      "grad_norm": 0.28550270199775696,
      "learning_rate": 4.602e-05,
      "loss": 0.0034,
      "step": 5970
    },
    {
      "epoch": 0.31893333333333335,
      "grad_norm": 0.18123453855514526,
      "learning_rate": 4.6013333333333336e-05,
      "loss": 0.0032,
      "step": 5980
    },
    {
      "epoch": 0.3194666666666667,
      "grad_norm": 0.5517683625221252,
      "learning_rate": 4.600666666666667e-05,
      "loss": 0.0025,
      "step": 5990
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.7837954759597778,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.0031,
      "step": 6000
    },
    {
      "epoch": 0.32053333333333334,
      "grad_norm": 0.4189073145389557,
      "learning_rate": 4.599333333333334e-05,
      "loss": 0.0022,
      "step": 6010
    },
    {
      "epoch": 0.32106666666666667,
      "grad_norm": 0.23052158951759338,
      "learning_rate": 4.5986666666666665e-05,
      "loss": 0.0023,
      "step": 6020
    },
    {
      "epoch": 0.3216,
      "grad_norm": 0.321388840675354,
      "learning_rate": 4.5980000000000004e-05,
      "loss": 0.0024,
      "step": 6030
    },
    {
      "epoch": 0.3221333333333333,
      "grad_norm": 0.6832919716835022,
      "learning_rate": 4.5973333333333336e-05,
      "loss": 0.0032,
      "step": 6040
    },
    {
      "epoch": 0.32266666666666666,
      "grad_norm": 0.11670204997062683,
      "learning_rate": 4.596666666666667e-05,
      "loss": 0.0041,
      "step": 6050
    },
    {
      "epoch": 0.3232,
      "grad_norm": 0.5118327140808105,
      "learning_rate": 4.596e-05,
      "loss": 0.0027,
      "step": 6060
    },
    {
      "epoch": 0.3237333333333333,
      "grad_norm": 0.5834195017814636,
      "learning_rate": 4.595333333333334e-05,
      "loss": 0.0024,
      "step": 6070
    },
    {
      "epoch": 0.32426666666666665,
      "grad_norm": 0.19289536774158478,
      "learning_rate": 4.594666666666667e-05,
      "loss": 0.0029,
      "step": 6080
    },
    {
      "epoch": 0.3248,
      "grad_norm": 0.19301779568195343,
      "learning_rate": 4.594e-05,
      "loss": 0.0023,
      "step": 6090
    },
    {
      "epoch": 0.3253333333333333,
      "grad_norm": 0.12441714107990265,
      "learning_rate": 4.5933333333333336e-05,
      "loss": 0.004,
      "step": 6100
    },
    {
      "epoch": 0.3258666666666667,
      "grad_norm": 0.1745491325855255,
      "learning_rate": 4.592666666666667e-05,
      "loss": 0.0031,
      "step": 6110
    },
    {
      "epoch": 0.3264,
      "grad_norm": 0.4521141052246094,
      "learning_rate": 4.592e-05,
      "loss": 0.0034,
      "step": 6120
    },
    {
      "epoch": 0.32693333333333335,
      "grad_norm": 0.17097729444503784,
      "learning_rate": 4.591333333333333e-05,
      "loss": 0.0034,
      "step": 6130
    },
    {
      "epoch": 0.3274666666666667,
      "grad_norm": 0.13108514249324799,
      "learning_rate": 4.590666666666667e-05,
      "loss": 0.004,
      "step": 6140
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.15966607630252838,
      "learning_rate": 4.5900000000000004e-05,
      "loss": 0.0026,
      "step": 6150
    },
    {
      "epoch": 0.32853333333333334,
      "grad_norm": 0.30592840909957886,
      "learning_rate": 4.589333333333334e-05,
      "loss": 0.0034,
      "step": 6160
    },
    {
      "epoch": 0.3290666666666667,
      "grad_norm": 0.5662443041801453,
      "learning_rate": 4.588666666666667e-05,
      "loss": 0.0025,
      "step": 6170
    },
    {
      "epoch": 0.3296,
      "grad_norm": 0.5834577083587646,
      "learning_rate": 4.588e-05,
      "loss": 0.003,
      "step": 6180
    },
    {
      "epoch": 0.33013333333333333,
      "grad_norm": 0.14137886464595795,
      "learning_rate": 4.5873333333333333e-05,
      "loss": 0.0024,
      "step": 6190
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 0.23264342546463013,
      "learning_rate": 4.5866666666666666e-05,
      "loss": 0.0033,
      "step": 6200
    },
    {
      "epoch": 0.3312,
      "grad_norm": 0.2529006600379944,
      "learning_rate": 4.5860000000000005e-05,
      "loss": 0.0029,
      "step": 6210
    },
    {
      "epoch": 0.3317333333333333,
      "grad_norm": 0.3963468670845032,
      "learning_rate": 4.585333333333334e-05,
      "loss": 0.0039,
      "step": 6220
    },
    {
      "epoch": 0.33226666666666665,
      "grad_norm": 0.28557538986206055,
      "learning_rate": 4.584666666666667e-05,
      "loss": 0.0022,
      "step": 6230
    },
    {
      "epoch": 0.3328,
      "grad_norm": 0.7676320672035217,
      "learning_rate": 4.584e-05,
      "loss": 0.0023,
      "step": 6240
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.6805170178413391,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 0.0025,
      "step": 6250
    },
    {
      "epoch": 0.33386666666666664,
      "grad_norm": 0.2623727023601532,
      "learning_rate": 4.5826666666666666e-05,
      "loss": 0.0036,
      "step": 6260
    },
    {
      "epoch": 0.3344,
      "grad_norm": 0.14240984618663788,
      "learning_rate": 4.5820000000000005e-05,
      "loss": 0.0039,
      "step": 6270
    },
    {
      "epoch": 0.33493333333333336,
      "grad_norm": 0.21310490369796753,
      "learning_rate": 4.581333333333334e-05,
      "loss": 0.0027,
      "step": 6280
    },
    {
      "epoch": 0.3354666666666667,
      "grad_norm": 0.2146952748298645,
      "learning_rate": 4.580666666666667e-05,
      "loss": 0.0035,
      "step": 6290
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.3704763352870941,
      "learning_rate": 4.58e-05,
      "loss": 0.0024,
      "step": 6300
    },
    {
      "epoch": 0.33653333333333335,
      "grad_norm": 0.4632244110107422,
      "learning_rate": 4.579333333333334e-05,
      "loss": 0.0035,
      "step": 6310
    },
    {
      "epoch": 0.3370666666666667,
      "grad_norm": 0.10888242721557617,
      "learning_rate": 4.5786666666666666e-05,
      "loss": 0.0023,
      "step": 6320
    },
    {
      "epoch": 0.3376,
      "grad_norm": 0.35196417570114136,
      "learning_rate": 4.578e-05,
      "loss": 0.0044,
      "step": 6330
    },
    {
      "epoch": 0.33813333333333334,
      "grad_norm": 0.1206723228096962,
      "learning_rate": 4.577333333333334e-05,
      "loss": 0.0032,
      "step": 6340
    },
    {
      "epoch": 0.33866666666666667,
      "grad_norm": 0.6103397607803345,
      "learning_rate": 4.576666666666667e-05,
      "loss": 0.0027,
      "step": 6350
    },
    {
      "epoch": 0.3392,
      "grad_norm": 0.5054044127464294,
      "learning_rate": 4.576e-05,
      "loss": 0.0023,
      "step": 6360
    },
    {
      "epoch": 0.33973333333333333,
      "grad_norm": 0.6686492562294006,
      "learning_rate": 4.5753333333333335e-05,
      "loss": 0.0023,
      "step": 6370
    },
    {
      "epoch": 0.34026666666666666,
      "grad_norm": 0.6831923127174377,
      "learning_rate": 4.5746666666666674e-05,
      "loss": 0.0031,
      "step": 6380
    },
    {
      "epoch": 0.3408,
      "grad_norm": 0.1545286923646927,
      "learning_rate": 4.574e-05,
      "loss": 0.0038,
      "step": 6390
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 0.14881004393100739,
      "learning_rate": 4.573333333333333e-05,
      "loss": 0.0042,
      "step": 6400
    },
    {
      "epoch": 0.34186666666666665,
      "grad_norm": 0.1784135103225708,
      "learning_rate": 4.572666666666667e-05,
      "loss": 0.0039,
      "step": 6410
    },
    {
      "epoch": 0.3424,
      "grad_norm": 0.32483983039855957,
      "learning_rate": 4.572e-05,
      "loss": 0.0034,
      "step": 6420
    },
    {
      "epoch": 0.3429333333333333,
      "grad_norm": 0.6065002083778381,
      "learning_rate": 4.5713333333333335e-05,
      "loss": 0.0042,
      "step": 6430
    },
    {
      "epoch": 0.34346666666666664,
      "grad_norm": 0.18294820189476013,
      "learning_rate": 4.570666666666667e-05,
      "loss": 0.0028,
      "step": 6440
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.3396157920360565,
      "learning_rate": 4.5700000000000006e-05,
      "loss": 0.0037,
      "step": 6450
    },
    {
      "epoch": 0.34453333333333336,
      "grad_norm": 0.19058997929096222,
      "learning_rate": 4.569333333333334e-05,
      "loss": 0.0024,
      "step": 6460
    },
    {
      "epoch": 0.3450666666666667,
      "grad_norm": 0.12620361149311066,
      "learning_rate": 4.5686666666666664e-05,
      "loss": 0.0029,
      "step": 6470
    },
    {
      "epoch": 0.3456,
      "grad_norm": 0.6264123320579529,
      "learning_rate": 4.568e-05,
      "loss": 0.0025,
      "step": 6480
    },
    {
      "epoch": 0.34613333333333335,
      "grad_norm": 0.5785465240478516,
      "learning_rate": 4.5673333333333335e-05,
      "loss": 0.0024,
      "step": 6490
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.40098994970321655,
      "learning_rate": 4.566666666666667e-05,
      "loss": 0.0028,
      "step": 6500
    },
    {
      "epoch": 0.3472,
      "grad_norm": 0.4222799837589264,
      "learning_rate": 4.566e-05,
      "loss": 0.0031,
      "step": 6510
    },
    {
      "epoch": 0.34773333333333334,
      "grad_norm": 0.23303526639938354,
      "learning_rate": 4.565333333333334e-05,
      "loss": 0.0024,
      "step": 6520
    },
    {
      "epoch": 0.34826666666666667,
      "grad_norm": 0.45802780985832214,
      "learning_rate": 4.564666666666667e-05,
      "loss": 0.0033,
      "step": 6530
    },
    {
      "epoch": 0.3488,
      "grad_norm": 0.2525695860385895,
      "learning_rate": 4.564e-05,
      "loss": 0.004,
      "step": 6540
    },
    {
      "epoch": 0.34933333333333333,
      "grad_norm": 1.0102137327194214,
      "learning_rate": 4.5633333333333336e-05,
      "loss": 0.003,
      "step": 6550
    },
    {
      "epoch": 0.34986666666666666,
      "grad_norm": 0.4532020092010498,
      "learning_rate": 4.562666666666667e-05,
      "loss": 0.0029,
      "step": 6560
    },
    {
      "epoch": 0.3504,
      "grad_norm": 0.4468570351600647,
      "learning_rate": 4.562e-05,
      "loss": 0.0024,
      "step": 6570
    },
    {
      "epoch": 0.3509333333333333,
      "grad_norm": 0.3581322729587555,
      "learning_rate": 4.561333333333333e-05,
      "loss": 0.0019,
      "step": 6580
    },
    {
      "epoch": 0.35146666666666665,
      "grad_norm": 0.7225591540336609,
      "learning_rate": 4.560666666666667e-05,
      "loss": 0.0024,
      "step": 6590
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.6336888074874878,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 0.0028,
      "step": 6600
    },
    {
      "epoch": 0.3525333333333333,
      "grad_norm": 0.5440236330032349,
      "learning_rate": 4.5593333333333336e-05,
      "loss": 0.002,
      "step": 6610
    },
    {
      "epoch": 0.35306666666666664,
      "grad_norm": 0.1755441576242447,
      "learning_rate": 4.558666666666667e-05,
      "loss": 0.0024,
      "step": 6620
    },
    {
      "epoch": 0.3536,
      "grad_norm": 0.09021978080272675,
      "learning_rate": 4.558e-05,
      "loss": 0.0045,
      "step": 6630
    },
    {
      "epoch": 0.35413333333333336,
      "grad_norm": 0.18286070227622986,
      "learning_rate": 4.557333333333333e-05,
      "loss": 0.0029,
      "step": 6640
    },
    {
      "epoch": 0.3546666666666667,
      "grad_norm": 0.2950824797153473,
      "learning_rate": 4.556666666666667e-05,
      "loss": 0.0046,
      "step": 6650
    },
    {
      "epoch": 0.3552,
      "grad_norm": 0.22495996952056885,
      "learning_rate": 4.5560000000000004e-05,
      "loss": 0.0034,
      "step": 6660
    },
    {
      "epoch": 0.35573333333333335,
      "grad_norm": 0.365462988615036,
      "learning_rate": 4.5553333333333337e-05,
      "loss": 0.0023,
      "step": 6670
    },
    {
      "epoch": 0.3562666666666667,
      "grad_norm": 0.13061745464801788,
      "learning_rate": 4.554666666666667e-05,
      "loss": 0.0025,
      "step": 6680
    },
    {
      "epoch": 0.3568,
      "grad_norm": 0.37507396936416626,
      "learning_rate": 4.554000000000001e-05,
      "loss": 0.003,
      "step": 6690
    },
    {
      "epoch": 0.35733333333333334,
      "grad_norm": 0.5692906975746155,
      "learning_rate": 4.553333333333333e-05,
      "loss": 0.0029,
      "step": 6700
    },
    {
      "epoch": 0.35786666666666667,
      "grad_norm": 0.21886439621448517,
      "learning_rate": 4.5526666666666666e-05,
      "loss": 0.0046,
      "step": 6710
    },
    {
      "epoch": 0.3584,
      "grad_norm": 0.4248025417327881,
      "learning_rate": 4.5520000000000005e-05,
      "loss": 0.003,
      "step": 6720
    },
    {
      "epoch": 0.3589333333333333,
      "grad_norm": 0.7586855888366699,
      "learning_rate": 4.551333333333334e-05,
      "loss": 0.0031,
      "step": 6730
    },
    {
      "epoch": 0.35946666666666666,
      "grad_norm": 0.09538846462965012,
      "learning_rate": 4.550666666666667e-05,
      "loss": 0.0036,
      "step": 6740
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.2636389434337616,
      "learning_rate": 4.55e-05,
      "loss": 0.0029,
      "step": 6750
    },
    {
      "epoch": 0.3605333333333333,
      "grad_norm": 0.13973760604858398,
      "learning_rate": 4.549333333333334e-05,
      "loss": 0.0042,
      "step": 6760
    },
    {
      "epoch": 0.36106666666666665,
      "grad_norm": 0.1801254004240036,
      "learning_rate": 4.5486666666666666e-05,
      "loss": 0.0031,
      "step": 6770
    },
    {
      "epoch": 0.3616,
      "grad_norm": 0.3542024493217468,
      "learning_rate": 4.548e-05,
      "loss": 0.0026,
      "step": 6780
    },
    {
      "epoch": 0.3621333333333333,
      "grad_norm": 0.2869955003261566,
      "learning_rate": 4.547333333333334e-05,
      "loss": 0.0029,
      "step": 6790
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 0.43828266859054565,
      "learning_rate": 4.546666666666667e-05,
      "loss": 0.0025,
      "step": 6800
    },
    {
      "epoch": 0.3632,
      "grad_norm": 0.6186182498931885,
      "learning_rate": 4.546e-05,
      "loss": 0.0026,
      "step": 6810
    },
    {
      "epoch": 0.36373333333333335,
      "grad_norm": 0.4193705916404724,
      "learning_rate": 4.5453333333333334e-05,
      "loss": 0.0031,
      "step": 6820
    },
    {
      "epoch": 0.3642666666666667,
      "grad_norm": 0.3441842794418335,
      "learning_rate": 4.544666666666667e-05,
      "loss": 0.0024,
      "step": 6830
    },
    {
      "epoch": 0.3648,
      "grad_norm": 0.09814420342445374,
      "learning_rate": 4.5440000000000005e-05,
      "loss": 0.0021,
      "step": 6840
    },
    {
      "epoch": 0.36533333333333334,
      "grad_norm": 0.4962320625782013,
      "learning_rate": 4.543333333333333e-05,
      "loss": 0.0034,
      "step": 6850
    },
    {
      "epoch": 0.3658666666666667,
      "grad_norm": 0.23544006049633026,
      "learning_rate": 4.542666666666667e-05,
      "loss": 0.0024,
      "step": 6860
    },
    {
      "epoch": 0.3664,
      "grad_norm": 0.34866082668304443,
      "learning_rate": 4.542e-05,
      "loss": 0.0025,
      "step": 6870
    },
    {
      "epoch": 0.36693333333333333,
      "grad_norm": 0.6356057524681091,
      "learning_rate": 4.5413333333333334e-05,
      "loss": 0.0023,
      "step": 6880
    },
    {
      "epoch": 0.36746666666666666,
      "grad_norm": 0.2019558548927307,
      "learning_rate": 4.540666666666667e-05,
      "loss": 0.0026,
      "step": 6890
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.4990593492984772,
      "learning_rate": 4.5400000000000006e-05,
      "loss": 0.0026,
      "step": 6900
    },
    {
      "epoch": 0.3685333333333333,
      "grad_norm": 0.4084700047969818,
      "learning_rate": 4.539333333333334e-05,
      "loss": 0.0024,
      "step": 6910
    },
    {
      "epoch": 0.36906666666666665,
      "grad_norm": 0.619032621383667,
      "learning_rate": 4.5386666666666664e-05,
      "loss": 0.0021,
      "step": 6920
    },
    {
      "epoch": 0.3696,
      "grad_norm": 0.1467323899269104,
      "learning_rate": 4.538e-05,
      "loss": 0.0025,
      "step": 6930
    },
    {
      "epoch": 0.3701333333333333,
      "grad_norm": 0.556462824344635,
      "learning_rate": 4.5373333333333335e-05,
      "loss": 0.003,
      "step": 6940
    },
    {
      "epoch": 0.37066666666666664,
      "grad_norm": 0.295541375875473,
      "learning_rate": 4.536666666666667e-05,
      "loss": 0.0022,
      "step": 6950
    },
    {
      "epoch": 0.3712,
      "grad_norm": 0.17975926399230957,
      "learning_rate": 4.536e-05,
      "loss": 0.003,
      "step": 6960
    },
    {
      "epoch": 0.37173333333333336,
      "grad_norm": 0.29617083072662354,
      "learning_rate": 4.535333333333334e-05,
      "loss": 0.0027,
      "step": 6970
    },
    {
      "epoch": 0.3722666666666667,
      "grad_norm": 0.5279077291488647,
      "learning_rate": 4.534666666666667e-05,
      "loss": 0.003,
      "step": 6980
    },
    {
      "epoch": 0.3728,
      "grad_norm": 0.4667852520942688,
      "learning_rate": 4.534e-05,
      "loss": 0.004,
      "step": 6990
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.6100226044654846,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 0.003,
      "step": 7000
    },
    {
      "epoch": 0.3738666666666667,
      "grad_norm": 0.3106078803539276,
      "learning_rate": 4.532666666666667e-05,
      "loss": 0.0036,
      "step": 7010
    },
    {
      "epoch": 0.3744,
      "grad_norm": 0.14694148302078247,
      "learning_rate": 4.532e-05,
      "loss": 0.0027,
      "step": 7020
    },
    {
      "epoch": 0.37493333333333334,
      "grad_norm": 0.46190139651298523,
      "learning_rate": 4.531333333333333e-05,
      "loss": 0.0024,
      "step": 7030
    },
    {
      "epoch": 0.37546666666666667,
      "grad_norm": 0.1847943812608719,
      "learning_rate": 4.530666666666667e-05,
      "loss": 0.0024,
      "step": 7040
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.17731858789920807,
      "learning_rate": 4.53e-05,
      "loss": 0.0027,
      "step": 7050
    },
    {
      "epoch": 0.37653333333333333,
      "grad_norm": 0.7664588689804077,
      "learning_rate": 4.5293333333333336e-05,
      "loss": 0.0023,
      "step": 7060
    },
    {
      "epoch": 0.37706666666666666,
      "grad_norm": 0.1379391849040985,
      "learning_rate": 4.528666666666667e-05,
      "loss": 0.0028,
      "step": 7070
    },
    {
      "epoch": 0.3776,
      "grad_norm": 0.14609397947788239,
      "learning_rate": 4.528e-05,
      "loss": 0.002,
      "step": 7080
    },
    {
      "epoch": 0.3781333333333333,
      "grad_norm": 0.34976184368133545,
      "learning_rate": 4.527333333333333e-05,
      "loss": 0.0035,
      "step": 7090
    },
    {
      "epoch": 0.37866666666666665,
      "grad_norm": 0.6985770463943481,
      "learning_rate": 4.526666666666667e-05,
      "loss": 0.0042,
      "step": 7100
    },
    {
      "epoch": 0.3792,
      "grad_norm": 0.09139002114534378,
      "learning_rate": 4.5260000000000004e-05,
      "loss": 0.002,
      "step": 7110
    },
    {
      "epoch": 0.3797333333333333,
      "grad_norm": 0.2144063115119934,
      "learning_rate": 4.5253333333333336e-05,
      "loss": 0.0031,
      "step": 7120
    },
    {
      "epoch": 0.38026666666666664,
      "grad_norm": 0.24014674127101898,
      "learning_rate": 4.524666666666667e-05,
      "loss": 0.0033,
      "step": 7130
    },
    {
      "epoch": 0.3808,
      "grad_norm": 0.35132691264152527,
      "learning_rate": 4.524000000000001e-05,
      "loss": 0.0029,
      "step": 7140
    },
    {
      "epoch": 0.38133333333333336,
      "grad_norm": 0.6174924969673157,
      "learning_rate": 4.523333333333333e-05,
      "loss": 0.0035,
      "step": 7150
    },
    {
      "epoch": 0.3818666666666667,
      "grad_norm": 0.39836180210113525,
      "learning_rate": 4.5226666666666665e-05,
      "loss": 0.0024,
      "step": 7160
    },
    {
      "epoch": 0.3824,
      "grad_norm": 0.4374343752861023,
      "learning_rate": 4.5220000000000004e-05,
      "loss": 0.0034,
      "step": 7170
    },
    {
      "epoch": 0.38293333333333335,
      "grad_norm": 0.3111799359321594,
      "learning_rate": 4.5213333333333336e-05,
      "loss": 0.0029,
      "step": 7180
    },
    {
      "epoch": 0.3834666666666667,
      "grad_norm": 0.5689002275466919,
      "learning_rate": 4.520666666666667e-05,
      "loss": 0.0023,
      "step": 7190
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.45469796657562256,
      "learning_rate": 4.52e-05,
      "loss": 0.0023,
      "step": 7200
    },
    {
      "epoch": 0.38453333333333334,
      "grad_norm": 0.23215292394161224,
      "learning_rate": 4.519333333333334e-05,
      "loss": 0.0021,
      "step": 7210
    },
    {
      "epoch": 0.38506666666666667,
      "grad_norm": 0.5105581879615784,
      "learning_rate": 4.518666666666667e-05,
      "loss": 0.0032,
      "step": 7220
    },
    {
      "epoch": 0.3856,
      "grad_norm": 0.7032122015953064,
      "learning_rate": 4.518e-05,
      "loss": 0.0024,
      "step": 7230
    },
    {
      "epoch": 0.38613333333333333,
      "grad_norm": 0.5178971290588379,
      "learning_rate": 4.517333333333334e-05,
      "loss": 0.003,
      "step": 7240
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.09744606912136078,
      "learning_rate": 4.516666666666667e-05,
      "loss": 0.0039,
      "step": 7250
    },
    {
      "epoch": 0.3872,
      "grad_norm": 0.6816825866699219,
      "learning_rate": 4.516e-05,
      "loss": 0.0021,
      "step": 7260
    },
    {
      "epoch": 0.3877333333333333,
      "grad_norm": 0.4784955382347107,
      "learning_rate": 4.5153333333333334e-05,
      "loss": 0.0025,
      "step": 7270
    },
    {
      "epoch": 0.38826666666666665,
      "grad_norm": 0.5398122072219849,
      "learning_rate": 4.514666666666667e-05,
      "loss": 0.0027,
      "step": 7280
    },
    {
      "epoch": 0.3888,
      "grad_norm": 0.5298687219619751,
      "learning_rate": 4.5140000000000005e-05,
      "loss": 0.0023,
      "step": 7290
    },
    {
      "epoch": 0.3893333333333333,
      "grad_norm": 0.5982925295829773,
      "learning_rate": 4.513333333333333e-05,
      "loss": 0.0024,
      "step": 7300
    },
    {
      "epoch": 0.38986666666666664,
      "grad_norm": 0.24049314856529236,
      "learning_rate": 4.512666666666667e-05,
      "loss": 0.0019,
      "step": 7310
    },
    {
      "epoch": 0.3904,
      "grad_norm": 0.09638724476099014,
      "learning_rate": 4.512e-05,
      "loss": 0.0019,
      "step": 7320
    },
    {
      "epoch": 0.39093333333333335,
      "grad_norm": 0.34326738119125366,
      "learning_rate": 4.5113333333333334e-05,
      "loss": 0.0023,
      "step": 7330
    },
    {
      "epoch": 0.3914666666666667,
      "grad_norm": 0.6423019766807556,
      "learning_rate": 4.5106666666666666e-05,
      "loss": 0.0026,
      "step": 7340
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.375956654548645,
      "learning_rate": 4.5100000000000005e-05,
      "loss": 0.0029,
      "step": 7350
    },
    {
      "epoch": 0.39253333333333335,
      "grad_norm": 0.13323059678077698,
      "learning_rate": 4.509333333333334e-05,
      "loss": 0.0039,
      "step": 7360
    },
    {
      "epoch": 0.3930666666666667,
      "grad_norm": 0.13803575932979584,
      "learning_rate": 4.508666666666667e-05,
      "loss": 0.0037,
      "step": 7370
    },
    {
      "epoch": 0.3936,
      "grad_norm": 0.6360557079315186,
      "learning_rate": 4.508e-05,
      "loss": 0.0036,
      "step": 7380
    },
    {
      "epoch": 0.39413333333333334,
      "grad_norm": 0.3213338851928711,
      "learning_rate": 4.5073333333333334e-05,
      "loss": 0.003,
      "step": 7390
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 0.19710838794708252,
      "learning_rate": 4.5066666666666667e-05,
      "loss": 0.0022,
      "step": 7400
    },
    {
      "epoch": 0.3952,
      "grad_norm": 0.6682311296463013,
      "learning_rate": 4.506e-05,
      "loss": 0.0043,
      "step": 7410
    },
    {
      "epoch": 0.3957333333333333,
      "grad_norm": 0.20394903421401978,
      "learning_rate": 4.505333333333334e-05,
      "loss": 0.0025,
      "step": 7420
    },
    {
      "epoch": 0.39626666666666666,
      "grad_norm": 0.6876933574676514,
      "learning_rate": 4.504666666666667e-05,
      "loss": 0.0036,
      "step": 7430
    },
    {
      "epoch": 0.3968,
      "grad_norm": 0.38749831914901733,
      "learning_rate": 4.504e-05,
      "loss": 0.0032,
      "step": 7440
    },
    {
      "epoch": 0.3973333333333333,
      "grad_norm": 0.6737465262413025,
      "learning_rate": 4.5033333333333335e-05,
      "loss": 0.0037,
      "step": 7450
    },
    {
      "epoch": 0.39786666666666665,
      "grad_norm": 0.647747814655304,
      "learning_rate": 4.502666666666667e-05,
      "loss": 0.0025,
      "step": 7460
    },
    {
      "epoch": 0.3984,
      "grad_norm": 0.211536705493927,
      "learning_rate": 4.502e-05,
      "loss": 0.0029,
      "step": 7470
    },
    {
      "epoch": 0.3989333333333333,
      "grad_norm": 0.5367608666419983,
      "learning_rate": 4.501333333333334e-05,
      "loss": 0.0037,
      "step": 7480
    },
    {
      "epoch": 0.3994666666666667,
      "grad_norm": 0.14213722944259644,
      "learning_rate": 4.500666666666667e-05,
      "loss": 0.0025,
      "step": 7490
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.08501049876213074,
      "learning_rate": 4.5e-05,
      "loss": 0.0022,
      "step": 7500
    },
    {
      "epoch": 0.40053333333333335,
      "grad_norm": 0.2800588011741638,
      "learning_rate": 4.4993333333333335e-05,
      "loss": 0.0028,
      "step": 7510
    },
    {
      "epoch": 0.4010666666666667,
      "grad_norm": 0.12123752385377884,
      "learning_rate": 4.4986666666666674e-05,
      "loss": 0.0034,
      "step": 7520
    },
    {
      "epoch": 0.4016,
      "grad_norm": 0.3718096613883972,
      "learning_rate": 4.498e-05,
      "loss": 0.0039,
      "step": 7530
    },
    {
      "epoch": 0.40213333333333334,
      "grad_norm": 0.1965794861316681,
      "learning_rate": 4.497333333333333e-05,
      "loss": 0.0029,
      "step": 7540
    },
    {
      "epoch": 0.4026666666666667,
      "grad_norm": 0.969935953617096,
      "learning_rate": 4.496666666666667e-05,
      "loss": 0.003,
      "step": 7550
    },
    {
      "epoch": 0.4032,
      "grad_norm": 0.12615840137004852,
      "learning_rate": 4.496e-05,
      "loss": 0.003,
      "step": 7560
    },
    {
      "epoch": 0.40373333333333333,
      "grad_norm": 0.4556472599506378,
      "learning_rate": 4.4953333333333335e-05,
      "loss": 0.0022,
      "step": 7570
    },
    {
      "epoch": 0.40426666666666666,
      "grad_norm": 0.7097449898719788,
      "learning_rate": 4.494666666666667e-05,
      "loss": 0.0028,
      "step": 7580
    },
    {
      "epoch": 0.4048,
      "grad_norm": 0.9101220369338989,
      "learning_rate": 4.494000000000001e-05,
      "loss": 0.0035,
      "step": 7590
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 0.1479693204164505,
      "learning_rate": 4.493333333333333e-05,
      "loss": 0.0024,
      "step": 7600
    },
    {
      "epoch": 0.40586666666666665,
      "grad_norm": 0.23226937651634216,
      "learning_rate": 4.4926666666666665e-05,
      "loss": 0.0027,
      "step": 7610
    },
    {
      "epoch": 0.4064,
      "grad_norm": 0.4930363893508911,
      "learning_rate": 4.4920000000000004e-05,
      "loss": 0.0036,
      "step": 7620
    },
    {
      "epoch": 0.4069333333333333,
      "grad_norm": 0.1845661848783493,
      "learning_rate": 4.4913333333333336e-05,
      "loss": 0.0034,
      "step": 7630
    },
    {
      "epoch": 0.40746666666666664,
      "grad_norm": 0.2716238498687744,
      "learning_rate": 4.490666666666667e-05,
      "loss": 0.0028,
      "step": 7640
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.34175175428390503,
      "learning_rate": 4.49e-05,
      "loss": 0.0026,
      "step": 7650
    },
    {
      "epoch": 0.40853333333333336,
      "grad_norm": 0.34277603030204773,
      "learning_rate": 4.489333333333334e-05,
      "loss": 0.0024,
      "step": 7660
    },
    {
      "epoch": 0.4090666666666667,
      "grad_norm": 0.6180797219276428,
      "learning_rate": 4.488666666666667e-05,
      "loss": 0.0033,
      "step": 7670
    },
    {
      "epoch": 0.4096,
      "grad_norm": 0.11194393783807755,
      "learning_rate": 4.488e-05,
      "loss": 0.0029,
      "step": 7680
    },
    {
      "epoch": 0.41013333333333335,
      "grad_norm": 0.1279079020023346,
      "learning_rate": 4.4873333333333336e-05,
      "loss": 0.0033,
      "step": 7690
    },
    {
      "epoch": 0.4106666666666667,
      "grad_norm": 0.37000685930252075,
      "learning_rate": 4.486666666666667e-05,
      "loss": 0.0022,
      "step": 7700
    },
    {
      "epoch": 0.4112,
      "grad_norm": 0.3729388415813446,
      "learning_rate": 4.486e-05,
      "loss": 0.003,
      "step": 7710
    },
    {
      "epoch": 0.41173333333333334,
      "grad_norm": 0.5130385756492615,
      "learning_rate": 4.485333333333333e-05,
      "loss": 0.0031,
      "step": 7720
    },
    {
      "epoch": 0.41226666666666667,
      "grad_norm": 0.22804927825927734,
      "learning_rate": 4.484666666666667e-05,
      "loss": 0.003,
      "step": 7730
    },
    {
      "epoch": 0.4128,
      "grad_norm": 0.11424120515584946,
      "learning_rate": 4.4840000000000004e-05,
      "loss": 0.0033,
      "step": 7740
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.9378360509872437,
      "learning_rate": 4.483333333333333e-05,
      "loss": 0.0032,
      "step": 7750
    },
    {
      "epoch": 0.41386666666666666,
      "grad_norm": 0.16023126244544983,
      "learning_rate": 4.482666666666667e-05,
      "loss": 0.0031,
      "step": 7760
    },
    {
      "epoch": 0.4144,
      "grad_norm": 0.22149379551410675,
      "learning_rate": 4.482e-05,
      "loss": 0.003,
      "step": 7770
    },
    {
      "epoch": 0.4149333333333333,
      "grad_norm": 0.42848047614097595,
      "learning_rate": 4.4813333333333333e-05,
      "loss": 0.0041,
      "step": 7780
    },
    {
      "epoch": 0.41546666666666665,
      "grad_norm": 0.4635053873062134,
      "learning_rate": 4.4806666666666666e-05,
      "loss": 0.0032,
      "step": 7790
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.11580628901720047,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 0.0031,
      "step": 7800
    },
    {
      "epoch": 0.4165333333333333,
      "grad_norm": 0.5475388169288635,
      "learning_rate": 4.479333333333334e-05,
      "loss": 0.0038,
      "step": 7810
    },
    {
      "epoch": 0.41706666666666664,
      "grad_norm": 0.46927645802497864,
      "learning_rate": 4.478666666666667e-05,
      "loss": 0.0044,
      "step": 7820
    },
    {
      "epoch": 0.4176,
      "grad_norm": 0.2887106239795685,
      "learning_rate": 4.478e-05,
      "loss": 0.0026,
      "step": 7830
    },
    {
      "epoch": 0.41813333333333336,
      "grad_norm": 0.4626152217388153,
      "learning_rate": 4.4773333333333334e-05,
      "loss": 0.0023,
      "step": 7840
    },
    {
      "epoch": 0.4186666666666667,
      "grad_norm": 0.13417230546474457,
      "learning_rate": 4.4766666666666666e-05,
      "loss": 0.0028,
      "step": 7850
    },
    {
      "epoch": 0.4192,
      "grad_norm": 0.636335551738739,
      "learning_rate": 4.4760000000000005e-05,
      "loss": 0.0025,
      "step": 7860
    },
    {
      "epoch": 0.41973333333333335,
      "grad_norm": 0.2741931080818176,
      "learning_rate": 4.475333333333334e-05,
      "loss": 0.0035,
      "step": 7870
    },
    {
      "epoch": 0.4202666666666667,
      "grad_norm": 0.47112345695495605,
      "learning_rate": 4.474666666666667e-05,
      "loss": 0.0026,
      "step": 7880
    },
    {
      "epoch": 0.4208,
      "grad_norm": 0.2843330204486847,
      "learning_rate": 4.474e-05,
      "loss": 0.0027,
      "step": 7890
    },
    {
      "epoch": 0.42133333333333334,
      "grad_norm": 0.1736486703157425,
      "learning_rate": 4.473333333333334e-05,
      "loss": 0.0031,
      "step": 7900
    },
    {
      "epoch": 0.42186666666666667,
      "grad_norm": 0.282896488904953,
      "learning_rate": 4.4726666666666666e-05,
      "loss": 0.0036,
      "step": 7910
    },
    {
      "epoch": 0.4224,
      "grad_norm": 0.3008783757686615,
      "learning_rate": 4.472e-05,
      "loss": 0.0035,
      "step": 7920
    },
    {
      "epoch": 0.42293333333333333,
      "grad_norm": 0.6182332634925842,
      "learning_rate": 4.471333333333334e-05,
      "loss": 0.0018,
      "step": 7930
    },
    {
      "epoch": 0.42346666666666666,
      "grad_norm": 0.5524815917015076,
      "learning_rate": 4.470666666666667e-05,
      "loss": 0.0028,
      "step": 7940
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.5127304792404175,
      "learning_rate": 4.47e-05,
      "loss": 0.0026,
      "step": 7950
    },
    {
      "epoch": 0.4245333333333333,
      "grad_norm": 0.17251531779766083,
      "learning_rate": 4.4693333333333335e-05,
      "loss": 0.0032,
      "step": 7960
    },
    {
      "epoch": 0.42506666666666665,
      "grad_norm": 0.46062248945236206,
      "learning_rate": 4.4686666666666674e-05,
      "loss": 0.0022,
      "step": 7970
    },
    {
      "epoch": 0.4256,
      "grad_norm": 0.4893331229686737,
      "learning_rate": 4.468e-05,
      "loss": 0.0027,
      "step": 7980
    },
    {
      "epoch": 0.4261333333333333,
      "grad_norm": 0.29760006070137024,
      "learning_rate": 4.467333333333333e-05,
      "loss": 0.0025,
      "step": 7990
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.7656008005142212,
      "learning_rate": 4.466666666666667e-05,
      "loss": 0.0028,
      "step": 8000
    },
    {
      "epoch": 0.4272,
      "grad_norm": 0.27271339297294617,
      "learning_rate": 4.466e-05,
      "loss": 0.0021,
      "step": 8010
    },
    {
      "epoch": 0.42773333333333335,
      "grad_norm": 0.45987293124198914,
      "learning_rate": 4.4653333333333335e-05,
      "loss": 0.0029,
      "step": 8020
    },
    {
      "epoch": 0.4282666666666667,
      "grad_norm": 0.23782843351364136,
      "learning_rate": 4.464666666666667e-05,
      "loss": 0.0037,
      "step": 8030
    },
    {
      "epoch": 0.4288,
      "grad_norm": 0.19951923191547394,
      "learning_rate": 4.4640000000000006e-05,
      "loss": 0.0022,
      "step": 8040
    },
    {
      "epoch": 0.42933333333333334,
      "grad_norm": 0.2604484558105469,
      "learning_rate": 4.463333333333334e-05,
      "loss": 0.0026,
      "step": 8050
    },
    {
      "epoch": 0.4298666666666667,
      "grad_norm": 0.13255487382411957,
      "learning_rate": 4.4626666666666664e-05,
      "loss": 0.0027,
      "step": 8060
    },
    {
      "epoch": 0.4304,
      "grad_norm": 0.4490504860877991,
      "learning_rate": 4.462e-05,
      "loss": 0.0035,
      "step": 8070
    },
    {
      "epoch": 0.43093333333333333,
      "grad_norm": 1.1864796876907349,
      "learning_rate": 4.4613333333333335e-05,
      "loss": 0.0023,
      "step": 8080
    },
    {
      "epoch": 0.43146666666666667,
      "grad_norm": 0.22708424925804138,
      "learning_rate": 4.460666666666667e-05,
      "loss": 0.0023,
      "step": 8090
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.14335839450359344,
      "learning_rate": 4.46e-05,
      "loss": 0.0032,
      "step": 8100
    },
    {
      "epoch": 0.4325333333333333,
      "grad_norm": 0.6825963258743286,
      "learning_rate": 4.459333333333334e-05,
      "loss": 0.0043,
      "step": 8110
    },
    {
      "epoch": 0.43306666666666666,
      "grad_norm": 0.3495529294013977,
      "learning_rate": 4.458666666666667e-05,
      "loss": 0.0042,
      "step": 8120
    },
    {
      "epoch": 0.4336,
      "grad_norm": 0.20259350538253784,
      "learning_rate": 4.458e-05,
      "loss": 0.0037,
      "step": 8130
    },
    {
      "epoch": 0.4341333333333333,
      "grad_norm": 0.5246257185935974,
      "learning_rate": 4.4573333333333336e-05,
      "loss": 0.0027,
      "step": 8140
    },
    {
      "epoch": 0.43466666666666665,
      "grad_norm": 0.1491851806640625,
      "learning_rate": 4.456666666666667e-05,
      "loss": 0.0033,
      "step": 8150
    },
    {
      "epoch": 0.4352,
      "grad_norm": 0.15655440092086792,
      "learning_rate": 4.456e-05,
      "loss": 0.0024,
      "step": 8160
    },
    {
      "epoch": 0.4357333333333333,
      "grad_norm": 0.15920166671276093,
      "learning_rate": 4.455333333333333e-05,
      "loss": 0.0032,
      "step": 8170
    },
    {
      "epoch": 0.4362666666666667,
      "grad_norm": 0.119514100253582,
      "learning_rate": 4.454666666666667e-05,
      "loss": 0.0032,
      "step": 8180
    },
    {
      "epoch": 0.4368,
      "grad_norm": 0.2251327931880951,
      "learning_rate": 4.4540000000000004e-05,
      "loss": 0.0028,
      "step": 8190
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 0.18504010140895844,
      "learning_rate": 4.4533333333333336e-05,
      "loss": 0.0036,
      "step": 8200
    },
    {
      "epoch": 0.4378666666666667,
      "grad_norm": 0.1687171757221222,
      "learning_rate": 4.452666666666667e-05,
      "loss": 0.0027,
      "step": 8210
    },
    {
      "epoch": 0.4384,
      "grad_norm": 0.1998334378004074,
      "learning_rate": 4.452e-05,
      "loss": 0.0032,
      "step": 8220
    },
    {
      "epoch": 0.43893333333333334,
      "grad_norm": 0.16369575262069702,
      "learning_rate": 4.451333333333333e-05,
      "loss": 0.0032,
      "step": 8230
    },
    {
      "epoch": 0.43946666666666667,
      "grad_norm": 0.4759334623813629,
      "learning_rate": 4.450666666666667e-05,
      "loss": 0.0031,
      "step": 8240
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.26255127787590027,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 0.0029,
      "step": 8250
    },
    {
      "epoch": 0.44053333333333333,
      "grad_norm": 0.6093299388885498,
      "learning_rate": 4.4493333333333337e-05,
      "loss": 0.002,
      "step": 8260
    },
    {
      "epoch": 0.44106666666666666,
      "grad_norm": 0.1736227124929428,
      "learning_rate": 4.448666666666667e-05,
      "loss": 0.0026,
      "step": 8270
    },
    {
      "epoch": 0.4416,
      "grad_norm": 0.47339195013046265,
      "learning_rate": 4.448e-05,
      "loss": 0.0036,
      "step": 8280
    },
    {
      "epoch": 0.4421333333333333,
      "grad_norm": 0.3354972004890442,
      "learning_rate": 4.447333333333333e-05,
      "loss": 0.0033,
      "step": 8290
    },
    {
      "epoch": 0.44266666666666665,
      "grad_norm": 0.4233408570289612,
      "learning_rate": 4.4466666666666666e-05,
      "loss": 0.0032,
      "step": 8300
    },
    {
      "epoch": 0.4432,
      "grad_norm": 0.1956171840429306,
      "learning_rate": 4.4460000000000005e-05,
      "loss": 0.0019,
      "step": 8310
    },
    {
      "epoch": 0.4437333333333333,
      "grad_norm": 0.19969405233860016,
      "learning_rate": 4.445333333333334e-05,
      "loss": 0.0024,
      "step": 8320
    },
    {
      "epoch": 0.44426666666666664,
      "grad_norm": 0.7547622919082642,
      "learning_rate": 4.444666666666667e-05,
      "loss": 0.0033,
      "step": 8330
    },
    {
      "epoch": 0.4448,
      "grad_norm": 0.603834867477417,
      "learning_rate": 4.444e-05,
      "loss": 0.0024,
      "step": 8340
    },
    {
      "epoch": 0.44533333333333336,
      "grad_norm": 0.18217040598392487,
      "learning_rate": 4.443333333333334e-05,
      "loss": 0.0026,
      "step": 8350
    },
    {
      "epoch": 0.4458666666666667,
      "grad_norm": 0.3675098121166229,
      "learning_rate": 4.4426666666666666e-05,
      "loss": 0.0029,
      "step": 8360
    },
    {
      "epoch": 0.4464,
      "grad_norm": 0.5120529532432556,
      "learning_rate": 4.442e-05,
      "loss": 0.0037,
      "step": 8370
    },
    {
      "epoch": 0.44693333333333335,
      "grad_norm": 0.4256035089492798,
      "learning_rate": 4.441333333333334e-05,
      "loss": 0.0028,
      "step": 8380
    },
    {
      "epoch": 0.4474666666666667,
      "grad_norm": 0.7564109563827515,
      "learning_rate": 4.440666666666667e-05,
      "loss": 0.0024,
      "step": 8390
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.16500532627105713,
      "learning_rate": 4.44e-05,
      "loss": 0.003,
      "step": 8400
    },
    {
      "epoch": 0.44853333333333334,
      "grad_norm": 0.21136140823364258,
      "learning_rate": 4.4393333333333334e-05,
      "loss": 0.0019,
      "step": 8410
    },
    {
      "epoch": 0.44906666666666667,
      "grad_norm": 0.19088634848594666,
      "learning_rate": 4.438666666666667e-05,
      "loss": 0.0022,
      "step": 8420
    },
    {
      "epoch": 0.4496,
      "grad_norm": 0.2919597327709198,
      "learning_rate": 4.438e-05,
      "loss": 0.0024,
      "step": 8430
    },
    {
      "epoch": 0.45013333333333333,
      "grad_norm": 0.29670262336730957,
      "learning_rate": 4.437333333333333e-05,
      "loss": 0.0027,
      "step": 8440
    },
    {
      "epoch": 0.45066666666666666,
      "grad_norm": 0.18638798594474792,
      "learning_rate": 4.436666666666667e-05,
      "loss": 0.0023,
      "step": 8450
    },
    {
      "epoch": 0.4512,
      "grad_norm": 0.4239961802959442,
      "learning_rate": 4.436e-05,
      "loss": 0.0029,
      "step": 8460
    },
    {
      "epoch": 0.4517333333333333,
      "grad_norm": 0.30393314361572266,
      "learning_rate": 4.4353333333333334e-05,
      "loss": 0.0023,
      "step": 8470
    },
    {
      "epoch": 0.45226666666666665,
      "grad_norm": 0.20932888984680176,
      "learning_rate": 4.434666666666667e-05,
      "loss": 0.0034,
      "step": 8480
    },
    {
      "epoch": 0.4528,
      "grad_norm": 0.2417798489332199,
      "learning_rate": 4.4340000000000006e-05,
      "loss": 0.0032,
      "step": 8490
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.26754844188690186,
      "learning_rate": 4.433333333333334e-05,
      "loss": 0.0026,
      "step": 8500
    },
    {
      "epoch": 0.45386666666666664,
      "grad_norm": 0.20089448988437653,
      "learning_rate": 4.4326666666666664e-05,
      "loss": 0.0034,
      "step": 8510
    },
    {
      "epoch": 0.4544,
      "grad_norm": 0.26216036081314087,
      "learning_rate": 4.432e-05,
      "loss": 0.0021,
      "step": 8520
    },
    {
      "epoch": 0.45493333333333336,
      "grad_norm": 0.2618551254272461,
      "learning_rate": 4.4313333333333335e-05,
      "loss": 0.0042,
      "step": 8530
    },
    {
      "epoch": 0.4554666666666667,
      "grad_norm": 0.551934003829956,
      "learning_rate": 4.430666666666667e-05,
      "loss": 0.0024,
      "step": 8540
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.5089883208274841,
      "learning_rate": 4.43e-05,
      "loss": 0.0018,
      "step": 8550
    },
    {
      "epoch": 0.45653333333333335,
      "grad_norm": 0.3458833396434784,
      "learning_rate": 4.429333333333334e-05,
      "loss": 0.0031,
      "step": 8560
    },
    {
      "epoch": 0.4570666666666667,
      "grad_norm": 0.2201109230518341,
      "learning_rate": 4.428666666666667e-05,
      "loss": 0.0023,
      "step": 8570
    },
    {
      "epoch": 0.4576,
      "grad_norm": 0.4651433229446411,
      "learning_rate": 4.428e-05,
      "loss": 0.0021,
      "step": 8580
    },
    {
      "epoch": 0.45813333333333334,
      "grad_norm": 0.4348059892654419,
      "learning_rate": 4.4273333333333335e-05,
      "loss": 0.0027,
      "step": 8590
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 0.468192458152771,
      "learning_rate": 4.426666666666667e-05,
      "loss": 0.0033,
      "step": 8600
    },
    {
      "epoch": 0.4592,
      "grad_norm": 0.3136083483695984,
      "learning_rate": 4.426e-05,
      "loss": 0.0027,
      "step": 8610
    },
    {
      "epoch": 0.4597333333333333,
      "grad_norm": 0.12078587710857391,
      "learning_rate": 4.425333333333334e-05,
      "loss": 0.0039,
      "step": 8620
    },
    {
      "epoch": 0.46026666666666666,
      "grad_norm": 0.7875892519950867,
      "learning_rate": 4.424666666666667e-05,
      "loss": 0.0027,
      "step": 8630
    },
    {
      "epoch": 0.4608,
      "grad_norm": 0.12847614288330078,
      "learning_rate": 4.424e-05,
      "loss": 0.0036,
      "step": 8640
    },
    {
      "epoch": 0.4613333333333333,
      "grad_norm": 0.34828370809555054,
      "learning_rate": 4.4233333333333336e-05,
      "loss": 0.0029,
      "step": 8650
    },
    {
      "epoch": 0.46186666666666665,
      "grad_norm": 0.3565598726272583,
      "learning_rate": 4.422666666666667e-05,
      "loss": 0.0029,
      "step": 8660
    },
    {
      "epoch": 0.4624,
      "grad_norm": 0.16005170345306396,
      "learning_rate": 4.422e-05,
      "loss": 0.0029,
      "step": 8670
    },
    {
      "epoch": 0.4629333333333333,
      "grad_norm": 0.15950794517993927,
      "learning_rate": 4.421333333333333e-05,
      "loss": 0.0036,
      "step": 8680
    },
    {
      "epoch": 0.4634666666666667,
      "grad_norm": 0.4292629361152649,
      "learning_rate": 4.420666666666667e-05,
      "loss": 0.0026,
      "step": 8690
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.4175444543361664,
      "learning_rate": 4.4200000000000004e-05,
      "loss": 0.004,
      "step": 8700
    },
    {
      "epoch": 0.46453333333333335,
      "grad_norm": 0.40938207507133484,
      "learning_rate": 4.4193333333333336e-05,
      "loss": 0.004,
      "step": 8710
    },
    {
      "epoch": 0.4650666666666667,
      "grad_norm": 0.20178544521331787,
      "learning_rate": 4.418666666666667e-05,
      "loss": 0.003,
      "step": 8720
    },
    {
      "epoch": 0.4656,
      "grad_norm": 0.390258252620697,
      "learning_rate": 4.418000000000001e-05,
      "loss": 0.0029,
      "step": 8730
    },
    {
      "epoch": 0.46613333333333334,
      "grad_norm": 0.3547222316265106,
      "learning_rate": 4.417333333333333e-05,
      "loss": 0.003,
      "step": 8740
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.19111287593841553,
      "learning_rate": 4.4166666666666665e-05,
      "loss": 0.0022,
      "step": 8750
    },
    {
      "epoch": 0.4672,
      "grad_norm": 0.7221256494522095,
      "learning_rate": 4.4160000000000004e-05,
      "loss": 0.0024,
      "step": 8760
    },
    {
      "epoch": 0.46773333333333333,
      "grad_norm": 0.16624775528907776,
      "learning_rate": 4.4153333333333336e-05,
      "loss": 0.0021,
      "step": 8770
    },
    {
      "epoch": 0.46826666666666666,
      "grad_norm": 0.4659462571144104,
      "learning_rate": 4.414666666666667e-05,
      "loss": 0.0036,
      "step": 8780
    },
    {
      "epoch": 0.4688,
      "grad_norm": 0.3206574320793152,
      "learning_rate": 4.414e-05,
      "loss": 0.0038,
      "step": 8790
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 0.7445860505104065,
      "learning_rate": 4.413333333333334e-05,
      "loss": 0.0023,
      "step": 8800
    },
    {
      "epoch": 0.46986666666666665,
      "grad_norm": 0.6251948475837708,
      "learning_rate": 4.4126666666666665e-05,
      "loss": 0.0031,
      "step": 8810
    },
    {
      "epoch": 0.4704,
      "grad_norm": 0.6014738082885742,
      "learning_rate": 4.412e-05,
      "loss": 0.0025,
      "step": 8820
    },
    {
      "epoch": 0.4709333333333333,
      "grad_norm": 0.9227374196052551,
      "learning_rate": 4.411333333333334e-05,
      "loss": 0.003,
      "step": 8830
    },
    {
      "epoch": 0.47146666666666665,
      "grad_norm": 0.3459160625934601,
      "learning_rate": 4.410666666666667e-05,
      "loss": 0.0035,
      "step": 8840
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.444139301776886,
      "learning_rate": 4.41e-05,
      "loss": 0.0022,
      "step": 8850
    },
    {
      "epoch": 0.47253333333333336,
      "grad_norm": 0.616522490978241,
      "learning_rate": 4.4093333333333334e-05,
      "loss": 0.0032,
      "step": 8860
    },
    {
      "epoch": 0.4730666666666667,
      "grad_norm": 0.38562843203544617,
      "learning_rate": 4.408666666666667e-05,
      "loss": 0.003,
      "step": 8870
    },
    {
      "epoch": 0.4736,
      "grad_norm": 0.3459113538265228,
      "learning_rate": 4.4080000000000005e-05,
      "loss": 0.0023,
      "step": 8880
    },
    {
      "epoch": 0.47413333333333335,
      "grad_norm": 0.11354190111160278,
      "learning_rate": 4.407333333333333e-05,
      "loss": 0.0027,
      "step": 8890
    },
    {
      "epoch": 0.4746666666666667,
      "grad_norm": 0.3096740245819092,
      "learning_rate": 4.406666666666667e-05,
      "loss": 0.0022,
      "step": 8900
    },
    {
      "epoch": 0.4752,
      "grad_norm": 0.1509842872619629,
      "learning_rate": 4.406e-05,
      "loss": 0.0031,
      "step": 8910
    },
    {
      "epoch": 0.47573333333333334,
      "grad_norm": 0.1734679490327835,
      "learning_rate": 4.4053333333333334e-05,
      "loss": 0.0035,
      "step": 8920
    },
    {
      "epoch": 0.47626666666666667,
      "grad_norm": 0.17819198966026306,
      "learning_rate": 4.4046666666666666e-05,
      "loss": 0.0036,
      "step": 8930
    },
    {
      "epoch": 0.4768,
      "grad_norm": 0.3110080361366272,
      "learning_rate": 4.4040000000000005e-05,
      "loss": 0.0043,
      "step": 8940
    },
    {
      "epoch": 0.47733333333333333,
      "grad_norm": 0.2146952748298645,
      "learning_rate": 4.403333333333334e-05,
      "loss": 0.004,
      "step": 8950
    },
    {
      "epoch": 0.47786666666666666,
      "grad_norm": 0.20565947890281677,
      "learning_rate": 4.402666666666666e-05,
      "loss": 0.002,
      "step": 8960
    },
    {
      "epoch": 0.4784,
      "grad_norm": 0.6376075744628906,
      "learning_rate": 4.402e-05,
      "loss": 0.0022,
      "step": 8970
    },
    {
      "epoch": 0.4789333333333333,
      "grad_norm": 0.3403169810771942,
      "learning_rate": 4.4013333333333334e-05,
      "loss": 0.0032,
      "step": 8980
    },
    {
      "epoch": 0.47946666666666665,
      "grad_norm": 0.499103307723999,
      "learning_rate": 4.4006666666666667e-05,
      "loss": 0.0035,
      "step": 8990
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.17107367515563965,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.0027,
      "step": 9000
    },
    {
      "epoch": 0.4805333333333333,
      "grad_norm": 0.6749059557914734,
      "learning_rate": 4.399333333333334e-05,
      "loss": 0.0022,
      "step": 9010
    },
    {
      "epoch": 0.48106666666666664,
      "grad_norm": 0.5354596972465515,
      "learning_rate": 4.398666666666667e-05,
      "loss": 0.0034,
      "step": 9020
    },
    {
      "epoch": 0.4816,
      "grad_norm": 0.31821107864379883,
      "learning_rate": 4.398e-05,
      "loss": 0.0026,
      "step": 9030
    },
    {
      "epoch": 0.48213333333333336,
      "grad_norm": 0.1691068708896637,
      "learning_rate": 4.3973333333333335e-05,
      "loss": 0.0022,
      "step": 9040
    },
    {
      "epoch": 0.4826666666666667,
      "grad_norm": 0.4276895821094513,
      "learning_rate": 4.396666666666667e-05,
      "loss": 0.0031,
      "step": 9050
    },
    {
      "epoch": 0.4832,
      "grad_norm": 0.2788293659687042,
      "learning_rate": 4.396e-05,
      "loss": 0.0029,
      "step": 9060
    },
    {
      "epoch": 0.48373333333333335,
      "grad_norm": 0.5977239012718201,
      "learning_rate": 4.395333333333334e-05,
      "loss": 0.0038,
      "step": 9070
    },
    {
      "epoch": 0.4842666666666667,
      "grad_norm": 0.4881131649017334,
      "learning_rate": 4.394666666666667e-05,
      "loss": 0.0038,
      "step": 9080
    },
    {
      "epoch": 0.4848,
      "grad_norm": 0.6111014485359192,
      "learning_rate": 4.394e-05,
      "loss": 0.0026,
      "step": 9090
    },
    {
      "epoch": 0.48533333333333334,
      "grad_norm": 0.364645779132843,
      "learning_rate": 4.3933333333333335e-05,
      "loss": 0.0036,
      "step": 9100
    },
    {
      "epoch": 0.48586666666666667,
      "grad_norm": 0.3911617696285248,
      "learning_rate": 4.3926666666666674e-05,
      "loss": 0.0027,
      "step": 9110
    },
    {
      "epoch": 0.4864,
      "grad_norm": 0.40551191568374634,
      "learning_rate": 4.392e-05,
      "loss": 0.0033,
      "step": 9120
    },
    {
      "epoch": 0.48693333333333333,
      "grad_norm": 0.1963038146495819,
      "learning_rate": 4.391333333333333e-05,
      "loss": 0.0046,
      "step": 9130
    },
    {
      "epoch": 0.48746666666666666,
      "grad_norm": 0.3946203291416168,
      "learning_rate": 4.390666666666667e-05,
      "loss": 0.0029,
      "step": 9140
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.5829674005508423,
      "learning_rate": 4.39e-05,
      "loss": 0.0023,
      "step": 9150
    },
    {
      "epoch": 0.4885333333333333,
      "grad_norm": 0.17791739106178284,
      "learning_rate": 4.3893333333333335e-05,
      "loss": 0.0025,
      "step": 9160
    },
    {
      "epoch": 0.48906666666666665,
      "grad_norm": 0.3869923949241638,
      "learning_rate": 4.388666666666667e-05,
      "loss": 0.0025,
      "step": 9170
    },
    {
      "epoch": 0.4896,
      "grad_norm": 0.1930931806564331,
      "learning_rate": 4.388000000000001e-05,
      "loss": 0.0024,
      "step": 9180
    },
    {
      "epoch": 0.4901333333333333,
      "grad_norm": 0.11017762124538422,
      "learning_rate": 4.387333333333333e-05,
      "loss": 0.0023,
      "step": 9190
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 0.2262515425682068,
      "learning_rate": 4.3866666666666665e-05,
      "loss": 0.0037,
      "step": 9200
    },
    {
      "epoch": 0.4912,
      "grad_norm": 0.6516813635826111,
      "learning_rate": 4.3860000000000004e-05,
      "loss": 0.0031,
      "step": 9210
    },
    {
      "epoch": 0.49173333333333336,
      "grad_norm": 0.08165150135755539,
      "learning_rate": 4.3853333333333336e-05,
      "loss": 0.0029,
      "step": 9220
    },
    {
      "epoch": 0.4922666666666667,
      "grad_norm": 0.7537733912467957,
      "learning_rate": 4.384666666666667e-05,
      "loss": 0.0042,
      "step": 9230
    },
    {
      "epoch": 0.4928,
      "grad_norm": 0.30312395095825195,
      "learning_rate": 4.384e-05,
      "loss": 0.003,
      "step": 9240
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.12512247264385223,
      "learning_rate": 4.383333333333334e-05,
      "loss": 0.0024,
      "step": 9250
    },
    {
      "epoch": 0.4938666666666667,
      "grad_norm": 0.943340539932251,
      "learning_rate": 4.382666666666667e-05,
      "loss": 0.0033,
      "step": 9260
    },
    {
      "epoch": 0.4944,
      "grad_norm": 0.6350918412208557,
      "learning_rate": 4.382e-05,
      "loss": 0.0027,
      "step": 9270
    },
    {
      "epoch": 0.49493333333333334,
      "grad_norm": 0.3444409966468811,
      "learning_rate": 4.3813333333333336e-05,
      "loss": 0.0026,
      "step": 9280
    },
    {
      "epoch": 0.49546666666666667,
      "grad_norm": 0.6603935956954956,
      "learning_rate": 4.380666666666667e-05,
      "loss": 0.0023,
      "step": 9290
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.28957852721214294,
      "learning_rate": 4.38e-05,
      "loss": 0.0029,
      "step": 9300
    },
    {
      "epoch": 0.4965333333333333,
      "grad_norm": 0.17303672432899475,
      "learning_rate": 4.379333333333333e-05,
      "loss": 0.0025,
      "step": 9310
    },
    {
      "epoch": 0.49706666666666666,
      "grad_norm": 0.17694638669490814,
      "learning_rate": 4.378666666666667e-05,
      "loss": 0.005,
      "step": 9320
    },
    {
      "epoch": 0.4976,
      "grad_norm": 0.42714592814445496,
      "learning_rate": 4.3780000000000004e-05,
      "loss": 0.0027,
      "step": 9330
    },
    {
      "epoch": 0.4981333333333333,
      "grad_norm": 0.16201111674308777,
      "learning_rate": 4.377333333333333e-05,
      "loss": 0.0032,
      "step": 9340
    },
    {
      "epoch": 0.49866666666666665,
      "grad_norm": 0.13762420415878296,
      "learning_rate": 4.376666666666667e-05,
      "loss": 0.0019,
      "step": 9350
    },
    {
      "epoch": 0.4992,
      "grad_norm": 0.5163976550102234,
      "learning_rate": 4.376e-05,
      "loss": 0.0023,
      "step": 9360
    },
    {
      "epoch": 0.4997333333333333,
      "grad_norm": 0.3568369150161743,
      "learning_rate": 4.3753333333333333e-05,
      "loss": 0.0036,
      "step": 9370
    },
    {
      "epoch": 0.5002666666666666,
      "grad_norm": 0.16458934545516968,
      "learning_rate": 4.374666666666667e-05,
      "loss": 0.0033,
      "step": 9380
    },
    {
      "epoch": 0.5008,
      "grad_norm": 0.17474767565727234,
      "learning_rate": 4.3740000000000005e-05,
      "loss": 0.0031,
      "step": 9390
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 0.21955657005310059,
      "learning_rate": 4.373333333333334e-05,
      "loss": 0.0027,
      "step": 9400
    },
    {
      "epoch": 0.5018666666666667,
      "grad_norm": 0.20459070801734924,
      "learning_rate": 4.372666666666667e-05,
      "loss": 0.0024,
      "step": 9410
    },
    {
      "epoch": 0.5024,
      "grad_norm": 0.4140029549598694,
      "learning_rate": 4.372e-05,
      "loss": 0.003,
      "step": 9420
    },
    {
      "epoch": 0.5029333333333333,
      "grad_norm": 0.28978481888771057,
      "learning_rate": 4.3713333333333334e-05,
      "loss": 0.0022,
      "step": 9430
    },
    {
      "epoch": 0.5034666666666666,
      "grad_norm": 0.32635876536369324,
      "learning_rate": 4.3706666666666666e-05,
      "loss": 0.0026,
      "step": 9440
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.7330906987190247,
      "learning_rate": 4.3700000000000005e-05,
      "loss": 0.0021,
      "step": 9450
    },
    {
      "epoch": 0.5045333333333333,
      "grad_norm": 0.5102440118789673,
      "learning_rate": 4.369333333333334e-05,
      "loss": 0.0031,
      "step": 9460
    },
    {
      "epoch": 0.5050666666666667,
      "grad_norm": 0.328964501619339,
      "learning_rate": 4.368666666666667e-05,
      "loss": 0.0032,
      "step": 9470
    },
    {
      "epoch": 0.5056,
      "grad_norm": 0.17423541843891144,
      "learning_rate": 4.368e-05,
      "loss": 0.0021,
      "step": 9480
    },
    {
      "epoch": 0.5061333333333333,
      "grad_norm": 0.24586103856563568,
      "learning_rate": 4.3673333333333334e-05,
      "loss": 0.0019,
      "step": 9490
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.5027732253074646,
      "learning_rate": 4.3666666666666666e-05,
      "loss": 0.002,
      "step": 9500
    },
    {
      "epoch": 0.5072,
      "grad_norm": 0.549625813961029,
      "learning_rate": 4.366e-05,
      "loss": 0.0029,
      "step": 9510
    },
    {
      "epoch": 0.5077333333333334,
      "grad_norm": 0.4010637104511261,
      "learning_rate": 4.365333333333334e-05,
      "loss": 0.0021,
      "step": 9520
    },
    {
      "epoch": 0.5082666666666666,
      "grad_norm": 0.47971275448799133,
      "learning_rate": 4.364666666666667e-05,
      "loss": 0.0032,
      "step": 9530
    },
    {
      "epoch": 0.5088,
      "grad_norm": 0.9818218350410461,
      "learning_rate": 4.364e-05,
      "loss": 0.0025,
      "step": 9540
    },
    {
      "epoch": 0.5093333333333333,
      "grad_norm": 0.16394557058811188,
      "learning_rate": 4.3633333333333335e-05,
      "loss": 0.0025,
      "step": 9550
    },
    {
      "epoch": 0.5098666666666667,
      "grad_norm": 0.16863244771957397,
      "learning_rate": 4.3626666666666674e-05,
      "loss": 0.0021,
      "step": 9560
    },
    {
      "epoch": 0.5104,
      "grad_norm": 0.07916121184825897,
      "learning_rate": 4.362e-05,
      "loss": 0.0026,
      "step": 9570
    },
    {
      "epoch": 0.5109333333333334,
      "grad_norm": 0.46094921231269836,
      "learning_rate": 4.361333333333333e-05,
      "loss": 0.0023,
      "step": 9580
    },
    {
      "epoch": 0.5114666666666666,
      "grad_norm": 0.4560569226741791,
      "learning_rate": 4.360666666666667e-05,
      "loss": 0.0026,
      "step": 9590
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.32665982842445374,
      "learning_rate": 4.36e-05,
      "loss": 0.0029,
      "step": 9600
    },
    {
      "epoch": 0.5125333333333333,
      "grad_norm": 0.26668816804885864,
      "learning_rate": 4.3593333333333335e-05,
      "loss": 0.0024,
      "step": 9610
    },
    {
      "epoch": 0.5130666666666667,
      "grad_norm": 0.32720986008644104,
      "learning_rate": 4.358666666666667e-05,
      "loss": 0.0028,
      "step": 9620
    },
    {
      "epoch": 0.5136,
      "grad_norm": 0.09911257028579712,
      "learning_rate": 4.3580000000000006e-05,
      "loss": 0.0029,
      "step": 9630
    },
    {
      "epoch": 0.5141333333333333,
      "grad_norm": 0.2525571584701538,
      "learning_rate": 4.357333333333333e-05,
      "loss": 0.0039,
      "step": 9640
    },
    {
      "epoch": 0.5146666666666667,
      "grad_norm": 0.2988072633743286,
      "learning_rate": 4.3566666666666664e-05,
      "loss": 0.0033,
      "step": 9650
    },
    {
      "epoch": 0.5152,
      "grad_norm": 0.3434596657752991,
      "learning_rate": 4.356e-05,
      "loss": 0.0037,
      "step": 9660
    },
    {
      "epoch": 0.5157333333333334,
      "grad_norm": 0.2197636514902115,
      "learning_rate": 4.3553333333333335e-05,
      "loss": 0.0023,
      "step": 9670
    },
    {
      "epoch": 0.5162666666666667,
      "grad_norm": 0.1913546621799469,
      "learning_rate": 4.354666666666667e-05,
      "loss": 0.0027,
      "step": 9680
    },
    {
      "epoch": 0.5168,
      "grad_norm": 0.9059590101242065,
      "learning_rate": 4.354e-05,
      "loss": 0.0023,
      "step": 9690
    },
    {
      "epoch": 0.5173333333333333,
      "grad_norm": 0.308466374874115,
      "learning_rate": 4.353333333333334e-05,
      "loss": 0.0026,
      "step": 9700
    },
    {
      "epoch": 0.5178666666666667,
      "grad_norm": 0.5396332740783691,
      "learning_rate": 4.352666666666667e-05,
      "loss": 0.0028,
      "step": 9710
    },
    {
      "epoch": 0.5184,
      "grad_norm": 0.47995221614837646,
      "learning_rate": 4.352e-05,
      "loss": 0.0048,
      "step": 9720
    },
    {
      "epoch": 0.5189333333333334,
      "grad_norm": 0.563904345035553,
      "learning_rate": 4.3513333333333336e-05,
      "loss": 0.0021,
      "step": 9730
    },
    {
      "epoch": 0.5194666666666666,
      "grad_norm": 0.12122057378292084,
      "learning_rate": 4.350666666666667e-05,
      "loss": 0.0034,
      "step": 9740
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.31181859970092773,
      "learning_rate": 4.35e-05,
      "loss": 0.0022,
      "step": 9750
    },
    {
      "epoch": 0.5205333333333333,
      "grad_norm": 0.7154260277748108,
      "learning_rate": 4.349333333333334e-05,
      "loss": 0.0022,
      "step": 9760
    },
    {
      "epoch": 0.5210666666666667,
      "grad_norm": 0.4389633238315582,
      "learning_rate": 4.348666666666667e-05,
      "loss": 0.0023,
      "step": 9770
    },
    {
      "epoch": 0.5216,
      "grad_norm": 0.3611229658126831,
      "learning_rate": 4.3480000000000004e-05,
      "loss": 0.004,
      "step": 9780
    },
    {
      "epoch": 0.5221333333333333,
      "grad_norm": 0.1893833726644516,
      "learning_rate": 4.3473333333333336e-05,
      "loss": 0.0028,
      "step": 9790
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 0.5620688199996948,
      "learning_rate": 4.346666666666667e-05,
      "loss": 0.0029,
      "step": 9800
    },
    {
      "epoch": 0.5232,
      "grad_norm": 0.5572052597999573,
      "learning_rate": 4.346e-05,
      "loss": 0.0037,
      "step": 9810
    },
    {
      "epoch": 0.5237333333333334,
      "grad_norm": 0.2520694136619568,
      "learning_rate": 4.345333333333333e-05,
      "loss": 0.0024,
      "step": 9820
    },
    {
      "epoch": 0.5242666666666667,
      "grad_norm": 0.24141430854797363,
      "learning_rate": 4.344666666666667e-05,
      "loss": 0.0026,
      "step": 9830
    },
    {
      "epoch": 0.5248,
      "grad_norm": 0.4392583668231964,
      "learning_rate": 4.3440000000000004e-05,
      "loss": 0.0022,
      "step": 9840
    },
    {
      "epoch": 0.5253333333333333,
      "grad_norm": 0.4779820144176483,
      "learning_rate": 4.3433333333333336e-05,
      "loss": 0.0034,
      "step": 9850
    },
    {
      "epoch": 0.5258666666666667,
      "grad_norm": 0.3975083529949188,
      "learning_rate": 4.342666666666667e-05,
      "loss": 0.0021,
      "step": 9860
    },
    {
      "epoch": 0.5264,
      "grad_norm": 0.14570266008377075,
      "learning_rate": 4.342e-05,
      "loss": 0.0029,
      "step": 9870
    },
    {
      "epoch": 0.5269333333333334,
      "grad_norm": 0.12332763522863388,
      "learning_rate": 4.341333333333333e-05,
      "loss": 0.0025,
      "step": 9880
    },
    {
      "epoch": 0.5274666666666666,
      "grad_norm": 0.3705861270427704,
      "learning_rate": 4.3406666666666666e-05,
      "loss": 0.0029,
      "step": 9890
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.4050012230873108,
      "learning_rate": 4.3400000000000005e-05,
      "loss": 0.0033,
      "step": 9900
    },
    {
      "epoch": 0.5285333333333333,
      "grad_norm": 0.22649811208248138,
      "learning_rate": 4.339333333333334e-05,
      "loss": 0.0032,
      "step": 9910
    },
    {
      "epoch": 0.5290666666666667,
      "grad_norm": 0.9556522965431213,
      "learning_rate": 4.338666666666667e-05,
      "loss": 0.0029,
      "step": 9920
    },
    {
      "epoch": 0.5296,
      "grad_norm": 0.2902585566043854,
      "learning_rate": 4.338e-05,
      "loss": 0.0036,
      "step": 9930
    },
    {
      "epoch": 0.5301333333333333,
      "grad_norm": 0.2741568088531494,
      "learning_rate": 4.337333333333334e-05,
      "loss": 0.0026,
      "step": 9940
    },
    {
      "epoch": 0.5306666666666666,
      "grad_norm": 0.44604262709617615,
      "learning_rate": 4.3366666666666666e-05,
      "loss": 0.0036,
      "step": 9950
    },
    {
      "epoch": 0.5312,
      "grad_norm": 0.1891869604587555,
      "learning_rate": 4.336e-05,
      "loss": 0.0046,
      "step": 9960
    },
    {
      "epoch": 0.5317333333333333,
      "grad_norm": 0.17435573041439056,
      "learning_rate": 4.335333333333334e-05,
      "loss": 0.0023,
      "step": 9970
    },
    {
      "epoch": 0.5322666666666667,
      "grad_norm": 0.7135106325149536,
      "learning_rate": 4.334666666666667e-05,
      "loss": 0.0027,
      "step": 9980
    },
    {
      "epoch": 0.5328,
      "grad_norm": 0.08306203782558441,
      "learning_rate": 4.334e-05,
      "loss": 0.0038,
      "step": 9990
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.32252851128578186,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.0023,
      "step": 10000
    },
    {
      "epoch": 0.5338666666666667,
      "grad_norm": 0.36551544070243835,
      "learning_rate": 4.332666666666667e-05,
      "loss": 0.0032,
      "step": 10010
    },
    {
      "epoch": 0.5344,
      "grad_norm": 0.9128720760345459,
      "learning_rate": 4.332e-05,
      "loss": 0.0029,
      "step": 10020
    },
    {
      "epoch": 0.5349333333333334,
      "grad_norm": 0.542908251285553,
      "learning_rate": 4.331333333333333e-05,
      "loss": 0.0027,
      "step": 10030
    },
    {
      "epoch": 0.5354666666666666,
      "grad_norm": 0.44058337807655334,
      "learning_rate": 4.330666666666667e-05,
      "loss": 0.0032,
      "step": 10040
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.15307319164276123,
      "learning_rate": 4.33e-05,
      "loss": 0.0036,
      "step": 10050
    },
    {
      "epoch": 0.5365333333333333,
      "grad_norm": 0.2296966016292572,
      "learning_rate": 4.3293333333333334e-05,
      "loss": 0.0048,
      "step": 10060
    },
    {
      "epoch": 0.5370666666666667,
      "grad_norm": 0.4010961949825287,
      "learning_rate": 4.328666666666667e-05,
      "loss": 0.0026,
      "step": 10070
    },
    {
      "epoch": 0.5376,
      "grad_norm": 0.3762926161289215,
      "learning_rate": 4.3280000000000006e-05,
      "loss": 0.0034,
      "step": 10080
    },
    {
      "epoch": 0.5381333333333334,
      "grad_norm": 0.15941806137561798,
      "learning_rate": 4.327333333333334e-05,
      "loss": 0.003,
      "step": 10090
    },
    {
      "epoch": 0.5386666666666666,
      "grad_norm": 0.5440637469291687,
      "learning_rate": 4.3266666666666664e-05,
      "loss": 0.0027,
      "step": 10100
    },
    {
      "epoch": 0.5392,
      "grad_norm": 0.4219284951686859,
      "learning_rate": 4.326e-05,
      "loss": 0.0022,
      "step": 10110
    },
    {
      "epoch": 0.5397333333333333,
      "grad_norm": 0.2649995982646942,
      "learning_rate": 4.3253333333333335e-05,
      "loss": 0.0034,
      "step": 10120
    },
    {
      "epoch": 0.5402666666666667,
      "grad_norm": 0.4971816837787628,
      "learning_rate": 4.324666666666667e-05,
      "loss": 0.0026,
      "step": 10130
    },
    {
      "epoch": 0.5408,
      "grad_norm": 0.5518788695335388,
      "learning_rate": 4.324e-05,
      "loss": 0.0026,
      "step": 10140
    },
    {
      "epoch": 0.5413333333333333,
      "grad_norm": 0.40509068965911865,
      "learning_rate": 4.323333333333334e-05,
      "loss": 0.0025,
      "step": 10150
    },
    {
      "epoch": 0.5418666666666667,
      "grad_norm": 0.40858933329582214,
      "learning_rate": 4.322666666666667e-05,
      "loss": 0.0038,
      "step": 10160
    },
    {
      "epoch": 0.5424,
      "grad_norm": 0.5955114364624023,
      "learning_rate": 4.3219999999999996e-05,
      "loss": 0.002,
      "step": 10170
    },
    {
      "epoch": 0.5429333333333334,
      "grad_norm": 0.3858603239059448,
      "learning_rate": 4.3213333333333335e-05,
      "loss": 0.0024,
      "step": 10180
    },
    {
      "epoch": 0.5434666666666667,
      "grad_norm": 0.42248138785362244,
      "learning_rate": 4.320666666666667e-05,
      "loss": 0.0035,
      "step": 10190
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.17952196300029755,
      "learning_rate": 4.32e-05,
      "loss": 0.0033,
      "step": 10200
    },
    {
      "epoch": 0.5445333333333333,
      "grad_norm": 0.19782403111457825,
      "learning_rate": 4.319333333333334e-05,
      "loss": 0.0027,
      "step": 10210
    },
    {
      "epoch": 0.5450666666666667,
      "grad_norm": 0.19503089785575867,
      "learning_rate": 4.318666666666667e-05,
      "loss": 0.0035,
      "step": 10220
    },
    {
      "epoch": 0.5456,
      "grad_norm": 0.1788206547498703,
      "learning_rate": 4.318e-05,
      "loss": 0.0034,
      "step": 10230
    },
    {
      "epoch": 0.5461333333333334,
      "grad_norm": 0.2641737163066864,
      "learning_rate": 4.3173333333333336e-05,
      "loss": 0.0027,
      "step": 10240
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.48453813791275024,
      "learning_rate": 4.316666666666667e-05,
      "loss": 0.0032,
      "step": 10250
    },
    {
      "epoch": 0.5472,
      "grad_norm": 0.24477116763591766,
      "learning_rate": 4.316e-05,
      "loss": 0.0027,
      "step": 10260
    },
    {
      "epoch": 0.5477333333333333,
      "grad_norm": 0.37143176794052124,
      "learning_rate": 4.315333333333333e-05,
      "loss": 0.0032,
      "step": 10270
    },
    {
      "epoch": 0.5482666666666667,
      "grad_norm": 0.190904900431633,
      "learning_rate": 4.314666666666667e-05,
      "loss": 0.0036,
      "step": 10280
    },
    {
      "epoch": 0.5488,
      "grad_norm": 0.33972615003585815,
      "learning_rate": 4.3140000000000004e-05,
      "loss": 0.0038,
      "step": 10290
    },
    {
      "epoch": 0.5493333333333333,
      "grad_norm": 0.43649131059646606,
      "learning_rate": 4.3133333333333336e-05,
      "loss": 0.0045,
      "step": 10300
    },
    {
      "epoch": 0.5498666666666666,
      "grad_norm": 0.4284892976284027,
      "learning_rate": 4.312666666666667e-05,
      "loss": 0.0022,
      "step": 10310
    },
    {
      "epoch": 0.5504,
      "grad_norm": 0.1394696831703186,
      "learning_rate": 4.312000000000001e-05,
      "loss": 0.0029,
      "step": 10320
    },
    {
      "epoch": 0.5509333333333334,
      "grad_norm": 0.13327936828136444,
      "learning_rate": 4.311333333333333e-05,
      "loss": 0.0024,
      "step": 10330
    },
    {
      "epoch": 0.5514666666666667,
      "grad_norm": 0.5465918779373169,
      "learning_rate": 4.3106666666666665e-05,
      "loss": 0.0027,
      "step": 10340
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.18429876863956451,
      "learning_rate": 4.3100000000000004e-05,
      "loss": 0.0022,
      "step": 10350
    },
    {
      "epoch": 0.5525333333333333,
      "grad_norm": 0.44618892669677734,
      "learning_rate": 4.3093333333333336e-05,
      "loss": 0.0022,
      "step": 10360
    },
    {
      "epoch": 0.5530666666666667,
      "grad_norm": 0.39048540592193604,
      "learning_rate": 4.308666666666667e-05,
      "loss": 0.0035,
      "step": 10370
    },
    {
      "epoch": 0.5536,
      "grad_norm": 0.37626996636390686,
      "learning_rate": 4.308e-05,
      "loss": 0.0038,
      "step": 10380
    },
    {
      "epoch": 0.5541333333333334,
      "grad_norm": 0.1286771595478058,
      "learning_rate": 4.307333333333334e-05,
      "loss": 0.0044,
      "step": 10390
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 0.2579054534435272,
      "learning_rate": 4.3066666666666665e-05,
      "loss": 0.0029,
      "step": 10400
    },
    {
      "epoch": 0.5552,
      "grad_norm": 0.2553507089614868,
      "learning_rate": 4.306e-05,
      "loss": 0.003,
      "step": 10410
    },
    {
      "epoch": 0.5557333333333333,
      "grad_norm": 0.2525271773338318,
      "learning_rate": 4.305333333333334e-05,
      "loss": 0.0039,
      "step": 10420
    },
    {
      "epoch": 0.5562666666666667,
      "grad_norm": 0.09048020094633102,
      "learning_rate": 4.304666666666667e-05,
      "loss": 0.0029,
      "step": 10430
    },
    {
      "epoch": 0.5568,
      "grad_norm": 0.6199448704719543,
      "learning_rate": 4.304e-05,
      "loss": 0.0024,
      "step": 10440
    },
    {
      "epoch": 0.5573333333333333,
      "grad_norm": 0.6057191491127014,
      "learning_rate": 4.3033333333333334e-05,
      "loss": 0.0047,
      "step": 10450
    },
    {
      "epoch": 0.5578666666666666,
      "grad_norm": 0.5128987431526184,
      "learning_rate": 4.302666666666667e-05,
      "loss": 0.0031,
      "step": 10460
    },
    {
      "epoch": 0.5584,
      "grad_norm": 0.08555775135755539,
      "learning_rate": 4.3020000000000005e-05,
      "loss": 0.0045,
      "step": 10470
    },
    {
      "epoch": 0.5589333333333333,
      "grad_norm": 0.36206507682800293,
      "learning_rate": 4.301333333333333e-05,
      "loss": 0.0033,
      "step": 10480
    },
    {
      "epoch": 0.5594666666666667,
      "grad_norm": 0.16886380314826965,
      "learning_rate": 4.300666666666667e-05,
      "loss": 0.0033,
      "step": 10490
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.12863901257514954,
      "learning_rate": 4.3e-05,
      "loss": 0.0034,
      "step": 10500
    },
    {
      "epoch": 0.5605333333333333,
      "grad_norm": 0.25604248046875,
      "learning_rate": 4.2993333333333334e-05,
      "loss": 0.002,
      "step": 10510
    },
    {
      "epoch": 0.5610666666666667,
      "grad_norm": 0.23640279471874237,
      "learning_rate": 4.2986666666666666e-05,
      "loss": 0.0023,
      "step": 10520
    },
    {
      "epoch": 0.5616,
      "grad_norm": 0.1723039448261261,
      "learning_rate": 4.2980000000000005e-05,
      "loss": 0.0034,
      "step": 10530
    },
    {
      "epoch": 0.5621333333333334,
      "grad_norm": 0.5044508576393127,
      "learning_rate": 4.297333333333334e-05,
      "loss": 0.0026,
      "step": 10540
    },
    {
      "epoch": 0.5626666666666666,
      "grad_norm": 0.12491411715745926,
      "learning_rate": 4.296666666666666e-05,
      "loss": 0.0024,
      "step": 10550
    },
    {
      "epoch": 0.5632,
      "grad_norm": 0.5164579153060913,
      "learning_rate": 4.296e-05,
      "loss": 0.003,
      "step": 10560
    },
    {
      "epoch": 0.5637333333333333,
      "grad_norm": 0.3044452667236328,
      "learning_rate": 4.2953333333333334e-05,
      "loss": 0.0026,
      "step": 10570
    },
    {
      "epoch": 0.5642666666666667,
      "grad_norm": 0.3441694676876068,
      "learning_rate": 4.2946666666666667e-05,
      "loss": 0.0018,
      "step": 10580
    },
    {
      "epoch": 0.5648,
      "grad_norm": 0.31325021386146545,
      "learning_rate": 4.2940000000000006e-05,
      "loss": 0.0024,
      "step": 10590
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 0.3765004277229309,
      "learning_rate": 4.293333333333334e-05,
      "loss": 0.0037,
      "step": 10600
    },
    {
      "epoch": 0.5658666666666666,
      "grad_norm": 0.10274748504161835,
      "learning_rate": 4.292666666666667e-05,
      "loss": 0.0033,
      "step": 10610
    },
    {
      "epoch": 0.5664,
      "grad_norm": 0.3429597020149231,
      "learning_rate": 4.292e-05,
      "loss": 0.0021,
      "step": 10620
    },
    {
      "epoch": 0.5669333333333333,
      "grad_norm": 0.2377433031797409,
      "learning_rate": 4.2913333333333335e-05,
      "loss": 0.0034,
      "step": 10630
    },
    {
      "epoch": 0.5674666666666667,
      "grad_norm": 0.4988443851470947,
      "learning_rate": 4.290666666666667e-05,
      "loss": 0.0027,
      "step": 10640
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.1216694787144661,
      "learning_rate": 4.29e-05,
      "loss": 0.0019,
      "step": 10650
    },
    {
      "epoch": 0.5685333333333333,
      "grad_norm": 0.45263582468032837,
      "learning_rate": 4.289333333333334e-05,
      "loss": 0.0027,
      "step": 10660
    },
    {
      "epoch": 0.5690666666666667,
      "grad_norm": 0.2362661510705948,
      "learning_rate": 4.288666666666667e-05,
      "loss": 0.0032,
      "step": 10670
    },
    {
      "epoch": 0.5696,
      "grad_norm": 0.1354282647371292,
      "learning_rate": 4.288e-05,
      "loss": 0.0034,
      "step": 10680
    },
    {
      "epoch": 0.5701333333333334,
      "grad_norm": 0.3591691553592682,
      "learning_rate": 4.2873333333333335e-05,
      "loss": 0.0022,
      "step": 10690
    },
    {
      "epoch": 0.5706666666666667,
      "grad_norm": 0.14545077085494995,
      "learning_rate": 4.286666666666667e-05,
      "loss": 0.0034,
      "step": 10700
    },
    {
      "epoch": 0.5712,
      "grad_norm": 0.11775404959917068,
      "learning_rate": 4.286e-05,
      "loss": 0.0022,
      "step": 10710
    },
    {
      "epoch": 0.5717333333333333,
      "grad_norm": 0.1745765656232834,
      "learning_rate": 4.285333333333333e-05,
      "loss": 0.0028,
      "step": 10720
    },
    {
      "epoch": 0.5722666666666667,
      "grad_norm": 0.3781369924545288,
      "learning_rate": 4.284666666666667e-05,
      "loss": 0.003,
      "step": 10730
    },
    {
      "epoch": 0.5728,
      "grad_norm": 0.5288980603218079,
      "learning_rate": 4.284e-05,
      "loss": 0.0025,
      "step": 10740
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.36771881580352783,
      "learning_rate": 4.2833333333333335e-05,
      "loss": 0.0032,
      "step": 10750
    },
    {
      "epoch": 0.5738666666666666,
      "grad_norm": 0.8104742169380188,
      "learning_rate": 4.282666666666667e-05,
      "loss": 0.0036,
      "step": 10760
    },
    {
      "epoch": 0.5744,
      "grad_norm": 0.19387292861938477,
      "learning_rate": 4.282000000000001e-05,
      "loss": 0.0038,
      "step": 10770
    },
    {
      "epoch": 0.5749333333333333,
      "grad_norm": 0.14690057933330536,
      "learning_rate": 4.281333333333333e-05,
      "loss": 0.0021,
      "step": 10780
    },
    {
      "epoch": 0.5754666666666667,
      "grad_norm": 0.09864906221628189,
      "learning_rate": 4.2806666666666665e-05,
      "loss": 0.0042,
      "step": 10790
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.32885363698005676,
      "learning_rate": 4.2800000000000004e-05,
      "loss": 0.0048,
      "step": 10800
    },
    {
      "epoch": 0.5765333333333333,
      "grad_norm": 0.3501861095428467,
      "learning_rate": 4.2793333333333336e-05,
      "loss": 0.0023,
      "step": 10810
    },
    {
      "epoch": 0.5770666666666666,
      "grad_norm": 0.2506159842014313,
      "learning_rate": 4.278666666666667e-05,
      "loss": 0.0032,
      "step": 10820
    },
    {
      "epoch": 0.5776,
      "grad_norm": 0.14004483819007874,
      "learning_rate": 4.278e-05,
      "loss": 0.0027,
      "step": 10830
    },
    {
      "epoch": 0.5781333333333334,
      "grad_norm": 0.7305253744125366,
      "learning_rate": 4.277333333333334e-05,
      "loss": 0.0028,
      "step": 10840
    },
    {
      "epoch": 0.5786666666666667,
      "grad_norm": 0.441974937915802,
      "learning_rate": 4.2766666666666665e-05,
      "loss": 0.0025,
      "step": 10850
    },
    {
      "epoch": 0.5792,
      "grad_norm": 0.16557426750659943,
      "learning_rate": 4.276e-05,
      "loss": 0.0036,
      "step": 10860
    },
    {
      "epoch": 0.5797333333333333,
      "grad_norm": 0.06400734186172485,
      "learning_rate": 4.2753333333333336e-05,
      "loss": 0.0024,
      "step": 10870
    },
    {
      "epoch": 0.5802666666666667,
      "grad_norm": 0.506014347076416,
      "learning_rate": 4.274666666666667e-05,
      "loss": 0.0031,
      "step": 10880
    },
    {
      "epoch": 0.5808,
      "grad_norm": 0.15839911997318268,
      "learning_rate": 4.274e-05,
      "loss": 0.0023,
      "step": 10890
    },
    {
      "epoch": 0.5813333333333334,
      "grad_norm": 0.26773619651794434,
      "learning_rate": 4.273333333333333e-05,
      "loss": 0.0022,
      "step": 10900
    },
    {
      "epoch": 0.5818666666666666,
      "grad_norm": 0.49400562047958374,
      "learning_rate": 4.272666666666667e-05,
      "loss": 0.0023,
      "step": 10910
    },
    {
      "epoch": 0.5824,
      "grad_norm": 0.2535925805568695,
      "learning_rate": 4.2720000000000004e-05,
      "loss": 0.0032,
      "step": 10920
    },
    {
      "epoch": 0.5829333333333333,
      "grad_norm": 0.36696484684944153,
      "learning_rate": 4.271333333333333e-05,
      "loss": 0.0028,
      "step": 10930
    },
    {
      "epoch": 0.5834666666666667,
      "grad_norm": 0.1314004361629486,
      "learning_rate": 4.270666666666667e-05,
      "loss": 0.0028,
      "step": 10940
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.8754209280014038,
      "learning_rate": 4.27e-05,
      "loss": 0.0036,
      "step": 10950
    },
    {
      "epoch": 0.5845333333333333,
      "grad_norm": 0.2790462076663971,
      "learning_rate": 4.2693333333333333e-05,
      "loss": 0.0027,
      "step": 10960
    },
    {
      "epoch": 0.5850666666666666,
      "grad_norm": 0.1165773794054985,
      "learning_rate": 4.268666666666667e-05,
      "loss": 0.0032,
      "step": 10970
    },
    {
      "epoch": 0.5856,
      "grad_norm": 0.15166285634040833,
      "learning_rate": 4.2680000000000005e-05,
      "loss": 0.0024,
      "step": 10980
    },
    {
      "epoch": 0.5861333333333333,
      "grad_norm": 0.2834233045578003,
      "learning_rate": 4.267333333333334e-05,
      "loss": 0.0032,
      "step": 10990
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.4960227906703949,
      "learning_rate": 4.266666666666667e-05,
      "loss": 0.0031,
      "step": 11000
    },
    {
      "epoch": 0.5872,
      "grad_norm": 0.46476680040359497,
      "learning_rate": 4.266e-05,
      "loss": 0.0023,
      "step": 11010
    },
    {
      "epoch": 0.5877333333333333,
      "grad_norm": 0.3199369013309479,
      "learning_rate": 4.2653333333333334e-05,
      "loss": 0.0039,
      "step": 11020
    },
    {
      "epoch": 0.5882666666666667,
      "grad_norm": 0.5065760612487793,
      "learning_rate": 4.2646666666666666e-05,
      "loss": 0.0026,
      "step": 11030
    },
    {
      "epoch": 0.5888,
      "grad_norm": 0.42564043402671814,
      "learning_rate": 4.2640000000000005e-05,
      "loss": 0.0022,
      "step": 11040
    },
    {
      "epoch": 0.5893333333333334,
      "grad_norm": 0.6626135110855103,
      "learning_rate": 4.263333333333334e-05,
      "loss": 0.0022,
      "step": 11050
    },
    {
      "epoch": 0.5898666666666667,
      "grad_norm": 0.5379294157028198,
      "learning_rate": 4.262666666666667e-05,
      "loss": 0.0025,
      "step": 11060
    },
    {
      "epoch": 0.5904,
      "grad_norm": 0.21784339845180511,
      "learning_rate": 4.262e-05,
      "loss": 0.0021,
      "step": 11070
    },
    {
      "epoch": 0.5909333333333333,
      "grad_norm": 0.19649691879749298,
      "learning_rate": 4.2613333333333334e-05,
      "loss": 0.0025,
      "step": 11080
    },
    {
      "epoch": 0.5914666666666667,
      "grad_norm": 0.7367050051689148,
      "learning_rate": 4.2606666666666666e-05,
      "loss": 0.0027,
      "step": 11090
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.1739223301410675,
      "learning_rate": 4.26e-05,
      "loss": 0.0023,
      "step": 11100
    },
    {
      "epoch": 0.5925333333333334,
      "grad_norm": 0.2074989378452301,
      "learning_rate": 4.259333333333334e-05,
      "loss": 0.0023,
      "step": 11110
    },
    {
      "epoch": 0.5930666666666666,
      "grad_norm": 0.18767260015010834,
      "learning_rate": 4.258666666666667e-05,
      "loss": 0.0025,
      "step": 11120
    },
    {
      "epoch": 0.5936,
      "grad_norm": 0.2468087077140808,
      "learning_rate": 4.258e-05,
      "loss": 0.0037,
      "step": 11130
    },
    {
      "epoch": 0.5941333333333333,
      "grad_norm": 0.6859143376350403,
      "learning_rate": 4.2573333333333335e-05,
      "loss": 0.0031,
      "step": 11140
    },
    {
      "epoch": 0.5946666666666667,
      "grad_norm": 0.4268144965171814,
      "learning_rate": 4.2566666666666674e-05,
      "loss": 0.0022,
      "step": 11150
    },
    {
      "epoch": 0.5952,
      "grad_norm": 0.23916926980018616,
      "learning_rate": 4.256e-05,
      "loss": 0.0028,
      "step": 11160
    },
    {
      "epoch": 0.5957333333333333,
      "grad_norm": 0.6978318691253662,
      "learning_rate": 4.255333333333333e-05,
      "loss": 0.0025,
      "step": 11170
    },
    {
      "epoch": 0.5962666666666666,
      "grad_norm": 0.5515463352203369,
      "learning_rate": 4.254666666666667e-05,
      "loss": 0.0029,
      "step": 11180
    },
    {
      "epoch": 0.5968,
      "grad_norm": 0.48915815353393555,
      "learning_rate": 4.254e-05,
      "loss": 0.0033,
      "step": 11190
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.7990826964378357,
      "learning_rate": 4.2533333333333335e-05,
      "loss": 0.0028,
      "step": 11200
    },
    {
      "epoch": 0.5978666666666667,
      "grad_norm": 0.15166829526424408,
      "learning_rate": 4.252666666666667e-05,
      "loss": 0.0029,
      "step": 11210
    },
    {
      "epoch": 0.5984,
      "grad_norm": 0.07462199032306671,
      "learning_rate": 4.2520000000000006e-05,
      "loss": 0.0021,
      "step": 11220
    },
    {
      "epoch": 0.5989333333333333,
      "grad_norm": 0.1431109756231308,
      "learning_rate": 4.251333333333333e-05,
      "loss": 0.0033,
      "step": 11230
    },
    {
      "epoch": 0.5994666666666667,
      "grad_norm": 0.11520718038082123,
      "learning_rate": 4.2506666666666664e-05,
      "loss": 0.0023,
      "step": 11240
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.3856566548347473,
      "learning_rate": 4.25e-05,
      "loss": 0.0034,
      "step": 11250
    },
    {
      "epoch": 0.6005333333333334,
      "grad_norm": 0.33262771368026733,
      "learning_rate": 4.2493333333333335e-05,
      "loss": 0.0026,
      "step": 11260
    },
    {
      "epoch": 0.6010666666666666,
      "grad_norm": 0.2900652289390564,
      "learning_rate": 4.248666666666667e-05,
      "loss": 0.0024,
      "step": 11270
    },
    {
      "epoch": 0.6016,
      "grad_norm": 0.45452865958213806,
      "learning_rate": 4.248e-05,
      "loss": 0.0031,
      "step": 11280
    },
    {
      "epoch": 0.6021333333333333,
      "grad_norm": 0.5497839450836182,
      "learning_rate": 4.247333333333334e-05,
      "loss": 0.0049,
      "step": 11290
    },
    {
      "epoch": 0.6026666666666667,
      "grad_norm": 0.6166640520095825,
      "learning_rate": 4.246666666666667e-05,
      "loss": 0.0027,
      "step": 11300
    },
    {
      "epoch": 0.6032,
      "grad_norm": 0.2505855858325958,
      "learning_rate": 4.246e-05,
      "loss": 0.002,
      "step": 11310
    },
    {
      "epoch": 0.6037333333333333,
      "grad_norm": 0.13359971344470978,
      "learning_rate": 4.2453333333333336e-05,
      "loss": 0.0031,
      "step": 11320
    },
    {
      "epoch": 0.6042666666666666,
      "grad_norm": 0.07438111305236816,
      "learning_rate": 4.244666666666667e-05,
      "loss": 0.0042,
      "step": 11330
    },
    {
      "epoch": 0.6048,
      "grad_norm": 0.5619397759437561,
      "learning_rate": 4.244e-05,
      "loss": 0.0022,
      "step": 11340
    },
    {
      "epoch": 0.6053333333333333,
      "grad_norm": 0.16112221777439117,
      "learning_rate": 4.243333333333334e-05,
      "loss": 0.0026,
      "step": 11350
    },
    {
      "epoch": 0.6058666666666667,
      "grad_norm": 0.1854783445596695,
      "learning_rate": 4.242666666666667e-05,
      "loss": 0.0027,
      "step": 11360
    },
    {
      "epoch": 0.6064,
      "grad_norm": 0.25204429030418396,
      "learning_rate": 4.2420000000000004e-05,
      "loss": 0.0019,
      "step": 11370
    },
    {
      "epoch": 0.6069333333333333,
      "grad_norm": 0.30293339490890503,
      "learning_rate": 4.241333333333333e-05,
      "loss": 0.0025,
      "step": 11380
    },
    {
      "epoch": 0.6074666666666667,
      "grad_norm": 0.37027931213378906,
      "learning_rate": 4.240666666666667e-05,
      "loss": 0.0026,
      "step": 11390
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.1695021241903305,
      "learning_rate": 4.24e-05,
      "loss": 0.0021,
      "step": 11400
    },
    {
      "epoch": 0.6085333333333334,
      "grad_norm": 0.0814685970544815,
      "learning_rate": 4.239333333333333e-05,
      "loss": 0.0023,
      "step": 11410
    },
    {
      "epoch": 0.6090666666666666,
      "grad_norm": 0.15271613001823425,
      "learning_rate": 4.238666666666667e-05,
      "loss": 0.0024,
      "step": 11420
    },
    {
      "epoch": 0.6096,
      "grad_norm": 0.30924707651138306,
      "learning_rate": 4.2380000000000004e-05,
      "loss": 0.0022,
      "step": 11430
    },
    {
      "epoch": 0.6101333333333333,
      "grad_norm": 0.7705342769622803,
      "learning_rate": 4.2373333333333336e-05,
      "loss": 0.0025,
      "step": 11440
    },
    {
      "epoch": 0.6106666666666667,
      "grad_norm": 0.21453849971294403,
      "learning_rate": 4.236666666666667e-05,
      "loss": 0.0026,
      "step": 11450
    },
    {
      "epoch": 0.6112,
      "grad_norm": 0.44970041513442993,
      "learning_rate": 4.236e-05,
      "loss": 0.0022,
      "step": 11460
    },
    {
      "epoch": 0.6117333333333334,
      "grad_norm": 0.6027247905731201,
      "learning_rate": 4.235333333333333e-05,
      "loss": 0.0039,
      "step": 11470
    },
    {
      "epoch": 0.6122666666666666,
      "grad_norm": 0.3194771409034729,
      "learning_rate": 4.2346666666666666e-05,
      "loss": 0.0036,
      "step": 11480
    },
    {
      "epoch": 0.6128,
      "grad_norm": 0.43749740719795227,
      "learning_rate": 4.2340000000000005e-05,
      "loss": 0.0021,
      "step": 11490
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.23953747749328613,
      "learning_rate": 4.233333333333334e-05,
      "loss": 0.0042,
      "step": 11500
    },
    {
      "epoch": 0.6138666666666667,
      "grad_norm": 0.6465930938720703,
      "learning_rate": 4.232666666666667e-05,
      "loss": 0.0043,
      "step": 11510
    },
    {
      "epoch": 0.6144,
      "grad_norm": 0.3824410140514374,
      "learning_rate": 4.232e-05,
      "loss": 0.0033,
      "step": 11520
    },
    {
      "epoch": 0.6149333333333333,
      "grad_norm": 0.3429661691188812,
      "learning_rate": 4.2313333333333334e-05,
      "loss": 0.0025,
      "step": 11530
    },
    {
      "epoch": 0.6154666666666667,
      "grad_norm": 0.3633200228214264,
      "learning_rate": 4.2306666666666666e-05,
      "loss": 0.0034,
      "step": 11540
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.11673460900783539,
      "learning_rate": 4.23e-05,
      "loss": 0.0026,
      "step": 11550
    },
    {
      "epoch": 0.6165333333333334,
      "grad_norm": 0.24634207785129547,
      "learning_rate": 4.229333333333334e-05,
      "loss": 0.0035,
      "step": 11560
    },
    {
      "epoch": 0.6170666666666667,
      "grad_norm": 0.3796781003475189,
      "learning_rate": 4.228666666666667e-05,
      "loss": 0.0041,
      "step": 11570
    },
    {
      "epoch": 0.6176,
      "grad_norm": 0.5772703289985657,
      "learning_rate": 4.228e-05,
      "loss": 0.004,
      "step": 11580
    },
    {
      "epoch": 0.6181333333333333,
      "grad_norm": 0.16379579901695251,
      "learning_rate": 4.2273333333333334e-05,
      "loss": 0.0035,
      "step": 11590
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 0.6806855797767639,
      "learning_rate": 4.226666666666667e-05,
      "loss": 0.003,
      "step": 11600
    },
    {
      "epoch": 0.6192,
      "grad_norm": 0.42521411180496216,
      "learning_rate": 4.226e-05,
      "loss": 0.003,
      "step": 11610
    },
    {
      "epoch": 0.6197333333333334,
      "grad_norm": 0.2526548206806183,
      "learning_rate": 4.225333333333333e-05,
      "loss": 0.0033,
      "step": 11620
    },
    {
      "epoch": 0.6202666666666666,
      "grad_norm": 0.4976978302001953,
      "learning_rate": 4.224666666666667e-05,
      "loss": 0.0036,
      "step": 11630
    },
    {
      "epoch": 0.6208,
      "grad_norm": 0.43018633127212524,
      "learning_rate": 4.224e-05,
      "loss": 0.0035,
      "step": 11640
    },
    {
      "epoch": 0.6213333333333333,
      "grad_norm": 0.6596704125404358,
      "learning_rate": 4.2233333333333334e-05,
      "loss": 0.0021,
      "step": 11650
    },
    {
      "epoch": 0.6218666666666667,
      "grad_norm": 0.3570736050605774,
      "learning_rate": 4.222666666666667e-05,
      "loss": 0.0037,
      "step": 11660
    },
    {
      "epoch": 0.6224,
      "grad_norm": 0.39055803418159485,
      "learning_rate": 4.2220000000000006e-05,
      "loss": 0.0032,
      "step": 11670
    },
    {
      "epoch": 0.6229333333333333,
      "grad_norm": 0.16802750527858734,
      "learning_rate": 4.221333333333334e-05,
      "loss": 0.0029,
      "step": 11680
    },
    {
      "epoch": 0.6234666666666666,
      "grad_norm": 0.17271865904331207,
      "learning_rate": 4.2206666666666663e-05,
      "loss": 0.0026,
      "step": 11690
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.2791309356689453,
      "learning_rate": 4.22e-05,
      "loss": 0.0022,
      "step": 11700
    },
    {
      "epoch": 0.6245333333333334,
      "grad_norm": 0.15945322811603546,
      "learning_rate": 4.2193333333333335e-05,
      "loss": 0.0036,
      "step": 11710
    },
    {
      "epoch": 0.6250666666666667,
      "grad_norm": 0.6316500902175903,
      "learning_rate": 4.218666666666667e-05,
      "loss": 0.0032,
      "step": 11720
    },
    {
      "epoch": 0.6256,
      "grad_norm": 0.3294379413127899,
      "learning_rate": 4.2180000000000006e-05,
      "loss": 0.0026,
      "step": 11730
    },
    {
      "epoch": 0.6261333333333333,
      "grad_norm": 0.2761000096797943,
      "learning_rate": 4.217333333333334e-05,
      "loss": 0.0027,
      "step": 11740
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.6596915125846863,
      "learning_rate": 4.216666666666667e-05,
      "loss": 0.0054,
      "step": 11750
    },
    {
      "epoch": 0.6272,
      "grad_norm": 0.8224225640296936,
      "learning_rate": 4.2159999999999996e-05,
      "loss": 0.0037,
      "step": 11760
    },
    {
      "epoch": 0.6277333333333334,
      "grad_norm": 0.34299755096435547,
      "learning_rate": 4.2153333333333335e-05,
      "loss": 0.0036,
      "step": 11770
    },
    {
      "epoch": 0.6282666666666666,
      "grad_norm": 0.20277170836925507,
      "learning_rate": 4.214666666666667e-05,
      "loss": 0.003,
      "step": 11780
    },
    {
      "epoch": 0.6288,
      "grad_norm": 0.2202843576669693,
      "learning_rate": 4.214e-05,
      "loss": 0.0032,
      "step": 11790
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 0.13202308118343353,
      "learning_rate": 4.213333333333334e-05,
      "loss": 0.0025,
      "step": 11800
    },
    {
      "epoch": 0.6298666666666667,
      "grad_norm": 0.21416029334068298,
      "learning_rate": 4.212666666666667e-05,
      "loss": 0.0033,
      "step": 11810
    },
    {
      "epoch": 0.6304,
      "grad_norm": 0.7091540694236755,
      "learning_rate": 4.212e-05,
      "loss": 0.0022,
      "step": 11820
    },
    {
      "epoch": 0.6309333333333333,
      "grad_norm": 0.2757911682128906,
      "learning_rate": 4.2113333333333336e-05,
      "loss": 0.002,
      "step": 11830
    },
    {
      "epoch": 0.6314666666666666,
      "grad_norm": 0.16129586100578308,
      "learning_rate": 4.210666666666667e-05,
      "loss": 0.0023,
      "step": 11840
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.4627034664154053,
      "learning_rate": 4.21e-05,
      "loss": 0.0026,
      "step": 11850
    },
    {
      "epoch": 0.6325333333333333,
      "grad_norm": 0.5400324463844299,
      "learning_rate": 4.209333333333333e-05,
      "loss": 0.0034,
      "step": 11860
    },
    {
      "epoch": 0.6330666666666667,
      "grad_norm": 0.4487258791923523,
      "learning_rate": 4.208666666666667e-05,
      "loss": 0.0042,
      "step": 11870
    },
    {
      "epoch": 0.6336,
      "grad_norm": 0.8845385909080505,
      "learning_rate": 4.2080000000000004e-05,
      "loss": 0.0026,
      "step": 11880
    },
    {
      "epoch": 0.6341333333333333,
      "grad_norm": 0.2632158398628235,
      "learning_rate": 4.2073333333333336e-05,
      "loss": 0.0036,
      "step": 11890
    },
    {
      "epoch": 0.6346666666666667,
      "grad_norm": 0.8372358679771423,
      "learning_rate": 4.206666666666667e-05,
      "loss": 0.0032,
      "step": 11900
    },
    {
      "epoch": 0.6352,
      "grad_norm": 0.3202607333660126,
      "learning_rate": 4.206e-05,
      "loss": 0.0038,
      "step": 11910
    },
    {
      "epoch": 0.6357333333333334,
      "grad_norm": 0.36542320251464844,
      "learning_rate": 4.205333333333333e-05,
      "loss": 0.0032,
      "step": 11920
    },
    {
      "epoch": 0.6362666666666666,
      "grad_norm": 0.20864039659500122,
      "learning_rate": 4.2046666666666665e-05,
      "loss": 0.0026,
      "step": 11930
    },
    {
      "epoch": 0.6368,
      "grad_norm": 0.22174327075481415,
      "learning_rate": 4.2040000000000004e-05,
      "loss": 0.0027,
      "step": 11940
    },
    {
      "epoch": 0.6373333333333333,
      "grad_norm": 0.3878100514411926,
      "learning_rate": 4.2033333333333336e-05,
      "loss": 0.0027,
      "step": 11950
    },
    {
      "epoch": 0.6378666666666667,
      "grad_norm": 0.28949111700057983,
      "learning_rate": 4.202666666666667e-05,
      "loss": 0.0031,
      "step": 11960
    },
    {
      "epoch": 0.6384,
      "grad_norm": 0.46845895051956177,
      "learning_rate": 4.202e-05,
      "loss": 0.0024,
      "step": 11970
    },
    {
      "epoch": 0.6389333333333334,
      "grad_norm": 0.6217939257621765,
      "learning_rate": 4.201333333333334e-05,
      "loss": 0.0031,
      "step": 11980
    },
    {
      "epoch": 0.6394666666666666,
      "grad_norm": 0.2429792881011963,
      "learning_rate": 4.2006666666666665e-05,
      "loss": 0.0035,
      "step": 11990
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.6112486720085144,
      "learning_rate": 4.2e-05,
      "loss": 0.0028,
      "step": 12000
    },
    {
      "epoch": 0.6405333333333333,
      "grad_norm": 0.755242645740509,
      "learning_rate": 4.199333333333334e-05,
      "loss": 0.0031,
      "step": 12010
    },
    {
      "epoch": 0.6410666666666667,
      "grad_norm": 0.9316303730010986,
      "learning_rate": 4.198666666666667e-05,
      "loss": 0.0024,
      "step": 12020
    },
    {
      "epoch": 0.6416,
      "grad_norm": 0.08204010874032974,
      "learning_rate": 4.198e-05,
      "loss": 0.0038,
      "step": 12030
    },
    {
      "epoch": 0.6421333333333333,
      "grad_norm": 0.9409721493721008,
      "learning_rate": 4.1973333333333334e-05,
      "loss": 0.0031,
      "step": 12040
    },
    {
      "epoch": 0.6426666666666667,
      "grad_norm": 0.23780983686447144,
      "learning_rate": 4.196666666666667e-05,
      "loss": 0.0048,
      "step": 12050
    },
    {
      "epoch": 0.6432,
      "grad_norm": 0.28525951504707336,
      "learning_rate": 4.196e-05,
      "loss": 0.0027,
      "step": 12060
    },
    {
      "epoch": 0.6437333333333334,
      "grad_norm": 0.34113144874572754,
      "learning_rate": 4.195333333333333e-05,
      "loss": 0.0043,
      "step": 12070
    },
    {
      "epoch": 0.6442666666666667,
      "grad_norm": 0.5623584389686584,
      "learning_rate": 4.194666666666667e-05,
      "loss": 0.0027,
      "step": 12080
    },
    {
      "epoch": 0.6448,
      "grad_norm": 0.2080966830253601,
      "learning_rate": 4.194e-05,
      "loss": 0.0024,
      "step": 12090
    },
    {
      "epoch": 0.6453333333333333,
      "grad_norm": 0.5655384659767151,
      "learning_rate": 4.1933333333333334e-05,
      "loss": 0.0032,
      "step": 12100
    },
    {
      "epoch": 0.6458666666666667,
      "grad_norm": 0.5876075029373169,
      "learning_rate": 4.192666666666667e-05,
      "loss": 0.0032,
      "step": 12110
    },
    {
      "epoch": 0.6464,
      "grad_norm": 0.22303377091884613,
      "learning_rate": 4.1920000000000005e-05,
      "loss": 0.0028,
      "step": 12120
    },
    {
      "epoch": 0.6469333333333334,
      "grad_norm": 0.4015696346759796,
      "learning_rate": 4.191333333333334e-05,
      "loss": 0.0034,
      "step": 12130
    },
    {
      "epoch": 0.6474666666666666,
      "grad_norm": 0.314337819814682,
      "learning_rate": 4.190666666666666e-05,
      "loss": 0.0026,
      "step": 12140
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.8433626294136047,
      "learning_rate": 4.19e-05,
      "loss": 0.0034,
      "step": 12150
    },
    {
      "epoch": 0.6485333333333333,
      "grad_norm": 0.5797644257545471,
      "learning_rate": 4.1893333333333334e-05,
      "loss": 0.0033,
      "step": 12160
    },
    {
      "epoch": 0.6490666666666667,
      "grad_norm": 0.13353759050369263,
      "learning_rate": 4.1886666666666667e-05,
      "loss": 0.0019,
      "step": 12170
    },
    {
      "epoch": 0.6496,
      "grad_norm": 0.20803597569465637,
      "learning_rate": 4.1880000000000006e-05,
      "loss": 0.0024,
      "step": 12180
    },
    {
      "epoch": 0.6501333333333333,
      "grad_norm": 0.16646532714366913,
      "learning_rate": 4.187333333333334e-05,
      "loss": 0.0036,
      "step": 12190
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 0.23211590945720673,
      "learning_rate": 4.186666666666667e-05,
      "loss": 0.0027,
      "step": 12200
    },
    {
      "epoch": 0.6512,
      "grad_norm": 0.22247600555419922,
      "learning_rate": 4.186e-05,
      "loss": 0.002,
      "step": 12210
    },
    {
      "epoch": 0.6517333333333334,
      "grad_norm": 0.3398231565952301,
      "learning_rate": 4.1853333333333335e-05,
      "loss": 0.0034,
      "step": 12220
    },
    {
      "epoch": 0.6522666666666667,
      "grad_norm": 0.3223353922367096,
      "learning_rate": 4.184666666666667e-05,
      "loss": 0.0031,
      "step": 12230
    },
    {
      "epoch": 0.6528,
      "grad_norm": 0.3709925711154938,
      "learning_rate": 4.184e-05,
      "loss": 0.0035,
      "step": 12240
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.19196869432926178,
      "learning_rate": 4.183333333333334e-05,
      "loss": 0.0028,
      "step": 12250
    },
    {
      "epoch": 0.6538666666666667,
      "grad_norm": 0.15236471593379974,
      "learning_rate": 4.182666666666667e-05,
      "loss": 0.0025,
      "step": 12260
    },
    {
      "epoch": 0.6544,
      "grad_norm": 0.555292546749115,
      "learning_rate": 4.182e-05,
      "loss": 0.0034,
      "step": 12270
    },
    {
      "epoch": 0.6549333333333334,
      "grad_norm": 0.2556823194026947,
      "learning_rate": 4.1813333333333335e-05,
      "loss": 0.002,
      "step": 12280
    },
    {
      "epoch": 0.6554666666666666,
      "grad_norm": 0.4986276626586914,
      "learning_rate": 4.180666666666667e-05,
      "loss": 0.0023,
      "step": 12290
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.39746469259262085,
      "learning_rate": 4.18e-05,
      "loss": 0.0033,
      "step": 12300
    },
    {
      "epoch": 0.6565333333333333,
      "grad_norm": 0.24801097810268402,
      "learning_rate": 4.179333333333333e-05,
      "loss": 0.0023,
      "step": 12310
    },
    {
      "epoch": 0.6570666666666667,
      "grad_norm": 0.19074587523937225,
      "learning_rate": 4.178666666666667e-05,
      "loss": 0.0026,
      "step": 12320
    },
    {
      "epoch": 0.6576,
      "grad_norm": 0.2548486590385437,
      "learning_rate": 4.178e-05,
      "loss": 0.0034,
      "step": 12330
    },
    {
      "epoch": 0.6581333333333333,
      "grad_norm": 0.29836198687553406,
      "learning_rate": 4.1773333333333335e-05,
      "loss": 0.0025,
      "step": 12340
    },
    {
      "epoch": 0.6586666666666666,
      "grad_norm": 0.491748183965683,
      "learning_rate": 4.176666666666667e-05,
      "loss": 0.0036,
      "step": 12350
    },
    {
      "epoch": 0.6592,
      "grad_norm": 0.155901238322258,
      "learning_rate": 4.176000000000001e-05,
      "loss": 0.0027,
      "step": 12360
    },
    {
      "epoch": 0.6597333333333333,
      "grad_norm": 0.48148512840270996,
      "learning_rate": 4.175333333333333e-05,
      "loss": 0.003,
      "step": 12370
    },
    {
      "epoch": 0.6602666666666667,
      "grad_norm": 0.2133297324180603,
      "learning_rate": 4.1746666666666665e-05,
      "loss": 0.0035,
      "step": 12380
    },
    {
      "epoch": 0.6608,
      "grad_norm": 0.5433481931686401,
      "learning_rate": 4.1740000000000004e-05,
      "loss": 0.0022,
      "step": 12390
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 0.6490170955657959,
      "learning_rate": 4.1733333333333336e-05,
      "loss": 0.0022,
      "step": 12400
    },
    {
      "epoch": 0.6618666666666667,
      "grad_norm": 0.3441034257411957,
      "learning_rate": 4.172666666666667e-05,
      "loss": 0.0028,
      "step": 12410
    },
    {
      "epoch": 0.6624,
      "grad_norm": 0.11061940342187881,
      "learning_rate": 4.172e-05,
      "loss": 0.002,
      "step": 12420
    },
    {
      "epoch": 0.6629333333333334,
      "grad_norm": 0.18645264208316803,
      "learning_rate": 4.171333333333334e-05,
      "loss": 0.0025,
      "step": 12430
    },
    {
      "epoch": 0.6634666666666666,
      "grad_norm": 0.3925347328186035,
      "learning_rate": 4.1706666666666665e-05,
      "loss": 0.0026,
      "step": 12440
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.6165386438369751,
      "learning_rate": 4.17e-05,
      "loss": 0.004,
      "step": 12450
    },
    {
      "epoch": 0.6645333333333333,
      "grad_norm": 0.08236368745565414,
      "learning_rate": 4.1693333333333336e-05,
      "loss": 0.0029,
      "step": 12460
    },
    {
      "epoch": 0.6650666666666667,
      "grad_norm": 0.25488314032554626,
      "learning_rate": 4.168666666666667e-05,
      "loss": 0.0033,
      "step": 12470
    },
    {
      "epoch": 0.6656,
      "grad_norm": 0.3629961907863617,
      "learning_rate": 4.168e-05,
      "loss": 0.0035,
      "step": 12480
    },
    {
      "epoch": 0.6661333333333334,
      "grad_norm": 0.2989182472229004,
      "learning_rate": 4.167333333333334e-05,
      "loss": 0.0031,
      "step": 12490
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.19363132119178772,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.0031,
      "step": 12500
    },
    {
      "epoch": 0.6672,
      "grad_norm": 0.1041787713766098,
      "learning_rate": 4.1660000000000004e-05,
      "loss": 0.0024,
      "step": 12510
    },
    {
      "epoch": 0.6677333333333333,
      "grad_norm": 0.299140989780426,
      "learning_rate": 4.165333333333333e-05,
      "loss": 0.0032,
      "step": 12520
    },
    {
      "epoch": 0.6682666666666667,
      "grad_norm": 0.23459729552268982,
      "learning_rate": 4.164666666666667e-05,
      "loss": 0.0035,
      "step": 12530
    },
    {
      "epoch": 0.6688,
      "grad_norm": 0.6946858763694763,
      "learning_rate": 4.164e-05,
      "loss": 0.0041,
      "step": 12540
    },
    {
      "epoch": 0.6693333333333333,
      "grad_norm": 0.5345081090927124,
      "learning_rate": 4.1633333333333333e-05,
      "loss": 0.0033,
      "step": 12550
    },
    {
      "epoch": 0.6698666666666667,
      "grad_norm": 0.2759835422039032,
      "learning_rate": 4.162666666666667e-05,
      "loss": 0.0036,
      "step": 12560
    },
    {
      "epoch": 0.6704,
      "grad_norm": 0.5271610021591187,
      "learning_rate": 4.1620000000000005e-05,
      "loss": 0.0026,
      "step": 12570
    },
    {
      "epoch": 0.6709333333333334,
      "grad_norm": 0.2121099978685379,
      "learning_rate": 4.161333333333334e-05,
      "loss": 0.0047,
      "step": 12580
    },
    {
      "epoch": 0.6714666666666667,
      "grad_norm": 0.1387411206960678,
      "learning_rate": 4.160666666666667e-05,
      "loss": 0.0041,
      "step": 12590
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.22425086796283722,
      "learning_rate": 4.16e-05,
      "loss": 0.0031,
      "step": 12600
    },
    {
      "epoch": 0.6725333333333333,
      "grad_norm": 0.7029504776000977,
      "learning_rate": 4.1593333333333334e-05,
      "loss": 0.0033,
      "step": 12610
    },
    {
      "epoch": 0.6730666666666667,
      "grad_norm": 0.48721301555633545,
      "learning_rate": 4.1586666666666666e-05,
      "loss": 0.0022,
      "step": 12620
    },
    {
      "epoch": 0.6736,
      "grad_norm": 0.63770592212677,
      "learning_rate": 4.1580000000000005e-05,
      "loss": 0.0024,
      "step": 12630
    },
    {
      "epoch": 0.6741333333333334,
      "grad_norm": 0.4481561779975891,
      "learning_rate": 4.157333333333334e-05,
      "loss": 0.0017,
      "step": 12640
    },
    {
      "epoch": 0.6746666666666666,
      "grad_norm": 0.2870011627674103,
      "learning_rate": 4.156666666666667e-05,
      "loss": 0.0036,
      "step": 12650
    },
    {
      "epoch": 0.6752,
      "grad_norm": 0.38244760036468506,
      "learning_rate": 4.156e-05,
      "loss": 0.0032,
      "step": 12660
    },
    {
      "epoch": 0.6757333333333333,
      "grad_norm": 0.11494095623493195,
      "learning_rate": 4.1553333333333334e-05,
      "loss": 0.002,
      "step": 12670
    },
    {
      "epoch": 0.6762666666666667,
      "grad_norm": 0.1748725324869156,
      "learning_rate": 4.1546666666666666e-05,
      "loss": 0.0024,
      "step": 12680
    },
    {
      "epoch": 0.6768,
      "grad_norm": 0.3682112693786621,
      "learning_rate": 4.154e-05,
      "loss": 0.0022,
      "step": 12690
    },
    {
      "epoch": 0.6773333333333333,
      "grad_norm": 0.29646503925323486,
      "learning_rate": 4.153333333333334e-05,
      "loss": 0.0021,
      "step": 12700
    },
    {
      "epoch": 0.6778666666666666,
      "grad_norm": 0.25108370184898376,
      "learning_rate": 4.152666666666667e-05,
      "loss": 0.0031,
      "step": 12710
    },
    {
      "epoch": 0.6784,
      "grad_norm": 0.39734718203544617,
      "learning_rate": 4.152e-05,
      "loss": 0.0026,
      "step": 12720
    },
    {
      "epoch": 0.6789333333333334,
      "grad_norm": 0.2768929600715637,
      "learning_rate": 4.1513333333333335e-05,
      "loss": 0.0037,
      "step": 12730
    },
    {
      "epoch": 0.6794666666666667,
      "grad_norm": 0.4160943031311035,
      "learning_rate": 4.150666666666667e-05,
      "loss": 0.0031,
      "step": 12740
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.2434101402759552,
      "learning_rate": 4.15e-05,
      "loss": 0.0022,
      "step": 12750
    },
    {
      "epoch": 0.6805333333333333,
      "grad_norm": 0.38843581080436707,
      "learning_rate": 4.149333333333333e-05,
      "loss": 0.0026,
      "step": 12760
    },
    {
      "epoch": 0.6810666666666667,
      "grad_norm": 0.1126365214586258,
      "learning_rate": 4.148666666666667e-05,
      "loss": 0.0022,
      "step": 12770
    },
    {
      "epoch": 0.6816,
      "grad_norm": 0.1619265377521515,
      "learning_rate": 4.148e-05,
      "loss": 0.0044,
      "step": 12780
    },
    {
      "epoch": 0.6821333333333334,
      "grad_norm": 0.5410717725753784,
      "learning_rate": 4.1473333333333335e-05,
      "loss": 0.0041,
      "step": 12790
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 0.16059163212776184,
      "learning_rate": 4.146666666666667e-05,
      "loss": 0.0025,
      "step": 12800
    },
    {
      "epoch": 0.6832,
      "grad_norm": 0.3479117453098297,
      "learning_rate": 4.1460000000000006e-05,
      "loss": 0.0027,
      "step": 12810
    },
    {
      "epoch": 0.6837333333333333,
      "grad_norm": 0.696746289730072,
      "learning_rate": 4.145333333333333e-05,
      "loss": 0.0034,
      "step": 12820
    },
    {
      "epoch": 0.6842666666666667,
      "grad_norm": 0.33676373958587646,
      "learning_rate": 4.1446666666666664e-05,
      "loss": 0.0037,
      "step": 12830
    },
    {
      "epoch": 0.6848,
      "grad_norm": 0.32657912373542786,
      "learning_rate": 4.144e-05,
      "loss": 0.0021,
      "step": 12840
    },
    {
      "epoch": 0.6853333333333333,
      "grad_norm": 0.38778167963027954,
      "learning_rate": 4.1433333333333335e-05,
      "loss": 0.0039,
      "step": 12850
    },
    {
      "epoch": 0.6858666666666666,
      "grad_norm": 0.24865497648715973,
      "learning_rate": 4.142666666666667e-05,
      "loss": 0.0033,
      "step": 12860
    },
    {
      "epoch": 0.6864,
      "grad_norm": 0.5013723373413086,
      "learning_rate": 4.142000000000001e-05,
      "loss": 0.0027,
      "step": 12870
    },
    {
      "epoch": 0.6869333333333333,
      "grad_norm": 0.7414566874504089,
      "learning_rate": 4.141333333333334e-05,
      "loss": 0.0031,
      "step": 12880
    },
    {
      "epoch": 0.6874666666666667,
      "grad_norm": 0.1898365616798401,
      "learning_rate": 4.140666666666667e-05,
      "loss": 0.0037,
      "step": 12890
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.273470938205719,
      "learning_rate": 4.14e-05,
      "loss": 0.0022,
      "step": 12900
    },
    {
      "epoch": 0.6885333333333333,
      "grad_norm": 0.558288037776947,
      "learning_rate": 4.1393333333333336e-05,
      "loss": 0.0039,
      "step": 12910
    },
    {
      "epoch": 0.6890666666666667,
      "grad_norm": 0.28125545382499695,
      "learning_rate": 4.138666666666667e-05,
      "loss": 0.0021,
      "step": 12920
    },
    {
      "epoch": 0.6896,
      "grad_norm": 0.4972599744796753,
      "learning_rate": 4.138e-05,
      "loss": 0.0019,
      "step": 12930
    },
    {
      "epoch": 0.6901333333333334,
      "grad_norm": 0.14722946286201477,
      "learning_rate": 4.137333333333334e-05,
      "loss": 0.0029,
      "step": 12940
    },
    {
      "epoch": 0.6906666666666667,
      "grad_norm": 0.08253726363182068,
      "learning_rate": 4.136666666666667e-05,
      "loss": 0.0018,
      "step": 12950
    },
    {
      "epoch": 0.6912,
      "grad_norm": 0.2172875702381134,
      "learning_rate": 4.1360000000000004e-05,
      "loss": 0.0028,
      "step": 12960
    },
    {
      "epoch": 0.6917333333333333,
      "grad_norm": 0.31978633999824524,
      "learning_rate": 4.1353333333333336e-05,
      "loss": 0.0023,
      "step": 12970
    },
    {
      "epoch": 0.6922666666666667,
      "grad_norm": 0.2600155174732208,
      "learning_rate": 4.134666666666667e-05,
      "loss": 0.0025,
      "step": 12980
    },
    {
      "epoch": 0.6928,
      "grad_norm": 0.4656693637371063,
      "learning_rate": 4.134e-05,
      "loss": 0.0027,
      "step": 12990
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.3678375482559204,
      "learning_rate": 4.133333333333333e-05,
      "loss": 0.004,
      "step": 13000
    },
    {
      "epoch": 0.6938666666666666,
      "grad_norm": 0.7017982006072998,
      "learning_rate": 4.132666666666667e-05,
      "loss": 0.0033,
      "step": 13010
    },
    {
      "epoch": 0.6944,
      "grad_norm": 0.43303489685058594,
      "learning_rate": 4.1320000000000004e-05,
      "loss": 0.0026,
      "step": 13020
    },
    {
      "epoch": 0.6949333333333333,
      "grad_norm": 0.24218687415122986,
      "learning_rate": 4.1313333333333336e-05,
      "loss": 0.0033,
      "step": 13030
    },
    {
      "epoch": 0.6954666666666667,
      "grad_norm": 0.649694561958313,
      "learning_rate": 4.130666666666667e-05,
      "loss": 0.0024,
      "step": 13040
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.6391398310661316,
      "learning_rate": 4.13e-05,
      "loss": 0.0025,
      "step": 13050
    },
    {
      "epoch": 0.6965333333333333,
      "grad_norm": 0.22323498129844666,
      "learning_rate": 4.129333333333333e-05,
      "loss": 0.003,
      "step": 13060
    },
    {
      "epoch": 0.6970666666666666,
      "grad_norm": 0.45931458473205566,
      "learning_rate": 4.1286666666666666e-05,
      "loss": 0.0029,
      "step": 13070
    },
    {
      "epoch": 0.6976,
      "grad_norm": 0.3032643795013428,
      "learning_rate": 4.1280000000000005e-05,
      "loss": 0.0031,
      "step": 13080
    },
    {
      "epoch": 0.6981333333333334,
      "grad_norm": 0.600501537322998,
      "learning_rate": 4.127333333333334e-05,
      "loss": 0.0037,
      "step": 13090
    },
    {
      "epoch": 0.6986666666666667,
      "grad_norm": 0.09529047459363937,
      "learning_rate": 4.126666666666667e-05,
      "loss": 0.0027,
      "step": 13100
    },
    {
      "epoch": 0.6992,
      "grad_norm": 0.20639215409755707,
      "learning_rate": 4.126e-05,
      "loss": 0.0024,
      "step": 13110
    },
    {
      "epoch": 0.6997333333333333,
      "grad_norm": 0.3890877962112427,
      "learning_rate": 4.1253333333333334e-05,
      "loss": 0.0026,
      "step": 13120
    },
    {
      "epoch": 0.7002666666666667,
      "grad_norm": 0.12929368019104004,
      "learning_rate": 4.1246666666666666e-05,
      "loss": 0.002,
      "step": 13130
    },
    {
      "epoch": 0.7008,
      "grad_norm": 0.4543119966983795,
      "learning_rate": 4.124e-05,
      "loss": 0.0027,
      "step": 13140
    },
    {
      "epoch": 0.7013333333333334,
      "grad_norm": 0.20808792114257812,
      "learning_rate": 4.123333333333334e-05,
      "loss": 0.0025,
      "step": 13150
    },
    {
      "epoch": 0.7018666666666666,
      "grad_norm": 0.15272381901741028,
      "learning_rate": 4.122666666666667e-05,
      "loss": 0.0022,
      "step": 13160
    },
    {
      "epoch": 0.7024,
      "grad_norm": 0.24515943229198456,
      "learning_rate": 4.122e-05,
      "loss": 0.0032,
      "step": 13170
    },
    {
      "epoch": 0.7029333333333333,
      "grad_norm": 0.25151464343070984,
      "learning_rate": 4.1213333333333334e-05,
      "loss": 0.0037,
      "step": 13180
    },
    {
      "epoch": 0.7034666666666667,
      "grad_norm": 0.3179783523082733,
      "learning_rate": 4.120666666666667e-05,
      "loss": 0.0025,
      "step": 13190
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.2654193043708801,
      "learning_rate": 4.12e-05,
      "loss": 0.0026,
      "step": 13200
    },
    {
      "epoch": 0.7045333333333333,
      "grad_norm": 0.40929874777793884,
      "learning_rate": 4.119333333333333e-05,
      "loss": 0.002,
      "step": 13210
    },
    {
      "epoch": 0.7050666666666666,
      "grad_norm": 0.11699990183115005,
      "learning_rate": 4.118666666666667e-05,
      "loss": 0.0025,
      "step": 13220
    },
    {
      "epoch": 0.7056,
      "grad_norm": 0.28409871459007263,
      "learning_rate": 4.118e-05,
      "loss": 0.0023,
      "step": 13230
    },
    {
      "epoch": 0.7061333333333333,
      "grad_norm": 0.2714323401451111,
      "learning_rate": 4.1173333333333334e-05,
      "loss": 0.0021,
      "step": 13240
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.2652681767940521,
      "learning_rate": 4.116666666666667e-05,
      "loss": 0.0023,
      "step": 13250
    },
    {
      "epoch": 0.7072,
      "grad_norm": 0.6193901300430298,
      "learning_rate": 4.1160000000000006e-05,
      "loss": 0.0022,
      "step": 13260
    },
    {
      "epoch": 0.7077333333333333,
      "grad_norm": 0.1560305804014206,
      "learning_rate": 4.115333333333333e-05,
      "loss": 0.0022,
      "step": 13270
    },
    {
      "epoch": 0.7082666666666667,
      "grad_norm": 0.4223983585834503,
      "learning_rate": 4.1146666666666663e-05,
      "loss": 0.0033,
      "step": 13280
    },
    {
      "epoch": 0.7088,
      "grad_norm": 0.5045851469039917,
      "learning_rate": 4.114e-05,
      "loss": 0.0034,
      "step": 13290
    },
    {
      "epoch": 0.7093333333333334,
      "grad_norm": 0.5373420119285583,
      "learning_rate": 4.1133333333333335e-05,
      "loss": 0.0022,
      "step": 13300
    },
    {
      "epoch": 0.7098666666666666,
      "grad_norm": 0.3612663447856903,
      "learning_rate": 4.112666666666667e-05,
      "loss": 0.0028,
      "step": 13310
    },
    {
      "epoch": 0.7104,
      "grad_norm": 0.19478335976600647,
      "learning_rate": 4.1120000000000006e-05,
      "loss": 0.0027,
      "step": 13320
    },
    {
      "epoch": 0.7109333333333333,
      "grad_norm": 0.3912971317768097,
      "learning_rate": 4.111333333333334e-05,
      "loss": 0.0025,
      "step": 13330
    },
    {
      "epoch": 0.7114666666666667,
      "grad_norm": 0.16764037311077118,
      "learning_rate": 4.110666666666667e-05,
      "loss": 0.0023,
      "step": 13340
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.3407818675041199,
      "learning_rate": 4.11e-05,
      "loss": 0.0032,
      "step": 13350
    },
    {
      "epoch": 0.7125333333333334,
      "grad_norm": 0.5785707831382751,
      "learning_rate": 4.1093333333333335e-05,
      "loss": 0.0034,
      "step": 13360
    },
    {
      "epoch": 0.7130666666666666,
      "grad_norm": 0.19146551191806793,
      "learning_rate": 4.108666666666667e-05,
      "loss": 0.0039,
      "step": 13370
    },
    {
      "epoch": 0.7136,
      "grad_norm": 0.4438075125217438,
      "learning_rate": 4.108e-05,
      "loss": 0.0035,
      "step": 13380
    },
    {
      "epoch": 0.7141333333333333,
      "grad_norm": 0.19698935747146606,
      "learning_rate": 4.107333333333334e-05,
      "loss": 0.0031,
      "step": 13390
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 0.2305193841457367,
      "learning_rate": 4.106666666666667e-05,
      "loss": 0.0022,
      "step": 13400
    },
    {
      "epoch": 0.7152,
      "grad_norm": 0.47786468267440796,
      "learning_rate": 4.106e-05,
      "loss": 0.0044,
      "step": 13410
    },
    {
      "epoch": 0.7157333333333333,
      "grad_norm": 0.08095551282167435,
      "learning_rate": 4.1053333333333336e-05,
      "loss": 0.0033,
      "step": 13420
    },
    {
      "epoch": 0.7162666666666667,
      "grad_norm": 0.41647815704345703,
      "learning_rate": 4.104666666666667e-05,
      "loss": 0.0031,
      "step": 13430
    },
    {
      "epoch": 0.7168,
      "grad_norm": 0.7538703680038452,
      "learning_rate": 4.104e-05,
      "loss": 0.0025,
      "step": 13440
    },
    {
      "epoch": 0.7173333333333334,
      "grad_norm": 0.4402441382408142,
      "learning_rate": 4.103333333333333e-05,
      "loss": 0.0035,
      "step": 13450
    },
    {
      "epoch": 0.7178666666666667,
      "grad_norm": 0.1304023712873459,
      "learning_rate": 4.102666666666667e-05,
      "loss": 0.0021,
      "step": 13460
    },
    {
      "epoch": 0.7184,
      "grad_norm": 0.2648065388202667,
      "learning_rate": 4.1020000000000004e-05,
      "loss": 0.0021,
      "step": 13470
    },
    {
      "epoch": 0.7189333333333333,
      "grad_norm": 0.38114944100379944,
      "learning_rate": 4.1013333333333336e-05,
      "loss": 0.0034,
      "step": 13480
    },
    {
      "epoch": 0.7194666666666667,
      "grad_norm": 0.829611599445343,
      "learning_rate": 4.100666666666667e-05,
      "loss": 0.003,
      "step": 13490
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.13716387748718262,
      "learning_rate": 4.1e-05,
      "loss": 0.0022,
      "step": 13500
    },
    {
      "epoch": 0.7205333333333334,
      "grad_norm": 0.2532762289047241,
      "learning_rate": 4.099333333333333e-05,
      "loss": 0.0023,
      "step": 13510
    },
    {
      "epoch": 0.7210666666666666,
      "grad_norm": 0.4461086094379425,
      "learning_rate": 4.0986666666666665e-05,
      "loss": 0.0047,
      "step": 13520
    },
    {
      "epoch": 0.7216,
      "grad_norm": 0.19089818000793457,
      "learning_rate": 4.0980000000000004e-05,
      "loss": 0.0021,
      "step": 13530
    },
    {
      "epoch": 0.7221333333333333,
      "grad_norm": 0.22753983736038208,
      "learning_rate": 4.0973333333333336e-05,
      "loss": 0.0023,
      "step": 13540
    },
    {
      "epoch": 0.7226666666666667,
      "grad_norm": 0.24380694329738617,
      "learning_rate": 4.096666666666667e-05,
      "loss": 0.0025,
      "step": 13550
    },
    {
      "epoch": 0.7232,
      "grad_norm": 0.18065360188484192,
      "learning_rate": 4.096e-05,
      "loss": 0.0023,
      "step": 13560
    },
    {
      "epoch": 0.7237333333333333,
      "grad_norm": 0.2756598889827728,
      "learning_rate": 4.095333333333334e-05,
      "loss": 0.0029,
      "step": 13570
    },
    {
      "epoch": 0.7242666666666666,
      "grad_norm": 0.7243565320968628,
      "learning_rate": 4.0946666666666665e-05,
      "loss": 0.0023,
      "step": 13580
    },
    {
      "epoch": 0.7248,
      "grad_norm": 0.19309185445308685,
      "learning_rate": 4.094e-05,
      "loss": 0.0043,
      "step": 13590
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 0.13537605106830597,
      "learning_rate": 4.093333333333334e-05,
      "loss": 0.0019,
      "step": 13600
    },
    {
      "epoch": 0.7258666666666667,
      "grad_norm": 0.09164933115243912,
      "learning_rate": 4.092666666666667e-05,
      "loss": 0.0038,
      "step": 13610
    },
    {
      "epoch": 0.7264,
      "grad_norm": 0.18106265366077423,
      "learning_rate": 4.092e-05,
      "loss": 0.0031,
      "step": 13620
    },
    {
      "epoch": 0.7269333333333333,
      "grad_norm": 0.5588526725769043,
      "learning_rate": 4.0913333333333334e-05,
      "loss": 0.002,
      "step": 13630
    },
    {
      "epoch": 0.7274666666666667,
      "grad_norm": 0.8872185945510864,
      "learning_rate": 4.090666666666667e-05,
      "loss": 0.0022,
      "step": 13640
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.10875701159238815,
      "learning_rate": 4.09e-05,
      "loss": 0.0027,
      "step": 13650
    },
    {
      "epoch": 0.7285333333333334,
      "grad_norm": 0.7221812605857849,
      "learning_rate": 4.089333333333333e-05,
      "loss": 0.0022,
      "step": 13660
    },
    {
      "epoch": 0.7290666666666666,
      "grad_norm": 0.4318941533565521,
      "learning_rate": 4.088666666666667e-05,
      "loss": 0.0026,
      "step": 13670
    },
    {
      "epoch": 0.7296,
      "grad_norm": 0.27879416942596436,
      "learning_rate": 4.088e-05,
      "loss": 0.0044,
      "step": 13680
    },
    {
      "epoch": 0.7301333333333333,
      "grad_norm": 0.7727189064025879,
      "learning_rate": 4.0873333333333334e-05,
      "loss": 0.0027,
      "step": 13690
    },
    {
      "epoch": 0.7306666666666667,
      "grad_norm": 0.608242392539978,
      "learning_rate": 4.086666666666667e-05,
      "loss": 0.0025,
      "step": 13700
    },
    {
      "epoch": 0.7312,
      "grad_norm": 0.32516512274742126,
      "learning_rate": 4.0860000000000005e-05,
      "loss": 0.0025,
      "step": 13710
    },
    {
      "epoch": 0.7317333333333333,
      "grad_norm": 0.25370004773139954,
      "learning_rate": 4.085333333333334e-05,
      "loss": 0.0025,
      "step": 13720
    },
    {
      "epoch": 0.7322666666666666,
      "grad_norm": 0.34200459718704224,
      "learning_rate": 4.084666666666667e-05,
      "loss": 0.0019,
      "step": 13730
    },
    {
      "epoch": 0.7328,
      "grad_norm": 1.0914535522460938,
      "learning_rate": 4.084e-05,
      "loss": 0.0026,
      "step": 13740
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.7220322489738464,
      "learning_rate": 4.0833333333333334e-05,
      "loss": 0.0029,
      "step": 13750
    },
    {
      "epoch": 0.7338666666666667,
      "grad_norm": 0.5482116937637329,
      "learning_rate": 4.0826666666666667e-05,
      "loss": 0.0029,
      "step": 13760
    },
    {
      "epoch": 0.7344,
      "grad_norm": 0.33435025811195374,
      "learning_rate": 4.0820000000000006e-05,
      "loss": 0.0029,
      "step": 13770
    },
    {
      "epoch": 0.7349333333333333,
      "grad_norm": 0.690499484539032,
      "learning_rate": 4.081333333333334e-05,
      "loss": 0.0022,
      "step": 13780
    },
    {
      "epoch": 0.7354666666666667,
      "grad_norm": 0.42284446954727173,
      "learning_rate": 4.080666666666667e-05,
      "loss": 0.003,
      "step": 13790
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.41468796133995056,
      "learning_rate": 4.08e-05,
      "loss": 0.0023,
      "step": 13800
    },
    {
      "epoch": 0.7365333333333334,
      "grad_norm": 0.17035377025604248,
      "learning_rate": 4.0793333333333335e-05,
      "loss": 0.0022,
      "step": 13810
    },
    {
      "epoch": 0.7370666666666666,
      "grad_norm": 0.5096272826194763,
      "learning_rate": 4.078666666666667e-05,
      "loss": 0.0033,
      "step": 13820
    },
    {
      "epoch": 0.7376,
      "grad_norm": 0.3824639618396759,
      "learning_rate": 4.078e-05,
      "loss": 0.0025,
      "step": 13830
    },
    {
      "epoch": 0.7381333333333333,
      "grad_norm": 0.6789349317550659,
      "learning_rate": 4.077333333333334e-05,
      "loss": 0.0031,
      "step": 13840
    },
    {
      "epoch": 0.7386666666666667,
      "grad_norm": 1.0365991592407227,
      "learning_rate": 4.076666666666667e-05,
      "loss": 0.0028,
      "step": 13850
    },
    {
      "epoch": 0.7392,
      "grad_norm": 0.41405728459358215,
      "learning_rate": 4.076e-05,
      "loss": 0.0034,
      "step": 13860
    },
    {
      "epoch": 0.7397333333333334,
      "grad_norm": 0.42137521505355835,
      "learning_rate": 4.0753333333333335e-05,
      "loss": 0.0023,
      "step": 13870
    },
    {
      "epoch": 0.7402666666666666,
      "grad_norm": 0.22959676384925842,
      "learning_rate": 4.074666666666667e-05,
      "loss": 0.0024,
      "step": 13880
    },
    {
      "epoch": 0.7408,
      "grad_norm": 0.32473695278167725,
      "learning_rate": 4.074e-05,
      "loss": 0.0019,
      "step": 13890
    },
    {
      "epoch": 0.7413333333333333,
      "grad_norm": 0.37052837014198303,
      "learning_rate": 4.073333333333333e-05,
      "loss": 0.0041,
      "step": 13900
    },
    {
      "epoch": 0.7418666666666667,
      "grad_norm": 0.3009316623210907,
      "learning_rate": 4.072666666666667e-05,
      "loss": 0.0028,
      "step": 13910
    },
    {
      "epoch": 0.7424,
      "grad_norm": 0.6277843713760376,
      "learning_rate": 4.072e-05,
      "loss": 0.003,
      "step": 13920
    },
    {
      "epoch": 0.7429333333333333,
      "grad_norm": 0.18087796866893768,
      "learning_rate": 4.0713333333333335e-05,
      "loss": 0.003,
      "step": 13930
    },
    {
      "epoch": 0.7434666666666667,
      "grad_norm": 0.15816400945186615,
      "learning_rate": 4.070666666666667e-05,
      "loss": 0.0023,
      "step": 13940
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.14736776053905487,
      "learning_rate": 4.07e-05,
      "loss": 0.0023,
      "step": 13950
    },
    {
      "epoch": 0.7445333333333334,
      "grad_norm": 0.64242023229599,
      "learning_rate": 4.069333333333333e-05,
      "loss": 0.0029,
      "step": 13960
    },
    {
      "epoch": 0.7450666666666667,
      "grad_norm": 0.4454585909843445,
      "learning_rate": 4.0686666666666664e-05,
      "loss": 0.0026,
      "step": 13970
    },
    {
      "epoch": 0.7456,
      "grad_norm": 0.34041300415992737,
      "learning_rate": 4.0680000000000004e-05,
      "loss": 0.0044,
      "step": 13980
    },
    {
      "epoch": 0.7461333333333333,
      "grad_norm": 0.9606331586837769,
      "learning_rate": 4.0673333333333336e-05,
      "loss": 0.0033,
      "step": 13990
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.3360706865787506,
      "learning_rate": 4.066666666666667e-05,
      "loss": 0.0043,
      "step": 14000
    },
    {
      "epoch": 0.7472,
      "grad_norm": 0.4660550355911255,
      "learning_rate": 4.066e-05,
      "loss": 0.0034,
      "step": 14010
    },
    {
      "epoch": 0.7477333333333334,
      "grad_norm": 0.4058111011981964,
      "learning_rate": 4.065333333333334e-05,
      "loss": 0.003,
      "step": 14020
    },
    {
      "epoch": 0.7482666666666666,
      "grad_norm": 0.15025915205478668,
      "learning_rate": 4.0646666666666665e-05,
      "loss": 0.0028,
      "step": 14030
    },
    {
      "epoch": 0.7488,
      "grad_norm": 0.15150819718837738,
      "learning_rate": 4.064e-05,
      "loss": 0.0026,
      "step": 14040
    },
    {
      "epoch": 0.7493333333333333,
      "grad_norm": 0.2162710428237915,
      "learning_rate": 4.0633333333333336e-05,
      "loss": 0.0026,
      "step": 14050
    },
    {
      "epoch": 0.7498666666666667,
      "grad_norm": 0.7122101187705994,
      "learning_rate": 4.062666666666667e-05,
      "loss": 0.0035,
      "step": 14060
    },
    {
      "epoch": 0.7504,
      "grad_norm": 0.4446792006492615,
      "learning_rate": 4.062e-05,
      "loss": 0.0022,
      "step": 14070
    },
    {
      "epoch": 0.7509333333333333,
      "grad_norm": 0.17359036207199097,
      "learning_rate": 4.061333333333334e-05,
      "loss": 0.0021,
      "step": 14080
    },
    {
      "epoch": 0.7514666666666666,
      "grad_norm": 0.49222955107688904,
      "learning_rate": 4.060666666666667e-05,
      "loss": 0.0029,
      "step": 14090
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.37260597944259644,
      "learning_rate": 4.0600000000000004e-05,
      "loss": 0.0033,
      "step": 14100
    },
    {
      "epoch": 0.7525333333333334,
      "grad_norm": 0.3393617272377014,
      "learning_rate": 4.0593333333333337e-05,
      "loss": 0.0041,
      "step": 14110
    },
    {
      "epoch": 0.7530666666666667,
      "grad_norm": 0.44050994515419006,
      "learning_rate": 4.058666666666667e-05,
      "loss": 0.0033,
      "step": 14120
    },
    {
      "epoch": 0.7536,
      "grad_norm": 0.15099012851715088,
      "learning_rate": 4.058e-05,
      "loss": 0.0033,
      "step": 14130
    },
    {
      "epoch": 0.7541333333333333,
      "grad_norm": 0.2583458125591278,
      "learning_rate": 4.057333333333333e-05,
      "loss": 0.0022,
      "step": 14140
    },
    {
      "epoch": 0.7546666666666667,
      "grad_norm": 0.2250448614358902,
      "learning_rate": 4.056666666666667e-05,
      "loss": 0.0033,
      "step": 14150
    },
    {
      "epoch": 0.7552,
      "grad_norm": 0.5706716179847717,
      "learning_rate": 4.0560000000000005e-05,
      "loss": 0.0025,
      "step": 14160
    },
    {
      "epoch": 0.7557333333333334,
      "grad_norm": 0.4908170700073242,
      "learning_rate": 4.055333333333334e-05,
      "loss": 0.0031,
      "step": 14170
    },
    {
      "epoch": 0.7562666666666666,
      "grad_norm": 0.7867682576179504,
      "learning_rate": 4.054666666666667e-05,
      "loss": 0.0023,
      "step": 14180
    },
    {
      "epoch": 0.7568,
      "grad_norm": 0.7180996537208557,
      "learning_rate": 4.054e-05,
      "loss": 0.0022,
      "step": 14190
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 0.37350890040397644,
      "learning_rate": 4.0533333333333334e-05,
      "loss": 0.0044,
      "step": 14200
    },
    {
      "epoch": 0.7578666666666667,
      "grad_norm": 0.44019123911857605,
      "learning_rate": 4.0526666666666666e-05,
      "loss": 0.0032,
      "step": 14210
    },
    {
      "epoch": 0.7584,
      "grad_norm": 0.3399287760257721,
      "learning_rate": 4.0520000000000005e-05,
      "loss": 0.002,
      "step": 14220
    },
    {
      "epoch": 0.7589333333333333,
      "grad_norm": 0.512016236782074,
      "learning_rate": 4.051333333333334e-05,
      "loss": 0.0022,
      "step": 14230
    },
    {
      "epoch": 0.7594666666666666,
      "grad_norm": 0.5219014286994934,
      "learning_rate": 4.050666666666667e-05,
      "loss": 0.0025,
      "step": 14240
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.345837265253067,
      "learning_rate": 4.05e-05,
      "loss": 0.0038,
      "step": 14250
    },
    {
      "epoch": 0.7605333333333333,
      "grad_norm": 0.4657328128814697,
      "learning_rate": 4.0493333333333334e-05,
      "loss": 0.0028,
      "step": 14260
    },
    {
      "epoch": 0.7610666666666667,
      "grad_norm": 0.18693166971206665,
      "learning_rate": 4.0486666666666666e-05,
      "loss": 0.0022,
      "step": 14270
    },
    {
      "epoch": 0.7616,
      "grad_norm": 0.127003014087677,
      "learning_rate": 4.048e-05,
      "loss": 0.0022,
      "step": 14280
    },
    {
      "epoch": 0.7621333333333333,
      "grad_norm": 0.5714036822319031,
      "learning_rate": 4.047333333333334e-05,
      "loss": 0.002,
      "step": 14290
    },
    {
      "epoch": 0.7626666666666667,
      "grad_norm": 0.12136179208755493,
      "learning_rate": 4.046666666666667e-05,
      "loss": 0.0036,
      "step": 14300
    },
    {
      "epoch": 0.7632,
      "grad_norm": 0.37110406160354614,
      "learning_rate": 4.046e-05,
      "loss": 0.0021,
      "step": 14310
    },
    {
      "epoch": 0.7637333333333334,
      "grad_norm": 0.4649471640586853,
      "learning_rate": 4.0453333333333335e-05,
      "loss": 0.003,
      "step": 14320
    },
    {
      "epoch": 0.7642666666666666,
      "grad_norm": 0.5110833644866943,
      "learning_rate": 4.044666666666667e-05,
      "loss": 0.002,
      "step": 14330
    },
    {
      "epoch": 0.7648,
      "grad_norm": 0.1844380795955658,
      "learning_rate": 4.044e-05,
      "loss": 0.0034,
      "step": 14340
    },
    {
      "epoch": 0.7653333333333333,
      "grad_norm": 0.5203414559364319,
      "learning_rate": 4.043333333333333e-05,
      "loss": 0.0029,
      "step": 14350
    },
    {
      "epoch": 0.7658666666666667,
      "grad_norm": 0.30123576521873474,
      "learning_rate": 4.042666666666667e-05,
      "loss": 0.0032,
      "step": 14360
    },
    {
      "epoch": 0.7664,
      "grad_norm": 0.4838128983974457,
      "learning_rate": 4.042e-05,
      "loss": 0.0035,
      "step": 14370
    },
    {
      "epoch": 0.7669333333333334,
      "grad_norm": 0.6099528670310974,
      "learning_rate": 4.0413333333333335e-05,
      "loss": 0.0026,
      "step": 14380
    },
    {
      "epoch": 0.7674666666666666,
      "grad_norm": 0.5741007924079895,
      "learning_rate": 4.040666666666667e-05,
      "loss": 0.0025,
      "step": 14390
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.3386825621128082,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 0.0022,
      "step": 14400
    },
    {
      "epoch": 0.7685333333333333,
      "grad_norm": 0.18558862805366516,
      "learning_rate": 4.039333333333333e-05,
      "loss": 0.0028,
      "step": 14410
    },
    {
      "epoch": 0.7690666666666667,
      "grad_norm": 0.20456984639167786,
      "learning_rate": 4.0386666666666664e-05,
      "loss": 0.0027,
      "step": 14420
    },
    {
      "epoch": 0.7696,
      "grad_norm": 0.8178259134292603,
      "learning_rate": 4.038e-05,
      "loss": 0.0039,
      "step": 14430
    },
    {
      "epoch": 0.7701333333333333,
      "grad_norm": 0.26315560936927795,
      "learning_rate": 4.0373333333333335e-05,
      "loss": 0.002,
      "step": 14440
    },
    {
      "epoch": 0.7706666666666667,
      "grad_norm": 0.9204275608062744,
      "learning_rate": 4.036666666666667e-05,
      "loss": 0.0026,
      "step": 14450
    },
    {
      "epoch": 0.7712,
      "grad_norm": 0.41794925928115845,
      "learning_rate": 4.0360000000000007e-05,
      "loss": 0.003,
      "step": 14460
    },
    {
      "epoch": 0.7717333333333334,
      "grad_norm": 0.29993751645088196,
      "learning_rate": 4.035333333333334e-05,
      "loss": 0.003,
      "step": 14470
    },
    {
      "epoch": 0.7722666666666667,
      "grad_norm": 0.1993144452571869,
      "learning_rate": 4.0346666666666664e-05,
      "loss": 0.0017,
      "step": 14480
    },
    {
      "epoch": 0.7728,
      "grad_norm": 0.26386067271232605,
      "learning_rate": 4.034e-05,
      "loss": 0.0026,
      "step": 14490
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.3375707268714905,
      "learning_rate": 4.0333333333333336e-05,
      "loss": 0.0028,
      "step": 14500
    },
    {
      "epoch": 0.7738666666666667,
      "grad_norm": 0.7896136045455933,
      "learning_rate": 4.032666666666667e-05,
      "loss": 0.0032,
      "step": 14510
    },
    {
      "epoch": 0.7744,
      "grad_norm": 0.7547271251678467,
      "learning_rate": 4.032e-05,
      "loss": 0.0024,
      "step": 14520
    },
    {
      "epoch": 0.7749333333333334,
      "grad_norm": 0.2676462233066559,
      "learning_rate": 4.031333333333334e-05,
      "loss": 0.0034,
      "step": 14530
    },
    {
      "epoch": 0.7754666666666666,
      "grad_norm": 0.1624930500984192,
      "learning_rate": 4.030666666666667e-05,
      "loss": 0.0029,
      "step": 14540
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.44603225588798523,
      "learning_rate": 4.0300000000000004e-05,
      "loss": 0.0022,
      "step": 14550
    },
    {
      "epoch": 0.7765333333333333,
      "grad_norm": 0.21706873178482056,
      "learning_rate": 4.0293333333333336e-05,
      "loss": 0.0032,
      "step": 14560
    },
    {
      "epoch": 0.7770666666666667,
      "grad_norm": 0.9416019320487976,
      "learning_rate": 4.028666666666667e-05,
      "loss": 0.0032,
      "step": 14570
    },
    {
      "epoch": 0.7776,
      "grad_norm": 0.3819001019001007,
      "learning_rate": 4.028e-05,
      "loss": 0.0023,
      "step": 14580
    },
    {
      "epoch": 0.7781333333333333,
      "grad_norm": 0.4119550287723541,
      "learning_rate": 4.027333333333333e-05,
      "loss": 0.004,
      "step": 14590
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 0.3211004436016083,
      "learning_rate": 4.026666666666667e-05,
      "loss": 0.0024,
      "step": 14600
    },
    {
      "epoch": 0.7792,
      "grad_norm": 0.270046591758728,
      "learning_rate": 4.0260000000000004e-05,
      "loss": 0.0025,
      "step": 14610
    },
    {
      "epoch": 0.7797333333333333,
      "grad_norm": 0.3055056631565094,
      "learning_rate": 4.0253333333333336e-05,
      "loss": 0.0025,
      "step": 14620
    },
    {
      "epoch": 0.7802666666666667,
      "grad_norm": 0.4721466600894928,
      "learning_rate": 4.024666666666667e-05,
      "loss": 0.0051,
      "step": 14630
    },
    {
      "epoch": 0.7808,
      "grad_norm": 0.3383811414241791,
      "learning_rate": 4.024e-05,
      "loss": 0.0023,
      "step": 14640
    },
    {
      "epoch": 0.7813333333333333,
      "grad_norm": 0.47435736656188965,
      "learning_rate": 4.023333333333333e-05,
      "loss": 0.0021,
      "step": 14650
    },
    {
      "epoch": 0.7818666666666667,
      "grad_norm": 0.37539348006248474,
      "learning_rate": 4.0226666666666666e-05,
      "loss": 0.0023,
      "step": 14660
    },
    {
      "epoch": 0.7824,
      "grad_norm": 0.14667288959026337,
      "learning_rate": 4.0220000000000005e-05,
      "loss": 0.0019,
      "step": 14670
    },
    {
      "epoch": 0.7829333333333334,
      "grad_norm": 0.17330977320671082,
      "learning_rate": 4.021333333333334e-05,
      "loss": 0.0037,
      "step": 14680
    },
    {
      "epoch": 0.7834666666666666,
      "grad_norm": 0.7060654759407043,
      "learning_rate": 4.020666666666667e-05,
      "loss": 0.0028,
      "step": 14690
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.7833850383758545,
      "learning_rate": 4.02e-05,
      "loss": 0.002,
      "step": 14700
    },
    {
      "epoch": 0.7845333333333333,
      "grad_norm": 0.2917207181453705,
      "learning_rate": 4.0193333333333334e-05,
      "loss": 0.0022,
      "step": 14710
    },
    {
      "epoch": 0.7850666666666667,
      "grad_norm": 0.4092027544975281,
      "learning_rate": 4.0186666666666666e-05,
      "loss": 0.0032,
      "step": 14720
    },
    {
      "epoch": 0.7856,
      "grad_norm": 0.5845651626586914,
      "learning_rate": 4.018e-05,
      "loss": 0.0023,
      "step": 14730
    },
    {
      "epoch": 0.7861333333333334,
      "grad_norm": 0.26601651310920715,
      "learning_rate": 4.017333333333334e-05,
      "loss": 0.0028,
      "step": 14740
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.3046794831752777,
      "learning_rate": 4.016666666666667e-05,
      "loss": 0.0032,
      "step": 14750
    },
    {
      "epoch": 0.7872,
      "grad_norm": 0.15078474581241608,
      "learning_rate": 4.016e-05,
      "loss": 0.0039,
      "step": 14760
    },
    {
      "epoch": 0.7877333333333333,
      "grad_norm": 0.4366946816444397,
      "learning_rate": 4.0153333333333334e-05,
      "loss": 0.0031,
      "step": 14770
    },
    {
      "epoch": 0.7882666666666667,
      "grad_norm": 0.392690509557724,
      "learning_rate": 4.014666666666667e-05,
      "loss": 0.0037,
      "step": 14780
    },
    {
      "epoch": 0.7888,
      "grad_norm": 0.6530476212501526,
      "learning_rate": 4.014e-05,
      "loss": 0.003,
      "step": 14790
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 0.5001906752586365,
      "learning_rate": 4.013333333333333e-05,
      "loss": 0.0024,
      "step": 14800
    },
    {
      "epoch": 0.7898666666666667,
      "grad_norm": 0.6616489887237549,
      "learning_rate": 4.012666666666667e-05,
      "loss": 0.0023,
      "step": 14810
    },
    {
      "epoch": 0.7904,
      "grad_norm": 0.1194537803530693,
      "learning_rate": 4.012e-05,
      "loss": 0.0034,
      "step": 14820
    },
    {
      "epoch": 0.7909333333333334,
      "grad_norm": 0.4401479661464691,
      "learning_rate": 4.0113333333333334e-05,
      "loss": 0.0039,
      "step": 14830
    },
    {
      "epoch": 0.7914666666666667,
      "grad_norm": 0.5896918177604675,
      "learning_rate": 4.0106666666666673e-05,
      "loss": 0.0038,
      "step": 14840
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.39562827348709106,
      "learning_rate": 4.0100000000000006e-05,
      "loss": 0.0027,
      "step": 14850
    },
    {
      "epoch": 0.7925333333333333,
      "grad_norm": 0.1390983760356903,
      "learning_rate": 4.009333333333333e-05,
      "loss": 0.0027,
      "step": 14860
    },
    {
      "epoch": 0.7930666666666667,
      "grad_norm": 0.2231973558664322,
      "learning_rate": 4.0086666666666663e-05,
      "loss": 0.0029,
      "step": 14870
    },
    {
      "epoch": 0.7936,
      "grad_norm": 0.4224536120891571,
      "learning_rate": 4.008e-05,
      "loss": 0.0026,
      "step": 14880
    },
    {
      "epoch": 0.7941333333333334,
      "grad_norm": 0.4832330644130707,
      "learning_rate": 4.0073333333333335e-05,
      "loss": 0.0032,
      "step": 14890
    },
    {
      "epoch": 0.7946666666666666,
      "grad_norm": 0.2352098971605301,
      "learning_rate": 4.006666666666667e-05,
      "loss": 0.0034,
      "step": 14900
    },
    {
      "epoch": 0.7952,
      "grad_norm": 0.09474608302116394,
      "learning_rate": 4.0060000000000006e-05,
      "loss": 0.0034,
      "step": 14910
    },
    {
      "epoch": 0.7957333333333333,
      "grad_norm": 0.11814375221729279,
      "learning_rate": 4.005333333333334e-05,
      "loss": 0.003,
      "step": 14920
    },
    {
      "epoch": 0.7962666666666667,
      "grad_norm": 0.5040594339370728,
      "learning_rate": 4.004666666666667e-05,
      "loss": 0.0029,
      "step": 14930
    },
    {
      "epoch": 0.7968,
      "grad_norm": 0.6379884481430054,
      "learning_rate": 4.004e-05,
      "loss": 0.0032,
      "step": 14940
    },
    {
      "epoch": 0.7973333333333333,
      "grad_norm": 1.0130757093429565,
      "learning_rate": 4.0033333333333335e-05,
      "loss": 0.0034,
      "step": 14950
    },
    {
      "epoch": 0.7978666666666666,
      "grad_norm": 0.21956436336040497,
      "learning_rate": 4.002666666666667e-05,
      "loss": 0.003,
      "step": 14960
    },
    {
      "epoch": 0.7984,
      "grad_norm": 0.6791253685951233,
      "learning_rate": 4.002e-05,
      "loss": 0.0026,
      "step": 14970
    },
    {
      "epoch": 0.7989333333333334,
      "grad_norm": 0.5051255226135254,
      "learning_rate": 4.001333333333334e-05,
      "loss": 0.0028,
      "step": 14980
    },
    {
      "epoch": 0.7994666666666667,
      "grad_norm": 0.5658956170082092,
      "learning_rate": 4.000666666666667e-05,
      "loss": 0.0028,
      "step": 14990
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.43718114495277405,
      "learning_rate": 4e-05,
      "loss": 0.0031,
      "step": 15000
    },
    {
      "epoch": 0.8005333333333333,
      "grad_norm": 0.1666978895664215,
      "learning_rate": 3.9993333333333336e-05,
      "loss": 0.0031,
      "step": 15010
    },
    {
      "epoch": 0.8010666666666667,
      "grad_norm": 0.4031083881855011,
      "learning_rate": 3.998666666666667e-05,
      "loss": 0.0022,
      "step": 15020
    },
    {
      "epoch": 0.8016,
      "grad_norm": 0.6667162179946899,
      "learning_rate": 3.998e-05,
      "loss": 0.0026,
      "step": 15030
    },
    {
      "epoch": 0.8021333333333334,
      "grad_norm": 0.42790234088897705,
      "learning_rate": 3.997333333333333e-05,
      "loss": 0.0034,
      "step": 15040
    },
    {
      "epoch": 0.8026666666666666,
      "grad_norm": 0.6543182730674744,
      "learning_rate": 3.996666666666667e-05,
      "loss": 0.0041,
      "step": 15050
    },
    {
      "epoch": 0.8032,
      "grad_norm": 0.4143296182155609,
      "learning_rate": 3.9960000000000004e-05,
      "loss": 0.0025,
      "step": 15060
    },
    {
      "epoch": 0.8037333333333333,
      "grad_norm": 0.46689993143081665,
      "learning_rate": 3.9953333333333336e-05,
      "loss": 0.0028,
      "step": 15070
    },
    {
      "epoch": 0.8042666666666667,
      "grad_norm": 0.24627935886383057,
      "learning_rate": 3.994666666666667e-05,
      "loss": 0.0021,
      "step": 15080
    },
    {
      "epoch": 0.8048,
      "grad_norm": 0.6897792816162109,
      "learning_rate": 3.994e-05,
      "loss": 0.0028,
      "step": 15090
    },
    {
      "epoch": 0.8053333333333333,
      "grad_norm": 0.30124321579933167,
      "learning_rate": 3.993333333333333e-05,
      "loss": 0.003,
      "step": 15100
    },
    {
      "epoch": 0.8058666666666666,
      "grad_norm": 0.21164517104625702,
      "learning_rate": 3.9926666666666665e-05,
      "loss": 0.0045,
      "step": 15110
    },
    {
      "epoch": 0.8064,
      "grad_norm": 0.5159816741943359,
      "learning_rate": 3.9920000000000004e-05,
      "loss": 0.0029,
      "step": 15120
    },
    {
      "epoch": 0.8069333333333333,
      "grad_norm": 0.2823043167591095,
      "learning_rate": 3.9913333333333336e-05,
      "loss": 0.0026,
      "step": 15130
    },
    {
      "epoch": 0.8074666666666667,
      "grad_norm": 0.1988990157842636,
      "learning_rate": 3.990666666666667e-05,
      "loss": 0.0036,
      "step": 15140
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.23928876221179962,
      "learning_rate": 3.99e-05,
      "loss": 0.0023,
      "step": 15150
    },
    {
      "epoch": 0.8085333333333333,
      "grad_norm": 0.6597429513931274,
      "learning_rate": 3.989333333333333e-05,
      "loss": 0.0024,
      "step": 15160
    },
    {
      "epoch": 0.8090666666666667,
      "grad_norm": 0.13634876906871796,
      "learning_rate": 3.9886666666666665e-05,
      "loss": 0.0029,
      "step": 15170
    },
    {
      "epoch": 0.8096,
      "grad_norm": 0.2887935936450958,
      "learning_rate": 3.988e-05,
      "loss": 0.0024,
      "step": 15180
    },
    {
      "epoch": 0.8101333333333334,
      "grad_norm": 0.2601405382156372,
      "learning_rate": 3.987333333333334e-05,
      "loss": 0.003,
      "step": 15190
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 0.19279611110687256,
      "learning_rate": 3.986666666666667e-05,
      "loss": 0.0028,
      "step": 15200
    },
    {
      "epoch": 0.8112,
      "grad_norm": 0.44306525588035583,
      "learning_rate": 3.986e-05,
      "loss": 0.0022,
      "step": 15210
    },
    {
      "epoch": 0.8117333333333333,
      "grad_norm": 0.2740240693092346,
      "learning_rate": 3.985333333333334e-05,
      "loss": 0.0033,
      "step": 15220
    },
    {
      "epoch": 0.8122666666666667,
      "grad_norm": 0.37203946709632874,
      "learning_rate": 3.984666666666667e-05,
      "loss": 0.0029,
      "step": 15230
    },
    {
      "epoch": 0.8128,
      "grad_norm": 0.9784919619560242,
      "learning_rate": 3.984e-05,
      "loss": 0.0037,
      "step": 15240
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.3460286855697632,
      "learning_rate": 3.983333333333333e-05,
      "loss": 0.0035,
      "step": 15250
    },
    {
      "epoch": 0.8138666666666666,
      "grad_norm": 0.7012962698936462,
      "learning_rate": 3.982666666666667e-05,
      "loss": 0.0032,
      "step": 15260
    },
    {
      "epoch": 0.8144,
      "grad_norm": 0.7319638133049011,
      "learning_rate": 3.982e-05,
      "loss": 0.0033,
      "step": 15270
    },
    {
      "epoch": 0.8149333333333333,
      "grad_norm": 0.6242443323135376,
      "learning_rate": 3.9813333333333334e-05,
      "loss": 0.0023,
      "step": 15280
    },
    {
      "epoch": 0.8154666666666667,
      "grad_norm": 0.3895014822483063,
      "learning_rate": 3.980666666666667e-05,
      "loss": 0.0022,
      "step": 15290
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.14202100038528442,
      "learning_rate": 3.9800000000000005e-05,
      "loss": 0.003,
      "step": 15300
    },
    {
      "epoch": 0.8165333333333333,
      "grad_norm": 0.40681377053260803,
      "learning_rate": 3.979333333333333e-05,
      "loss": 0.0026,
      "step": 15310
    },
    {
      "epoch": 0.8170666666666667,
      "grad_norm": 0.7133480906486511,
      "learning_rate": 3.978666666666667e-05,
      "loss": 0.0027,
      "step": 15320
    },
    {
      "epoch": 0.8176,
      "grad_norm": 0.08245941251516342,
      "learning_rate": 3.978e-05,
      "loss": 0.0029,
      "step": 15330
    },
    {
      "epoch": 0.8181333333333334,
      "grad_norm": 0.12722045183181763,
      "learning_rate": 3.9773333333333334e-05,
      "loss": 0.0036,
      "step": 15340
    },
    {
      "epoch": 0.8186666666666667,
      "grad_norm": 0.16293174028396606,
      "learning_rate": 3.9766666666666667e-05,
      "loss": 0.0029,
      "step": 15350
    },
    {
      "epoch": 0.8192,
      "grad_norm": 1.0064460039138794,
      "learning_rate": 3.9760000000000006e-05,
      "loss": 0.0033,
      "step": 15360
    },
    {
      "epoch": 0.8197333333333333,
      "grad_norm": 0.28066831827163696,
      "learning_rate": 3.975333333333334e-05,
      "loss": 0.0026,
      "step": 15370
    },
    {
      "epoch": 0.8202666666666667,
      "grad_norm": 0.13891102373600006,
      "learning_rate": 3.974666666666667e-05,
      "loss": 0.0023,
      "step": 15380
    },
    {
      "epoch": 0.8208,
      "grad_norm": 0.12303409725427628,
      "learning_rate": 3.974e-05,
      "loss": 0.0027,
      "step": 15390
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 0.20109911262989044,
      "learning_rate": 3.9733333333333335e-05,
      "loss": 0.0032,
      "step": 15400
    },
    {
      "epoch": 0.8218666666666666,
      "grad_norm": 0.9318114519119263,
      "learning_rate": 3.972666666666667e-05,
      "loss": 0.0032,
      "step": 15410
    },
    {
      "epoch": 0.8224,
      "grad_norm": 0.2515805959701538,
      "learning_rate": 3.972e-05,
      "loss": 0.0022,
      "step": 15420
    },
    {
      "epoch": 0.8229333333333333,
      "grad_norm": 0.16894738376140594,
      "learning_rate": 3.971333333333334e-05,
      "loss": 0.0022,
      "step": 15430
    },
    {
      "epoch": 0.8234666666666667,
      "grad_norm": 0.32677266001701355,
      "learning_rate": 3.970666666666667e-05,
      "loss": 0.0035,
      "step": 15440
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.17231228947639465,
      "learning_rate": 3.97e-05,
      "loss": 0.0021,
      "step": 15450
    },
    {
      "epoch": 0.8245333333333333,
      "grad_norm": 0.3594358563423157,
      "learning_rate": 3.9693333333333335e-05,
      "loss": 0.0023,
      "step": 15460
    },
    {
      "epoch": 0.8250666666666666,
      "grad_norm": 0.7946720123291016,
      "learning_rate": 3.968666666666667e-05,
      "loss": 0.0039,
      "step": 15470
    },
    {
      "epoch": 0.8256,
      "grad_norm": 0.32448282837867737,
      "learning_rate": 3.968e-05,
      "loss": 0.0043,
      "step": 15480
    },
    {
      "epoch": 0.8261333333333334,
      "grad_norm": 0.24764057993888855,
      "learning_rate": 3.967333333333333e-05,
      "loss": 0.0041,
      "step": 15490
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.5049744248390198,
      "learning_rate": 3.966666666666667e-05,
      "loss": 0.0032,
      "step": 15500
    },
    {
      "epoch": 0.8272,
      "grad_norm": 0.35053005814552307,
      "learning_rate": 3.966e-05,
      "loss": 0.0022,
      "step": 15510
    },
    {
      "epoch": 0.8277333333333333,
      "grad_norm": 0.17170915007591248,
      "learning_rate": 3.9653333333333335e-05,
      "loss": 0.004,
      "step": 15520
    },
    {
      "epoch": 0.8282666666666667,
      "grad_norm": 0.15695925056934357,
      "learning_rate": 3.964666666666667e-05,
      "loss": 0.0022,
      "step": 15530
    },
    {
      "epoch": 0.8288,
      "grad_norm": 0.14034709334373474,
      "learning_rate": 3.964e-05,
      "loss": 0.0018,
      "step": 15540
    },
    {
      "epoch": 0.8293333333333334,
      "grad_norm": 0.8617635369300842,
      "learning_rate": 3.963333333333333e-05,
      "loss": 0.0022,
      "step": 15550
    },
    {
      "epoch": 0.8298666666666666,
      "grad_norm": 0.589013934135437,
      "learning_rate": 3.9626666666666664e-05,
      "loss": 0.0039,
      "step": 15560
    },
    {
      "epoch": 0.8304,
      "grad_norm": 0.11901853233575821,
      "learning_rate": 3.9620000000000004e-05,
      "loss": 0.002,
      "step": 15570
    },
    {
      "epoch": 0.8309333333333333,
      "grad_norm": 0.16807065904140472,
      "learning_rate": 3.9613333333333336e-05,
      "loss": 0.0032,
      "step": 15580
    },
    {
      "epoch": 0.8314666666666667,
      "grad_norm": 0.4236333966255188,
      "learning_rate": 3.960666666666667e-05,
      "loss": 0.0019,
      "step": 15590
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.9269288778305054,
      "learning_rate": 3.960000000000001e-05,
      "loss": 0.0033,
      "step": 15600
    },
    {
      "epoch": 0.8325333333333333,
      "grad_norm": 0.09026646614074707,
      "learning_rate": 3.959333333333334e-05,
      "loss": 0.0031,
      "step": 15610
    },
    {
      "epoch": 0.8330666666666666,
      "grad_norm": 0.10732236504554749,
      "learning_rate": 3.9586666666666665e-05,
      "loss": 0.0025,
      "step": 15620
    },
    {
      "epoch": 0.8336,
      "grad_norm": 0.30861958861351013,
      "learning_rate": 3.958e-05,
      "loss": 0.0021,
      "step": 15630
    },
    {
      "epoch": 0.8341333333333333,
      "grad_norm": 0.6651737093925476,
      "learning_rate": 3.9573333333333336e-05,
      "loss": 0.0029,
      "step": 15640
    },
    {
      "epoch": 0.8346666666666667,
      "grad_norm": 0.1279328167438507,
      "learning_rate": 3.956666666666667e-05,
      "loss": 0.0032,
      "step": 15650
    },
    {
      "epoch": 0.8352,
      "grad_norm": 0.1495562642812729,
      "learning_rate": 3.956e-05,
      "loss": 0.0022,
      "step": 15660
    },
    {
      "epoch": 0.8357333333333333,
      "grad_norm": 0.17192469537258148,
      "learning_rate": 3.955333333333334e-05,
      "loss": 0.003,
      "step": 15670
    },
    {
      "epoch": 0.8362666666666667,
      "grad_norm": 0.13639870285987854,
      "learning_rate": 3.954666666666667e-05,
      "loss": 0.0021,
      "step": 15680
    },
    {
      "epoch": 0.8368,
      "grad_norm": 0.3892669975757599,
      "learning_rate": 3.954e-05,
      "loss": 0.0018,
      "step": 15690
    },
    {
      "epoch": 0.8373333333333334,
      "grad_norm": 0.41133227944374084,
      "learning_rate": 3.9533333333333337e-05,
      "loss": 0.0021,
      "step": 15700
    },
    {
      "epoch": 0.8378666666666666,
      "grad_norm": 0.32861757278442383,
      "learning_rate": 3.952666666666667e-05,
      "loss": 0.004,
      "step": 15710
    },
    {
      "epoch": 0.8384,
      "grad_norm": 0.6839634776115417,
      "learning_rate": 3.952e-05,
      "loss": 0.0024,
      "step": 15720
    },
    {
      "epoch": 0.8389333333333333,
      "grad_norm": 0.843694269657135,
      "learning_rate": 3.951333333333333e-05,
      "loss": 0.0034,
      "step": 15730
    },
    {
      "epoch": 0.8394666666666667,
      "grad_norm": 0.530682384967804,
      "learning_rate": 3.950666666666667e-05,
      "loss": 0.0021,
      "step": 15740
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.2523380517959595,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 0.0025,
      "step": 15750
    },
    {
      "epoch": 0.8405333333333334,
      "grad_norm": 0.28023362159729004,
      "learning_rate": 3.949333333333334e-05,
      "loss": 0.0029,
      "step": 15760
    },
    {
      "epoch": 0.8410666666666666,
      "grad_norm": 0.4193999767303467,
      "learning_rate": 3.948666666666667e-05,
      "loss": 0.003,
      "step": 15770
    },
    {
      "epoch": 0.8416,
      "grad_norm": 0.4732397794723511,
      "learning_rate": 3.948e-05,
      "loss": 0.0032,
      "step": 15780
    },
    {
      "epoch": 0.8421333333333333,
      "grad_norm": 0.5399267673492432,
      "learning_rate": 3.9473333333333334e-05,
      "loss": 0.0033,
      "step": 15790
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 0.19387967884540558,
      "learning_rate": 3.9466666666666666e-05,
      "loss": 0.0026,
      "step": 15800
    },
    {
      "epoch": 0.8432,
      "grad_norm": 0.17869524657726288,
      "learning_rate": 3.9460000000000005e-05,
      "loss": 0.0024,
      "step": 15810
    },
    {
      "epoch": 0.8437333333333333,
      "grad_norm": 0.23000188171863556,
      "learning_rate": 3.945333333333334e-05,
      "loss": 0.0019,
      "step": 15820
    },
    {
      "epoch": 0.8442666666666667,
      "grad_norm": 0.23996283113956451,
      "learning_rate": 3.944666666666667e-05,
      "loss": 0.0024,
      "step": 15830
    },
    {
      "epoch": 0.8448,
      "grad_norm": 0.12003115564584732,
      "learning_rate": 3.944e-05,
      "loss": 0.0021,
      "step": 15840
    },
    {
      "epoch": 0.8453333333333334,
      "grad_norm": 0.45613738894462585,
      "learning_rate": 3.9433333333333334e-05,
      "loss": 0.0029,
      "step": 15850
    },
    {
      "epoch": 0.8458666666666667,
      "grad_norm": 0.6161516904830933,
      "learning_rate": 3.9426666666666666e-05,
      "loss": 0.0021,
      "step": 15860
    },
    {
      "epoch": 0.8464,
      "grad_norm": 0.07260502129793167,
      "learning_rate": 3.942e-05,
      "loss": 0.0037,
      "step": 15870
    },
    {
      "epoch": 0.8469333333333333,
      "grad_norm": 0.6037734150886536,
      "learning_rate": 3.941333333333334e-05,
      "loss": 0.0035,
      "step": 15880
    },
    {
      "epoch": 0.8474666666666667,
      "grad_norm": 0.32405588030815125,
      "learning_rate": 3.940666666666667e-05,
      "loss": 0.0024,
      "step": 15890
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.3196459412574768,
      "learning_rate": 3.94e-05,
      "loss": 0.0023,
      "step": 15900
    },
    {
      "epoch": 0.8485333333333334,
      "grad_norm": 0.25594040751457214,
      "learning_rate": 3.9393333333333335e-05,
      "loss": 0.0031,
      "step": 15910
    },
    {
      "epoch": 0.8490666666666666,
      "grad_norm": 0.5474270582199097,
      "learning_rate": 3.938666666666667e-05,
      "loss": 0.0035,
      "step": 15920
    },
    {
      "epoch": 0.8496,
      "grad_norm": 0.5808007717132568,
      "learning_rate": 3.938e-05,
      "loss": 0.0028,
      "step": 15930
    },
    {
      "epoch": 0.8501333333333333,
      "grad_norm": 0.5882290005683899,
      "learning_rate": 3.937333333333333e-05,
      "loss": 0.0024,
      "step": 15940
    },
    {
      "epoch": 0.8506666666666667,
      "grad_norm": 0.46276047825813293,
      "learning_rate": 3.936666666666667e-05,
      "loss": 0.003,
      "step": 15950
    },
    {
      "epoch": 0.8512,
      "grad_norm": 0.7389944195747375,
      "learning_rate": 3.936e-05,
      "loss": 0.0028,
      "step": 15960
    },
    {
      "epoch": 0.8517333333333333,
      "grad_norm": 0.584598958492279,
      "learning_rate": 3.9353333333333335e-05,
      "loss": 0.0026,
      "step": 15970
    },
    {
      "epoch": 0.8522666666666666,
      "grad_norm": 0.26457223296165466,
      "learning_rate": 3.9346666666666674e-05,
      "loss": 0.0021,
      "step": 15980
    },
    {
      "epoch": 0.8528,
      "grad_norm": 0.2658196985721588,
      "learning_rate": 3.9340000000000006e-05,
      "loss": 0.0025,
      "step": 15990
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.3084178864955902,
      "learning_rate": 3.933333333333333e-05,
      "loss": 0.0026,
      "step": 16000
    },
    {
      "epoch": 0.8538666666666667,
      "grad_norm": 0.6459878087043762,
      "learning_rate": 3.9326666666666664e-05,
      "loss": 0.0027,
      "step": 16010
    },
    {
      "epoch": 0.8544,
      "grad_norm": 1.0075560808181763,
      "learning_rate": 3.932e-05,
      "loss": 0.0036,
      "step": 16020
    },
    {
      "epoch": 0.8549333333333333,
      "grad_norm": 0.3833639323711395,
      "learning_rate": 3.9313333333333335e-05,
      "loss": 0.0022,
      "step": 16030
    },
    {
      "epoch": 0.8554666666666667,
      "grad_norm": 0.13841155171394348,
      "learning_rate": 3.930666666666667e-05,
      "loss": 0.0022,
      "step": 16040
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.42957112193107605,
      "learning_rate": 3.9300000000000007e-05,
      "loss": 0.0043,
      "step": 16050
    },
    {
      "epoch": 0.8565333333333334,
      "grad_norm": 0.3409769535064697,
      "learning_rate": 3.929333333333334e-05,
      "loss": 0.0027,
      "step": 16060
    },
    {
      "epoch": 0.8570666666666666,
      "grad_norm": 0.351601243019104,
      "learning_rate": 3.9286666666666664e-05,
      "loss": 0.0025,
      "step": 16070
    },
    {
      "epoch": 0.8576,
      "grad_norm": 0.2083626091480255,
      "learning_rate": 3.9280000000000003e-05,
      "loss": 0.0023,
      "step": 16080
    },
    {
      "epoch": 0.8581333333333333,
      "grad_norm": 0.5103210806846619,
      "learning_rate": 3.9273333333333336e-05,
      "loss": 0.0026,
      "step": 16090
    },
    {
      "epoch": 0.8586666666666667,
      "grad_norm": 0.2742021679878235,
      "learning_rate": 3.926666666666667e-05,
      "loss": 0.0033,
      "step": 16100
    },
    {
      "epoch": 0.8592,
      "grad_norm": 0.28252360224723816,
      "learning_rate": 3.926e-05,
      "loss": 0.0019,
      "step": 16110
    },
    {
      "epoch": 0.8597333333333333,
      "grad_norm": 0.6163440942764282,
      "learning_rate": 3.925333333333334e-05,
      "loss": 0.0022,
      "step": 16120
    },
    {
      "epoch": 0.8602666666666666,
      "grad_norm": 0.861449122428894,
      "learning_rate": 3.924666666666667e-05,
      "loss": 0.0033,
      "step": 16130
    },
    {
      "epoch": 0.8608,
      "grad_norm": 0.5502455234527588,
      "learning_rate": 3.9240000000000004e-05,
      "loss": 0.0019,
      "step": 16140
    },
    {
      "epoch": 0.8613333333333333,
      "grad_norm": 0.32479792833328247,
      "learning_rate": 3.9233333333333336e-05,
      "loss": 0.0034,
      "step": 16150
    },
    {
      "epoch": 0.8618666666666667,
      "grad_norm": 0.1910247951745987,
      "learning_rate": 3.922666666666667e-05,
      "loss": 0.0025,
      "step": 16160
    },
    {
      "epoch": 0.8624,
      "grad_norm": 0.6298068165779114,
      "learning_rate": 3.922e-05,
      "loss": 0.0025,
      "step": 16170
    },
    {
      "epoch": 0.8629333333333333,
      "grad_norm": 0.38647326827049255,
      "learning_rate": 3.921333333333333e-05,
      "loss": 0.0024,
      "step": 16180
    },
    {
      "epoch": 0.8634666666666667,
      "grad_norm": 0.4919653832912445,
      "learning_rate": 3.920666666666667e-05,
      "loss": 0.0038,
      "step": 16190
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.1887139081954956,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 0.0022,
      "step": 16200
    },
    {
      "epoch": 0.8645333333333334,
      "grad_norm": 0.7670006155967712,
      "learning_rate": 3.9193333333333336e-05,
      "loss": 0.003,
      "step": 16210
    },
    {
      "epoch": 0.8650666666666667,
      "grad_norm": 0.35246437788009644,
      "learning_rate": 3.918666666666667e-05,
      "loss": 0.0029,
      "step": 16220
    },
    {
      "epoch": 0.8656,
      "grad_norm": 0.1095738485455513,
      "learning_rate": 3.918e-05,
      "loss": 0.0023,
      "step": 16230
    },
    {
      "epoch": 0.8661333333333333,
      "grad_norm": 0.3403368592262268,
      "learning_rate": 3.917333333333333e-05,
      "loss": 0.003,
      "step": 16240
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.3157309889793396,
      "learning_rate": 3.9166666666666665e-05,
      "loss": 0.0026,
      "step": 16250
    },
    {
      "epoch": 0.8672,
      "grad_norm": 0.5172054171562195,
      "learning_rate": 3.9160000000000005e-05,
      "loss": 0.003,
      "step": 16260
    },
    {
      "epoch": 0.8677333333333334,
      "grad_norm": 0.452301561832428,
      "learning_rate": 3.915333333333334e-05,
      "loss": 0.0036,
      "step": 16270
    },
    {
      "epoch": 0.8682666666666666,
      "grad_norm": 0.3544182777404785,
      "learning_rate": 3.914666666666667e-05,
      "loss": 0.0026,
      "step": 16280
    },
    {
      "epoch": 0.8688,
      "grad_norm": 0.3090059757232666,
      "learning_rate": 3.914e-05,
      "loss": 0.0023,
      "step": 16290
    },
    {
      "epoch": 0.8693333333333333,
      "grad_norm": 0.3915342688560486,
      "learning_rate": 3.9133333333333334e-05,
      "loss": 0.0033,
      "step": 16300
    },
    {
      "epoch": 0.8698666666666667,
      "grad_norm": 0.27884334325790405,
      "learning_rate": 3.9126666666666666e-05,
      "loss": 0.0038,
      "step": 16310
    },
    {
      "epoch": 0.8704,
      "grad_norm": 0.25173988938331604,
      "learning_rate": 3.912e-05,
      "loss": 0.0034,
      "step": 16320
    },
    {
      "epoch": 0.8709333333333333,
      "grad_norm": 0.4925805628299713,
      "learning_rate": 3.911333333333334e-05,
      "loss": 0.0043,
      "step": 16330
    },
    {
      "epoch": 0.8714666666666666,
      "grad_norm": 0.09730063378810883,
      "learning_rate": 3.910666666666667e-05,
      "loss": 0.0029,
      "step": 16340
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.6759371161460876,
      "learning_rate": 3.91e-05,
      "loss": 0.0027,
      "step": 16350
    },
    {
      "epoch": 0.8725333333333334,
      "grad_norm": 0.682183027267456,
      "learning_rate": 3.9093333333333334e-05,
      "loss": 0.0029,
      "step": 16360
    },
    {
      "epoch": 0.8730666666666667,
      "grad_norm": 0.2837415635585785,
      "learning_rate": 3.9086666666666666e-05,
      "loss": 0.0026,
      "step": 16370
    },
    {
      "epoch": 0.8736,
      "grad_norm": 0.2756958603858948,
      "learning_rate": 3.908e-05,
      "loss": 0.0019,
      "step": 16380
    },
    {
      "epoch": 0.8741333333333333,
      "grad_norm": 0.1459253579378128,
      "learning_rate": 3.907333333333333e-05,
      "loss": 0.0024,
      "step": 16390
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 0.8943252563476562,
      "learning_rate": 3.906666666666667e-05,
      "loss": 0.0022,
      "step": 16400
    },
    {
      "epoch": 0.8752,
      "grad_norm": 0.09136500954627991,
      "learning_rate": 3.906e-05,
      "loss": 0.0022,
      "step": 16410
    },
    {
      "epoch": 0.8757333333333334,
      "grad_norm": 0.6429674029350281,
      "learning_rate": 3.9053333333333334e-05,
      "loss": 0.0037,
      "step": 16420
    },
    {
      "epoch": 0.8762666666666666,
      "grad_norm": 0.35728907585144043,
      "learning_rate": 3.9046666666666673e-05,
      "loss": 0.0034,
      "step": 16430
    },
    {
      "epoch": 0.8768,
      "grad_norm": 0.24187526106834412,
      "learning_rate": 3.9040000000000006e-05,
      "loss": 0.0022,
      "step": 16440
    },
    {
      "epoch": 0.8773333333333333,
      "grad_norm": 0.6326298117637634,
      "learning_rate": 3.903333333333333e-05,
      "loss": 0.0032,
      "step": 16450
    },
    {
      "epoch": 0.8778666666666667,
      "grad_norm": 0.7223407626152039,
      "learning_rate": 3.902666666666667e-05,
      "loss": 0.0031,
      "step": 16460
    },
    {
      "epoch": 0.8784,
      "grad_norm": 0.7590034604072571,
      "learning_rate": 3.902e-05,
      "loss": 0.0029,
      "step": 16470
    },
    {
      "epoch": 0.8789333333333333,
      "grad_norm": 0.3018777668476105,
      "learning_rate": 3.9013333333333335e-05,
      "loss": 0.0031,
      "step": 16480
    },
    {
      "epoch": 0.8794666666666666,
      "grad_norm": 0.2103538066148758,
      "learning_rate": 3.900666666666667e-05,
      "loss": 0.0024,
      "step": 16490
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.28730177879333496,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.0025,
      "step": 16500
    },
    {
      "epoch": 0.8805333333333333,
      "grad_norm": 0.1554725617170334,
      "learning_rate": 3.899333333333334e-05,
      "loss": 0.0029,
      "step": 16510
    },
    {
      "epoch": 0.8810666666666667,
      "grad_norm": 0.40886616706848145,
      "learning_rate": 3.8986666666666664e-05,
      "loss": 0.0032,
      "step": 16520
    },
    {
      "epoch": 0.8816,
      "grad_norm": 0.14862914383411407,
      "learning_rate": 3.898e-05,
      "loss": 0.0024,
      "step": 16530
    },
    {
      "epoch": 0.8821333333333333,
      "grad_norm": 0.43296122550964355,
      "learning_rate": 3.8973333333333335e-05,
      "loss": 0.0035,
      "step": 16540
    },
    {
      "epoch": 0.8826666666666667,
      "grad_norm": 0.12946777045726776,
      "learning_rate": 3.896666666666667e-05,
      "loss": 0.0031,
      "step": 16550
    },
    {
      "epoch": 0.8832,
      "grad_norm": 0.1461736559867859,
      "learning_rate": 3.896e-05,
      "loss": 0.0034,
      "step": 16560
    },
    {
      "epoch": 0.8837333333333334,
      "grad_norm": 0.09844577312469482,
      "learning_rate": 3.895333333333334e-05,
      "loss": 0.0028,
      "step": 16570
    },
    {
      "epoch": 0.8842666666666666,
      "grad_norm": 0.2786382734775543,
      "learning_rate": 3.894666666666667e-05,
      "loss": 0.0023,
      "step": 16580
    },
    {
      "epoch": 0.8848,
      "grad_norm": 0.4332396388053894,
      "learning_rate": 3.894e-05,
      "loss": 0.0028,
      "step": 16590
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 0.17926481366157532,
      "learning_rate": 3.8933333333333336e-05,
      "loss": 0.0021,
      "step": 16600
    },
    {
      "epoch": 0.8858666666666667,
      "grad_norm": 0.35361775755882263,
      "learning_rate": 3.892666666666667e-05,
      "loss": 0.0028,
      "step": 16610
    },
    {
      "epoch": 0.8864,
      "grad_norm": 0.3002072870731354,
      "learning_rate": 3.892e-05,
      "loss": 0.002,
      "step": 16620
    },
    {
      "epoch": 0.8869333333333334,
      "grad_norm": 0.46834710240364075,
      "learning_rate": 3.891333333333333e-05,
      "loss": 0.0034,
      "step": 16630
    },
    {
      "epoch": 0.8874666666666666,
      "grad_norm": 0.47708481550216675,
      "learning_rate": 3.890666666666667e-05,
      "loss": 0.0024,
      "step": 16640
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.5087969899177551,
      "learning_rate": 3.8900000000000004e-05,
      "loss": 0.0034,
      "step": 16650
    },
    {
      "epoch": 0.8885333333333333,
      "grad_norm": 0.13617394864559174,
      "learning_rate": 3.8893333333333336e-05,
      "loss": 0.0023,
      "step": 16660
    },
    {
      "epoch": 0.8890666666666667,
      "grad_norm": 0.2781344950199127,
      "learning_rate": 3.888666666666667e-05,
      "loss": 0.0019,
      "step": 16670
    },
    {
      "epoch": 0.8896,
      "grad_norm": 0.5398953557014465,
      "learning_rate": 3.888e-05,
      "loss": 0.0021,
      "step": 16680
    },
    {
      "epoch": 0.8901333333333333,
      "grad_norm": 0.30623507499694824,
      "learning_rate": 3.887333333333333e-05,
      "loss": 0.002,
      "step": 16690
    },
    {
      "epoch": 0.8906666666666667,
      "grad_norm": 0.4367843270301819,
      "learning_rate": 3.8866666666666665e-05,
      "loss": 0.0029,
      "step": 16700
    },
    {
      "epoch": 0.8912,
      "grad_norm": 0.31662195920944214,
      "learning_rate": 3.8860000000000004e-05,
      "loss": 0.0024,
      "step": 16710
    },
    {
      "epoch": 0.8917333333333334,
      "grad_norm": 0.8518286347389221,
      "learning_rate": 3.8853333333333336e-05,
      "loss": 0.0028,
      "step": 16720
    },
    {
      "epoch": 0.8922666666666667,
      "grad_norm": 0.7301765084266663,
      "learning_rate": 3.884666666666667e-05,
      "loss": 0.0023,
      "step": 16730
    },
    {
      "epoch": 0.8928,
      "grad_norm": 0.6300510764122009,
      "learning_rate": 3.884e-05,
      "loss": 0.0043,
      "step": 16740
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.19151395559310913,
      "learning_rate": 3.883333333333333e-05,
      "loss": 0.0031,
      "step": 16750
    },
    {
      "epoch": 0.8938666666666667,
      "grad_norm": 0.5631080865859985,
      "learning_rate": 3.8826666666666665e-05,
      "loss": 0.0036,
      "step": 16760
    },
    {
      "epoch": 0.8944,
      "grad_norm": 0.18040545284748077,
      "learning_rate": 3.882e-05,
      "loss": 0.002,
      "step": 16770
    },
    {
      "epoch": 0.8949333333333334,
      "grad_norm": 0.11270157992839813,
      "learning_rate": 3.881333333333334e-05,
      "loss": 0.0022,
      "step": 16780
    },
    {
      "epoch": 0.8954666666666666,
      "grad_norm": 0.3436245620250702,
      "learning_rate": 3.880666666666667e-05,
      "loss": 0.0023,
      "step": 16790
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.13127745687961578,
      "learning_rate": 3.88e-05,
      "loss": 0.0022,
      "step": 16800
    },
    {
      "epoch": 0.8965333333333333,
      "grad_norm": 0.5307778716087341,
      "learning_rate": 3.879333333333334e-05,
      "loss": 0.0028,
      "step": 16810
    },
    {
      "epoch": 0.8970666666666667,
      "grad_norm": 0.36136594414711,
      "learning_rate": 3.878666666666667e-05,
      "loss": 0.0028,
      "step": 16820
    },
    {
      "epoch": 0.8976,
      "grad_norm": 0.22990532219409943,
      "learning_rate": 3.878e-05,
      "loss": 0.0032,
      "step": 16830
    },
    {
      "epoch": 0.8981333333333333,
      "grad_norm": 0.12109179049730301,
      "learning_rate": 3.877333333333334e-05,
      "loss": 0.0028,
      "step": 16840
    },
    {
      "epoch": 0.8986666666666666,
      "grad_norm": 0.4326978027820587,
      "learning_rate": 3.876666666666667e-05,
      "loss": 0.0033,
      "step": 16850
    },
    {
      "epoch": 0.8992,
      "grad_norm": 0.7961755394935608,
      "learning_rate": 3.876e-05,
      "loss": 0.004,
      "step": 16860
    },
    {
      "epoch": 0.8997333333333334,
      "grad_norm": 0.27592700719833374,
      "learning_rate": 3.8753333333333334e-05,
      "loss": 0.0036,
      "step": 16870
    },
    {
      "epoch": 0.9002666666666667,
      "grad_norm": 0.1077469065785408,
      "learning_rate": 3.874666666666667e-05,
      "loss": 0.0023,
      "step": 16880
    },
    {
      "epoch": 0.9008,
      "grad_norm": 0.2212478220462799,
      "learning_rate": 3.8740000000000005e-05,
      "loss": 0.0029,
      "step": 16890
    },
    {
      "epoch": 0.9013333333333333,
      "grad_norm": 0.22113806009292603,
      "learning_rate": 3.873333333333333e-05,
      "loss": 0.0028,
      "step": 16900
    },
    {
      "epoch": 0.9018666666666667,
      "grad_norm": 0.7738146185874939,
      "learning_rate": 3.872666666666667e-05,
      "loss": 0.0032,
      "step": 16910
    },
    {
      "epoch": 0.9024,
      "grad_norm": 0.23362919688224792,
      "learning_rate": 3.872e-05,
      "loss": 0.0032,
      "step": 16920
    },
    {
      "epoch": 0.9029333333333334,
      "grad_norm": 0.5819116830825806,
      "learning_rate": 3.8713333333333334e-05,
      "loss": 0.0033,
      "step": 16930
    },
    {
      "epoch": 0.9034666666666666,
      "grad_norm": 0.2770005166530609,
      "learning_rate": 3.8706666666666667e-05,
      "loss": 0.0029,
      "step": 16940
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.49850666522979736,
      "learning_rate": 3.8700000000000006e-05,
      "loss": 0.0041,
      "step": 16950
    },
    {
      "epoch": 0.9045333333333333,
      "grad_norm": 0.160338893532753,
      "learning_rate": 3.869333333333334e-05,
      "loss": 0.0022,
      "step": 16960
    },
    {
      "epoch": 0.9050666666666667,
      "grad_norm": 0.10741084069013596,
      "learning_rate": 3.868666666666667e-05,
      "loss": 0.0036,
      "step": 16970
    },
    {
      "epoch": 0.9056,
      "grad_norm": 0.1777810901403427,
      "learning_rate": 3.868e-05,
      "loss": 0.0025,
      "step": 16980
    },
    {
      "epoch": 0.9061333333333333,
      "grad_norm": 0.5193305015563965,
      "learning_rate": 3.8673333333333335e-05,
      "loss": 0.0023,
      "step": 16990
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.2785385847091675,
      "learning_rate": 3.866666666666667e-05,
      "loss": 0.0025,
      "step": 17000
    },
    {
      "epoch": 0.9072,
      "grad_norm": 0.17677341401576996,
      "learning_rate": 3.866e-05,
      "loss": 0.0027,
      "step": 17010
    },
    {
      "epoch": 0.9077333333333333,
      "grad_norm": 0.14548705518245697,
      "learning_rate": 3.865333333333334e-05,
      "loss": 0.0025,
      "step": 17020
    },
    {
      "epoch": 0.9082666666666667,
      "grad_norm": 0.4991177022457123,
      "learning_rate": 3.864666666666667e-05,
      "loss": 0.002,
      "step": 17030
    },
    {
      "epoch": 0.9088,
      "grad_norm": 0.15167978405952454,
      "learning_rate": 3.864e-05,
      "loss": 0.0038,
      "step": 17040
    },
    {
      "epoch": 0.9093333333333333,
      "grad_norm": 0.5448954105377197,
      "learning_rate": 3.8633333333333335e-05,
      "loss": 0.0024,
      "step": 17050
    },
    {
      "epoch": 0.9098666666666667,
      "grad_norm": 0.2677730917930603,
      "learning_rate": 3.862666666666667e-05,
      "loss": 0.0029,
      "step": 17060
    },
    {
      "epoch": 0.9104,
      "grad_norm": 0.1877250373363495,
      "learning_rate": 3.862e-05,
      "loss": 0.0028,
      "step": 17070
    },
    {
      "epoch": 0.9109333333333334,
      "grad_norm": 0.2612883150577545,
      "learning_rate": 3.861333333333333e-05,
      "loss": 0.0029,
      "step": 17080
    },
    {
      "epoch": 0.9114666666666666,
      "grad_norm": 0.41258227825164795,
      "learning_rate": 3.860666666666667e-05,
      "loss": 0.0038,
      "step": 17090
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.1877647489309311,
      "learning_rate": 3.86e-05,
      "loss": 0.0045,
      "step": 17100
    },
    {
      "epoch": 0.9125333333333333,
      "grad_norm": 0.3842347264289856,
      "learning_rate": 3.8593333333333335e-05,
      "loss": 0.0037,
      "step": 17110
    },
    {
      "epoch": 0.9130666666666667,
      "grad_norm": 0.29513880610466003,
      "learning_rate": 3.858666666666667e-05,
      "loss": 0.0043,
      "step": 17120
    },
    {
      "epoch": 0.9136,
      "grad_norm": 0.5272364616394043,
      "learning_rate": 3.858e-05,
      "loss": 0.003,
      "step": 17130
    },
    {
      "epoch": 0.9141333333333334,
      "grad_norm": 0.631417989730835,
      "learning_rate": 3.857333333333333e-05,
      "loss": 0.0027,
      "step": 17140
    },
    {
      "epoch": 0.9146666666666666,
      "grad_norm": 0.42696765065193176,
      "learning_rate": 3.8566666666666664e-05,
      "loss": 0.002,
      "step": 17150
    },
    {
      "epoch": 0.9152,
      "grad_norm": 0.5375933051109314,
      "learning_rate": 3.8560000000000004e-05,
      "loss": 0.004,
      "step": 17160
    },
    {
      "epoch": 0.9157333333333333,
      "grad_norm": 0.5459578633308411,
      "learning_rate": 3.8553333333333336e-05,
      "loss": 0.0029,
      "step": 17170
    },
    {
      "epoch": 0.9162666666666667,
      "grad_norm": 0.3373715579509735,
      "learning_rate": 3.854666666666667e-05,
      "loss": 0.0025,
      "step": 17180
    },
    {
      "epoch": 0.9168,
      "grad_norm": 0.4064420759677887,
      "learning_rate": 3.854000000000001e-05,
      "loss": 0.0031,
      "step": 17190
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 0.37253180146217346,
      "learning_rate": 3.853333333333334e-05,
      "loss": 0.0029,
      "step": 17200
    },
    {
      "epoch": 0.9178666666666667,
      "grad_norm": 0.20847107470035553,
      "learning_rate": 3.8526666666666665e-05,
      "loss": 0.0017,
      "step": 17210
    },
    {
      "epoch": 0.9184,
      "grad_norm": 0.09529584646224976,
      "learning_rate": 3.8520000000000004e-05,
      "loss": 0.002,
      "step": 17220
    },
    {
      "epoch": 0.9189333333333334,
      "grad_norm": 0.40558382868766785,
      "learning_rate": 3.8513333333333336e-05,
      "loss": 0.0047,
      "step": 17230
    },
    {
      "epoch": 0.9194666666666667,
      "grad_norm": 0.23624369502067566,
      "learning_rate": 3.850666666666667e-05,
      "loss": 0.0025,
      "step": 17240
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.3535548746585846,
      "learning_rate": 3.85e-05,
      "loss": 0.0033,
      "step": 17250
    },
    {
      "epoch": 0.9205333333333333,
      "grad_norm": 0.13376949727535248,
      "learning_rate": 3.849333333333334e-05,
      "loss": 0.0054,
      "step": 17260
    },
    {
      "epoch": 0.9210666666666667,
      "grad_norm": 0.5361871719360352,
      "learning_rate": 3.848666666666667e-05,
      "loss": 0.0021,
      "step": 17270
    },
    {
      "epoch": 0.9216,
      "grad_norm": 0.3946467936038971,
      "learning_rate": 3.848e-05,
      "loss": 0.0034,
      "step": 17280
    },
    {
      "epoch": 0.9221333333333334,
      "grad_norm": 0.1354929655790329,
      "learning_rate": 3.8473333333333337e-05,
      "loss": 0.0042,
      "step": 17290
    },
    {
      "epoch": 0.9226666666666666,
      "grad_norm": 0.3965054452419281,
      "learning_rate": 3.846666666666667e-05,
      "loss": 0.0022,
      "step": 17300
    },
    {
      "epoch": 0.9232,
      "grad_norm": 0.2694775462150574,
      "learning_rate": 3.846e-05,
      "loss": 0.0039,
      "step": 17310
    },
    {
      "epoch": 0.9237333333333333,
      "grad_norm": 0.10367457568645477,
      "learning_rate": 3.845333333333333e-05,
      "loss": 0.0019,
      "step": 17320
    },
    {
      "epoch": 0.9242666666666667,
      "grad_norm": 0.21962061524391174,
      "learning_rate": 3.844666666666667e-05,
      "loss": 0.0026,
      "step": 17330
    },
    {
      "epoch": 0.9248,
      "grad_norm": 0.20348088443279266,
      "learning_rate": 3.8440000000000005e-05,
      "loss": 0.0021,
      "step": 17340
    },
    {
      "epoch": 0.9253333333333333,
      "grad_norm": 0.45300567150115967,
      "learning_rate": 3.843333333333334e-05,
      "loss": 0.0028,
      "step": 17350
    },
    {
      "epoch": 0.9258666666666666,
      "grad_norm": 0.24969780445098877,
      "learning_rate": 3.842666666666667e-05,
      "loss": 0.0027,
      "step": 17360
    },
    {
      "epoch": 0.9264,
      "grad_norm": 0.5772358179092407,
      "learning_rate": 3.842e-05,
      "loss": 0.0023,
      "step": 17370
    },
    {
      "epoch": 0.9269333333333334,
      "grad_norm": 0.3260096311569214,
      "learning_rate": 3.8413333333333334e-05,
      "loss": 0.003,
      "step": 17380
    },
    {
      "epoch": 0.9274666666666667,
      "grad_norm": 0.15553990006446838,
      "learning_rate": 3.8406666666666666e-05,
      "loss": 0.0029,
      "step": 17390
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.3549368679523468,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 0.0034,
      "step": 17400
    },
    {
      "epoch": 0.9285333333333333,
      "grad_norm": 0.2127993404865265,
      "learning_rate": 3.839333333333334e-05,
      "loss": 0.0024,
      "step": 17410
    },
    {
      "epoch": 0.9290666666666667,
      "grad_norm": 0.5047901272773743,
      "learning_rate": 3.838666666666667e-05,
      "loss": 0.0027,
      "step": 17420
    },
    {
      "epoch": 0.9296,
      "grad_norm": 0.4307739734649658,
      "learning_rate": 3.838e-05,
      "loss": 0.0027,
      "step": 17430
    },
    {
      "epoch": 0.9301333333333334,
      "grad_norm": 0.39437931776046753,
      "learning_rate": 3.8373333333333334e-05,
      "loss": 0.0024,
      "step": 17440
    },
    {
      "epoch": 0.9306666666666666,
      "grad_norm": 0.34147363901138306,
      "learning_rate": 3.8366666666666666e-05,
      "loss": 0.002,
      "step": 17450
    },
    {
      "epoch": 0.9312,
      "grad_norm": 0.5422515273094177,
      "learning_rate": 3.836e-05,
      "loss": 0.0022,
      "step": 17460
    },
    {
      "epoch": 0.9317333333333333,
      "grad_norm": 0.07420559972524643,
      "learning_rate": 3.835333333333334e-05,
      "loss": 0.0041,
      "step": 17470
    },
    {
      "epoch": 0.9322666666666667,
      "grad_norm": 0.14919865131378174,
      "learning_rate": 3.834666666666667e-05,
      "loss": 0.003,
      "step": 17480
    },
    {
      "epoch": 0.9328,
      "grad_norm": 0.3997579514980316,
      "learning_rate": 3.834e-05,
      "loss": 0.0022,
      "step": 17490
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.3562183082103729,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 0.0019,
      "step": 17500
    },
    {
      "epoch": 0.9338666666666666,
      "grad_norm": 0.4128434360027313,
      "learning_rate": 3.832666666666667e-05,
      "loss": 0.0028,
      "step": 17510
    },
    {
      "epoch": 0.9344,
      "grad_norm": 0.12108215689659119,
      "learning_rate": 3.832e-05,
      "loss": 0.0033,
      "step": 17520
    },
    {
      "epoch": 0.9349333333333333,
      "grad_norm": 0.38308054208755493,
      "learning_rate": 3.831333333333333e-05,
      "loss": 0.0029,
      "step": 17530
    },
    {
      "epoch": 0.9354666666666667,
      "grad_norm": 0.15695703029632568,
      "learning_rate": 3.830666666666667e-05,
      "loss": 0.0032,
      "step": 17540
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.8647889494895935,
      "learning_rate": 3.83e-05,
      "loss": 0.0023,
      "step": 17550
    },
    {
      "epoch": 0.9365333333333333,
      "grad_norm": 0.40856385231018066,
      "learning_rate": 3.8293333333333335e-05,
      "loss": 0.0026,
      "step": 17560
    },
    {
      "epoch": 0.9370666666666667,
      "grad_norm": 0.4392563998699188,
      "learning_rate": 3.8286666666666674e-05,
      "loss": 0.0032,
      "step": 17570
    },
    {
      "epoch": 0.9376,
      "grad_norm": 0.19825959205627441,
      "learning_rate": 3.828e-05,
      "loss": 0.0025,
      "step": 17580
    },
    {
      "epoch": 0.9381333333333334,
      "grad_norm": 0.3892107903957367,
      "learning_rate": 3.827333333333333e-05,
      "loss": 0.0025,
      "step": 17590
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 0.13335342705249786,
      "learning_rate": 3.8266666666666664e-05,
      "loss": 0.0039,
      "step": 17600
    },
    {
      "epoch": 0.9392,
      "grad_norm": 0.32634761929512024,
      "learning_rate": 3.826e-05,
      "loss": 0.0037,
      "step": 17610
    },
    {
      "epoch": 0.9397333333333333,
      "grad_norm": 0.33182328939437866,
      "learning_rate": 3.8253333333333335e-05,
      "loss": 0.0022,
      "step": 17620
    },
    {
      "epoch": 0.9402666666666667,
      "grad_norm": 0.5427140593528748,
      "learning_rate": 3.824666666666667e-05,
      "loss": 0.0052,
      "step": 17630
    },
    {
      "epoch": 0.9408,
      "grad_norm": 0.5255841612815857,
      "learning_rate": 3.8240000000000007e-05,
      "loss": 0.0024,
      "step": 17640
    },
    {
      "epoch": 0.9413333333333334,
      "grad_norm": 0.30846691131591797,
      "learning_rate": 3.823333333333334e-05,
      "loss": 0.002,
      "step": 17650
    },
    {
      "epoch": 0.9418666666666666,
      "grad_norm": 0.4182763397693634,
      "learning_rate": 3.8226666666666664e-05,
      "loss": 0.0022,
      "step": 17660
    },
    {
      "epoch": 0.9424,
      "grad_norm": 0.3496323525905609,
      "learning_rate": 3.822e-05,
      "loss": 0.0036,
      "step": 17670
    },
    {
      "epoch": 0.9429333333333333,
      "grad_norm": 0.0951806977391243,
      "learning_rate": 3.8213333333333336e-05,
      "loss": 0.0027,
      "step": 17680
    },
    {
      "epoch": 0.9434666666666667,
      "grad_norm": 0.4023318588733673,
      "learning_rate": 3.820666666666667e-05,
      "loss": 0.0034,
      "step": 17690
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.13208283483982086,
      "learning_rate": 3.82e-05,
      "loss": 0.0025,
      "step": 17700
    },
    {
      "epoch": 0.9445333333333333,
      "grad_norm": 0.0971231535077095,
      "learning_rate": 3.819333333333334e-05,
      "loss": 0.0018,
      "step": 17710
    },
    {
      "epoch": 0.9450666666666667,
      "grad_norm": 0.5077412128448486,
      "learning_rate": 3.818666666666667e-05,
      "loss": 0.003,
      "step": 17720
    },
    {
      "epoch": 0.9456,
      "grad_norm": 0.20443043112754822,
      "learning_rate": 3.818e-05,
      "loss": 0.0025,
      "step": 17730
    },
    {
      "epoch": 0.9461333333333334,
      "grad_norm": 0.23565326631069183,
      "learning_rate": 3.8173333333333336e-05,
      "loss": 0.0029,
      "step": 17740
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.6956666111946106,
      "learning_rate": 3.816666666666667e-05,
      "loss": 0.0027,
      "step": 17750
    },
    {
      "epoch": 0.9472,
      "grad_norm": 0.11057592183351517,
      "learning_rate": 3.816e-05,
      "loss": 0.0024,
      "step": 17760
    },
    {
      "epoch": 0.9477333333333333,
      "grad_norm": 0.41125208139419556,
      "learning_rate": 3.815333333333333e-05,
      "loss": 0.0032,
      "step": 17770
    },
    {
      "epoch": 0.9482666666666667,
      "grad_norm": 0.17469975352287292,
      "learning_rate": 3.814666666666667e-05,
      "loss": 0.0027,
      "step": 17780
    },
    {
      "epoch": 0.9488,
      "grad_norm": 0.25735095143318176,
      "learning_rate": 3.8140000000000004e-05,
      "loss": 0.003,
      "step": 17790
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 0.20072691142559052,
      "learning_rate": 3.8133333333333336e-05,
      "loss": 0.0022,
      "step": 17800
    },
    {
      "epoch": 0.9498666666666666,
      "grad_norm": 0.33433040976524353,
      "learning_rate": 3.812666666666667e-05,
      "loss": 0.0029,
      "step": 17810
    },
    {
      "epoch": 0.9504,
      "grad_norm": 0.35121744871139526,
      "learning_rate": 3.812e-05,
      "loss": 0.0029,
      "step": 17820
    },
    {
      "epoch": 0.9509333333333333,
      "grad_norm": 0.36813217401504517,
      "learning_rate": 3.811333333333333e-05,
      "loss": 0.0018,
      "step": 17830
    },
    {
      "epoch": 0.9514666666666667,
      "grad_norm": 0.20473726093769073,
      "learning_rate": 3.8106666666666665e-05,
      "loss": 0.0026,
      "step": 17840
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.4679476022720337,
      "learning_rate": 3.8100000000000005e-05,
      "loss": 0.0032,
      "step": 17850
    },
    {
      "epoch": 0.9525333333333333,
      "grad_norm": 0.33160215616226196,
      "learning_rate": 3.809333333333334e-05,
      "loss": 0.002,
      "step": 17860
    },
    {
      "epoch": 0.9530666666666666,
      "grad_norm": 0.5181406736373901,
      "learning_rate": 3.808666666666667e-05,
      "loss": 0.0029,
      "step": 17870
    },
    {
      "epoch": 0.9536,
      "grad_norm": 0.5134826898574829,
      "learning_rate": 3.808e-05,
      "loss": 0.0024,
      "step": 17880
    },
    {
      "epoch": 0.9541333333333334,
      "grad_norm": 1.0731121301651,
      "learning_rate": 3.8073333333333334e-05,
      "loss": 0.0045,
      "step": 17890
    },
    {
      "epoch": 0.9546666666666667,
      "grad_norm": 0.8414022326469421,
      "learning_rate": 3.8066666666666666e-05,
      "loss": 0.0037,
      "step": 17900
    },
    {
      "epoch": 0.9552,
      "grad_norm": 0.19467109441757202,
      "learning_rate": 3.806e-05,
      "loss": 0.0033,
      "step": 17910
    },
    {
      "epoch": 0.9557333333333333,
      "grad_norm": 0.18610088527202606,
      "learning_rate": 3.805333333333334e-05,
      "loss": 0.0026,
      "step": 17920
    },
    {
      "epoch": 0.9562666666666667,
      "grad_norm": 0.6268470883369446,
      "learning_rate": 3.804666666666667e-05,
      "loss": 0.0045,
      "step": 17930
    },
    {
      "epoch": 0.9568,
      "grad_norm": 0.261817067861557,
      "learning_rate": 3.804e-05,
      "loss": 0.0038,
      "step": 17940
    },
    {
      "epoch": 0.9573333333333334,
      "grad_norm": 0.3856542110443115,
      "learning_rate": 3.803333333333334e-05,
      "loss": 0.0024,
      "step": 17950
    },
    {
      "epoch": 0.9578666666666666,
      "grad_norm": 0.6376402974128723,
      "learning_rate": 3.8026666666666666e-05,
      "loss": 0.0031,
      "step": 17960
    },
    {
      "epoch": 0.9584,
      "grad_norm": 0.46408170461654663,
      "learning_rate": 3.802e-05,
      "loss": 0.0023,
      "step": 17970
    },
    {
      "epoch": 0.9589333333333333,
      "grad_norm": 0.5883524417877197,
      "learning_rate": 3.801333333333333e-05,
      "loss": 0.0026,
      "step": 17980
    },
    {
      "epoch": 0.9594666666666667,
      "grad_norm": 0.4020208418369293,
      "learning_rate": 3.800666666666667e-05,
      "loss": 0.0035,
      "step": 17990
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.6580013036727905,
      "learning_rate": 3.8e-05,
      "loss": 0.0029,
      "step": 18000
    },
    {
      "epoch": 0.9605333333333334,
      "grad_norm": 0.06838946789503098,
      "learning_rate": 3.7993333333333334e-05,
      "loss": 0.0022,
      "step": 18010
    },
    {
      "epoch": 0.9610666666666666,
      "grad_norm": 0.2569425404071808,
      "learning_rate": 3.7986666666666673e-05,
      "loss": 0.0021,
      "step": 18020
    },
    {
      "epoch": 0.9616,
      "grad_norm": 0.4669554829597473,
      "learning_rate": 3.7980000000000006e-05,
      "loss": 0.0042,
      "step": 18030
    },
    {
      "epoch": 0.9621333333333333,
      "grad_norm": 0.8942239880561829,
      "learning_rate": 3.797333333333333e-05,
      "loss": 0.0025,
      "step": 18040
    },
    {
      "epoch": 0.9626666666666667,
      "grad_norm": 0.29693830013275146,
      "learning_rate": 3.796666666666667e-05,
      "loss": 0.0024,
      "step": 18050
    },
    {
      "epoch": 0.9632,
      "grad_norm": 0.25991755723953247,
      "learning_rate": 3.796e-05,
      "loss": 0.0038,
      "step": 18060
    },
    {
      "epoch": 0.9637333333333333,
      "grad_norm": 0.20040416717529297,
      "learning_rate": 3.7953333333333335e-05,
      "loss": 0.0033,
      "step": 18070
    },
    {
      "epoch": 0.9642666666666667,
      "grad_norm": 0.17187786102294922,
      "learning_rate": 3.794666666666667e-05,
      "loss": 0.0046,
      "step": 18080
    },
    {
      "epoch": 0.9648,
      "grad_norm": 0.1966138631105423,
      "learning_rate": 3.7940000000000006e-05,
      "loss": 0.002,
      "step": 18090
    },
    {
      "epoch": 0.9653333333333334,
      "grad_norm": 0.17075049877166748,
      "learning_rate": 3.793333333333334e-05,
      "loss": 0.0021,
      "step": 18100
    },
    {
      "epoch": 0.9658666666666667,
      "grad_norm": 0.11449621617794037,
      "learning_rate": 3.7926666666666664e-05,
      "loss": 0.0024,
      "step": 18110
    },
    {
      "epoch": 0.9664,
      "grad_norm": 0.2922976315021515,
      "learning_rate": 3.792e-05,
      "loss": 0.0026,
      "step": 18120
    },
    {
      "epoch": 0.9669333333333333,
      "grad_norm": 0.7201458811759949,
      "learning_rate": 3.7913333333333335e-05,
      "loss": 0.0039,
      "step": 18130
    },
    {
      "epoch": 0.9674666666666667,
      "grad_norm": 0.7612759470939636,
      "learning_rate": 3.790666666666667e-05,
      "loss": 0.0031,
      "step": 18140
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.4223543405532837,
      "learning_rate": 3.79e-05,
      "loss": 0.0027,
      "step": 18150
    },
    {
      "epoch": 0.9685333333333334,
      "grad_norm": 0.1702117919921875,
      "learning_rate": 3.789333333333334e-05,
      "loss": 0.0021,
      "step": 18160
    },
    {
      "epoch": 0.9690666666666666,
      "grad_norm": 0.6442193984985352,
      "learning_rate": 3.788666666666667e-05,
      "loss": 0.002,
      "step": 18170
    },
    {
      "epoch": 0.9696,
      "grad_norm": 0.5926980972290039,
      "learning_rate": 3.788e-05,
      "loss": 0.0041,
      "step": 18180
    },
    {
      "epoch": 0.9701333333333333,
      "grad_norm": 0.20954878628253937,
      "learning_rate": 3.7873333333333336e-05,
      "loss": 0.0024,
      "step": 18190
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 0.29348745942115784,
      "learning_rate": 3.786666666666667e-05,
      "loss": 0.0034,
      "step": 18200
    },
    {
      "epoch": 0.9712,
      "grad_norm": 0.37118247151374817,
      "learning_rate": 3.786e-05,
      "loss": 0.0026,
      "step": 18210
    },
    {
      "epoch": 0.9717333333333333,
      "grad_norm": 0.1666014939546585,
      "learning_rate": 3.785333333333333e-05,
      "loss": 0.0035,
      "step": 18220
    },
    {
      "epoch": 0.9722666666666666,
      "grad_norm": 0.10228951275348663,
      "learning_rate": 3.784666666666667e-05,
      "loss": 0.0025,
      "step": 18230
    },
    {
      "epoch": 0.9728,
      "grad_norm": 0.5156924724578857,
      "learning_rate": 3.7840000000000004e-05,
      "loss": 0.0031,
      "step": 18240
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.20739863812923431,
      "learning_rate": 3.7833333333333336e-05,
      "loss": 0.0022,
      "step": 18250
    },
    {
      "epoch": 0.9738666666666667,
      "grad_norm": 0.34412500262260437,
      "learning_rate": 3.782666666666667e-05,
      "loss": 0.0021,
      "step": 18260
    },
    {
      "epoch": 0.9744,
      "grad_norm": 0.25828859210014343,
      "learning_rate": 3.782e-05,
      "loss": 0.002,
      "step": 18270
    },
    {
      "epoch": 0.9749333333333333,
      "grad_norm": 0.7248446345329285,
      "learning_rate": 3.781333333333333e-05,
      "loss": 0.0018,
      "step": 18280
    },
    {
      "epoch": 0.9754666666666667,
      "grad_norm": 0.19131073355674744,
      "learning_rate": 3.7806666666666665e-05,
      "loss": 0.0023,
      "step": 18290
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.32576507329940796,
      "learning_rate": 3.7800000000000004e-05,
      "loss": 0.0029,
      "step": 18300
    },
    {
      "epoch": 0.9765333333333334,
      "grad_norm": 0.22633586823940277,
      "learning_rate": 3.7793333333333336e-05,
      "loss": 0.0034,
      "step": 18310
    },
    {
      "epoch": 0.9770666666666666,
      "grad_norm": 0.3692898750305176,
      "learning_rate": 3.778666666666667e-05,
      "loss": 0.003,
      "step": 18320
    },
    {
      "epoch": 0.9776,
      "grad_norm": 0.30659356713294983,
      "learning_rate": 3.778000000000001e-05,
      "loss": 0.0035,
      "step": 18330
    },
    {
      "epoch": 0.9781333333333333,
      "grad_norm": 0.21213652193546295,
      "learning_rate": 3.777333333333333e-05,
      "loss": 0.0021,
      "step": 18340
    },
    {
      "epoch": 0.9786666666666667,
      "grad_norm": 0.1819077581167221,
      "learning_rate": 3.7766666666666665e-05,
      "loss": 0.0029,
      "step": 18350
    },
    {
      "epoch": 0.9792,
      "grad_norm": 0.0993913859128952,
      "learning_rate": 3.776e-05,
      "loss": 0.0022,
      "step": 18360
    },
    {
      "epoch": 0.9797333333333333,
      "grad_norm": 0.15565524995326996,
      "learning_rate": 3.775333333333334e-05,
      "loss": 0.0023,
      "step": 18370
    },
    {
      "epoch": 0.9802666666666666,
      "grad_norm": 0.7085014581680298,
      "learning_rate": 3.774666666666667e-05,
      "loss": 0.0021,
      "step": 18380
    },
    {
      "epoch": 0.9808,
      "grad_norm": 0.36056363582611084,
      "learning_rate": 3.774e-05,
      "loss": 0.0028,
      "step": 18390
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 0.11484070867300034,
      "learning_rate": 3.773333333333334e-05,
      "loss": 0.0033,
      "step": 18400
    },
    {
      "epoch": 0.9818666666666667,
      "grad_norm": 0.46932241320610046,
      "learning_rate": 3.7726666666666666e-05,
      "loss": 0.0024,
      "step": 18410
    },
    {
      "epoch": 0.9824,
      "grad_norm": 0.3947776257991791,
      "learning_rate": 3.772e-05,
      "loss": 0.003,
      "step": 18420
    },
    {
      "epoch": 0.9829333333333333,
      "grad_norm": 0.15188747644424438,
      "learning_rate": 3.771333333333334e-05,
      "loss": 0.0023,
      "step": 18430
    },
    {
      "epoch": 0.9834666666666667,
      "grad_norm": 0.15563400089740753,
      "learning_rate": 3.770666666666667e-05,
      "loss": 0.0019,
      "step": 18440
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.1550782173871994,
      "learning_rate": 3.77e-05,
      "loss": 0.0022,
      "step": 18450
    },
    {
      "epoch": 0.9845333333333334,
      "grad_norm": 0.2311638444662094,
      "learning_rate": 3.7693333333333334e-05,
      "loss": 0.0018,
      "step": 18460
    },
    {
      "epoch": 0.9850666666666666,
      "grad_norm": 0.5024987459182739,
      "learning_rate": 3.768666666666667e-05,
      "loss": 0.0024,
      "step": 18470
    },
    {
      "epoch": 0.9856,
      "grad_norm": 0.28430765867233276,
      "learning_rate": 3.7680000000000005e-05,
      "loss": 0.0028,
      "step": 18480
    },
    {
      "epoch": 0.9861333333333333,
      "grad_norm": 0.33236369490623474,
      "learning_rate": 3.767333333333333e-05,
      "loss": 0.0024,
      "step": 18490
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.1690698117017746,
      "learning_rate": 3.766666666666667e-05,
      "loss": 0.0023,
      "step": 18500
    },
    {
      "epoch": 0.9872,
      "grad_norm": 0.33448049426078796,
      "learning_rate": 3.766e-05,
      "loss": 0.0037,
      "step": 18510
    },
    {
      "epoch": 0.9877333333333334,
      "grad_norm": 0.0792970210313797,
      "learning_rate": 3.7653333333333334e-05,
      "loss": 0.0033,
      "step": 18520
    },
    {
      "epoch": 0.9882666666666666,
      "grad_norm": 0.3092625141143799,
      "learning_rate": 3.7646666666666666e-05,
      "loss": 0.0026,
      "step": 18530
    },
    {
      "epoch": 0.9888,
      "grad_norm": 0.5674415826797485,
      "learning_rate": 3.7640000000000006e-05,
      "loss": 0.0025,
      "step": 18540
    },
    {
      "epoch": 0.9893333333333333,
      "grad_norm": 0.1688203364610672,
      "learning_rate": 3.763333333333334e-05,
      "loss": 0.0026,
      "step": 18550
    },
    {
      "epoch": 0.9898666666666667,
      "grad_norm": 0.2627052366733551,
      "learning_rate": 3.762666666666667e-05,
      "loss": 0.0022,
      "step": 18560
    },
    {
      "epoch": 0.9904,
      "grad_norm": 0.1818847507238388,
      "learning_rate": 3.762e-05,
      "loss": 0.0029,
      "step": 18570
    },
    {
      "epoch": 0.9909333333333333,
      "grad_norm": 0.8701866865158081,
      "learning_rate": 3.7613333333333335e-05,
      "loss": 0.0029,
      "step": 18580
    },
    {
      "epoch": 0.9914666666666667,
      "grad_norm": 0.39403560757637024,
      "learning_rate": 3.760666666666667e-05,
      "loss": 0.0024,
      "step": 18590
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.27761581540107727,
      "learning_rate": 3.76e-05,
      "loss": 0.0023,
      "step": 18600
    },
    {
      "epoch": 0.9925333333333334,
      "grad_norm": 0.20020173490047455,
      "learning_rate": 3.759333333333334e-05,
      "loss": 0.0016,
      "step": 18610
    },
    {
      "epoch": 0.9930666666666667,
      "grad_norm": 0.20733126997947693,
      "learning_rate": 3.758666666666667e-05,
      "loss": 0.0032,
      "step": 18620
    },
    {
      "epoch": 0.9936,
      "grad_norm": 0.3709911108016968,
      "learning_rate": 3.758e-05,
      "loss": 0.0022,
      "step": 18630
    },
    {
      "epoch": 0.9941333333333333,
      "grad_norm": 0.5475229024887085,
      "learning_rate": 3.7573333333333335e-05,
      "loss": 0.0021,
      "step": 18640
    },
    {
      "epoch": 0.9946666666666667,
      "grad_norm": 0.2563403248786926,
      "learning_rate": 3.756666666666667e-05,
      "loss": 0.0023,
      "step": 18650
    },
    {
      "epoch": 0.9952,
      "grad_norm": 0.23646363615989685,
      "learning_rate": 3.756e-05,
      "loss": 0.0034,
      "step": 18660
    },
    {
      "epoch": 0.9957333333333334,
      "grad_norm": 0.45244938135147095,
      "learning_rate": 3.755333333333333e-05,
      "loss": 0.0022,
      "step": 18670
    },
    {
      "epoch": 0.9962666666666666,
      "grad_norm": 0.4082847237586975,
      "learning_rate": 3.754666666666667e-05,
      "loss": 0.0035,
      "step": 18680
    },
    {
      "epoch": 0.9968,
      "grad_norm": 0.4758669435977936,
      "learning_rate": 3.754e-05,
      "loss": 0.003,
      "step": 18690
    },
    {
      "epoch": 0.9973333333333333,
      "grad_norm": 0.3307899236679077,
      "learning_rate": 3.7533333333333335e-05,
      "loss": 0.002,
      "step": 18700
    },
    {
      "epoch": 0.9978666666666667,
      "grad_norm": 0.49111178517341614,
      "learning_rate": 3.7526666666666674e-05,
      "loss": 0.0026,
      "step": 18710
    },
    {
      "epoch": 0.9984,
      "grad_norm": 0.2736140489578247,
      "learning_rate": 3.752e-05,
      "loss": 0.0019,
      "step": 18720
    },
    {
      "epoch": 0.9989333333333333,
      "grad_norm": 0.41259780526161194,
      "learning_rate": 3.751333333333333e-05,
      "loss": 0.0028,
      "step": 18730
    },
    {
      "epoch": 0.9994666666666666,
      "grad_norm": 0.47599637508392334,
      "learning_rate": 3.7506666666666664e-05,
      "loss": 0.0025,
      "step": 18740
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.41819998621940613,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.0025,
      "step": 18750
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.002747009741142392,
      "eval_runtime": 165.948,
      "eval_samples_per_second": 1506.496,
      "eval_steps_per_second": 37.662,
      "step": 18750
    },
    {
      "epoch": 1.0005333333333333,
      "grad_norm": 0.9830917119979858,
      "learning_rate": 3.7493333333333336e-05,
      "loss": 0.0049,
      "step": 18760
    },
    {
      "epoch": 1.0010666666666668,
      "grad_norm": 0.10288651287555695,
      "learning_rate": 3.748666666666667e-05,
      "loss": 0.0032,
      "step": 18770
    },
    {
      "epoch": 1.0016,
      "grad_norm": 0.32751354575157166,
      "learning_rate": 3.748000000000001e-05,
      "loss": 0.002,
      "step": 18780
    },
    {
      "epoch": 1.0021333333333333,
      "grad_norm": 0.3393935263156891,
      "learning_rate": 3.747333333333333e-05,
      "loss": 0.0024,
      "step": 18790
    },
    {
      "epoch": 1.0026666666666666,
      "grad_norm": 0.558078944683075,
      "learning_rate": 3.7466666666666665e-05,
      "loss": 0.002,
      "step": 18800
    },
    {
      "epoch": 1.0032,
      "grad_norm": 0.11822311580181122,
      "learning_rate": 3.7460000000000004e-05,
      "loss": 0.0036,
      "step": 18810
    },
    {
      "epoch": 1.0037333333333334,
      "grad_norm": 0.1284903734922409,
      "learning_rate": 3.7453333333333336e-05,
      "loss": 0.0026,
      "step": 18820
    },
    {
      "epoch": 1.0042666666666666,
      "grad_norm": 0.12485870718955994,
      "learning_rate": 3.744666666666667e-05,
      "loss": 0.0029,
      "step": 18830
    },
    {
      "epoch": 1.0048,
      "grad_norm": 0.16485634446144104,
      "learning_rate": 3.744e-05,
      "loss": 0.0026,
      "step": 18840
    },
    {
      "epoch": 1.0053333333333334,
      "grad_norm": 0.2077091783285141,
      "learning_rate": 3.743333333333334e-05,
      "loss": 0.0023,
      "step": 18850
    },
    {
      "epoch": 1.0058666666666667,
      "grad_norm": 0.6640917658805847,
      "learning_rate": 3.742666666666667e-05,
      "loss": 0.0032,
      "step": 18860
    },
    {
      "epoch": 1.0064,
      "grad_norm": 0.8133165836334229,
      "learning_rate": 3.742e-05,
      "loss": 0.002,
      "step": 18870
    },
    {
      "epoch": 1.0069333333333332,
      "grad_norm": 0.3298242688179016,
      "learning_rate": 3.7413333333333337e-05,
      "loss": 0.0037,
      "step": 18880
    },
    {
      "epoch": 1.0074666666666667,
      "grad_norm": 0.22709843516349792,
      "learning_rate": 3.740666666666667e-05,
      "loss": 0.0029,
      "step": 18890
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.9544540047645569,
      "learning_rate": 3.74e-05,
      "loss": 0.0018,
      "step": 18900
    },
    {
      "epoch": 1.0085333333333333,
      "grad_norm": 0.5555612444877625,
      "learning_rate": 3.739333333333333e-05,
      "loss": 0.0021,
      "step": 18910
    },
    {
      "epoch": 1.0090666666666666,
      "grad_norm": 0.3668770492076874,
      "learning_rate": 3.738666666666667e-05,
      "loss": 0.0031,
      "step": 18920
    },
    {
      "epoch": 1.0096,
      "grad_norm": 0.5307333469390869,
      "learning_rate": 3.7380000000000005e-05,
      "loss": 0.0026,
      "step": 18930
    },
    {
      "epoch": 1.0101333333333333,
      "grad_norm": 0.6384351849555969,
      "learning_rate": 3.737333333333333e-05,
      "loss": 0.0021,
      "step": 18940
    },
    {
      "epoch": 1.0106666666666666,
      "grad_norm": 0.1273522526025772,
      "learning_rate": 3.736666666666667e-05,
      "loss": 0.004,
      "step": 18950
    },
    {
      "epoch": 1.0112,
      "grad_norm": 0.2416277825832367,
      "learning_rate": 3.736e-05,
      "loss": 0.003,
      "step": 18960
    },
    {
      "epoch": 1.0117333333333334,
      "grad_norm": 0.4196242690086365,
      "learning_rate": 3.7353333333333334e-05,
      "loss": 0.0022,
      "step": 18970
    },
    {
      "epoch": 1.0122666666666666,
      "grad_norm": 0.2798183858394623,
      "learning_rate": 3.7346666666666666e-05,
      "loss": 0.0041,
      "step": 18980
    },
    {
      "epoch": 1.0128,
      "grad_norm": 0.2524608075618744,
      "learning_rate": 3.7340000000000005e-05,
      "loss": 0.0022,
      "step": 18990
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.27641749382019043,
      "learning_rate": 3.733333333333334e-05,
      "loss": 0.0024,
      "step": 19000
    },
    {
      "epoch": 1.0138666666666667,
      "grad_norm": 0.12878890335559845,
      "learning_rate": 3.732666666666667e-05,
      "loss": 0.0021,
      "step": 19010
    },
    {
      "epoch": 1.0144,
      "grad_norm": 0.24459528923034668,
      "learning_rate": 3.732e-05,
      "loss": 0.0024,
      "step": 19020
    },
    {
      "epoch": 1.0149333333333332,
      "grad_norm": 0.20015104115009308,
      "learning_rate": 3.7313333333333334e-05,
      "loss": 0.0024,
      "step": 19030
    },
    {
      "epoch": 1.0154666666666667,
      "grad_norm": 0.15500149130821228,
      "learning_rate": 3.7306666666666666e-05,
      "loss": 0.003,
      "step": 19040
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.15892769396305084,
      "learning_rate": 3.73e-05,
      "loss": 0.0023,
      "step": 19050
    },
    {
      "epoch": 1.0165333333333333,
      "grad_norm": 0.6661450862884521,
      "learning_rate": 3.729333333333334e-05,
      "loss": 0.0025,
      "step": 19060
    },
    {
      "epoch": 1.0170666666666666,
      "grad_norm": 0.47604814171791077,
      "learning_rate": 3.728666666666667e-05,
      "loss": 0.0021,
      "step": 19070
    },
    {
      "epoch": 1.0176,
      "grad_norm": 0.19103467464447021,
      "learning_rate": 3.728e-05,
      "loss": 0.0021,
      "step": 19080
    },
    {
      "epoch": 1.0181333333333333,
      "grad_norm": 0.07624194771051407,
      "learning_rate": 3.727333333333334e-05,
      "loss": 0.0025,
      "step": 19090
    },
    {
      "epoch": 1.0186666666666666,
      "grad_norm": 0.6602358818054199,
      "learning_rate": 3.726666666666667e-05,
      "loss": 0.0026,
      "step": 19100
    },
    {
      "epoch": 1.0192,
      "grad_norm": 0.6695899963378906,
      "learning_rate": 3.726e-05,
      "loss": 0.0044,
      "step": 19110
    },
    {
      "epoch": 1.0197333333333334,
      "grad_norm": 0.5973671078681946,
      "learning_rate": 3.725333333333333e-05,
      "loss": 0.0025,
      "step": 19120
    },
    {
      "epoch": 1.0202666666666667,
      "grad_norm": 0.48236942291259766,
      "learning_rate": 3.724666666666667e-05,
      "loss": 0.0038,
      "step": 19130
    },
    {
      "epoch": 1.0208,
      "grad_norm": 0.16151538491249084,
      "learning_rate": 3.724e-05,
      "loss": 0.002,
      "step": 19140
    },
    {
      "epoch": 1.0213333333333334,
      "grad_norm": 0.3165629804134369,
      "learning_rate": 3.7233333333333335e-05,
      "loss": 0.0032,
      "step": 19150
    },
    {
      "epoch": 1.0218666666666667,
      "grad_norm": 0.1964414268732071,
      "learning_rate": 3.7226666666666674e-05,
      "loss": 0.0024,
      "step": 19160
    },
    {
      "epoch": 1.0224,
      "grad_norm": 0.5271133780479431,
      "learning_rate": 3.722e-05,
      "loss": 0.0021,
      "step": 19170
    },
    {
      "epoch": 1.0229333333333333,
      "grad_norm": 0.3664250075817108,
      "learning_rate": 3.721333333333333e-05,
      "loss": 0.0023,
      "step": 19180
    },
    {
      "epoch": 1.0234666666666667,
      "grad_norm": 0.1429128497838974,
      "learning_rate": 3.720666666666667e-05,
      "loss": 0.0031,
      "step": 19190
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.8554267287254333,
      "learning_rate": 3.72e-05,
      "loss": 0.0035,
      "step": 19200
    },
    {
      "epoch": 1.0245333333333333,
      "grad_norm": 0.2453322857618332,
      "learning_rate": 3.7193333333333335e-05,
      "loss": 0.002,
      "step": 19210
    },
    {
      "epoch": 1.0250666666666666,
      "grad_norm": 0.39620107412338257,
      "learning_rate": 3.718666666666667e-05,
      "loss": 0.0041,
      "step": 19220
    },
    {
      "epoch": 1.0256,
      "grad_norm": 0.5228675007820129,
      "learning_rate": 3.7180000000000007e-05,
      "loss": 0.0025,
      "step": 19230
    },
    {
      "epoch": 1.0261333333333333,
      "grad_norm": 0.6812137961387634,
      "learning_rate": 3.717333333333334e-05,
      "loss": 0.0028,
      "step": 19240
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.2575854957103729,
      "learning_rate": 3.7166666666666664e-05,
      "loss": 0.0029,
      "step": 19250
    },
    {
      "epoch": 1.0272,
      "grad_norm": 0.5902405977249146,
      "learning_rate": 3.716e-05,
      "loss": 0.0026,
      "step": 19260
    },
    {
      "epoch": 1.0277333333333334,
      "grad_norm": 0.221487358212471,
      "learning_rate": 3.7153333333333336e-05,
      "loss": 0.004,
      "step": 19270
    },
    {
      "epoch": 1.0282666666666667,
      "grad_norm": 0.22512955963611603,
      "learning_rate": 3.714666666666667e-05,
      "loss": 0.0024,
      "step": 19280
    },
    {
      "epoch": 1.0288,
      "grad_norm": 0.16299624741077423,
      "learning_rate": 3.714e-05,
      "loss": 0.0031,
      "step": 19290
    },
    {
      "epoch": 1.0293333333333334,
      "grad_norm": 0.190332293510437,
      "learning_rate": 3.713333333333334e-05,
      "loss": 0.0024,
      "step": 19300
    },
    {
      "epoch": 1.0298666666666667,
      "grad_norm": 0.7669602036476135,
      "learning_rate": 3.712666666666667e-05,
      "loss": 0.0032,
      "step": 19310
    },
    {
      "epoch": 1.0304,
      "grad_norm": 0.21758127212524414,
      "learning_rate": 3.712e-05,
      "loss": 0.0027,
      "step": 19320
    },
    {
      "epoch": 1.0309333333333333,
      "grad_norm": 0.24604840576648712,
      "learning_rate": 3.7113333333333336e-05,
      "loss": 0.0021,
      "step": 19330
    },
    {
      "epoch": 1.0314666666666668,
      "grad_norm": 0.10379859060049057,
      "learning_rate": 3.710666666666667e-05,
      "loss": 0.0033,
      "step": 19340
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.1542099416255951,
      "learning_rate": 3.71e-05,
      "loss": 0.0041,
      "step": 19350
    },
    {
      "epoch": 1.0325333333333333,
      "grad_norm": 0.3708879351615906,
      "learning_rate": 3.709333333333333e-05,
      "loss": 0.0038,
      "step": 19360
    },
    {
      "epoch": 1.0330666666666666,
      "grad_norm": 0.46666842699050903,
      "learning_rate": 3.708666666666667e-05,
      "loss": 0.0024,
      "step": 19370
    },
    {
      "epoch": 1.0336,
      "grad_norm": 0.31050044298171997,
      "learning_rate": 3.7080000000000004e-05,
      "loss": 0.0038,
      "step": 19380
    },
    {
      "epoch": 1.0341333333333333,
      "grad_norm": 0.3803830146789551,
      "learning_rate": 3.7073333333333336e-05,
      "loss": 0.0024,
      "step": 19390
    },
    {
      "epoch": 1.0346666666666666,
      "grad_norm": 0.3060493469238281,
      "learning_rate": 3.706666666666667e-05,
      "loss": 0.0033,
      "step": 19400
    },
    {
      "epoch": 1.0352,
      "grad_norm": 0.25946253538131714,
      "learning_rate": 3.706e-05,
      "loss": 0.0028,
      "step": 19410
    },
    {
      "epoch": 1.0357333333333334,
      "grad_norm": 0.11533168703317642,
      "learning_rate": 3.705333333333333e-05,
      "loss": 0.003,
      "step": 19420
    },
    {
      "epoch": 1.0362666666666667,
      "grad_norm": 0.10949722677469254,
      "learning_rate": 3.7046666666666665e-05,
      "loss": 0.0023,
      "step": 19430
    },
    {
      "epoch": 1.0368,
      "grad_norm": 0.31597667932510376,
      "learning_rate": 3.7040000000000005e-05,
      "loss": 0.0023,
      "step": 19440
    },
    {
      "epoch": 1.0373333333333334,
      "grad_norm": 0.1340084969997406,
      "learning_rate": 3.703333333333334e-05,
      "loss": 0.0023,
      "step": 19450
    },
    {
      "epoch": 1.0378666666666667,
      "grad_norm": 0.41349610686302185,
      "learning_rate": 3.702666666666667e-05,
      "loss": 0.0034,
      "step": 19460
    },
    {
      "epoch": 1.0384,
      "grad_norm": 0.4110158681869507,
      "learning_rate": 3.702e-05,
      "loss": 0.0023,
      "step": 19470
    },
    {
      "epoch": 1.0389333333333333,
      "grad_norm": 0.49510806798934937,
      "learning_rate": 3.7013333333333334e-05,
      "loss": 0.0033,
      "step": 19480
    },
    {
      "epoch": 1.0394666666666668,
      "grad_norm": 0.33090031147003174,
      "learning_rate": 3.7006666666666666e-05,
      "loss": 0.0035,
      "step": 19490
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.2735815942287445,
      "learning_rate": 3.7e-05,
      "loss": 0.0026,
      "step": 19500
    },
    {
      "epoch": 1.0405333333333333,
      "grad_norm": 0.23569028079509735,
      "learning_rate": 3.699333333333334e-05,
      "loss": 0.004,
      "step": 19510
    },
    {
      "epoch": 1.0410666666666666,
      "grad_norm": 0.23868991434574127,
      "learning_rate": 3.698666666666667e-05,
      "loss": 0.0018,
      "step": 19520
    },
    {
      "epoch": 1.0416,
      "grad_norm": 0.3348454236984253,
      "learning_rate": 3.698e-05,
      "loss": 0.003,
      "step": 19530
    },
    {
      "epoch": 1.0421333333333334,
      "grad_norm": 0.9266185760498047,
      "learning_rate": 3.697333333333334e-05,
      "loss": 0.0044,
      "step": 19540
    },
    {
      "epoch": 1.0426666666666666,
      "grad_norm": 0.2573327124118805,
      "learning_rate": 3.6966666666666666e-05,
      "loss": 0.0027,
      "step": 19550
    },
    {
      "epoch": 1.0432,
      "grad_norm": 0.387766033411026,
      "learning_rate": 3.696e-05,
      "loss": 0.0021,
      "step": 19560
    },
    {
      "epoch": 1.0437333333333334,
      "grad_norm": 0.38037315011024475,
      "learning_rate": 3.695333333333334e-05,
      "loss": 0.0035,
      "step": 19570
    },
    {
      "epoch": 1.0442666666666667,
      "grad_norm": 0.09907719492912292,
      "learning_rate": 3.694666666666667e-05,
      "loss": 0.0022,
      "step": 19580
    },
    {
      "epoch": 1.0448,
      "grad_norm": 0.32504141330718994,
      "learning_rate": 3.694e-05,
      "loss": 0.0033,
      "step": 19590
    },
    {
      "epoch": 1.0453333333333332,
      "grad_norm": 0.41895362734794617,
      "learning_rate": 3.6933333333333334e-05,
      "loss": 0.0027,
      "step": 19600
    },
    {
      "epoch": 1.0458666666666667,
      "grad_norm": 0.23489339649677277,
      "learning_rate": 3.6926666666666673e-05,
      "loss": 0.0026,
      "step": 19610
    },
    {
      "epoch": 1.0464,
      "grad_norm": 0.7746996283531189,
      "learning_rate": 3.692e-05,
      "loss": 0.0022,
      "step": 19620
    },
    {
      "epoch": 1.0469333333333333,
      "grad_norm": 0.4833386242389679,
      "learning_rate": 3.691333333333333e-05,
      "loss": 0.002,
      "step": 19630
    },
    {
      "epoch": 1.0474666666666668,
      "grad_norm": 1.1767573356628418,
      "learning_rate": 3.690666666666667e-05,
      "loss": 0.0027,
      "step": 19640
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.1451767235994339,
      "learning_rate": 3.69e-05,
      "loss": 0.0037,
      "step": 19650
    },
    {
      "epoch": 1.0485333333333333,
      "grad_norm": 0.12717103958129883,
      "learning_rate": 3.6893333333333335e-05,
      "loss": 0.003,
      "step": 19660
    },
    {
      "epoch": 1.0490666666666666,
      "grad_norm": 0.8546773791313171,
      "learning_rate": 3.688666666666667e-05,
      "loss": 0.0023,
      "step": 19670
    },
    {
      "epoch": 1.0496,
      "grad_norm": 0.328969806432724,
      "learning_rate": 3.6880000000000006e-05,
      "loss": 0.003,
      "step": 19680
    },
    {
      "epoch": 1.0501333333333334,
      "grad_norm": 0.5637457370758057,
      "learning_rate": 3.687333333333334e-05,
      "loss": 0.0026,
      "step": 19690
    },
    {
      "epoch": 1.0506666666666666,
      "grad_norm": 0.5788878202438354,
      "learning_rate": 3.6866666666666664e-05,
      "loss": 0.0029,
      "step": 19700
    },
    {
      "epoch": 1.0512,
      "grad_norm": 0.07298321276903152,
      "learning_rate": 3.686e-05,
      "loss": 0.0025,
      "step": 19710
    },
    {
      "epoch": 1.0517333333333334,
      "grad_norm": 0.16346876323223114,
      "learning_rate": 3.6853333333333335e-05,
      "loss": 0.0027,
      "step": 19720
    },
    {
      "epoch": 1.0522666666666667,
      "grad_norm": 0.2212025374174118,
      "learning_rate": 3.684666666666667e-05,
      "loss": 0.003,
      "step": 19730
    },
    {
      "epoch": 1.0528,
      "grad_norm": 0.5057135224342346,
      "learning_rate": 3.684e-05,
      "loss": 0.0028,
      "step": 19740
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.30020666122436523,
      "learning_rate": 3.683333333333334e-05,
      "loss": 0.002,
      "step": 19750
    },
    {
      "epoch": 1.0538666666666667,
      "grad_norm": 0.6165010929107666,
      "learning_rate": 3.682666666666667e-05,
      "loss": 0.0025,
      "step": 19760
    },
    {
      "epoch": 1.0544,
      "grad_norm": 0.3665124177932739,
      "learning_rate": 3.682e-05,
      "loss": 0.0037,
      "step": 19770
    },
    {
      "epoch": 1.0549333333333333,
      "grad_norm": 0.5077183842658997,
      "learning_rate": 3.6813333333333335e-05,
      "loss": 0.0021,
      "step": 19780
    },
    {
      "epoch": 1.0554666666666668,
      "grad_norm": 0.09215310215950012,
      "learning_rate": 3.680666666666667e-05,
      "loss": 0.0038,
      "step": 19790
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.6123944520950317,
      "learning_rate": 3.68e-05,
      "loss": 0.003,
      "step": 19800
    },
    {
      "epoch": 1.0565333333333333,
      "grad_norm": 0.39722004532814026,
      "learning_rate": 3.679333333333333e-05,
      "loss": 0.0023,
      "step": 19810
    },
    {
      "epoch": 1.0570666666666666,
      "grad_norm": 0.5259652137756348,
      "learning_rate": 3.678666666666667e-05,
      "loss": 0.0028,
      "step": 19820
    },
    {
      "epoch": 1.0576,
      "grad_norm": 0.34796538949012756,
      "learning_rate": 3.6780000000000004e-05,
      "loss": 0.0029,
      "step": 19830
    },
    {
      "epoch": 1.0581333333333334,
      "grad_norm": 0.4456743001937866,
      "learning_rate": 3.6773333333333336e-05,
      "loss": 0.0041,
      "step": 19840
    },
    {
      "epoch": 1.0586666666666666,
      "grad_norm": 0.15163108706474304,
      "learning_rate": 3.676666666666667e-05,
      "loss": 0.0022,
      "step": 19850
    },
    {
      "epoch": 1.0592,
      "grad_norm": 0.2463807314634323,
      "learning_rate": 3.676e-05,
      "loss": 0.0029,
      "step": 19860
    },
    {
      "epoch": 1.0597333333333334,
      "grad_norm": 0.30344951152801514,
      "learning_rate": 3.675333333333333e-05,
      "loss": 0.0032,
      "step": 19870
    },
    {
      "epoch": 1.0602666666666667,
      "grad_norm": 0.2850179076194763,
      "learning_rate": 3.6746666666666665e-05,
      "loss": 0.0029,
      "step": 19880
    },
    {
      "epoch": 1.0608,
      "grad_norm": 0.11384635418653488,
      "learning_rate": 3.6740000000000004e-05,
      "loss": 0.0032,
      "step": 19890
    },
    {
      "epoch": 1.0613333333333332,
      "grad_norm": 0.12952403724193573,
      "learning_rate": 3.6733333333333336e-05,
      "loss": 0.0027,
      "step": 19900
    },
    {
      "epoch": 1.0618666666666667,
      "grad_norm": 0.3024609386920929,
      "learning_rate": 3.672666666666667e-05,
      "loss": 0.003,
      "step": 19910
    },
    {
      "epoch": 1.0624,
      "grad_norm": 0.13304859399795532,
      "learning_rate": 3.672000000000001e-05,
      "loss": 0.0026,
      "step": 19920
    },
    {
      "epoch": 1.0629333333333333,
      "grad_norm": 0.669586181640625,
      "learning_rate": 3.671333333333333e-05,
      "loss": 0.0022,
      "step": 19930
    },
    {
      "epoch": 1.0634666666666668,
      "grad_norm": 0.6739941835403442,
      "learning_rate": 3.6706666666666665e-05,
      "loss": 0.0027,
      "step": 19940
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.1432826668024063,
      "learning_rate": 3.6700000000000004e-05,
      "loss": 0.0031,
      "step": 19950
    },
    {
      "epoch": 1.0645333333333333,
      "grad_norm": 0.18708640336990356,
      "learning_rate": 3.669333333333334e-05,
      "loss": 0.0037,
      "step": 19960
    },
    {
      "epoch": 1.0650666666666666,
      "grad_norm": 0.11727412045001984,
      "learning_rate": 3.668666666666667e-05,
      "loss": 0.0021,
      "step": 19970
    },
    {
      "epoch": 1.0656,
      "grad_norm": 0.4237981140613556,
      "learning_rate": 3.668e-05,
      "loss": 0.0031,
      "step": 19980
    },
    {
      "epoch": 1.0661333333333334,
      "grad_norm": 0.39689648151397705,
      "learning_rate": 3.667333333333334e-05,
      "loss": 0.0027,
      "step": 19990
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.08536132425069809,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.0022,
      "step": 20000
    },
    {
      "epoch": 1.0672,
      "grad_norm": 0.20341607928276062,
      "learning_rate": 3.666e-05,
      "loss": 0.0029,
      "step": 20010
    },
    {
      "epoch": 1.0677333333333334,
      "grad_norm": 0.23386545479297638,
      "learning_rate": 3.665333333333334e-05,
      "loss": 0.0027,
      "step": 20020
    },
    {
      "epoch": 1.0682666666666667,
      "grad_norm": 0.7069717049598694,
      "learning_rate": 3.664666666666667e-05,
      "loss": 0.0022,
      "step": 20030
    },
    {
      "epoch": 1.0688,
      "grad_norm": 0.2878054082393646,
      "learning_rate": 3.664e-05,
      "loss": 0.0024,
      "step": 20040
    },
    {
      "epoch": 1.0693333333333332,
      "grad_norm": 0.5633187890052795,
      "learning_rate": 3.6633333333333334e-05,
      "loss": 0.0025,
      "step": 20050
    },
    {
      "epoch": 1.0698666666666667,
      "grad_norm": 0.1320330947637558,
      "learning_rate": 3.662666666666667e-05,
      "loss": 0.0026,
      "step": 20060
    },
    {
      "epoch": 1.0704,
      "grad_norm": 0.29136788845062256,
      "learning_rate": 3.6620000000000005e-05,
      "loss": 0.0019,
      "step": 20070
    },
    {
      "epoch": 1.0709333333333333,
      "grad_norm": 0.12004657834768295,
      "learning_rate": 3.661333333333333e-05,
      "loss": 0.0023,
      "step": 20080
    },
    {
      "epoch": 1.0714666666666666,
      "grad_norm": 0.24824786186218262,
      "learning_rate": 3.660666666666667e-05,
      "loss": 0.0038,
      "step": 20090
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.2726999819278717,
      "learning_rate": 3.66e-05,
      "loss": 0.0025,
      "step": 20100
    },
    {
      "epoch": 1.0725333333333333,
      "grad_norm": 0.3147783875465393,
      "learning_rate": 3.6593333333333334e-05,
      "loss": 0.0036,
      "step": 20110
    },
    {
      "epoch": 1.0730666666666666,
      "grad_norm": 0.4435868263244629,
      "learning_rate": 3.6586666666666666e-05,
      "loss": 0.0025,
      "step": 20120
    },
    {
      "epoch": 1.0735999999999999,
      "grad_norm": 0.21627956628799438,
      "learning_rate": 3.6580000000000006e-05,
      "loss": 0.002,
      "step": 20130
    },
    {
      "epoch": 1.0741333333333334,
      "grad_norm": 0.11908580362796783,
      "learning_rate": 3.657333333333334e-05,
      "loss": 0.004,
      "step": 20140
    },
    {
      "epoch": 1.0746666666666667,
      "grad_norm": 0.221422016620636,
      "learning_rate": 3.656666666666666e-05,
      "loss": 0.0025,
      "step": 20150
    },
    {
      "epoch": 1.0752,
      "grad_norm": 0.24706430733203888,
      "learning_rate": 3.656e-05,
      "loss": 0.0027,
      "step": 20160
    },
    {
      "epoch": 1.0757333333333334,
      "grad_norm": 0.49458345770835876,
      "learning_rate": 3.6553333333333335e-05,
      "loss": 0.0021,
      "step": 20170
    },
    {
      "epoch": 1.0762666666666667,
      "grad_norm": 0.19289223849773407,
      "learning_rate": 3.654666666666667e-05,
      "loss": 0.003,
      "step": 20180
    },
    {
      "epoch": 1.0768,
      "grad_norm": 0.07346731424331665,
      "learning_rate": 3.654e-05,
      "loss": 0.0025,
      "step": 20190
    },
    {
      "epoch": 1.0773333333333333,
      "grad_norm": 0.1717936396598816,
      "learning_rate": 3.653333333333334e-05,
      "loss": 0.0029,
      "step": 20200
    },
    {
      "epoch": 1.0778666666666668,
      "grad_norm": 0.2565810978412628,
      "learning_rate": 3.652666666666667e-05,
      "loss": 0.0025,
      "step": 20210
    },
    {
      "epoch": 1.0784,
      "grad_norm": 0.5062713623046875,
      "learning_rate": 3.652e-05,
      "loss": 0.0029,
      "step": 20220
    },
    {
      "epoch": 1.0789333333333333,
      "grad_norm": 0.20333829522132874,
      "learning_rate": 3.6513333333333335e-05,
      "loss": 0.0031,
      "step": 20230
    },
    {
      "epoch": 1.0794666666666666,
      "grad_norm": 0.13685712218284607,
      "learning_rate": 3.650666666666667e-05,
      "loss": 0.0026,
      "step": 20240
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.3401612937450409,
      "learning_rate": 3.65e-05,
      "loss": 0.003,
      "step": 20250
    },
    {
      "epoch": 1.0805333333333333,
      "grad_norm": 0.49876585602760315,
      "learning_rate": 3.649333333333333e-05,
      "loss": 0.0026,
      "step": 20260
    },
    {
      "epoch": 1.0810666666666666,
      "grad_norm": 0.5435515642166138,
      "learning_rate": 3.648666666666667e-05,
      "loss": 0.0024,
      "step": 20270
    },
    {
      "epoch": 1.0816,
      "grad_norm": 0.7404513359069824,
      "learning_rate": 3.648e-05,
      "loss": 0.0024,
      "step": 20280
    },
    {
      "epoch": 1.0821333333333334,
      "grad_norm": 0.3588607609272003,
      "learning_rate": 3.6473333333333335e-05,
      "loss": 0.0027,
      "step": 20290
    },
    {
      "epoch": 1.0826666666666667,
      "grad_norm": 0.2143900841474533,
      "learning_rate": 3.646666666666667e-05,
      "loss": 0.0037,
      "step": 20300
    },
    {
      "epoch": 1.0832,
      "grad_norm": 0.14140261709690094,
      "learning_rate": 3.646e-05,
      "loss": 0.0023,
      "step": 20310
    },
    {
      "epoch": 1.0837333333333334,
      "grad_norm": 0.14434075355529785,
      "learning_rate": 3.645333333333333e-05,
      "loss": 0.003,
      "step": 20320
    },
    {
      "epoch": 1.0842666666666667,
      "grad_norm": 0.26161473989486694,
      "learning_rate": 3.644666666666667e-05,
      "loss": 0.0047,
      "step": 20330
    },
    {
      "epoch": 1.0848,
      "grad_norm": 0.5011413097381592,
      "learning_rate": 3.6440000000000003e-05,
      "loss": 0.003,
      "step": 20340
    },
    {
      "epoch": 1.0853333333333333,
      "grad_norm": 0.46809497475624084,
      "learning_rate": 3.6433333333333336e-05,
      "loss": 0.0021,
      "step": 20350
    },
    {
      "epoch": 1.0858666666666668,
      "grad_norm": 0.41394925117492676,
      "learning_rate": 3.642666666666667e-05,
      "loss": 0.002,
      "step": 20360
    },
    {
      "epoch": 1.0864,
      "grad_norm": 0.11020641773939133,
      "learning_rate": 3.642000000000001e-05,
      "loss": 0.0025,
      "step": 20370
    },
    {
      "epoch": 1.0869333333333333,
      "grad_norm": 0.3068121373653412,
      "learning_rate": 3.641333333333333e-05,
      "loss": 0.0039,
      "step": 20380
    },
    {
      "epoch": 1.0874666666666666,
      "grad_norm": 0.9868955016136169,
      "learning_rate": 3.6406666666666665e-05,
      "loss": 0.0027,
      "step": 20390
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.1469345986843109,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 0.0041,
      "step": 20400
    },
    {
      "epoch": 1.0885333333333334,
      "grad_norm": 0.13149064779281616,
      "learning_rate": 3.6393333333333336e-05,
      "loss": 0.0034,
      "step": 20410
    },
    {
      "epoch": 1.0890666666666666,
      "grad_norm": 0.31423911452293396,
      "learning_rate": 3.638666666666667e-05,
      "loss": 0.0041,
      "step": 20420
    },
    {
      "epoch": 1.0896,
      "grad_norm": 0.46605509519577026,
      "learning_rate": 3.638e-05,
      "loss": 0.0022,
      "step": 20430
    },
    {
      "epoch": 1.0901333333333334,
      "grad_norm": 0.28869378566741943,
      "learning_rate": 3.637333333333334e-05,
      "loss": 0.0026,
      "step": 20440
    },
    {
      "epoch": 1.0906666666666667,
      "grad_norm": 0.576442301273346,
      "learning_rate": 3.636666666666667e-05,
      "loss": 0.0027,
      "step": 20450
    },
    {
      "epoch": 1.0912,
      "grad_norm": 0.26705583930015564,
      "learning_rate": 3.636e-05,
      "loss": 0.0031,
      "step": 20460
    },
    {
      "epoch": 1.0917333333333334,
      "grad_norm": 0.41909560561180115,
      "learning_rate": 3.6353333333333337e-05,
      "loss": 0.0035,
      "step": 20470
    },
    {
      "epoch": 1.0922666666666667,
      "grad_norm": 0.6166057586669922,
      "learning_rate": 3.634666666666667e-05,
      "loss": 0.0023,
      "step": 20480
    },
    {
      "epoch": 1.0928,
      "grad_norm": 0.45643842220306396,
      "learning_rate": 3.634e-05,
      "loss": 0.0025,
      "step": 20490
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.4573579430580139,
      "learning_rate": 3.633333333333333e-05,
      "loss": 0.0022,
      "step": 20500
    },
    {
      "epoch": 1.0938666666666668,
      "grad_norm": 0.601517379283905,
      "learning_rate": 3.632666666666667e-05,
      "loss": 0.0022,
      "step": 20510
    },
    {
      "epoch": 1.0944,
      "grad_norm": 0.5486723184585571,
      "learning_rate": 3.6320000000000005e-05,
      "loss": 0.0036,
      "step": 20520
    },
    {
      "epoch": 1.0949333333333333,
      "grad_norm": 0.43297359347343445,
      "learning_rate": 3.631333333333333e-05,
      "loss": 0.0043,
      "step": 20530
    },
    {
      "epoch": 1.0954666666666666,
      "grad_norm": 0.7766885161399841,
      "learning_rate": 3.630666666666667e-05,
      "loss": 0.0022,
      "step": 20540
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.7116096019744873,
      "learning_rate": 3.63e-05,
      "loss": 0.0025,
      "step": 20550
    },
    {
      "epoch": 1.0965333333333334,
      "grad_norm": 0.4095103442668915,
      "learning_rate": 3.6293333333333334e-05,
      "loss": 0.0031,
      "step": 20560
    },
    {
      "epoch": 1.0970666666666666,
      "grad_norm": 0.14134876430034637,
      "learning_rate": 3.6286666666666666e-05,
      "loss": 0.0028,
      "step": 20570
    },
    {
      "epoch": 1.0976,
      "grad_norm": 0.2173927128314972,
      "learning_rate": 3.6280000000000005e-05,
      "loss": 0.0028,
      "step": 20580
    },
    {
      "epoch": 1.0981333333333334,
      "grad_norm": 0.6754743456840515,
      "learning_rate": 3.627333333333334e-05,
      "loss": 0.0051,
      "step": 20590
    },
    {
      "epoch": 1.0986666666666667,
      "grad_norm": 0.5424152612686157,
      "learning_rate": 3.626666666666667e-05,
      "loss": 0.0023,
      "step": 20600
    },
    {
      "epoch": 1.0992,
      "grad_norm": 0.5383650064468384,
      "learning_rate": 3.626e-05,
      "loss": 0.0028,
      "step": 20610
    },
    {
      "epoch": 1.0997333333333332,
      "grad_norm": 0.7483442425727844,
      "learning_rate": 3.6253333333333334e-05,
      "loss": 0.0023,
      "step": 20620
    },
    {
      "epoch": 1.1002666666666667,
      "grad_norm": 0.5987154245376587,
      "learning_rate": 3.6246666666666666e-05,
      "loss": 0.0022,
      "step": 20630
    },
    {
      "epoch": 1.1008,
      "grad_norm": 0.0974687710404396,
      "learning_rate": 3.624e-05,
      "loss": 0.0035,
      "step": 20640
    },
    {
      "epoch": 1.1013333333333333,
      "grad_norm": 0.1305803507566452,
      "learning_rate": 3.623333333333334e-05,
      "loss": 0.0027,
      "step": 20650
    },
    {
      "epoch": 1.1018666666666665,
      "grad_norm": 0.48738372325897217,
      "learning_rate": 3.622666666666667e-05,
      "loss": 0.0028,
      "step": 20660
    },
    {
      "epoch": 1.1024,
      "grad_norm": 0.7478143572807312,
      "learning_rate": 3.622e-05,
      "loss": 0.0033,
      "step": 20670
    },
    {
      "epoch": 1.1029333333333333,
      "grad_norm": 0.38397473096847534,
      "learning_rate": 3.6213333333333334e-05,
      "loss": 0.0036,
      "step": 20680
    },
    {
      "epoch": 1.1034666666666666,
      "grad_norm": 0.28720417618751526,
      "learning_rate": 3.620666666666667e-05,
      "loss": 0.0024,
      "step": 20690
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.23648697137832642,
      "learning_rate": 3.62e-05,
      "loss": 0.0029,
      "step": 20700
    },
    {
      "epoch": 1.1045333333333334,
      "grad_norm": 0.32471147179603577,
      "learning_rate": 3.619333333333333e-05,
      "loss": 0.0023,
      "step": 20710
    },
    {
      "epoch": 1.1050666666666666,
      "grad_norm": 0.2620432674884796,
      "learning_rate": 3.618666666666667e-05,
      "loss": 0.0023,
      "step": 20720
    },
    {
      "epoch": 1.1056,
      "grad_norm": 0.7336317896842957,
      "learning_rate": 3.618e-05,
      "loss": 0.0041,
      "step": 20730
    },
    {
      "epoch": 1.1061333333333334,
      "grad_norm": 0.17337806522846222,
      "learning_rate": 3.6173333333333335e-05,
      "loss": 0.0022,
      "step": 20740
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.38416096568107605,
      "learning_rate": 3.6166666666666674e-05,
      "loss": 0.003,
      "step": 20750
    },
    {
      "epoch": 1.1072,
      "grad_norm": 0.4600337743759155,
      "learning_rate": 3.616e-05,
      "loss": 0.0026,
      "step": 20760
    },
    {
      "epoch": 1.1077333333333332,
      "grad_norm": 0.2381359338760376,
      "learning_rate": 3.615333333333333e-05,
      "loss": 0.0026,
      "step": 20770
    },
    {
      "epoch": 1.1082666666666667,
      "grad_norm": 0.24913868308067322,
      "learning_rate": 3.614666666666667e-05,
      "loss": 0.002,
      "step": 20780
    },
    {
      "epoch": 1.1088,
      "grad_norm": 0.3506380617618561,
      "learning_rate": 3.614e-05,
      "loss": 0.0027,
      "step": 20790
    },
    {
      "epoch": 1.1093333333333333,
      "grad_norm": 0.19667546451091766,
      "learning_rate": 3.6133333333333335e-05,
      "loss": 0.0024,
      "step": 20800
    },
    {
      "epoch": 1.1098666666666666,
      "grad_norm": 0.25483036041259766,
      "learning_rate": 3.612666666666667e-05,
      "loss": 0.0025,
      "step": 20810
    },
    {
      "epoch": 1.1104,
      "grad_norm": 0.23220069706439972,
      "learning_rate": 3.6120000000000007e-05,
      "loss": 0.002,
      "step": 20820
    },
    {
      "epoch": 1.1109333333333333,
      "grad_norm": 0.11845098435878754,
      "learning_rate": 3.611333333333333e-05,
      "loss": 0.0027,
      "step": 20830
    },
    {
      "epoch": 1.1114666666666666,
      "grad_norm": 0.2991608679294586,
      "learning_rate": 3.6106666666666664e-05,
      "loss": 0.0018,
      "step": 20840
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.26345962285995483,
      "learning_rate": 3.61e-05,
      "loss": 0.002,
      "step": 20850
    },
    {
      "epoch": 1.1125333333333334,
      "grad_norm": 0.7399133443832397,
      "learning_rate": 3.6093333333333336e-05,
      "loss": 0.0029,
      "step": 20860
    },
    {
      "epoch": 1.1130666666666666,
      "grad_norm": 0.1381480097770691,
      "learning_rate": 3.608666666666667e-05,
      "loss": 0.0024,
      "step": 20870
    },
    {
      "epoch": 1.1136,
      "grad_norm": 0.26097798347473145,
      "learning_rate": 3.608e-05,
      "loss": 0.0024,
      "step": 20880
    },
    {
      "epoch": 1.1141333333333334,
      "grad_norm": 0.37008610367774963,
      "learning_rate": 3.607333333333334e-05,
      "loss": 0.0027,
      "step": 20890
    },
    {
      "epoch": 1.1146666666666667,
      "grad_norm": 0.23349377512931824,
      "learning_rate": 3.606666666666667e-05,
      "loss": 0.0022,
      "step": 20900
    },
    {
      "epoch": 1.1152,
      "grad_norm": 0.36240366101264954,
      "learning_rate": 3.606e-05,
      "loss": 0.0026,
      "step": 20910
    },
    {
      "epoch": 1.1157333333333332,
      "grad_norm": 0.1154487207531929,
      "learning_rate": 3.6053333333333336e-05,
      "loss": 0.0031,
      "step": 20920
    },
    {
      "epoch": 1.1162666666666667,
      "grad_norm": 0.10976516455411911,
      "learning_rate": 3.604666666666667e-05,
      "loss": 0.003,
      "step": 20930
    },
    {
      "epoch": 1.1168,
      "grad_norm": 0.1948372721672058,
      "learning_rate": 3.604e-05,
      "loss": 0.0024,
      "step": 20940
    },
    {
      "epoch": 1.1173333333333333,
      "grad_norm": 0.1559678465127945,
      "learning_rate": 3.603333333333333e-05,
      "loss": 0.0023,
      "step": 20950
    },
    {
      "epoch": 1.1178666666666666,
      "grad_norm": 0.3559519946575165,
      "learning_rate": 3.602666666666667e-05,
      "loss": 0.0028,
      "step": 20960
    },
    {
      "epoch": 1.1184,
      "grad_norm": 0.13955387473106384,
      "learning_rate": 3.6020000000000004e-05,
      "loss": 0.0028,
      "step": 20970
    },
    {
      "epoch": 1.1189333333333333,
      "grad_norm": 0.7633119225502014,
      "learning_rate": 3.6013333333333336e-05,
      "loss": 0.0031,
      "step": 20980
    },
    {
      "epoch": 1.1194666666666666,
      "grad_norm": 0.18601129949092865,
      "learning_rate": 3.600666666666667e-05,
      "loss": 0.0033,
      "step": 20990
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.5849977731704712,
      "learning_rate": 3.6e-05,
      "loss": 0.0031,
      "step": 21000
    },
    {
      "epoch": 1.1205333333333334,
      "grad_norm": 0.23898755013942719,
      "learning_rate": 3.599333333333333e-05,
      "loss": 0.0031,
      "step": 21010
    },
    {
      "epoch": 1.1210666666666667,
      "grad_norm": 0.355123907327652,
      "learning_rate": 3.5986666666666665e-05,
      "loss": 0.0038,
      "step": 21020
    },
    {
      "epoch": 1.1216,
      "grad_norm": 0.191856250166893,
      "learning_rate": 3.5980000000000004e-05,
      "loss": 0.0044,
      "step": 21030
    },
    {
      "epoch": 1.1221333333333334,
      "grad_norm": 0.3729974627494812,
      "learning_rate": 3.597333333333334e-05,
      "loss": 0.0022,
      "step": 21040
    },
    {
      "epoch": 1.1226666666666667,
      "grad_norm": 0.452380895614624,
      "learning_rate": 3.596666666666667e-05,
      "loss": 0.0031,
      "step": 21050
    },
    {
      "epoch": 1.1232,
      "grad_norm": 0.23120571672916412,
      "learning_rate": 3.596e-05,
      "loss": 0.004,
      "step": 21060
    },
    {
      "epoch": 1.1237333333333333,
      "grad_norm": 0.5805094242095947,
      "learning_rate": 3.5953333333333334e-05,
      "loss": 0.0027,
      "step": 21070
    },
    {
      "epoch": 1.1242666666666667,
      "grad_norm": 0.3981998562812805,
      "learning_rate": 3.5946666666666666e-05,
      "loss": 0.0022,
      "step": 21080
    },
    {
      "epoch": 1.1248,
      "grad_norm": 0.3646317720413208,
      "learning_rate": 3.594e-05,
      "loss": 0.0041,
      "step": 21090
    },
    {
      "epoch": 1.1253333333333333,
      "grad_norm": 0.34019604325294495,
      "learning_rate": 3.593333333333334e-05,
      "loss": 0.0032,
      "step": 21100
    },
    {
      "epoch": 1.1258666666666666,
      "grad_norm": 0.1753590852022171,
      "learning_rate": 3.592666666666667e-05,
      "loss": 0.0023,
      "step": 21110
    },
    {
      "epoch": 1.1264,
      "grad_norm": 0.32807764410972595,
      "learning_rate": 3.592e-05,
      "loss": 0.0028,
      "step": 21120
    },
    {
      "epoch": 1.1269333333333333,
      "grad_norm": 0.2806938588619232,
      "learning_rate": 3.591333333333334e-05,
      "loss": 0.0024,
      "step": 21130
    },
    {
      "epoch": 1.1274666666666666,
      "grad_norm": 0.60869300365448,
      "learning_rate": 3.5906666666666666e-05,
      "loss": 0.0034,
      "step": 21140
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.33180293440818787,
      "learning_rate": 3.59e-05,
      "loss": 0.0022,
      "step": 21150
    },
    {
      "epoch": 1.1285333333333334,
      "grad_norm": 0.6717820167541504,
      "learning_rate": 3.589333333333334e-05,
      "loss": 0.0029,
      "step": 21160
    },
    {
      "epoch": 1.1290666666666667,
      "grad_norm": 0.15701843798160553,
      "learning_rate": 3.588666666666667e-05,
      "loss": 0.003,
      "step": 21170
    },
    {
      "epoch": 1.1296,
      "grad_norm": 0.4930213987827301,
      "learning_rate": 3.588e-05,
      "loss": 0.0021,
      "step": 21180
    },
    {
      "epoch": 1.1301333333333332,
      "grad_norm": 0.6690936088562012,
      "learning_rate": 3.5873333333333334e-05,
      "loss": 0.0022,
      "step": 21190
    },
    {
      "epoch": 1.1306666666666667,
      "grad_norm": 0.6779159307479858,
      "learning_rate": 3.586666666666667e-05,
      "loss": 0.0026,
      "step": 21200
    },
    {
      "epoch": 1.1312,
      "grad_norm": 0.42029228806495667,
      "learning_rate": 3.586e-05,
      "loss": 0.0022,
      "step": 21210
    },
    {
      "epoch": 1.1317333333333333,
      "grad_norm": 0.6235911250114441,
      "learning_rate": 3.585333333333333e-05,
      "loss": 0.0033,
      "step": 21220
    },
    {
      "epoch": 1.1322666666666668,
      "grad_norm": 0.7511329650878906,
      "learning_rate": 3.584666666666667e-05,
      "loss": 0.0035,
      "step": 21230
    },
    {
      "epoch": 1.1328,
      "grad_norm": 0.07689958810806274,
      "learning_rate": 3.584e-05,
      "loss": 0.0028,
      "step": 21240
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.15137441456317902,
      "learning_rate": 3.5833333333333335e-05,
      "loss": 0.0026,
      "step": 21250
    },
    {
      "epoch": 1.1338666666666666,
      "grad_norm": 0.6401317715644836,
      "learning_rate": 3.582666666666667e-05,
      "loss": 0.002,
      "step": 21260
    },
    {
      "epoch": 1.1344,
      "grad_norm": 0.5322428345680237,
      "learning_rate": 3.5820000000000006e-05,
      "loss": 0.0022,
      "step": 21270
    },
    {
      "epoch": 1.1349333333333333,
      "grad_norm": 0.3520989418029785,
      "learning_rate": 3.581333333333334e-05,
      "loss": 0.0025,
      "step": 21280
    },
    {
      "epoch": 1.1354666666666666,
      "grad_norm": 0.1295597404241562,
      "learning_rate": 3.5806666666666664e-05,
      "loss": 0.0028,
      "step": 21290
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.42819154262542725,
      "learning_rate": 3.58e-05,
      "loss": 0.0023,
      "step": 21300
    },
    {
      "epoch": 1.1365333333333334,
      "grad_norm": 0.30864205956459045,
      "learning_rate": 3.5793333333333335e-05,
      "loss": 0.0025,
      "step": 21310
    },
    {
      "epoch": 1.1370666666666667,
      "grad_norm": 0.4059179425239563,
      "learning_rate": 3.578666666666667e-05,
      "loss": 0.003,
      "step": 21320
    },
    {
      "epoch": 1.1376,
      "grad_norm": 0.3607136905193329,
      "learning_rate": 3.578e-05,
      "loss": 0.0036,
      "step": 21330
    },
    {
      "epoch": 1.1381333333333332,
      "grad_norm": 0.5015813112258911,
      "learning_rate": 3.577333333333334e-05,
      "loss": 0.0035,
      "step": 21340
    },
    {
      "epoch": 1.1386666666666667,
      "grad_norm": 0.14680002629756927,
      "learning_rate": 3.576666666666667e-05,
      "loss": 0.003,
      "step": 21350
    },
    {
      "epoch": 1.1392,
      "grad_norm": 0.17743298411369324,
      "learning_rate": 3.5759999999999996e-05,
      "loss": 0.0037,
      "step": 21360
    },
    {
      "epoch": 1.1397333333333333,
      "grad_norm": 0.21253696084022522,
      "learning_rate": 3.5753333333333335e-05,
      "loss": 0.003,
      "step": 21370
    },
    {
      "epoch": 1.1402666666666668,
      "grad_norm": 0.48704397678375244,
      "learning_rate": 3.574666666666667e-05,
      "loss": 0.0028,
      "step": 21380
    },
    {
      "epoch": 1.1408,
      "grad_norm": 0.10183794796466827,
      "learning_rate": 3.574e-05,
      "loss": 0.0019,
      "step": 21390
    },
    {
      "epoch": 1.1413333333333333,
      "grad_norm": 0.24268612265586853,
      "learning_rate": 3.573333333333333e-05,
      "loss": 0.0021,
      "step": 21400
    },
    {
      "epoch": 1.1418666666666666,
      "grad_norm": 0.08959940820932388,
      "learning_rate": 3.572666666666667e-05,
      "loss": 0.0018,
      "step": 21410
    },
    {
      "epoch": 1.1424,
      "grad_norm": 0.8980046510696411,
      "learning_rate": 3.5720000000000004e-05,
      "loss": 0.0025,
      "step": 21420
    },
    {
      "epoch": 1.1429333333333334,
      "grad_norm": 0.6482993364334106,
      "learning_rate": 3.5713333333333336e-05,
      "loss": 0.0042,
      "step": 21430
    },
    {
      "epoch": 1.1434666666666666,
      "grad_norm": 0.18874287605285645,
      "learning_rate": 3.570666666666667e-05,
      "loss": 0.0046,
      "step": 21440
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.2558515667915344,
      "learning_rate": 3.57e-05,
      "loss": 0.0029,
      "step": 21450
    },
    {
      "epoch": 1.1445333333333334,
      "grad_norm": 0.09535713493824005,
      "learning_rate": 3.569333333333333e-05,
      "loss": 0.0028,
      "step": 21460
    },
    {
      "epoch": 1.1450666666666667,
      "grad_norm": 0.6676490902900696,
      "learning_rate": 3.5686666666666665e-05,
      "loss": 0.0022,
      "step": 21470
    },
    {
      "epoch": 1.1456,
      "grad_norm": 0.42959126830101013,
      "learning_rate": 3.5680000000000004e-05,
      "loss": 0.0027,
      "step": 21480
    },
    {
      "epoch": 1.1461333333333332,
      "grad_norm": 0.35554927587509155,
      "learning_rate": 3.5673333333333336e-05,
      "loss": 0.0024,
      "step": 21490
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.3492910861968994,
      "learning_rate": 3.566666666666667e-05,
      "loss": 0.0024,
      "step": 21500
    },
    {
      "epoch": 1.1472,
      "grad_norm": 0.13700595498085022,
      "learning_rate": 3.566e-05,
      "loss": 0.003,
      "step": 21510
    },
    {
      "epoch": 1.1477333333333333,
      "grad_norm": 0.24124161899089813,
      "learning_rate": 3.565333333333333e-05,
      "loss": 0.0026,
      "step": 21520
    },
    {
      "epoch": 1.1482666666666668,
      "grad_norm": 0.29022014141082764,
      "learning_rate": 3.5646666666666665e-05,
      "loss": 0.003,
      "step": 21530
    },
    {
      "epoch": 1.1488,
      "grad_norm": 0.14538884162902832,
      "learning_rate": 3.5640000000000004e-05,
      "loss": 0.0027,
      "step": 21540
    },
    {
      "epoch": 1.1493333333333333,
      "grad_norm": 0.2605498433113098,
      "learning_rate": 3.563333333333334e-05,
      "loss": 0.0025,
      "step": 21550
    },
    {
      "epoch": 1.1498666666666666,
      "grad_norm": 0.2682647705078125,
      "learning_rate": 3.562666666666667e-05,
      "loss": 0.002,
      "step": 21560
    },
    {
      "epoch": 1.1504,
      "grad_norm": 0.0970344990491867,
      "learning_rate": 3.562e-05,
      "loss": 0.0033,
      "step": 21570
    },
    {
      "epoch": 1.1509333333333334,
      "grad_norm": 0.3325696289539337,
      "learning_rate": 3.561333333333334e-05,
      "loss": 0.0038,
      "step": 21580
    },
    {
      "epoch": 1.1514666666666666,
      "grad_norm": 0.3953174948692322,
      "learning_rate": 3.5606666666666666e-05,
      "loss": 0.0026,
      "step": 21590
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.5608490705490112,
      "learning_rate": 3.56e-05,
      "loss": 0.0024,
      "step": 21600
    },
    {
      "epoch": 1.1525333333333334,
      "grad_norm": 0.3997342586517334,
      "learning_rate": 3.559333333333334e-05,
      "loss": 0.0021,
      "step": 21610
    },
    {
      "epoch": 1.1530666666666667,
      "grad_norm": 0.38271528482437134,
      "learning_rate": 3.558666666666667e-05,
      "loss": 0.0037,
      "step": 21620
    },
    {
      "epoch": 1.1536,
      "grad_norm": 0.3292432129383087,
      "learning_rate": 3.558e-05,
      "loss": 0.0036,
      "step": 21630
    },
    {
      "epoch": 1.1541333333333332,
      "grad_norm": 0.1631457358598709,
      "learning_rate": 3.5573333333333334e-05,
      "loss": 0.002,
      "step": 21640
    },
    {
      "epoch": 1.1546666666666667,
      "grad_norm": 0.8164300322532654,
      "learning_rate": 3.556666666666667e-05,
      "loss": 0.0022,
      "step": 21650
    },
    {
      "epoch": 1.1552,
      "grad_norm": 0.7481412887573242,
      "learning_rate": 3.5560000000000005e-05,
      "loss": 0.0026,
      "step": 21660
    },
    {
      "epoch": 1.1557333333333333,
      "grad_norm": 0.14331817626953125,
      "learning_rate": 3.555333333333333e-05,
      "loss": 0.0026,
      "step": 21670
    },
    {
      "epoch": 1.1562666666666668,
      "grad_norm": 0.41711047291755676,
      "learning_rate": 3.554666666666667e-05,
      "loss": 0.0019,
      "step": 21680
    },
    {
      "epoch": 1.1568,
      "grad_norm": 0.27099138498306274,
      "learning_rate": 3.554e-05,
      "loss": 0.0025,
      "step": 21690
    },
    {
      "epoch": 1.1573333333333333,
      "grad_norm": 0.48347485065460205,
      "learning_rate": 3.5533333333333334e-05,
      "loss": 0.0028,
      "step": 21700
    },
    {
      "epoch": 1.1578666666666666,
      "grad_norm": 0.4378744959831238,
      "learning_rate": 3.5526666666666666e-05,
      "loss": 0.005,
      "step": 21710
    },
    {
      "epoch": 1.1584,
      "grad_norm": 0.10578098148107529,
      "learning_rate": 3.5520000000000006e-05,
      "loss": 0.0026,
      "step": 21720
    },
    {
      "epoch": 1.1589333333333334,
      "grad_norm": 0.15915165841579437,
      "learning_rate": 3.551333333333334e-05,
      "loss": 0.0032,
      "step": 21730
    },
    {
      "epoch": 1.1594666666666666,
      "grad_norm": 0.30511361360549927,
      "learning_rate": 3.550666666666666e-05,
      "loss": 0.0024,
      "step": 21740
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.23036062717437744,
      "learning_rate": 3.55e-05,
      "loss": 0.003,
      "step": 21750
    },
    {
      "epoch": 1.1605333333333334,
      "grad_norm": 0.16972921788692474,
      "learning_rate": 3.5493333333333335e-05,
      "loss": 0.0022,
      "step": 21760
    },
    {
      "epoch": 1.1610666666666667,
      "grad_norm": 0.253653883934021,
      "learning_rate": 3.548666666666667e-05,
      "loss": 0.0032,
      "step": 21770
    },
    {
      "epoch": 1.1616,
      "grad_norm": 0.5368991494178772,
      "learning_rate": 3.548e-05,
      "loss": 0.0029,
      "step": 21780
    },
    {
      "epoch": 1.1621333333333332,
      "grad_norm": 0.35167741775512695,
      "learning_rate": 3.547333333333334e-05,
      "loss": 0.0027,
      "step": 21790
    },
    {
      "epoch": 1.1626666666666667,
      "grad_norm": 0.20956988632678986,
      "learning_rate": 3.546666666666667e-05,
      "loss": 0.0019,
      "step": 21800
    },
    {
      "epoch": 1.1632,
      "grad_norm": 0.3437459468841553,
      "learning_rate": 3.546e-05,
      "loss": 0.0029,
      "step": 21810
    },
    {
      "epoch": 1.1637333333333333,
      "grad_norm": 0.26322847604751587,
      "learning_rate": 3.5453333333333335e-05,
      "loss": 0.0029,
      "step": 21820
    },
    {
      "epoch": 1.1642666666666668,
      "grad_norm": 0.6597527265548706,
      "learning_rate": 3.544666666666667e-05,
      "loss": 0.0025,
      "step": 21830
    },
    {
      "epoch": 1.1648,
      "grad_norm": 0.5904451012611389,
      "learning_rate": 3.544e-05,
      "loss": 0.0029,
      "step": 21840
    },
    {
      "epoch": 1.1653333333333333,
      "grad_norm": 0.3655650317668915,
      "learning_rate": 3.543333333333333e-05,
      "loss": 0.0031,
      "step": 21850
    },
    {
      "epoch": 1.1658666666666666,
      "grad_norm": 0.20164704322814941,
      "learning_rate": 3.542666666666667e-05,
      "loss": 0.0022,
      "step": 21860
    },
    {
      "epoch": 1.1663999999999999,
      "grad_norm": 0.15361817181110382,
      "learning_rate": 3.542e-05,
      "loss": 0.0027,
      "step": 21870
    },
    {
      "epoch": 1.1669333333333334,
      "grad_norm": 0.3635239005088806,
      "learning_rate": 3.5413333333333335e-05,
      "loss": 0.0033,
      "step": 21880
    },
    {
      "epoch": 1.1674666666666667,
      "grad_norm": 0.5087541937828064,
      "learning_rate": 3.540666666666667e-05,
      "loss": 0.0023,
      "step": 21890
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.45691943168640137,
      "learning_rate": 3.54e-05,
      "loss": 0.0036,
      "step": 21900
    },
    {
      "epoch": 1.1685333333333334,
      "grad_norm": 0.17915305495262146,
      "learning_rate": 3.539333333333333e-05,
      "loss": 0.0035,
      "step": 21910
    },
    {
      "epoch": 1.1690666666666667,
      "grad_norm": 0.3162641227245331,
      "learning_rate": 3.538666666666667e-05,
      "loss": 0.0039,
      "step": 21920
    },
    {
      "epoch": 1.1696,
      "grad_norm": 0.19629834592342377,
      "learning_rate": 3.5380000000000003e-05,
      "loss": 0.002,
      "step": 21930
    },
    {
      "epoch": 1.1701333333333332,
      "grad_norm": 0.5331494808197021,
      "learning_rate": 3.5373333333333336e-05,
      "loss": 0.0025,
      "step": 21940
    },
    {
      "epoch": 1.1706666666666667,
      "grad_norm": 0.15771785378456116,
      "learning_rate": 3.536666666666667e-05,
      "loss": 0.0034,
      "step": 21950
    },
    {
      "epoch": 1.1712,
      "grad_norm": 0.20828098058700562,
      "learning_rate": 3.536000000000001e-05,
      "loss": 0.0022,
      "step": 21960
    },
    {
      "epoch": 1.1717333333333333,
      "grad_norm": 0.4113802909851074,
      "learning_rate": 3.535333333333333e-05,
      "loss": 0.0021,
      "step": 21970
    },
    {
      "epoch": 1.1722666666666668,
      "grad_norm": 0.35655224323272705,
      "learning_rate": 3.5346666666666665e-05,
      "loss": 0.0039,
      "step": 21980
    },
    {
      "epoch": 1.1728,
      "grad_norm": 0.261559396982193,
      "learning_rate": 3.5340000000000004e-05,
      "loss": 0.0022,
      "step": 21990
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.3203577697277069,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 0.0022,
      "step": 22000
    },
    {
      "epoch": 1.1738666666666666,
      "grad_norm": 0.14843878149986267,
      "learning_rate": 3.532666666666667e-05,
      "loss": 0.002,
      "step": 22010
    },
    {
      "epoch": 1.1743999999999999,
      "grad_norm": 0.22240889072418213,
      "learning_rate": 3.532e-05,
      "loss": 0.0033,
      "step": 22020
    },
    {
      "epoch": 1.1749333333333334,
      "grad_norm": 0.4979809522628784,
      "learning_rate": 3.531333333333334e-05,
      "loss": 0.0029,
      "step": 22030
    },
    {
      "epoch": 1.1754666666666667,
      "grad_norm": 0.21842072904109955,
      "learning_rate": 3.5306666666666665e-05,
      "loss": 0.0041,
      "step": 22040
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.13494135439395905,
      "learning_rate": 3.53e-05,
      "loss": 0.003,
      "step": 22050
    },
    {
      "epoch": 1.1765333333333334,
      "grad_norm": 0.3085835874080658,
      "learning_rate": 3.5293333333333336e-05,
      "loss": 0.0036,
      "step": 22060
    },
    {
      "epoch": 1.1770666666666667,
      "grad_norm": 0.28081440925598145,
      "learning_rate": 3.528666666666667e-05,
      "loss": 0.0028,
      "step": 22070
    },
    {
      "epoch": 1.1776,
      "grad_norm": 0.37494805455207825,
      "learning_rate": 3.528e-05,
      "loss": 0.0021,
      "step": 22080
    },
    {
      "epoch": 1.1781333333333333,
      "grad_norm": 0.30145108699798584,
      "learning_rate": 3.527333333333333e-05,
      "loss": 0.0024,
      "step": 22090
    },
    {
      "epoch": 1.1786666666666668,
      "grad_norm": 0.23561260104179382,
      "learning_rate": 3.526666666666667e-05,
      "loss": 0.0024,
      "step": 22100
    },
    {
      "epoch": 1.1792,
      "grad_norm": 0.687449038028717,
      "learning_rate": 3.5260000000000005e-05,
      "loss": 0.0023,
      "step": 22110
    },
    {
      "epoch": 1.1797333333333333,
      "grad_norm": 0.3938866853713989,
      "learning_rate": 3.525333333333333e-05,
      "loss": 0.0028,
      "step": 22120
    },
    {
      "epoch": 1.1802666666666666,
      "grad_norm": 0.5454837083816528,
      "learning_rate": 3.524666666666667e-05,
      "loss": 0.0036,
      "step": 22130
    },
    {
      "epoch": 1.1808,
      "grad_norm": 0.3637210428714752,
      "learning_rate": 3.524e-05,
      "loss": 0.0029,
      "step": 22140
    },
    {
      "epoch": 1.1813333333333333,
      "grad_norm": 0.37184515595436096,
      "learning_rate": 3.5233333333333334e-05,
      "loss": 0.0035,
      "step": 22150
    },
    {
      "epoch": 1.1818666666666666,
      "grad_norm": 0.41198787093162537,
      "learning_rate": 3.5226666666666666e-05,
      "loss": 0.003,
      "step": 22160
    },
    {
      "epoch": 1.1824,
      "grad_norm": 0.4516516327857971,
      "learning_rate": 3.5220000000000005e-05,
      "loss": 0.0022,
      "step": 22170
    },
    {
      "epoch": 1.1829333333333334,
      "grad_norm": 0.15064764022827148,
      "learning_rate": 3.521333333333334e-05,
      "loss": 0.0019,
      "step": 22180
    },
    {
      "epoch": 1.1834666666666667,
      "grad_norm": 0.3117985725402832,
      "learning_rate": 3.520666666666667e-05,
      "loss": 0.0019,
      "step": 22190
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.20213089883327484,
      "learning_rate": 3.52e-05,
      "loss": 0.0029,
      "step": 22200
    },
    {
      "epoch": 1.1845333333333334,
      "grad_norm": 0.43050095438957214,
      "learning_rate": 3.5193333333333334e-05,
      "loss": 0.0028,
      "step": 22210
    },
    {
      "epoch": 1.1850666666666667,
      "grad_norm": 0.41768404841423035,
      "learning_rate": 3.5186666666666666e-05,
      "loss": 0.0023,
      "step": 22220
    },
    {
      "epoch": 1.1856,
      "grad_norm": 0.258291631937027,
      "learning_rate": 3.518e-05,
      "loss": 0.0027,
      "step": 22230
    },
    {
      "epoch": 1.1861333333333333,
      "grad_norm": 0.5564084053039551,
      "learning_rate": 3.517333333333334e-05,
      "loss": 0.0028,
      "step": 22240
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.5934734344482422,
      "learning_rate": 3.516666666666667e-05,
      "loss": 0.0022,
      "step": 22250
    },
    {
      "epoch": 1.1872,
      "grad_norm": 0.4164809286594391,
      "learning_rate": 3.516e-05,
      "loss": 0.0032,
      "step": 22260
    },
    {
      "epoch": 1.1877333333333333,
      "grad_norm": 0.33239489793777466,
      "learning_rate": 3.5153333333333334e-05,
      "loss": 0.0032,
      "step": 22270
    },
    {
      "epoch": 1.1882666666666666,
      "grad_norm": 0.30430877208709717,
      "learning_rate": 3.514666666666667e-05,
      "loss": 0.003,
      "step": 22280
    },
    {
      "epoch": 1.1888,
      "grad_norm": 0.6578379273414612,
      "learning_rate": 3.514e-05,
      "loss": 0.0028,
      "step": 22290
    },
    {
      "epoch": 1.1893333333333334,
      "grad_norm": 0.6614792943000793,
      "learning_rate": 3.513333333333334e-05,
      "loss": 0.0024,
      "step": 22300
    },
    {
      "epoch": 1.1898666666666666,
      "grad_norm": 0.08002723008394241,
      "learning_rate": 3.512666666666667e-05,
      "loss": 0.0019,
      "step": 22310
    },
    {
      "epoch": 1.1904,
      "grad_norm": 0.7532665729522705,
      "learning_rate": 3.512e-05,
      "loss": 0.0026,
      "step": 22320
    },
    {
      "epoch": 1.1909333333333334,
      "grad_norm": 0.34591686725616455,
      "learning_rate": 3.5113333333333335e-05,
      "loss": 0.0023,
      "step": 22330
    },
    {
      "epoch": 1.1914666666666667,
      "grad_norm": 0.16004981100559235,
      "learning_rate": 3.5106666666666674e-05,
      "loss": 0.0032,
      "step": 22340
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.1482517570257187,
      "learning_rate": 3.51e-05,
      "loss": 0.002,
      "step": 22350
    },
    {
      "epoch": 1.1925333333333334,
      "grad_norm": 0.06530778110027313,
      "learning_rate": 3.509333333333333e-05,
      "loss": 0.0025,
      "step": 22360
    },
    {
      "epoch": 1.1930666666666667,
      "grad_norm": 0.10309697687625885,
      "learning_rate": 3.508666666666667e-05,
      "loss": 0.0034,
      "step": 22370
    },
    {
      "epoch": 1.1936,
      "grad_norm": 0.1436651647090912,
      "learning_rate": 3.508e-05,
      "loss": 0.0023,
      "step": 22380
    },
    {
      "epoch": 1.1941333333333333,
      "grad_norm": 0.21201826632022858,
      "learning_rate": 3.5073333333333335e-05,
      "loss": 0.0038,
      "step": 22390
    },
    {
      "epoch": 1.1946666666666665,
      "grad_norm": 0.21895825862884521,
      "learning_rate": 3.506666666666667e-05,
      "loss": 0.0027,
      "step": 22400
    },
    {
      "epoch": 1.1952,
      "grad_norm": 0.509307324886322,
      "learning_rate": 3.5060000000000007e-05,
      "loss": 0.0035,
      "step": 22410
    },
    {
      "epoch": 1.1957333333333333,
      "grad_norm": 0.7209094762802124,
      "learning_rate": 3.505333333333333e-05,
      "loss": 0.0032,
      "step": 22420
    },
    {
      "epoch": 1.1962666666666666,
      "grad_norm": 0.7060336470603943,
      "learning_rate": 3.5046666666666664e-05,
      "loss": 0.0036,
      "step": 22430
    },
    {
      "epoch": 1.1968,
      "grad_norm": 0.42565497756004333,
      "learning_rate": 3.504e-05,
      "loss": 0.0029,
      "step": 22440
    },
    {
      "epoch": 1.1973333333333334,
      "grad_norm": 0.1951267123222351,
      "learning_rate": 3.5033333333333336e-05,
      "loss": 0.0023,
      "step": 22450
    },
    {
      "epoch": 1.1978666666666666,
      "grad_norm": 0.41030949354171753,
      "learning_rate": 3.502666666666667e-05,
      "loss": 0.0033,
      "step": 22460
    },
    {
      "epoch": 1.1984,
      "grad_norm": 0.8412209749221802,
      "learning_rate": 3.502e-05,
      "loss": 0.0027,
      "step": 22470
    },
    {
      "epoch": 1.1989333333333334,
      "grad_norm": 0.2646370530128479,
      "learning_rate": 3.501333333333334e-05,
      "loss": 0.0031,
      "step": 22480
    },
    {
      "epoch": 1.1994666666666667,
      "grad_norm": 0.17459051311016083,
      "learning_rate": 3.500666666666667e-05,
      "loss": 0.0024,
      "step": 22490
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.22459225356578827,
      "learning_rate": 3.5e-05,
      "loss": 0.0019,
      "step": 22500
    },
    {
      "epoch": 1.2005333333333335,
      "grad_norm": 0.2570764422416687,
      "learning_rate": 3.4993333333333336e-05,
      "loss": 0.0022,
      "step": 22510
    },
    {
      "epoch": 1.2010666666666667,
      "grad_norm": 0.2711826264858246,
      "learning_rate": 3.498666666666667e-05,
      "loss": 0.0033,
      "step": 22520
    },
    {
      "epoch": 1.2016,
      "grad_norm": 0.24541011452674866,
      "learning_rate": 3.498e-05,
      "loss": 0.0034,
      "step": 22530
    },
    {
      "epoch": 1.2021333333333333,
      "grad_norm": 0.2202073484659195,
      "learning_rate": 3.497333333333333e-05,
      "loss": 0.0033,
      "step": 22540
    },
    {
      "epoch": 1.2026666666666666,
      "grad_norm": 0.37803369760513306,
      "learning_rate": 3.496666666666667e-05,
      "loss": 0.0032,
      "step": 22550
    },
    {
      "epoch": 1.2032,
      "grad_norm": 0.16170111298561096,
      "learning_rate": 3.4960000000000004e-05,
      "loss": 0.003,
      "step": 22560
    },
    {
      "epoch": 1.2037333333333333,
      "grad_norm": 0.10211274772882462,
      "learning_rate": 3.495333333333333e-05,
      "loss": 0.0029,
      "step": 22570
    },
    {
      "epoch": 1.2042666666666666,
      "grad_norm": 0.13444432616233826,
      "learning_rate": 3.494666666666667e-05,
      "loss": 0.0034,
      "step": 22580
    },
    {
      "epoch": 1.2048,
      "grad_norm": 0.42172157764434814,
      "learning_rate": 3.494e-05,
      "loss": 0.002,
      "step": 22590
    },
    {
      "epoch": 1.2053333333333334,
      "grad_norm": 0.18678554892539978,
      "learning_rate": 3.493333333333333e-05,
      "loss": 0.0019,
      "step": 22600
    },
    {
      "epoch": 1.2058666666666666,
      "grad_norm": 0.4582923948764801,
      "learning_rate": 3.4926666666666665e-05,
      "loss": 0.0022,
      "step": 22610
    },
    {
      "epoch": 1.2064,
      "grad_norm": 0.4602592885494232,
      "learning_rate": 3.4920000000000004e-05,
      "loss": 0.0025,
      "step": 22620
    },
    {
      "epoch": 1.2069333333333334,
      "grad_norm": 0.34252607822418213,
      "learning_rate": 3.491333333333334e-05,
      "loss": 0.0038,
      "step": 22630
    },
    {
      "epoch": 1.2074666666666667,
      "grad_norm": 0.12484098970890045,
      "learning_rate": 3.490666666666667e-05,
      "loss": 0.0029,
      "step": 22640
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.649935245513916,
      "learning_rate": 3.49e-05,
      "loss": 0.0033,
      "step": 22650
    },
    {
      "epoch": 1.2085333333333332,
      "grad_norm": 0.5384218692779541,
      "learning_rate": 3.4893333333333334e-05,
      "loss": 0.0018,
      "step": 22660
    },
    {
      "epoch": 1.2090666666666667,
      "grad_norm": 0.1156177669763565,
      "learning_rate": 3.4886666666666666e-05,
      "loss": 0.0038,
      "step": 22670
    },
    {
      "epoch": 1.2096,
      "grad_norm": 0.5241186022758484,
      "learning_rate": 3.4880000000000005e-05,
      "loss": 0.0031,
      "step": 22680
    },
    {
      "epoch": 1.2101333333333333,
      "grad_norm": 0.12644226849079132,
      "learning_rate": 3.487333333333334e-05,
      "loss": 0.0026,
      "step": 22690
    },
    {
      "epoch": 1.2106666666666666,
      "grad_norm": 0.4458853602409363,
      "learning_rate": 3.486666666666667e-05,
      "loss": 0.0035,
      "step": 22700
    },
    {
      "epoch": 1.2112,
      "grad_norm": 0.5210944414138794,
      "learning_rate": 3.486e-05,
      "loss": 0.0023,
      "step": 22710
    },
    {
      "epoch": 1.2117333333333333,
      "grad_norm": 0.6551195383071899,
      "learning_rate": 3.4853333333333334e-05,
      "loss": 0.003,
      "step": 22720
    },
    {
      "epoch": 1.2122666666666666,
      "grad_norm": 0.3113533556461334,
      "learning_rate": 3.4846666666666666e-05,
      "loss": 0.0045,
      "step": 22730
    },
    {
      "epoch": 1.2128,
      "grad_norm": 0.41659510135650635,
      "learning_rate": 3.484e-05,
      "loss": 0.0037,
      "step": 22740
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.098289854824543,
      "learning_rate": 3.483333333333334e-05,
      "loss": 0.0028,
      "step": 22750
    },
    {
      "epoch": 1.2138666666666666,
      "grad_norm": 0.4871911108493805,
      "learning_rate": 3.482666666666667e-05,
      "loss": 0.0027,
      "step": 22760
    },
    {
      "epoch": 1.2144,
      "grad_norm": 0.5022264122962952,
      "learning_rate": 3.482e-05,
      "loss": 0.0039,
      "step": 22770
    },
    {
      "epoch": 1.2149333333333334,
      "grad_norm": 0.30206647515296936,
      "learning_rate": 3.4813333333333334e-05,
      "loss": 0.0034,
      "step": 22780
    },
    {
      "epoch": 1.2154666666666667,
      "grad_norm": 0.45830339193344116,
      "learning_rate": 3.480666666666667e-05,
      "loss": 0.0023,
      "step": 22790
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.10565853118896484,
      "learning_rate": 3.48e-05,
      "loss": 0.0024,
      "step": 22800
    },
    {
      "epoch": 1.2165333333333332,
      "grad_norm": 0.45576804876327515,
      "learning_rate": 3.479333333333333e-05,
      "loss": 0.0028,
      "step": 22810
    },
    {
      "epoch": 1.2170666666666667,
      "grad_norm": 0.1168503686785698,
      "learning_rate": 3.478666666666667e-05,
      "loss": 0.0024,
      "step": 22820
    },
    {
      "epoch": 1.2176,
      "grad_norm": 0.49119290709495544,
      "learning_rate": 3.478e-05,
      "loss": 0.0029,
      "step": 22830
    },
    {
      "epoch": 1.2181333333333333,
      "grad_norm": 0.14202533662319183,
      "learning_rate": 3.4773333333333335e-05,
      "loss": 0.002,
      "step": 22840
    },
    {
      "epoch": 1.2186666666666666,
      "grad_norm": 0.36515048146247864,
      "learning_rate": 3.476666666666667e-05,
      "loss": 0.0029,
      "step": 22850
    },
    {
      "epoch": 1.2192,
      "grad_norm": 0.28844550251960754,
      "learning_rate": 3.4760000000000006e-05,
      "loss": 0.0023,
      "step": 22860
    },
    {
      "epoch": 1.2197333333333333,
      "grad_norm": 0.25412002205848694,
      "learning_rate": 3.475333333333334e-05,
      "loss": 0.0038,
      "step": 22870
    },
    {
      "epoch": 1.2202666666666666,
      "grad_norm": 0.2993580102920532,
      "learning_rate": 3.4746666666666664e-05,
      "loss": 0.0028,
      "step": 22880
    },
    {
      "epoch": 1.2208,
      "grad_norm": 0.182422935962677,
      "learning_rate": 3.474e-05,
      "loss": 0.0042,
      "step": 22890
    },
    {
      "epoch": 1.2213333333333334,
      "grad_norm": 0.18988414108753204,
      "learning_rate": 3.4733333333333335e-05,
      "loss": 0.003,
      "step": 22900
    },
    {
      "epoch": 1.2218666666666667,
      "grad_norm": 0.45418524742126465,
      "learning_rate": 3.472666666666667e-05,
      "loss": 0.0048,
      "step": 22910
    },
    {
      "epoch": 1.2224,
      "grad_norm": 0.460391104221344,
      "learning_rate": 3.472e-05,
      "loss": 0.0024,
      "step": 22920
    },
    {
      "epoch": 1.2229333333333334,
      "grad_norm": 0.23221294581890106,
      "learning_rate": 3.471333333333334e-05,
      "loss": 0.0022,
      "step": 22930
    },
    {
      "epoch": 1.2234666666666667,
      "grad_norm": 0.709491491317749,
      "learning_rate": 3.470666666666667e-05,
      "loss": 0.0022,
      "step": 22940
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.5118277072906494,
      "learning_rate": 3.4699999999999996e-05,
      "loss": 0.0028,
      "step": 22950
    },
    {
      "epoch": 1.2245333333333333,
      "grad_norm": 0.07545999437570572,
      "learning_rate": 3.4693333333333335e-05,
      "loss": 0.0028,
      "step": 22960
    },
    {
      "epoch": 1.2250666666666667,
      "grad_norm": 0.20007365942001343,
      "learning_rate": 3.468666666666667e-05,
      "loss": 0.0022,
      "step": 22970
    },
    {
      "epoch": 1.2256,
      "grad_norm": 0.5653579235076904,
      "learning_rate": 3.468e-05,
      "loss": 0.0032,
      "step": 22980
    },
    {
      "epoch": 1.2261333333333333,
      "grad_norm": 0.11309854686260223,
      "learning_rate": 3.467333333333333e-05,
      "loss": 0.0036,
      "step": 22990
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.5591845512390137,
      "learning_rate": 3.466666666666667e-05,
      "loss": 0.0021,
      "step": 23000
    },
    {
      "epoch": 1.2272,
      "grad_norm": 0.6270094513893127,
      "learning_rate": 3.4660000000000004e-05,
      "loss": 0.0021,
      "step": 23010
    },
    {
      "epoch": 1.2277333333333333,
      "grad_norm": 0.28584182262420654,
      "learning_rate": 3.4653333333333336e-05,
      "loss": 0.0029,
      "step": 23020
    },
    {
      "epoch": 1.2282666666666666,
      "grad_norm": 0.13067865371704102,
      "learning_rate": 3.464666666666667e-05,
      "loss": 0.0037,
      "step": 23030
    },
    {
      "epoch": 1.2288000000000001,
      "grad_norm": 0.8128954768180847,
      "learning_rate": 3.464e-05,
      "loss": 0.0022,
      "step": 23040
    },
    {
      "epoch": 1.2293333333333334,
      "grad_norm": 0.3710041046142578,
      "learning_rate": 3.463333333333333e-05,
      "loss": 0.0026,
      "step": 23050
    },
    {
      "epoch": 1.2298666666666667,
      "grad_norm": 0.47516781091690063,
      "learning_rate": 3.462666666666667e-05,
      "loss": 0.002,
      "step": 23060
    },
    {
      "epoch": 1.2304,
      "grad_norm": 0.11002666503190994,
      "learning_rate": 3.4620000000000004e-05,
      "loss": 0.0029,
      "step": 23070
    },
    {
      "epoch": 1.2309333333333332,
      "grad_norm": 0.37795737385749817,
      "learning_rate": 3.4613333333333336e-05,
      "loss": 0.0021,
      "step": 23080
    },
    {
      "epoch": 1.2314666666666667,
      "grad_norm": 0.40645653009414673,
      "learning_rate": 3.460666666666667e-05,
      "loss": 0.0031,
      "step": 23090
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.18097494542598724,
      "learning_rate": 3.46e-05,
      "loss": 0.002,
      "step": 23100
    },
    {
      "epoch": 1.2325333333333333,
      "grad_norm": 0.2906776964664459,
      "learning_rate": 3.459333333333333e-05,
      "loss": 0.0031,
      "step": 23110
    },
    {
      "epoch": 1.2330666666666668,
      "grad_norm": 0.09341967105865479,
      "learning_rate": 3.4586666666666665e-05,
      "loss": 0.0027,
      "step": 23120
    },
    {
      "epoch": 1.2336,
      "grad_norm": 0.0845155194401741,
      "learning_rate": 3.4580000000000004e-05,
      "loss": 0.002,
      "step": 23130
    },
    {
      "epoch": 1.2341333333333333,
      "grad_norm": 0.14231552183628082,
      "learning_rate": 3.4573333333333337e-05,
      "loss": 0.0028,
      "step": 23140
    },
    {
      "epoch": 1.2346666666666666,
      "grad_norm": 0.6307698488235474,
      "learning_rate": 3.456666666666667e-05,
      "loss": 0.0023,
      "step": 23150
    },
    {
      "epoch": 1.2352,
      "grad_norm": 0.2637284994125366,
      "learning_rate": 3.456e-05,
      "loss": 0.0024,
      "step": 23160
    },
    {
      "epoch": 1.2357333333333334,
      "grad_norm": 0.13210684061050415,
      "learning_rate": 3.455333333333334e-05,
      "loss": 0.0027,
      "step": 23170
    },
    {
      "epoch": 1.2362666666666666,
      "grad_norm": 0.0860123410820961,
      "learning_rate": 3.4546666666666666e-05,
      "loss": 0.0017,
      "step": 23180
    },
    {
      "epoch": 1.2368000000000001,
      "grad_norm": 0.4130021333694458,
      "learning_rate": 3.454e-05,
      "loss": 0.0023,
      "step": 23190
    },
    {
      "epoch": 1.2373333333333334,
      "grad_norm": 0.1653916835784912,
      "learning_rate": 3.453333333333334e-05,
      "loss": 0.004,
      "step": 23200
    },
    {
      "epoch": 1.2378666666666667,
      "grad_norm": 0.6430980563163757,
      "learning_rate": 3.452666666666667e-05,
      "loss": 0.002,
      "step": 23210
    },
    {
      "epoch": 1.2384,
      "grad_norm": 0.17920349538326263,
      "learning_rate": 3.452e-05,
      "loss": 0.0033,
      "step": 23220
    },
    {
      "epoch": 1.2389333333333332,
      "grad_norm": 0.5580312609672546,
      "learning_rate": 3.4513333333333334e-05,
      "loss": 0.0017,
      "step": 23230
    },
    {
      "epoch": 1.2394666666666667,
      "grad_norm": 0.5304586291313171,
      "learning_rate": 3.450666666666667e-05,
      "loss": 0.0025,
      "step": 23240
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.07524816691875458,
      "learning_rate": 3.45e-05,
      "loss": 0.0028,
      "step": 23250
    },
    {
      "epoch": 1.2405333333333333,
      "grad_norm": 0.20305034518241882,
      "learning_rate": 3.449333333333333e-05,
      "loss": 0.0025,
      "step": 23260
    },
    {
      "epoch": 1.2410666666666668,
      "grad_norm": 0.17650431394577026,
      "learning_rate": 3.448666666666667e-05,
      "loss": 0.0023,
      "step": 23270
    },
    {
      "epoch": 1.2416,
      "grad_norm": 0.20358626544475555,
      "learning_rate": 3.448e-05,
      "loss": 0.002,
      "step": 23280
    },
    {
      "epoch": 1.2421333333333333,
      "grad_norm": 0.7235226035118103,
      "learning_rate": 3.4473333333333334e-05,
      "loss": 0.0044,
      "step": 23290
    },
    {
      "epoch": 1.2426666666666666,
      "grad_norm": 0.4549756646156311,
      "learning_rate": 3.4466666666666666e-05,
      "loss": 0.0036,
      "step": 23300
    },
    {
      "epoch": 1.2432,
      "grad_norm": 0.19940972328186035,
      "learning_rate": 3.4460000000000005e-05,
      "loss": 0.0029,
      "step": 23310
    },
    {
      "epoch": 1.2437333333333334,
      "grad_norm": 0.29969948530197144,
      "learning_rate": 3.445333333333334e-05,
      "loss": 0.0031,
      "step": 23320
    },
    {
      "epoch": 1.2442666666666666,
      "grad_norm": 0.14116860926151276,
      "learning_rate": 3.444666666666666e-05,
      "loss": 0.0025,
      "step": 23330
    },
    {
      "epoch": 1.2448,
      "grad_norm": 0.7946236729621887,
      "learning_rate": 3.444e-05,
      "loss": 0.0033,
      "step": 23340
    },
    {
      "epoch": 1.2453333333333334,
      "grad_norm": 0.33668944239616394,
      "learning_rate": 3.4433333333333335e-05,
      "loss": 0.0024,
      "step": 23350
    },
    {
      "epoch": 1.2458666666666667,
      "grad_norm": 0.2342689484357834,
      "learning_rate": 3.442666666666667e-05,
      "loss": 0.0023,
      "step": 23360
    },
    {
      "epoch": 1.2464,
      "grad_norm": 0.3574081361293793,
      "learning_rate": 3.442e-05,
      "loss": 0.0028,
      "step": 23370
    },
    {
      "epoch": 1.2469333333333332,
      "grad_norm": 0.33205288648605347,
      "learning_rate": 3.441333333333334e-05,
      "loss": 0.0028,
      "step": 23380
    },
    {
      "epoch": 1.2474666666666667,
      "grad_norm": 0.5887121558189392,
      "learning_rate": 3.440666666666667e-05,
      "loss": 0.0021,
      "step": 23390
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.3376447558403015,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 0.003,
      "step": 23400
    },
    {
      "epoch": 1.2485333333333333,
      "grad_norm": 0.7585123181343079,
      "learning_rate": 3.4393333333333335e-05,
      "loss": 0.0038,
      "step": 23410
    },
    {
      "epoch": 1.2490666666666668,
      "grad_norm": 0.31920304894447327,
      "learning_rate": 3.438666666666667e-05,
      "loss": 0.0025,
      "step": 23420
    },
    {
      "epoch": 1.2496,
      "grad_norm": 0.20841331779956818,
      "learning_rate": 3.438e-05,
      "loss": 0.0021,
      "step": 23430
    },
    {
      "epoch": 1.2501333333333333,
      "grad_norm": 0.694807231426239,
      "learning_rate": 3.437333333333334e-05,
      "loss": 0.0025,
      "step": 23440
    },
    {
      "epoch": 1.2506666666666666,
      "grad_norm": 0.3354496657848358,
      "learning_rate": 3.436666666666667e-05,
      "loss": 0.0035,
      "step": 23450
    },
    {
      "epoch": 1.2511999999999999,
      "grad_norm": 0.3003097474575043,
      "learning_rate": 3.436e-05,
      "loss": 0.0031,
      "step": 23460
    },
    {
      "epoch": 1.2517333333333334,
      "grad_norm": 0.12416417896747589,
      "learning_rate": 3.4353333333333335e-05,
      "loss": 0.0041,
      "step": 23470
    },
    {
      "epoch": 1.2522666666666666,
      "grad_norm": 0.5146346092224121,
      "learning_rate": 3.434666666666667e-05,
      "loss": 0.0027,
      "step": 23480
    },
    {
      "epoch": 1.2528000000000001,
      "grad_norm": 0.5060132741928101,
      "learning_rate": 3.434e-05,
      "loss": 0.0028,
      "step": 23490
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.44194358587265015,
      "learning_rate": 3.433333333333333e-05,
      "loss": 0.0021,
      "step": 23500
    },
    {
      "epoch": 1.2538666666666667,
      "grad_norm": 0.15391485393047333,
      "learning_rate": 3.432666666666667e-05,
      "loss": 0.0034,
      "step": 23510
    },
    {
      "epoch": 1.2544,
      "grad_norm": 0.3529069721698761,
      "learning_rate": 3.4320000000000003e-05,
      "loss": 0.0023,
      "step": 23520
    },
    {
      "epoch": 1.2549333333333332,
      "grad_norm": 0.7214385867118835,
      "learning_rate": 3.4313333333333336e-05,
      "loss": 0.003,
      "step": 23530
    },
    {
      "epoch": 1.2554666666666667,
      "grad_norm": 0.0960695669054985,
      "learning_rate": 3.430666666666667e-05,
      "loss": 0.0027,
      "step": 23540
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.48664915561676025,
      "learning_rate": 3.430000000000001e-05,
      "loss": 0.0024,
      "step": 23550
    },
    {
      "epoch": 1.2565333333333333,
      "grad_norm": 0.4640170931816101,
      "learning_rate": 3.429333333333333e-05,
      "loss": 0.0028,
      "step": 23560
    },
    {
      "epoch": 1.2570666666666668,
      "grad_norm": 0.32853245735168457,
      "learning_rate": 3.4286666666666665e-05,
      "loss": 0.0042,
      "step": 23570
    },
    {
      "epoch": 1.2576,
      "grad_norm": 0.3577864468097687,
      "learning_rate": 3.4280000000000004e-05,
      "loss": 0.0035,
      "step": 23580
    },
    {
      "epoch": 1.2581333333333333,
      "grad_norm": 0.06489460915327072,
      "learning_rate": 3.4273333333333336e-05,
      "loss": 0.0035,
      "step": 23590
    },
    {
      "epoch": 1.2586666666666666,
      "grad_norm": 0.5435391068458557,
      "learning_rate": 3.426666666666667e-05,
      "loss": 0.0036,
      "step": 23600
    },
    {
      "epoch": 1.2591999999999999,
      "grad_norm": 0.1952940970659256,
      "learning_rate": 3.426e-05,
      "loss": 0.0026,
      "step": 23610
    },
    {
      "epoch": 1.2597333333333334,
      "grad_norm": 0.26197078824043274,
      "learning_rate": 3.425333333333334e-05,
      "loss": 0.0019,
      "step": 23620
    },
    {
      "epoch": 1.2602666666666666,
      "grad_norm": 0.4005313217639923,
      "learning_rate": 3.4246666666666665e-05,
      "loss": 0.0028,
      "step": 23630
    },
    {
      "epoch": 1.2608,
      "grad_norm": 0.6701254844665527,
      "learning_rate": 3.424e-05,
      "loss": 0.0027,
      "step": 23640
    },
    {
      "epoch": 1.2613333333333334,
      "grad_norm": 0.22725510597229004,
      "learning_rate": 3.4233333333333336e-05,
      "loss": 0.0025,
      "step": 23650
    },
    {
      "epoch": 1.2618666666666667,
      "grad_norm": 0.469154417514801,
      "learning_rate": 3.422666666666667e-05,
      "loss": 0.0031,
      "step": 23660
    },
    {
      "epoch": 1.2624,
      "grad_norm": 0.07971055060625076,
      "learning_rate": 3.422e-05,
      "loss": 0.0027,
      "step": 23670
    },
    {
      "epoch": 1.2629333333333332,
      "grad_norm": 0.2686293423175812,
      "learning_rate": 3.421333333333333e-05,
      "loss": 0.0021,
      "step": 23680
    },
    {
      "epoch": 1.2634666666666667,
      "grad_norm": 0.4762345850467682,
      "learning_rate": 3.420666666666667e-05,
      "loss": 0.0035,
      "step": 23690
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.10625069588422775,
      "learning_rate": 3.4200000000000005e-05,
      "loss": 0.0019,
      "step": 23700
    },
    {
      "epoch": 1.2645333333333333,
      "grad_norm": 0.24390825629234314,
      "learning_rate": 3.419333333333333e-05,
      "loss": 0.0027,
      "step": 23710
    },
    {
      "epoch": 1.2650666666666668,
      "grad_norm": 0.22256295382976532,
      "learning_rate": 3.418666666666667e-05,
      "loss": 0.004,
      "step": 23720
    },
    {
      "epoch": 1.2656,
      "grad_norm": 0.6408386826515198,
      "learning_rate": 3.418e-05,
      "loss": 0.0041,
      "step": 23730
    },
    {
      "epoch": 1.2661333333333333,
      "grad_norm": 0.4390419125556946,
      "learning_rate": 3.4173333333333334e-05,
      "loss": 0.002,
      "step": 23740
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.20141522586345673,
      "learning_rate": 3.4166666666666666e-05,
      "loss": 0.0034,
      "step": 23750
    },
    {
      "epoch": 1.2671999999999999,
      "grad_norm": 0.3002462387084961,
      "learning_rate": 3.4160000000000005e-05,
      "loss": 0.0021,
      "step": 23760
    },
    {
      "epoch": 1.2677333333333334,
      "grad_norm": 0.38474157452583313,
      "learning_rate": 3.415333333333334e-05,
      "loss": 0.0019,
      "step": 23770
    },
    {
      "epoch": 1.2682666666666667,
      "grad_norm": 0.2137189656496048,
      "learning_rate": 3.414666666666666e-05,
      "loss": 0.0033,
      "step": 23780
    },
    {
      "epoch": 1.2688,
      "grad_norm": 0.5859922766685486,
      "learning_rate": 3.414e-05,
      "loss": 0.003,
      "step": 23790
    },
    {
      "epoch": 1.2693333333333334,
      "grad_norm": 0.19538255035877228,
      "learning_rate": 3.4133333333333334e-05,
      "loss": 0.002,
      "step": 23800
    },
    {
      "epoch": 1.2698666666666667,
      "grad_norm": 0.13214793801307678,
      "learning_rate": 3.4126666666666666e-05,
      "loss": 0.0021,
      "step": 23810
    },
    {
      "epoch": 1.2704,
      "grad_norm": 0.32334959506988525,
      "learning_rate": 3.412e-05,
      "loss": 0.0021,
      "step": 23820
    },
    {
      "epoch": 1.2709333333333332,
      "grad_norm": 0.305549681186676,
      "learning_rate": 3.411333333333334e-05,
      "loss": 0.003,
      "step": 23830
    },
    {
      "epoch": 1.2714666666666667,
      "grad_norm": 0.2633495628833771,
      "learning_rate": 3.410666666666667e-05,
      "loss": 0.0027,
      "step": 23840
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.16117794811725616,
      "learning_rate": 3.41e-05,
      "loss": 0.0033,
      "step": 23850
    },
    {
      "epoch": 1.2725333333333333,
      "grad_norm": 0.5946194529533386,
      "learning_rate": 3.4093333333333334e-05,
      "loss": 0.0026,
      "step": 23860
    },
    {
      "epoch": 1.2730666666666668,
      "grad_norm": 0.3134384751319885,
      "learning_rate": 3.408666666666667e-05,
      "loss": 0.0028,
      "step": 23870
    },
    {
      "epoch": 1.2736,
      "grad_norm": 0.29036250710487366,
      "learning_rate": 3.408e-05,
      "loss": 0.0022,
      "step": 23880
    },
    {
      "epoch": 1.2741333333333333,
      "grad_norm": 0.6387247443199158,
      "learning_rate": 3.407333333333334e-05,
      "loss": 0.0021,
      "step": 23890
    },
    {
      "epoch": 1.2746666666666666,
      "grad_norm": 0.2624807357788086,
      "learning_rate": 3.406666666666667e-05,
      "loss": 0.002,
      "step": 23900
    },
    {
      "epoch": 1.2752,
      "grad_norm": 0.5681189298629761,
      "learning_rate": 3.406e-05,
      "loss": 0.0027,
      "step": 23910
    },
    {
      "epoch": 1.2757333333333334,
      "grad_norm": 0.17739583551883698,
      "learning_rate": 3.4053333333333335e-05,
      "loss": 0.0024,
      "step": 23920
    },
    {
      "epoch": 1.2762666666666667,
      "grad_norm": 0.3102714419364929,
      "learning_rate": 3.404666666666667e-05,
      "loss": 0.0038,
      "step": 23930
    },
    {
      "epoch": 1.2768,
      "grad_norm": 0.8162795901298523,
      "learning_rate": 3.404e-05,
      "loss": 0.0023,
      "step": 23940
    },
    {
      "epoch": 1.2773333333333334,
      "grad_norm": 0.10188360512256622,
      "learning_rate": 3.403333333333333e-05,
      "loss": 0.0031,
      "step": 23950
    },
    {
      "epoch": 1.2778666666666667,
      "grad_norm": 0.17235067486763,
      "learning_rate": 3.402666666666667e-05,
      "loss": 0.0034,
      "step": 23960
    },
    {
      "epoch": 1.2784,
      "grad_norm": 0.3642643094062805,
      "learning_rate": 3.402e-05,
      "loss": 0.003,
      "step": 23970
    },
    {
      "epoch": 1.2789333333333333,
      "grad_norm": 0.2902476191520691,
      "learning_rate": 3.4013333333333335e-05,
      "loss": 0.0028,
      "step": 23980
    },
    {
      "epoch": 1.2794666666666665,
      "grad_norm": 0.5044888257980347,
      "learning_rate": 3.400666666666667e-05,
      "loss": 0.0026,
      "step": 23990
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.147978737950325,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.0035,
      "step": 24000
    },
    {
      "epoch": 1.2805333333333333,
      "grad_norm": 0.7172043919563293,
      "learning_rate": 3.399333333333333e-05,
      "loss": 0.0022,
      "step": 24010
    },
    {
      "epoch": 1.2810666666666668,
      "grad_norm": 0.40636521577835083,
      "learning_rate": 3.3986666666666664e-05,
      "loss": 0.0035,
      "step": 24020
    },
    {
      "epoch": 1.2816,
      "grad_norm": 0.1142798364162445,
      "learning_rate": 3.398e-05,
      "loss": 0.003,
      "step": 24030
    },
    {
      "epoch": 1.2821333333333333,
      "grad_norm": 0.4896250069141388,
      "learning_rate": 3.3973333333333336e-05,
      "loss": 0.0022,
      "step": 24040
    },
    {
      "epoch": 1.2826666666666666,
      "grad_norm": 0.16073934733867645,
      "learning_rate": 3.396666666666667e-05,
      "loss": 0.002,
      "step": 24050
    },
    {
      "epoch": 1.2832,
      "grad_norm": 0.5603684186935425,
      "learning_rate": 3.396e-05,
      "loss": 0.0019,
      "step": 24060
    },
    {
      "epoch": 1.2837333333333334,
      "grad_norm": 0.32744544744491577,
      "learning_rate": 3.395333333333334e-05,
      "loss": 0.0033,
      "step": 24070
    },
    {
      "epoch": 1.2842666666666667,
      "grad_norm": 0.12268047779798508,
      "learning_rate": 3.394666666666667e-05,
      "loss": 0.0024,
      "step": 24080
    },
    {
      "epoch": 1.2848,
      "grad_norm": 0.3277319669723511,
      "learning_rate": 3.394e-05,
      "loss": 0.0025,
      "step": 24090
    },
    {
      "epoch": 1.2853333333333334,
      "grad_norm": 0.34337711334228516,
      "learning_rate": 3.3933333333333336e-05,
      "loss": 0.0023,
      "step": 24100
    },
    {
      "epoch": 1.2858666666666667,
      "grad_norm": 0.22525948286056519,
      "learning_rate": 3.392666666666667e-05,
      "loss": 0.0028,
      "step": 24110
    },
    {
      "epoch": 1.2864,
      "grad_norm": 0.26575514674186707,
      "learning_rate": 3.392e-05,
      "loss": 0.0025,
      "step": 24120
    },
    {
      "epoch": 1.2869333333333333,
      "grad_norm": 0.584245502948761,
      "learning_rate": 3.391333333333333e-05,
      "loss": 0.0028,
      "step": 24130
    },
    {
      "epoch": 1.2874666666666665,
      "grad_norm": 0.5298862457275391,
      "learning_rate": 3.390666666666667e-05,
      "loss": 0.0022,
      "step": 24140
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.1048966571688652,
      "learning_rate": 3.3900000000000004e-05,
      "loss": 0.0034,
      "step": 24150
    },
    {
      "epoch": 1.2885333333333333,
      "grad_norm": 0.31956127285957336,
      "learning_rate": 3.389333333333333e-05,
      "loss": 0.0023,
      "step": 24160
    },
    {
      "epoch": 1.2890666666666668,
      "grad_norm": 0.6597433090209961,
      "learning_rate": 3.388666666666667e-05,
      "loss": 0.0025,
      "step": 24170
    },
    {
      "epoch": 1.2896,
      "grad_norm": 0.18048399686813354,
      "learning_rate": 3.388e-05,
      "loss": 0.0024,
      "step": 24180
    },
    {
      "epoch": 1.2901333333333334,
      "grad_norm": 0.2838907539844513,
      "learning_rate": 3.387333333333333e-05,
      "loss": 0.0034,
      "step": 24190
    },
    {
      "epoch": 1.2906666666666666,
      "grad_norm": 0.6127181053161621,
      "learning_rate": 3.3866666666666665e-05,
      "loss": 0.003,
      "step": 24200
    },
    {
      "epoch": 1.2912,
      "grad_norm": 0.1876453459262848,
      "learning_rate": 3.3860000000000004e-05,
      "loss": 0.0019,
      "step": 24210
    },
    {
      "epoch": 1.2917333333333334,
      "grad_norm": 0.10731913894414902,
      "learning_rate": 3.385333333333334e-05,
      "loss": 0.002,
      "step": 24220
    },
    {
      "epoch": 1.2922666666666667,
      "grad_norm": 0.14895853400230408,
      "learning_rate": 3.384666666666667e-05,
      "loss": 0.0026,
      "step": 24230
    },
    {
      "epoch": 1.2928,
      "grad_norm": 0.6179641485214233,
      "learning_rate": 3.384e-05,
      "loss": 0.0036,
      "step": 24240
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.3843827545642853,
      "learning_rate": 3.3833333333333334e-05,
      "loss": 0.0042,
      "step": 24250
    },
    {
      "epoch": 1.2938666666666667,
      "grad_norm": 0.3928488492965698,
      "learning_rate": 3.3826666666666666e-05,
      "loss": 0.0031,
      "step": 24260
    },
    {
      "epoch": 1.2944,
      "grad_norm": 0.7936025857925415,
      "learning_rate": 3.3820000000000005e-05,
      "loss": 0.0033,
      "step": 24270
    },
    {
      "epoch": 1.2949333333333333,
      "grad_norm": 0.5352520942687988,
      "learning_rate": 3.381333333333334e-05,
      "loss": 0.0032,
      "step": 24280
    },
    {
      "epoch": 1.2954666666666665,
      "grad_norm": 0.11062552779912949,
      "learning_rate": 3.380666666666667e-05,
      "loss": 0.0032,
      "step": 24290
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.36093610525131226,
      "learning_rate": 3.38e-05,
      "loss": 0.0028,
      "step": 24300
    },
    {
      "epoch": 1.2965333333333333,
      "grad_norm": 0.21112534403800964,
      "learning_rate": 3.3793333333333334e-05,
      "loss": 0.0024,
      "step": 24310
    },
    {
      "epoch": 1.2970666666666666,
      "grad_norm": 0.4687355160713196,
      "learning_rate": 3.3786666666666666e-05,
      "loss": 0.0032,
      "step": 24320
    },
    {
      "epoch": 1.2976,
      "grad_norm": 0.3398856222629547,
      "learning_rate": 3.378e-05,
      "loss": 0.0031,
      "step": 24330
    },
    {
      "epoch": 1.2981333333333334,
      "grad_norm": 0.3871660530567169,
      "learning_rate": 3.377333333333334e-05,
      "loss": 0.0025,
      "step": 24340
    },
    {
      "epoch": 1.2986666666666666,
      "grad_norm": 0.7464662194252014,
      "learning_rate": 3.376666666666667e-05,
      "loss": 0.0027,
      "step": 24350
    },
    {
      "epoch": 1.2992,
      "grad_norm": 0.4422728419303894,
      "learning_rate": 3.376e-05,
      "loss": 0.0034,
      "step": 24360
    },
    {
      "epoch": 1.2997333333333334,
      "grad_norm": 0.16598378121852875,
      "learning_rate": 3.3753333333333334e-05,
      "loss": 0.0031,
      "step": 24370
    },
    {
      "epoch": 1.3002666666666667,
      "grad_norm": 0.5409064888954163,
      "learning_rate": 3.374666666666667e-05,
      "loss": 0.0035,
      "step": 24380
    },
    {
      "epoch": 1.3008,
      "grad_norm": 0.3503877818584442,
      "learning_rate": 3.374e-05,
      "loss": 0.0046,
      "step": 24390
    },
    {
      "epoch": 1.3013333333333335,
      "grad_norm": 0.1969911754131317,
      "learning_rate": 3.373333333333333e-05,
      "loss": 0.0022,
      "step": 24400
    },
    {
      "epoch": 1.3018666666666667,
      "grad_norm": 0.4816259443759918,
      "learning_rate": 3.372666666666667e-05,
      "loss": 0.0029,
      "step": 24410
    },
    {
      "epoch": 1.3024,
      "grad_norm": 0.5705407857894897,
      "learning_rate": 3.372e-05,
      "loss": 0.0028,
      "step": 24420
    },
    {
      "epoch": 1.3029333333333333,
      "grad_norm": 0.6489459872245789,
      "learning_rate": 3.3713333333333335e-05,
      "loss": 0.003,
      "step": 24430
    },
    {
      "epoch": 1.3034666666666666,
      "grad_norm": 0.2330661416053772,
      "learning_rate": 3.370666666666667e-05,
      "loss": 0.0036,
      "step": 24440
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.27420079708099365,
      "learning_rate": 3.3700000000000006e-05,
      "loss": 0.0036,
      "step": 24450
    },
    {
      "epoch": 1.3045333333333333,
      "grad_norm": 0.4445965886116028,
      "learning_rate": 3.369333333333333e-05,
      "loss": 0.0021,
      "step": 24460
    },
    {
      "epoch": 1.3050666666666666,
      "grad_norm": 0.5441886782646179,
      "learning_rate": 3.3686666666666664e-05,
      "loss": 0.0025,
      "step": 24470
    },
    {
      "epoch": 1.3056,
      "grad_norm": 0.48081719875335693,
      "learning_rate": 3.368e-05,
      "loss": 0.0021,
      "step": 24480
    },
    {
      "epoch": 1.3061333333333334,
      "grad_norm": 0.2605476379394531,
      "learning_rate": 3.3673333333333335e-05,
      "loss": 0.0023,
      "step": 24490
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.6934381723403931,
      "learning_rate": 3.366666666666667e-05,
      "loss": 0.0026,
      "step": 24500
    },
    {
      "epoch": 1.3072,
      "grad_norm": 0.21713848412036896,
      "learning_rate": 3.366e-05,
      "loss": 0.0024,
      "step": 24510
    },
    {
      "epoch": 1.3077333333333334,
      "grad_norm": 0.2999062240123749,
      "learning_rate": 3.365333333333334e-05,
      "loss": 0.0021,
      "step": 24520
    },
    {
      "epoch": 1.3082666666666667,
      "grad_norm": 0.2876320779323578,
      "learning_rate": 3.364666666666667e-05,
      "loss": 0.004,
      "step": 24530
    },
    {
      "epoch": 1.3088,
      "grad_norm": 0.10298699140548706,
      "learning_rate": 3.3639999999999996e-05,
      "loss": 0.0028,
      "step": 24540
    },
    {
      "epoch": 1.3093333333333335,
      "grad_norm": 0.257323294878006,
      "learning_rate": 3.3633333333333335e-05,
      "loss": 0.0029,
      "step": 24550
    },
    {
      "epoch": 1.3098666666666667,
      "grad_norm": 0.10949944704771042,
      "learning_rate": 3.362666666666667e-05,
      "loss": 0.0026,
      "step": 24560
    },
    {
      "epoch": 1.3104,
      "grad_norm": 0.2776101529598236,
      "learning_rate": 3.362e-05,
      "loss": 0.0033,
      "step": 24570
    },
    {
      "epoch": 1.3109333333333333,
      "grad_norm": 0.14718230068683624,
      "learning_rate": 3.361333333333333e-05,
      "loss": 0.0019,
      "step": 24580
    },
    {
      "epoch": 1.3114666666666666,
      "grad_norm": 0.24938388168811798,
      "learning_rate": 3.360666666666667e-05,
      "loss": 0.004,
      "step": 24590
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.1722562164068222,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 0.0022,
      "step": 24600
    },
    {
      "epoch": 1.3125333333333333,
      "grad_norm": 0.2585558295249939,
      "learning_rate": 3.359333333333333e-05,
      "loss": 0.0023,
      "step": 24610
    },
    {
      "epoch": 1.3130666666666666,
      "grad_norm": 0.3189804255962372,
      "learning_rate": 3.358666666666667e-05,
      "loss": 0.0019,
      "step": 24620
    },
    {
      "epoch": 1.3136,
      "grad_norm": 0.2941438853740692,
      "learning_rate": 3.358e-05,
      "loss": 0.0029,
      "step": 24630
    },
    {
      "epoch": 1.3141333333333334,
      "grad_norm": 0.4457796514034271,
      "learning_rate": 3.357333333333333e-05,
      "loss": 0.002,
      "step": 24640
    },
    {
      "epoch": 1.3146666666666667,
      "grad_norm": 0.14794869720935822,
      "learning_rate": 3.356666666666667e-05,
      "loss": 0.0022,
      "step": 24650
    },
    {
      "epoch": 1.3152,
      "grad_norm": 0.2884472906589508,
      "learning_rate": 3.3560000000000004e-05,
      "loss": 0.0021,
      "step": 24660
    },
    {
      "epoch": 1.3157333333333332,
      "grad_norm": 0.10338035970926285,
      "learning_rate": 3.3553333333333336e-05,
      "loss": 0.0029,
      "step": 24670
    },
    {
      "epoch": 1.3162666666666667,
      "grad_norm": 0.34530994296073914,
      "learning_rate": 3.354666666666667e-05,
      "loss": 0.0024,
      "step": 24680
    },
    {
      "epoch": 1.3168,
      "grad_norm": 0.575301468372345,
      "learning_rate": 3.354e-05,
      "loss": 0.0026,
      "step": 24690
    },
    {
      "epoch": 1.3173333333333335,
      "grad_norm": 0.4157809913158417,
      "learning_rate": 3.353333333333333e-05,
      "loss": 0.0025,
      "step": 24700
    },
    {
      "epoch": 1.3178666666666667,
      "grad_norm": 0.6507766842842102,
      "learning_rate": 3.3526666666666665e-05,
      "loss": 0.0033,
      "step": 24710
    },
    {
      "epoch": 1.3184,
      "grad_norm": 0.17450909316539764,
      "learning_rate": 3.3520000000000004e-05,
      "loss": 0.0033,
      "step": 24720
    },
    {
      "epoch": 1.3189333333333333,
      "grad_norm": 0.45755207538604736,
      "learning_rate": 3.3513333333333337e-05,
      "loss": 0.0031,
      "step": 24730
    },
    {
      "epoch": 1.3194666666666666,
      "grad_norm": 0.7123336791992188,
      "learning_rate": 3.350666666666667e-05,
      "loss": 0.0025,
      "step": 24740
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.3013010025024414,
      "learning_rate": 3.35e-05,
      "loss": 0.0022,
      "step": 24750
    },
    {
      "epoch": 1.3205333333333333,
      "grad_norm": 0.29437464475631714,
      "learning_rate": 3.349333333333334e-05,
      "loss": 0.0023,
      "step": 24760
    },
    {
      "epoch": 1.3210666666666666,
      "grad_norm": 0.3647398054599762,
      "learning_rate": 3.3486666666666666e-05,
      "loss": 0.003,
      "step": 24770
    },
    {
      "epoch": 1.3216,
      "grad_norm": 0.0772886648774147,
      "learning_rate": 3.348e-05,
      "loss": 0.0024,
      "step": 24780
    },
    {
      "epoch": 1.3221333333333334,
      "grad_norm": 0.15200446546077728,
      "learning_rate": 3.347333333333334e-05,
      "loss": 0.0019,
      "step": 24790
    },
    {
      "epoch": 1.3226666666666667,
      "grad_norm": 0.5538679957389832,
      "learning_rate": 3.346666666666667e-05,
      "loss": 0.0023,
      "step": 24800
    },
    {
      "epoch": 1.3232,
      "grad_norm": 0.16087287664413452,
      "learning_rate": 3.346e-05,
      "loss": 0.0025,
      "step": 24810
    },
    {
      "epoch": 1.3237333333333332,
      "grad_norm": 0.7002245187759399,
      "learning_rate": 3.3453333333333334e-05,
      "loss": 0.0042,
      "step": 24820
    },
    {
      "epoch": 1.3242666666666667,
      "grad_norm": 0.4252144396305084,
      "learning_rate": 3.344666666666667e-05,
      "loss": 0.0027,
      "step": 24830
    },
    {
      "epoch": 1.3248,
      "grad_norm": 0.42983052134513855,
      "learning_rate": 3.344e-05,
      "loss": 0.0027,
      "step": 24840
    },
    {
      "epoch": 1.3253333333333333,
      "grad_norm": 0.1582961082458496,
      "learning_rate": 3.343333333333333e-05,
      "loss": 0.0026,
      "step": 24850
    },
    {
      "epoch": 1.3258666666666667,
      "grad_norm": 0.5135539770126343,
      "learning_rate": 3.342666666666667e-05,
      "loss": 0.0026,
      "step": 24860
    },
    {
      "epoch": 1.3264,
      "grad_norm": 0.32422858476638794,
      "learning_rate": 3.342e-05,
      "loss": 0.0024,
      "step": 24870
    },
    {
      "epoch": 1.3269333333333333,
      "grad_norm": 0.10870596766471863,
      "learning_rate": 3.3413333333333334e-05,
      "loss": 0.0029,
      "step": 24880
    },
    {
      "epoch": 1.3274666666666666,
      "grad_norm": 0.2618333399295807,
      "learning_rate": 3.3406666666666666e-05,
      "loss": 0.0025,
      "step": 24890
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.47204500436782837,
      "learning_rate": 3.3400000000000005e-05,
      "loss": 0.0032,
      "step": 24900
    },
    {
      "epoch": 1.3285333333333333,
      "grad_norm": 0.7770198583602905,
      "learning_rate": 3.339333333333334e-05,
      "loss": 0.0024,
      "step": 24910
    },
    {
      "epoch": 1.3290666666666666,
      "grad_norm": 0.4952801764011383,
      "learning_rate": 3.338666666666666e-05,
      "loss": 0.0035,
      "step": 24920
    },
    {
      "epoch": 1.3296000000000001,
      "grad_norm": 0.12720037996768951,
      "learning_rate": 3.338e-05,
      "loss": 0.0028,
      "step": 24930
    },
    {
      "epoch": 1.3301333333333334,
      "grad_norm": 0.23143303394317627,
      "learning_rate": 3.3373333333333335e-05,
      "loss": 0.0028,
      "step": 24940
    },
    {
      "epoch": 1.3306666666666667,
      "grad_norm": 0.20669744908809662,
      "learning_rate": 3.336666666666667e-05,
      "loss": 0.0022,
      "step": 24950
    },
    {
      "epoch": 1.3312,
      "grad_norm": 0.3495473861694336,
      "learning_rate": 3.336e-05,
      "loss": 0.002,
      "step": 24960
    },
    {
      "epoch": 1.3317333333333332,
      "grad_norm": 0.19962722063064575,
      "learning_rate": 3.335333333333334e-05,
      "loss": 0.003,
      "step": 24970
    },
    {
      "epoch": 1.3322666666666667,
      "grad_norm": 0.5133247375488281,
      "learning_rate": 3.334666666666667e-05,
      "loss": 0.0023,
      "step": 24980
    },
    {
      "epoch": 1.3328,
      "grad_norm": 0.2701343595981598,
      "learning_rate": 3.3339999999999996e-05,
      "loss": 0.003,
      "step": 24990
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.24752087891101837,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.0025,
      "step": 25000
    },
    {
      "epoch": 1.3338666666666668,
      "grad_norm": 0.5680327415466309,
      "learning_rate": 3.332666666666667e-05,
      "loss": 0.0027,
      "step": 25010
    },
    {
      "epoch": 1.3344,
      "grad_norm": 0.4311125874519348,
      "learning_rate": 3.332e-05,
      "loss": 0.0036,
      "step": 25020
    },
    {
      "epoch": 1.3349333333333333,
      "grad_norm": 0.5461152791976929,
      "learning_rate": 3.331333333333334e-05,
      "loss": 0.0029,
      "step": 25030
    },
    {
      "epoch": 1.3354666666666666,
      "grad_norm": 0.24752284586429596,
      "learning_rate": 3.330666666666667e-05,
      "loss": 0.005,
      "step": 25040
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.42970725893974304,
      "learning_rate": 3.33e-05,
      "loss": 0.0024,
      "step": 25050
    },
    {
      "epoch": 1.3365333333333334,
      "grad_norm": 0.3492342531681061,
      "learning_rate": 3.3293333333333335e-05,
      "loss": 0.0018,
      "step": 25060
    },
    {
      "epoch": 1.3370666666666666,
      "grad_norm": 0.18437513709068298,
      "learning_rate": 3.328666666666667e-05,
      "loss": 0.0024,
      "step": 25070
    },
    {
      "epoch": 1.3376000000000001,
      "grad_norm": 0.4742046892642975,
      "learning_rate": 3.328e-05,
      "loss": 0.002,
      "step": 25080
    },
    {
      "epoch": 1.3381333333333334,
      "grad_norm": 0.28692805767059326,
      "learning_rate": 3.327333333333333e-05,
      "loss": 0.0031,
      "step": 25090
    },
    {
      "epoch": 1.3386666666666667,
      "grad_norm": 0.20901136100292206,
      "learning_rate": 3.326666666666667e-05,
      "loss": 0.0026,
      "step": 25100
    },
    {
      "epoch": 1.3392,
      "grad_norm": 0.33889704942703247,
      "learning_rate": 3.3260000000000003e-05,
      "loss": 0.0028,
      "step": 25110
    },
    {
      "epoch": 1.3397333333333332,
      "grad_norm": 0.11577433347702026,
      "learning_rate": 3.3253333333333336e-05,
      "loss": 0.0032,
      "step": 25120
    },
    {
      "epoch": 1.3402666666666667,
      "grad_norm": 0.3004201650619507,
      "learning_rate": 3.324666666666667e-05,
      "loss": 0.0029,
      "step": 25130
    },
    {
      "epoch": 1.3408,
      "grad_norm": 0.2101096361875534,
      "learning_rate": 3.324e-05,
      "loss": 0.0023,
      "step": 25140
    },
    {
      "epoch": 1.3413333333333333,
      "grad_norm": 0.07017949223518372,
      "learning_rate": 3.323333333333333e-05,
      "loss": 0.0024,
      "step": 25150
    },
    {
      "epoch": 1.3418666666666668,
      "grad_norm": 0.2156364917755127,
      "learning_rate": 3.3226666666666665e-05,
      "loss": 0.0033,
      "step": 25160
    },
    {
      "epoch": 1.3424,
      "grad_norm": 0.361511766910553,
      "learning_rate": 3.3220000000000004e-05,
      "loss": 0.0026,
      "step": 25170
    },
    {
      "epoch": 1.3429333333333333,
      "grad_norm": 0.3380797207355499,
      "learning_rate": 3.3213333333333336e-05,
      "loss": 0.0033,
      "step": 25180
    },
    {
      "epoch": 1.3434666666666666,
      "grad_norm": 0.20846788585186005,
      "learning_rate": 3.320666666666667e-05,
      "loss": 0.0029,
      "step": 25190
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.08350590616464615,
      "learning_rate": 3.32e-05,
      "loss": 0.0031,
      "step": 25200
    },
    {
      "epoch": 1.3445333333333334,
      "grad_norm": 0.19861029088497162,
      "learning_rate": 3.319333333333334e-05,
      "loss": 0.0017,
      "step": 25210
    },
    {
      "epoch": 1.3450666666666666,
      "grad_norm": 0.3810364007949829,
      "learning_rate": 3.3186666666666665e-05,
      "loss": 0.0026,
      "step": 25220
    },
    {
      "epoch": 1.3456000000000001,
      "grad_norm": 0.44552043080329895,
      "learning_rate": 3.318e-05,
      "loss": 0.002,
      "step": 25230
    },
    {
      "epoch": 1.3461333333333334,
      "grad_norm": 0.22991177439689636,
      "learning_rate": 3.3173333333333336e-05,
      "loss": 0.0023,
      "step": 25240
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.29384469985961914,
      "learning_rate": 3.316666666666667e-05,
      "loss": 0.0038,
      "step": 25250
    },
    {
      "epoch": 1.3472,
      "grad_norm": 0.3032802641391754,
      "learning_rate": 3.316e-05,
      "loss": 0.0034,
      "step": 25260
    },
    {
      "epoch": 1.3477333333333332,
      "grad_norm": 0.3332419991493225,
      "learning_rate": 3.315333333333333e-05,
      "loss": 0.0032,
      "step": 25270
    },
    {
      "epoch": 1.3482666666666667,
      "grad_norm": 0.3042418956756592,
      "learning_rate": 3.314666666666667e-05,
      "loss": 0.0035,
      "step": 25280
    },
    {
      "epoch": 1.3488,
      "grad_norm": 0.10924491286277771,
      "learning_rate": 3.314e-05,
      "loss": 0.0023,
      "step": 25290
    },
    {
      "epoch": 1.3493333333333333,
      "grad_norm": 0.0993485078215599,
      "learning_rate": 3.313333333333333e-05,
      "loss": 0.0025,
      "step": 25300
    },
    {
      "epoch": 1.3498666666666668,
      "grad_norm": 0.399890661239624,
      "learning_rate": 3.312666666666667e-05,
      "loss": 0.0024,
      "step": 25310
    },
    {
      "epoch": 1.3504,
      "grad_norm": 0.09149439632892609,
      "learning_rate": 3.312e-05,
      "loss": 0.0031,
      "step": 25320
    },
    {
      "epoch": 1.3509333333333333,
      "grad_norm": 0.4612995386123657,
      "learning_rate": 3.3113333333333334e-05,
      "loss": 0.0016,
      "step": 25330
    },
    {
      "epoch": 1.3514666666666666,
      "grad_norm": 0.4756952226161957,
      "learning_rate": 3.3106666666666666e-05,
      "loss": 0.0025,
      "step": 25340
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.7592286467552185,
      "learning_rate": 3.3100000000000005e-05,
      "loss": 0.002,
      "step": 25350
    },
    {
      "epoch": 1.3525333333333334,
      "grad_norm": 0.4765957295894623,
      "learning_rate": 3.309333333333334e-05,
      "loss": 0.0018,
      "step": 25360
    },
    {
      "epoch": 1.3530666666666666,
      "grad_norm": 0.3456915616989136,
      "learning_rate": 3.308666666666666e-05,
      "loss": 0.004,
      "step": 25370
    },
    {
      "epoch": 1.3536000000000001,
      "grad_norm": 0.40710505843162537,
      "learning_rate": 3.308e-05,
      "loss": 0.0027,
      "step": 25380
    },
    {
      "epoch": 1.3541333333333334,
      "grad_norm": 0.13702601194381714,
      "learning_rate": 3.3073333333333334e-05,
      "loss": 0.0018,
      "step": 25390
    },
    {
      "epoch": 1.3546666666666667,
      "grad_norm": 0.1519218236207962,
      "learning_rate": 3.3066666666666666e-05,
      "loss": 0.0024,
      "step": 25400
    },
    {
      "epoch": 1.3552,
      "grad_norm": 0.34747564792633057,
      "learning_rate": 3.3060000000000005e-05,
      "loss": 0.0034,
      "step": 25410
    },
    {
      "epoch": 1.3557333333333332,
      "grad_norm": 0.19259783625602722,
      "learning_rate": 3.305333333333334e-05,
      "loss": 0.002,
      "step": 25420
    },
    {
      "epoch": 1.3562666666666667,
      "grad_norm": 0.5020635724067688,
      "learning_rate": 3.304666666666667e-05,
      "loss": 0.002,
      "step": 25430
    },
    {
      "epoch": 1.3568,
      "grad_norm": 0.13054156303405762,
      "learning_rate": 3.304e-05,
      "loss": 0.0019,
      "step": 25440
    },
    {
      "epoch": 1.3573333333333333,
      "grad_norm": 0.7248405814170837,
      "learning_rate": 3.3033333333333334e-05,
      "loss": 0.0021,
      "step": 25450
    },
    {
      "epoch": 1.3578666666666668,
      "grad_norm": 0.2812318205833435,
      "learning_rate": 3.302666666666667e-05,
      "loss": 0.0048,
      "step": 25460
    },
    {
      "epoch": 1.3584,
      "grad_norm": 0.5902683734893799,
      "learning_rate": 3.302e-05,
      "loss": 0.0025,
      "step": 25470
    },
    {
      "epoch": 1.3589333333333333,
      "grad_norm": 0.12117008119821548,
      "learning_rate": 3.301333333333334e-05,
      "loss": 0.003,
      "step": 25480
    },
    {
      "epoch": 1.3594666666666666,
      "grad_norm": 0.6789407730102539,
      "learning_rate": 3.300666666666667e-05,
      "loss": 0.0021,
      "step": 25490
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.2760304808616638,
      "learning_rate": 3.3e-05,
      "loss": 0.0023,
      "step": 25500
    },
    {
      "epoch": 1.3605333333333334,
      "grad_norm": 0.0636003315448761,
      "learning_rate": 3.2993333333333335e-05,
      "loss": 0.0021,
      "step": 25510
    },
    {
      "epoch": 1.3610666666666666,
      "grad_norm": 0.1738506704568863,
      "learning_rate": 3.298666666666667e-05,
      "loss": 0.0027,
      "step": 25520
    },
    {
      "epoch": 1.3616,
      "grad_norm": 0.08425439894199371,
      "learning_rate": 3.298e-05,
      "loss": 0.0027,
      "step": 25530
    },
    {
      "epoch": 1.3621333333333334,
      "grad_norm": 0.13184595108032227,
      "learning_rate": 3.297333333333333e-05,
      "loss": 0.0024,
      "step": 25540
    },
    {
      "epoch": 1.3626666666666667,
      "grad_norm": 0.6500648260116577,
      "learning_rate": 3.296666666666667e-05,
      "loss": 0.0023,
      "step": 25550
    },
    {
      "epoch": 1.3632,
      "grad_norm": 0.20479600131511688,
      "learning_rate": 3.296e-05,
      "loss": 0.0029,
      "step": 25560
    },
    {
      "epoch": 1.3637333333333332,
      "grad_norm": 0.35026589035987854,
      "learning_rate": 3.2953333333333335e-05,
      "loss": 0.003,
      "step": 25570
    },
    {
      "epoch": 1.3642666666666667,
      "grad_norm": 0.11021485924720764,
      "learning_rate": 3.294666666666667e-05,
      "loss": 0.0023,
      "step": 25580
    },
    {
      "epoch": 1.3648,
      "grad_norm": 0.1515258252620697,
      "learning_rate": 3.2940000000000006e-05,
      "loss": 0.0022,
      "step": 25590
    },
    {
      "epoch": 1.3653333333333333,
      "grad_norm": 0.15806210041046143,
      "learning_rate": 3.293333333333333e-05,
      "loss": 0.0034,
      "step": 25600
    },
    {
      "epoch": 1.3658666666666668,
      "grad_norm": 0.1720208078622818,
      "learning_rate": 3.2926666666666664e-05,
      "loss": 0.0033,
      "step": 25610
    },
    {
      "epoch": 1.3664,
      "grad_norm": 0.3632263243198395,
      "learning_rate": 3.292e-05,
      "loss": 0.0036,
      "step": 25620
    },
    {
      "epoch": 1.3669333333333333,
      "grad_norm": 0.37376219034194946,
      "learning_rate": 3.2913333333333336e-05,
      "loss": 0.0032,
      "step": 25630
    },
    {
      "epoch": 1.3674666666666666,
      "grad_norm": 0.288622111082077,
      "learning_rate": 3.290666666666667e-05,
      "loss": 0.0027,
      "step": 25640
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.20823927223682404,
      "learning_rate": 3.29e-05,
      "loss": 0.002,
      "step": 25650
    },
    {
      "epoch": 1.3685333333333334,
      "grad_norm": 0.657109797000885,
      "learning_rate": 3.289333333333334e-05,
      "loss": 0.0031,
      "step": 25660
    },
    {
      "epoch": 1.3690666666666667,
      "grad_norm": 0.27191150188446045,
      "learning_rate": 3.2886666666666665e-05,
      "loss": 0.003,
      "step": 25670
    },
    {
      "epoch": 1.3696,
      "grad_norm": 0.16484631597995758,
      "learning_rate": 3.288e-05,
      "loss": 0.0024,
      "step": 25680
    },
    {
      "epoch": 1.3701333333333334,
      "grad_norm": 0.8057348132133484,
      "learning_rate": 3.2873333333333336e-05,
      "loss": 0.0024,
      "step": 25690
    },
    {
      "epoch": 1.3706666666666667,
      "grad_norm": 0.19271960854530334,
      "learning_rate": 3.286666666666667e-05,
      "loss": 0.0035,
      "step": 25700
    },
    {
      "epoch": 1.3712,
      "grad_norm": 0.23978453874588013,
      "learning_rate": 3.286e-05,
      "loss": 0.0028,
      "step": 25710
    },
    {
      "epoch": 1.3717333333333332,
      "grad_norm": 0.11986556649208069,
      "learning_rate": 3.285333333333333e-05,
      "loss": 0.0023,
      "step": 25720
    },
    {
      "epoch": 1.3722666666666667,
      "grad_norm": 0.17399796843528748,
      "learning_rate": 3.284666666666667e-05,
      "loss": 0.0046,
      "step": 25730
    },
    {
      "epoch": 1.3728,
      "grad_norm": 0.3740507960319519,
      "learning_rate": 3.2840000000000004e-05,
      "loss": 0.0032,
      "step": 25740
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.07431589812040329,
      "learning_rate": 3.283333333333333e-05,
      "loss": 0.0031,
      "step": 25750
    },
    {
      "epoch": 1.3738666666666668,
      "grad_norm": 0.3768744468688965,
      "learning_rate": 3.282666666666667e-05,
      "loss": 0.0026,
      "step": 25760
    },
    {
      "epoch": 1.3744,
      "grad_norm": 0.31910696625709534,
      "learning_rate": 3.282e-05,
      "loss": 0.003,
      "step": 25770
    },
    {
      "epoch": 1.3749333333333333,
      "grad_norm": 0.1408458948135376,
      "learning_rate": 3.281333333333333e-05,
      "loss": 0.0025,
      "step": 25780
    },
    {
      "epoch": 1.3754666666666666,
      "grad_norm": 0.4412260949611664,
      "learning_rate": 3.280666666666667e-05,
      "loss": 0.002,
      "step": 25790
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.2455127239227295,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 0.0023,
      "step": 25800
    },
    {
      "epoch": 1.3765333333333334,
      "grad_norm": 0.6053543090820312,
      "learning_rate": 3.279333333333334e-05,
      "loss": 0.0019,
      "step": 25810
    },
    {
      "epoch": 1.3770666666666667,
      "grad_norm": 0.5422885417938232,
      "learning_rate": 3.278666666666666e-05,
      "loss": 0.0023,
      "step": 25820
    },
    {
      "epoch": 1.3776,
      "grad_norm": 0.07643944770097733,
      "learning_rate": 3.278e-05,
      "loss": 0.0042,
      "step": 25830
    },
    {
      "epoch": 1.3781333333333334,
      "grad_norm": 0.41919997334480286,
      "learning_rate": 3.2773333333333334e-05,
      "loss": 0.0023,
      "step": 25840
    },
    {
      "epoch": 1.3786666666666667,
      "grad_norm": 0.45180433988571167,
      "learning_rate": 3.2766666666666666e-05,
      "loss": 0.0029,
      "step": 25850
    },
    {
      "epoch": 1.3792,
      "grad_norm": 0.4737626910209656,
      "learning_rate": 3.2760000000000005e-05,
      "loss": 0.003,
      "step": 25860
    },
    {
      "epoch": 1.3797333333333333,
      "grad_norm": 0.19987308979034424,
      "learning_rate": 3.275333333333334e-05,
      "loss": 0.0026,
      "step": 25870
    },
    {
      "epoch": 1.3802666666666665,
      "grad_norm": 0.5787431597709656,
      "learning_rate": 3.274666666666667e-05,
      "loss": 0.0031,
      "step": 25880
    },
    {
      "epoch": 1.3808,
      "grad_norm": 0.24180179834365845,
      "learning_rate": 3.274e-05,
      "loss": 0.0019,
      "step": 25890
    },
    {
      "epoch": 1.3813333333333333,
      "grad_norm": 0.38536959886550903,
      "learning_rate": 3.2733333333333334e-05,
      "loss": 0.0021,
      "step": 25900
    },
    {
      "epoch": 1.3818666666666668,
      "grad_norm": 0.23078174889087677,
      "learning_rate": 3.2726666666666666e-05,
      "loss": 0.0032,
      "step": 25910
    },
    {
      "epoch": 1.3824,
      "grad_norm": 0.45713186264038086,
      "learning_rate": 3.272e-05,
      "loss": 0.0025,
      "step": 25920
    },
    {
      "epoch": 1.3829333333333333,
      "grad_norm": 0.26191583275794983,
      "learning_rate": 3.271333333333334e-05,
      "loss": 0.0028,
      "step": 25930
    },
    {
      "epoch": 1.3834666666666666,
      "grad_norm": 0.26238757371902466,
      "learning_rate": 3.270666666666667e-05,
      "loss": 0.003,
      "step": 25940
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.4347759187221527,
      "learning_rate": 3.27e-05,
      "loss": 0.0028,
      "step": 25950
    },
    {
      "epoch": 1.3845333333333334,
      "grad_norm": 0.39760711789131165,
      "learning_rate": 3.2693333333333334e-05,
      "loss": 0.0023,
      "step": 25960
    },
    {
      "epoch": 1.3850666666666667,
      "grad_norm": 0.38150274753570557,
      "learning_rate": 3.268666666666667e-05,
      "loss": 0.0025,
      "step": 25970
    },
    {
      "epoch": 1.3856,
      "grad_norm": 0.28486350178718567,
      "learning_rate": 3.268e-05,
      "loss": 0.0039,
      "step": 25980
    },
    {
      "epoch": 1.3861333333333334,
      "grad_norm": 0.13621212542057037,
      "learning_rate": 3.267333333333333e-05,
      "loss": 0.0026,
      "step": 25990
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.2960027754306793,
      "learning_rate": 3.266666666666667e-05,
      "loss": 0.0029,
      "step": 26000
    },
    {
      "epoch": 1.3872,
      "grad_norm": 0.28196048736572266,
      "learning_rate": 3.266e-05,
      "loss": 0.0039,
      "step": 26010
    },
    {
      "epoch": 1.3877333333333333,
      "grad_norm": 0.4364856779575348,
      "learning_rate": 3.2653333333333335e-05,
      "loss": 0.002,
      "step": 26020
    },
    {
      "epoch": 1.3882666666666665,
      "grad_norm": 0.20255985856056213,
      "learning_rate": 3.264666666666667e-05,
      "loss": 0.0026,
      "step": 26030
    },
    {
      "epoch": 1.3888,
      "grad_norm": 0.4718708395957947,
      "learning_rate": 3.2640000000000006e-05,
      "loss": 0.0032,
      "step": 26040
    },
    {
      "epoch": 1.3893333333333333,
      "grad_norm": 0.38704442977905273,
      "learning_rate": 3.263333333333333e-05,
      "loss": 0.0028,
      "step": 26050
    },
    {
      "epoch": 1.3898666666666666,
      "grad_norm": 0.07456235587596893,
      "learning_rate": 3.2626666666666664e-05,
      "loss": 0.002,
      "step": 26060
    },
    {
      "epoch": 1.3904,
      "grad_norm": 0.3317018747329712,
      "learning_rate": 3.262e-05,
      "loss": 0.0021,
      "step": 26070
    },
    {
      "epoch": 1.3909333333333334,
      "grad_norm": 0.6198607683181763,
      "learning_rate": 3.2613333333333335e-05,
      "loss": 0.0025,
      "step": 26080
    },
    {
      "epoch": 1.3914666666666666,
      "grad_norm": 0.38139837980270386,
      "learning_rate": 3.260666666666667e-05,
      "loss": 0.0028,
      "step": 26090
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.40593674778938293,
      "learning_rate": 3.26e-05,
      "loss": 0.0026,
      "step": 26100
    },
    {
      "epoch": 1.3925333333333334,
      "grad_norm": 0.2762509882450104,
      "learning_rate": 3.259333333333334e-05,
      "loss": 0.0041,
      "step": 26110
    },
    {
      "epoch": 1.3930666666666667,
      "grad_norm": 0.10807579010725021,
      "learning_rate": 3.258666666666667e-05,
      "loss": 0.0041,
      "step": 26120
    },
    {
      "epoch": 1.3936,
      "grad_norm": 0.1133151724934578,
      "learning_rate": 3.2579999999999996e-05,
      "loss": 0.0034,
      "step": 26130
    },
    {
      "epoch": 1.3941333333333334,
      "grad_norm": 0.38807758688926697,
      "learning_rate": 3.2573333333333335e-05,
      "loss": 0.0035,
      "step": 26140
    },
    {
      "epoch": 1.3946666666666667,
      "grad_norm": 0.18711736798286438,
      "learning_rate": 3.256666666666667e-05,
      "loss": 0.0034,
      "step": 26150
    },
    {
      "epoch": 1.3952,
      "grad_norm": 0.6639977097511292,
      "learning_rate": 3.256e-05,
      "loss": 0.0024,
      "step": 26160
    },
    {
      "epoch": 1.3957333333333333,
      "grad_norm": 0.6275253891944885,
      "learning_rate": 3.255333333333334e-05,
      "loss": 0.004,
      "step": 26170
    },
    {
      "epoch": 1.3962666666666665,
      "grad_norm": 0.12237454950809479,
      "learning_rate": 3.254666666666667e-05,
      "loss": 0.0024,
      "step": 26180
    },
    {
      "epoch": 1.3968,
      "grad_norm": 0.12794865667819977,
      "learning_rate": 3.2540000000000004e-05,
      "loss": 0.003,
      "step": 26190
    },
    {
      "epoch": 1.3973333333333333,
      "grad_norm": 0.24891094863414764,
      "learning_rate": 3.253333333333333e-05,
      "loss": 0.0023,
      "step": 26200
    },
    {
      "epoch": 1.3978666666666666,
      "grad_norm": 0.06622695922851562,
      "learning_rate": 3.252666666666667e-05,
      "loss": 0.0034,
      "step": 26210
    },
    {
      "epoch": 1.3984,
      "grad_norm": 0.10639366507530212,
      "learning_rate": 3.252e-05,
      "loss": 0.0029,
      "step": 26220
    },
    {
      "epoch": 1.3989333333333334,
      "grad_norm": 0.17327086627483368,
      "learning_rate": 3.251333333333333e-05,
      "loss": 0.0024,
      "step": 26230
    },
    {
      "epoch": 1.3994666666666666,
      "grad_norm": 0.6164692640304565,
      "learning_rate": 3.250666666666667e-05,
      "loss": 0.0026,
      "step": 26240
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.1287209689617157,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 0.0022,
      "step": 26250
    },
    {
      "epoch": 1.4005333333333334,
      "grad_norm": 1.1046191453933716,
      "learning_rate": 3.2493333333333336e-05,
      "loss": 0.0028,
      "step": 26260
    },
    {
      "epoch": 1.4010666666666667,
      "grad_norm": 0.19634664058685303,
      "learning_rate": 3.248666666666667e-05,
      "loss": 0.0024,
      "step": 26270
    },
    {
      "epoch": 1.4016,
      "grad_norm": 0.5526947975158691,
      "learning_rate": 3.248e-05,
      "loss": 0.0036,
      "step": 26280
    },
    {
      "epoch": 1.4021333333333335,
      "grad_norm": 0.48324957489967346,
      "learning_rate": 3.247333333333333e-05,
      "loss": 0.0031,
      "step": 26290
    },
    {
      "epoch": 1.4026666666666667,
      "grad_norm": 0.19220218062400818,
      "learning_rate": 3.2466666666666665e-05,
      "loss": 0.0034,
      "step": 26300
    },
    {
      "epoch": 1.4032,
      "grad_norm": 0.7694095373153687,
      "learning_rate": 3.2460000000000004e-05,
      "loss": 0.0032,
      "step": 26310
    },
    {
      "epoch": 1.4037333333333333,
      "grad_norm": 0.2461523562669754,
      "learning_rate": 3.2453333333333337e-05,
      "loss": 0.002,
      "step": 26320
    },
    {
      "epoch": 1.4042666666666666,
      "grad_norm": 0.4307243525981903,
      "learning_rate": 3.244666666666667e-05,
      "loss": 0.0026,
      "step": 26330
    },
    {
      "epoch": 1.4048,
      "grad_norm": 0.43521857261657715,
      "learning_rate": 3.244e-05,
      "loss": 0.002,
      "step": 26340
    },
    {
      "epoch": 1.4053333333333333,
      "grad_norm": 0.21869097650051117,
      "learning_rate": 3.243333333333333e-05,
      "loss": 0.0022,
      "step": 26350
    },
    {
      "epoch": 1.4058666666666666,
      "grad_norm": 0.3155236840248108,
      "learning_rate": 3.2426666666666666e-05,
      "loss": 0.0027,
      "step": 26360
    },
    {
      "epoch": 1.4064,
      "grad_norm": 0.6277117729187012,
      "learning_rate": 3.242e-05,
      "loss": 0.0029,
      "step": 26370
    },
    {
      "epoch": 1.4069333333333334,
      "grad_norm": 0.2528791129589081,
      "learning_rate": 3.241333333333334e-05,
      "loss": 0.002,
      "step": 26380
    },
    {
      "epoch": 1.4074666666666666,
      "grad_norm": 0.5072833299636841,
      "learning_rate": 3.240666666666667e-05,
      "loss": 0.0027,
      "step": 26390
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.17997334897518158,
      "learning_rate": 3.24e-05,
      "loss": 0.0038,
      "step": 26400
    },
    {
      "epoch": 1.4085333333333334,
      "grad_norm": 0.24841591715812683,
      "learning_rate": 3.2393333333333334e-05,
      "loss": 0.0026,
      "step": 26410
    },
    {
      "epoch": 1.4090666666666667,
      "grad_norm": 0.16501827538013458,
      "learning_rate": 3.238666666666667e-05,
      "loss": 0.002,
      "step": 26420
    },
    {
      "epoch": 1.4096,
      "grad_norm": 0.4166722297668457,
      "learning_rate": 3.238e-05,
      "loss": 0.0043,
      "step": 26430
    },
    {
      "epoch": 1.4101333333333335,
      "grad_norm": 0.5831817984580994,
      "learning_rate": 3.237333333333333e-05,
      "loss": 0.0036,
      "step": 26440
    },
    {
      "epoch": 1.4106666666666667,
      "grad_norm": 0.308795303106308,
      "learning_rate": 3.236666666666667e-05,
      "loss": 0.0031,
      "step": 26450
    },
    {
      "epoch": 1.4112,
      "grad_norm": 0.5327373743057251,
      "learning_rate": 3.236e-05,
      "loss": 0.0021,
      "step": 26460
    },
    {
      "epoch": 1.4117333333333333,
      "grad_norm": 0.42407095432281494,
      "learning_rate": 3.2353333333333334e-05,
      "loss": 0.0019,
      "step": 26470
    },
    {
      "epoch": 1.4122666666666666,
      "grad_norm": 0.22111964225769043,
      "learning_rate": 3.2346666666666666e-05,
      "loss": 0.0027,
      "step": 26480
    },
    {
      "epoch": 1.4128,
      "grad_norm": 0.07617220282554626,
      "learning_rate": 3.2340000000000005e-05,
      "loss": 0.0022,
      "step": 26490
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.1791311353445053,
      "learning_rate": 3.233333333333333e-05,
      "loss": 0.002,
      "step": 26500
    },
    {
      "epoch": 1.4138666666666666,
      "grad_norm": 0.33626288175582886,
      "learning_rate": 3.232666666666666e-05,
      "loss": 0.003,
      "step": 26510
    },
    {
      "epoch": 1.4144,
      "grad_norm": 0.2629168927669525,
      "learning_rate": 3.232e-05,
      "loss": 0.0023,
      "step": 26520
    },
    {
      "epoch": 1.4149333333333334,
      "grad_norm": 0.2771930694580078,
      "learning_rate": 3.2313333333333335e-05,
      "loss": 0.0028,
      "step": 26530
    },
    {
      "epoch": 1.4154666666666667,
      "grad_norm": 0.3683573305606842,
      "learning_rate": 3.230666666666667e-05,
      "loss": 0.0023,
      "step": 26540
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.3947719633579254,
      "learning_rate": 3.2300000000000006e-05,
      "loss": 0.0032,
      "step": 26550
    },
    {
      "epoch": 1.4165333333333332,
      "grad_norm": 0.5522356629371643,
      "learning_rate": 3.229333333333334e-05,
      "loss": 0.0042,
      "step": 26560
    },
    {
      "epoch": 1.4170666666666667,
      "grad_norm": 0.13952957093715668,
      "learning_rate": 3.228666666666667e-05,
      "loss": 0.0023,
      "step": 26570
    },
    {
      "epoch": 1.4176,
      "grad_norm": 0.6384028196334839,
      "learning_rate": 3.2279999999999996e-05,
      "loss": 0.0028,
      "step": 26580
    },
    {
      "epoch": 1.4181333333333335,
      "grad_norm": 0.5376649498939514,
      "learning_rate": 3.2273333333333335e-05,
      "loss": 0.0021,
      "step": 26590
    },
    {
      "epoch": 1.4186666666666667,
      "grad_norm": 0.30991673469543457,
      "learning_rate": 3.226666666666667e-05,
      "loss": 0.0022,
      "step": 26600
    },
    {
      "epoch": 1.4192,
      "grad_norm": 0.17637744545936584,
      "learning_rate": 3.226e-05,
      "loss": 0.0026,
      "step": 26610
    },
    {
      "epoch": 1.4197333333333333,
      "grad_norm": 0.3771889805793762,
      "learning_rate": 3.225333333333334e-05,
      "loss": 0.002,
      "step": 26620
    },
    {
      "epoch": 1.4202666666666666,
      "grad_norm": 0.408401221036911,
      "learning_rate": 3.224666666666667e-05,
      "loss": 0.003,
      "step": 26630
    },
    {
      "epoch": 1.4208,
      "grad_norm": 0.35227149724960327,
      "learning_rate": 3.224e-05,
      "loss": 0.0025,
      "step": 26640
    },
    {
      "epoch": 1.4213333333333333,
      "grad_norm": 0.3231876790523529,
      "learning_rate": 3.2233333333333335e-05,
      "loss": 0.0026,
      "step": 26650
    },
    {
      "epoch": 1.4218666666666666,
      "grad_norm": 0.10826414823532104,
      "learning_rate": 3.222666666666667e-05,
      "loss": 0.0034,
      "step": 26660
    },
    {
      "epoch": 1.4224,
      "grad_norm": 0.39999958872795105,
      "learning_rate": 3.222e-05,
      "loss": 0.0026,
      "step": 26670
    },
    {
      "epoch": 1.4229333333333334,
      "grad_norm": 0.7947831749916077,
      "learning_rate": 3.221333333333333e-05,
      "loss": 0.0028,
      "step": 26680
    },
    {
      "epoch": 1.4234666666666667,
      "grad_norm": 0.15514996647834778,
      "learning_rate": 3.220666666666667e-05,
      "loss": 0.0019,
      "step": 26690
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.4855135679244995,
      "learning_rate": 3.2200000000000003e-05,
      "loss": 0.002,
      "step": 26700
    },
    {
      "epoch": 1.4245333333333332,
      "grad_norm": 0.15838579833507538,
      "learning_rate": 3.2193333333333336e-05,
      "loss": 0.0026,
      "step": 26710
    },
    {
      "epoch": 1.4250666666666667,
      "grad_norm": 0.3863876461982727,
      "learning_rate": 3.218666666666667e-05,
      "loss": 0.0019,
      "step": 26720
    },
    {
      "epoch": 1.4256,
      "grad_norm": 0.8765779733657837,
      "learning_rate": 3.218e-05,
      "loss": 0.0038,
      "step": 26730
    },
    {
      "epoch": 1.4261333333333333,
      "grad_norm": 0.16942955553531647,
      "learning_rate": 3.217333333333333e-05,
      "loss": 0.0019,
      "step": 26740
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.30999207496643066,
      "learning_rate": 3.2166666666666665e-05,
      "loss": 0.0024,
      "step": 26750
    },
    {
      "epoch": 1.4272,
      "grad_norm": 0.43964236974716187,
      "learning_rate": 3.2160000000000004e-05,
      "loss": 0.0022,
      "step": 26760
    },
    {
      "epoch": 1.4277333333333333,
      "grad_norm": 0.21696603298187256,
      "learning_rate": 3.2153333333333336e-05,
      "loss": 0.0034,
      "step": 26770
    },
    {
      "epoch": 1.4282666666666666,
      "grad_norm": 0.41317012906074524,
      "learning_rate": 3.214666666666667e-05,
      "loss": 0.0037,
      "step": 26780
    },
    {
      "epoch": 1.4288,
      "grad_norm": 0.13798412680625916,
      "learning_rate": 3.214e-05,
      "loss": 0.0032,
      "step": 26790
    },
    {
      "epoch": 1.4293333333333333,
      "grad_norm": 0.09240905195474625,
      "learning_rate": 3.213333333333334e-05,
      "loss": 0.0021,
      "step": 26800
    },
    {
      "epoch": 1.4298666666666666,
      "grad_norm": 0.6594820022583008,
      "learning_rate": 3.2126666666666665e-05,
      "loss": 0.0043,
      "step": 26810
    },
    {
      "epoch": 1.4304000000000001,
      "grad_norm": 0.19516092538833618,
      "learning_rate": 3.212e-05,
      "loss": 0.0022,
      "step": 26820
    },
    {
      "epoch": 1.4309333333333334,
      "grad_norm": 0.5486022233963013,
      "learning_rate": 3.2113333333333336e-05,
      "loss": 0.0021,
      "step": 26830
    },
    {
      "epoch": 1.4314666666666667,
      "grad_norm": 0.2002008706331253,
      "learning_rate": 3.210666666666667e-05,
      "loss": 0.0034,
      "step": 26840
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.31821951270103455,
      "learning_rate": 3.21e-05,
      "loss": 0.0019,
      "step": 26850
    },
    {
      "epoch": 1.4325333333333332,
      "grad_norm": 0.32233646512031555,
      "learning_rate": 3.209333333333333e-05,
      "loss": 0.0026,
      "step": 26860
    },
    {
      "epoch": 1.4330666666666667,
      "grad_norm": 0.32848668098449707,
      "learning_rate": 3.208666666666667e-05,
      "loss": 0.0019,
      "step": 26870
    },
    {
      "epoch": 1.4336,
      "grad_norm": 0.17344605922698975,
      "learning_rate": 3.208e-05,
      "loss": 0.0035,
      "step": 26880
    },
    {
      "epoch": 1.4341333333333333,
      "grad_norm": 0.2691608965396881,
      "learning_rate": 3.207333333333333e-05,
      "loss": 0.002,
      "step": 26890
    },
    {
      "epoch": 1.4346666666666668,
      "grad_norm": 0.4667862355709076,
      "learning_rate": 3.206666666666667e-05,
      "loss": 0.0022,
      "step": 26900
    },
    {
      "epoch": 1.4352,
      "grad_norm": 0.32915693521499634,
      "learning_rate": 3.206e-05,
      "loss": 0.0034,
      "step": 26910
    },
    {
      "epoch": 1.4357333333333333,
      "grad_norm": 0.3558174967765808,
      "learning_rate": 3.2053333333333334e-05,
      "loss": 0.0025,
      "step": 26920
    },
    {
      "epoch": 1.4362666666666666,
      "grad_norm": 0.19229677319526672,
      "learning_rate": 3.2046666666666666e-05,
      "loss": 0.0023,
      "step": 26930
    },
    {
      "epoch": 1.4368,
      "grad_norm": 0.2655305564403534,
      "learning_rate": 3.2040000000000005e-05,
      "loss": 0.002,
      "step": 26940
    },
    {
      "epoch": 1.4373333333333334,
      "grad_norm": 0.3933636546134949,
      "learning_rate": 3.203333333333334e-05,
      "loss": 0.0021,
      "step": 26950
    },
    {
      "epoch": 1.4378666666666666,
      "grad_norm": 0.3527369499206543,
      "learning_rate": 3.202666666666666e-05,
      "loss": 0.0026,
      "step": 26960
    },
    {
      "epoch": 1.4384000000000001,
      "grad_norm": 0.4965149164199829,
      "learning_rate": 3.202e-05,
      "loss": 0.0018,
      "step": 26970
    },
    {
      "epoch": 1.4389333333333334,
      "grad_norm": 0.45303332805633545,
      "learning_rate": 3.2013333333333334e-05,
      "loss": 0.003,
      "step": 26980
    },
    {
      "epoch": 1.4394666666666667,
      "grad_norm": 1.0821422338485718,
      "learning_rate": 3.2006666666666666e-05,
      "loss": 0.0018,
      "step": 26990
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.2073269635438919,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.0021,
      "step": 27000
    },
    {
      "epoch": 1.4405333333333332,
      "grad_norm": 0.6770918369293213,
      "learning_rate": 3.199333333333334e-05,
      "loss": 0.0021,
      "step": 27010
    },
    {
      "epoch": 1.4410666666666667,
      "grad_norm": 0.7057942152023315,
      "learning_rate": 3.198666666666667e-05,
      "loss": 0.0024,
      "step": 27020
    },
    {
      "epoch": 1.4416,
      "grad_norm": 0.24229705333709717,
      "learning_rate": 3.198e-05,
      "loss": 0.0019,
      "step": 27030
    },
    {
      "epoch": 1.4421333333333333,
      "grad_norm": 0.470601350069046,
      "learning_rate": 3.1973333333333334e-05,
      "loss": 0.0028,
      "step": 27040
    },
    {
      "epoch": 1.4426666666666668,
      "grad_norm": 0.13224215805530548,
      "learning_rate": 3.196666666666667e-05,
      "loss": 0.0026,
      "step": 27050
    },
    {
      "epoch": 1.4432,
      "grad_norm": 0.26360437273979187,
      "learning_rate": 3.196e-05,
      "loss": 0.0032,
      "step": 27060
    },
    {
      "epoch": 1.4437333333333333,
      "grad_norm": 0.485313355922699,
      "learning_rate": 3.195333333333334e-05,
      "loss": 0.0027,
      "step": 27070
    },
    {
      "epoch": 1.4442666666666666,
      "grad_norm": 0.35833099484443665,
      "learning_rate": 3.194666666666667e-05,
      "loss": 0.0028,
      "step": 27080
    },
    {
      "epoch": 1.4447999999999999,
      "grad_norm": 0.05653698742389679,
      "learning_rate": 3.194e-05,
      "loss": 0.0029,
      "step": 27090
    },
    {
      "epoch": 1.4453333333333334,
      "grad_norm": 0.3657216727733612,
      "learning_rate": 3.1933333333333335e-05,
      "loss": 0.0024,
      "step": 27100
    },
    {
      "epoch": 1.4458666666666666,
      "grad_norm": 0.13507471978664398,
      "learning_rate": 3.192666666666667e-05,
      "loss": 0.0023,
      "step": 27110
    },
    {
      "epoch": 1.4464000000000001,
      "grad_norm": 0.2494610846042633,
      "learning_rate": 3.192e-05,
      "loss": 0.0032,
      "step": 27120
    },
    {
      "epoch": 1.4469333333333334,
      "grad_norm": 0.09276450425386429,
      "learning_rate": 3.191333333333333e-05,
      "loss": 0.0027,
      "step": 27130
    },
    {
      "epoch": 1.4474666666666667,
      "grad_norm": 0.7564230561256409,
      "learning_rate": 3.190666666666667e-05,
      "loss": 0.002,
      "step": 27140
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.5105196833610535,
      "learning_rate": 3.19e-05,
      "loss": 0.0022,
      "step": 27150
    },
    {
      "epoch": 1.4485333333333332,
      "grad_norm": 0.13764801621437073,
      "learning_rate": 3.1893333333333335e-05,
      "loss": 0.0034,
      "step": 27160
    },
    {
      "epoch": 1.4490666666666667,
      "grad_norm": 0.5519195795059204,
      "learning_rate": 3.188666666666667e-05,
      "loss": 0.0027,
      "step": 27170
    },
    {
      "epoch": 1.4496,
      "grad_norm": 0.24377843737602234,
      "learning_rate": 3.188e-05,
      "loss": 0.0027,
      "step": 27180
    },
    {
      "epoch": 1.4501333333333333,
      "grad_norm": 0.19001689553260803,
      "learning_rate": 3.187333333333333e-05,
      "loss": 0.0029,
      "step": 27190
    },
    {
      "epoch": 1.4506666666666668,
      "grad_norm": 0.14741778373718262,
      "learning_rate": 3.1866666666666664e-05,
      "loss": 0.0043,
      "step": 27200
    },
    {
      "epoch": 1.4512,
      "grad_norm": 0.5865888595581055,
      "learning_rate": 3.186e-05,
      "loss": 0.0033,
      "step": 27210
    },
    {
      "epoch": 1.4517333333333333,
      "grad_norm": 0.3189382255077362,
      "learning_rate": 3.1853333333333336e-05,
      "loss": 0.0021,
      "step": 27220
    },
    {
      "epoch": 1.4522666666666666,
      "grad_norm": 0.4167414903640747,
      "learning_rate": 3.184666666666667e-05,
      "loss": 0.0035,
      "step": 27230
    },
    {
      "epoch": 1.4527999999999999,
      "grad_norm": 0.4329177737236023,
      "learning_rate": 3.184e-05,
      "loss": 0.0023,
      "step": 27240
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.4038051664829254,
      "learning_rate": 3.183333333333334e-05,
      "loss": 0.0035,
      "step": 27250
    },
    {
      "epoch": 1.4538666666666666,
      "grad_norm": 0.32023143768310547,
      "learning_rate": 3.1826666666666665e-05,
      "loss": 0.003,
      "step": 27260
    },
    {
      "epoch": 1.4544000000000001,
      "grad_norm": 0.19069692492485046,
      "learning_rate": 3.182e-05,
      "loss": 0.002,
      "step": 27270
    },
    {
      "epoch": 1.4549333333333334,
      "grad_norm": 0.6064380407333374,
      "learning_rate": 3.1813333333333336e-05,
      "loss": 0.0047,
      "step": 27280
    },
    {
      "epoch": 1.4554666666666667,
      "grad_norm": 0.5588524341583252,
      "learning_rate": 3.180666666666667e-05,
      "loss": 0.0024,
      "step": 27290
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.3688144087791443,
      "learning_rate": 3.18e-05,
      "loss": 0.0024,
      "step": 27300
    },
    {
      "epoch": 1.4565333333333332,
      "grad_norm": 0.16251109540462494,
      "learning_rate": 3.179333333333333e-05,
      "loss": 0.002,
      "step": 27310
    },
    {
      "epoch": 1.4570666666666667,
      "grad_norm": 0.5979400873184204,
      "learning_rate": 3.178666666666667e-05,
      "loss": 0.0019,
      "step": 27320
    },
    {
      "epoch": 1.4576,
      "grad_norm": 0.08663675934076309,
      "learning_rate": 3.1780000000000004e-05,
      "loss": 0.003,
      "step": 27330
    },
    {
      "epoch": 1.4581333333333333,
      "grad_norm": 0.3766189515590668,
      "learning_rate": 3.177333333333333e-05,
      "loss": 0.0035,
      "step": 27340
    },
    {
      "epoch": 1.4586666666666668,
      "grad_norm": 0.14174005389213562,
      "learning_rate": 3.176666666666667e-05,
      "loss": 0.0034,
      "step": 27350
    },
    {
      "epoch": 1.4592,
      "grad_norm": 0.22929659485816956,
      "learning_rate": 3.176e-05,
      "loss": 0.0023,
      "step": 27360
    },
    {
      "epoch": 1.4597333333333333,
      "grad_norm": 0.3366098403930664,
      "learning_rate": 3.175333333333333e-05,
      "loss": 0.0028,
      "step": 27370
    },
    {
      "epoch": 1.4602666666666666,
      "grad_norm": 0.13763552904129028,
      "learning_rate": 3.174666666666667e-05,
      "loss": 0.0028,
      "step": 27380
    },
    {
      "epoch": 1.4607999999999999,
      "grad_norm": 0.08622254431247711,
      "learning_rate": 3.1740000000000004e-05,
      "loss": 0.0022,
      "step": 27390
    },
    {
      "epoch": 1.4613333333333334,
      "grad_norm": 0.33613815903663635,
      "learning_rate": 3.173333333333334e-05,
      "loss": 0.0025,
      "step": 27400
    },
    {
      "epoch": 1.4618666666666666,
      "grad_norm": 0.5935376286506653,
      "learning_rate": 3.172666666666667e-05,
      "loss": 0.0022,
      "step": 27410
    },
    {
      "epoch": 1.4624,
      "grad_norm": 0.20258378982543945,
      "learning_rate": 3.172e-05,
      "loss": 0.0021,
      "step": 27420
    },
    {
      "epoch": 1.4629333333333334,
      "grad_norm": 0.10011966526508331,
      "learning_rate": 3.1713333333333334e-05,
      "loss": 0.0025,
      "step": 27430
    },
    {
      "epoch": 1.4634666666666667,
      "grad_norm": 0.10750603675842285,
      "learning_rate": 3.1706666666666666e-05,
      "loss": 0.0024,
      "step": 27440
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.4078805148601532,
      "learning_rate": 3.1700000000000005e-05,
      "loss": 0.0027,
      "step": 27450
    },
    {
      "epoch": 1.4645333333333332,
      "grad_norm": 0.33241698145866394,
      "learning_rate": 3.169333333333334e-05,
      "loss": 0.0024,
      "step": 27460
    },
    {
      "epoch": 1.4650666666666667,
      "grad_norm": 0.4517182409763336,
      "learning_rate": 3.168666666666667e-05,
      "loss": 0.0032,
      "step": 27470
    },
    {
      "epoch": 1.4656,
      "grad_norm": 0.25774839520454407,
      "learning_rate": 3.168e-05,
      "loss": 0.003,
      "step": 27480
    },
    {
      "epoch": 1.4661333333333333,
      "grad_norm": 0.2897982895374298,
      "learning_rate": 3.1673333333333334e-05,
      "loss": 0.0018,
      "step": 27490
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.19398517906665802,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 0.0026,
      "step": 27500
    },
    {
      "epoch": 1.4672,
      "grad_norm": 0.22461257874965668,
      "learning_rate": 3.166e-05,
      "loss": 0.0024,
      "step": 27510
    },
    {
      "epoch": 1.4677333333333333,
      "grad_norm": 0.452732115983963,
      "learning_rate": 3.165333333333334e-05,
      "loss": 0.0027,
      "step": 27520
    },
    {
      "epoch": 1.4682666666666666,
      "grad_norm": 0.1548391580581665,
      "learning_rate": 3.164666666666667e-05,
      "loss": 0.0031,
      "step": 27530
    },
    {
      "epoch": 1.4687999999999999,
      "grad_norm": 0.4246886670589447,
      "learning_rate": 3.164e-05,
      "loss": 0.0022,
      "step": 27540
    },
    {
      "epoch": 1.4693333333333334,
      "grad_norm": 0.28573480248451233,
      "learning_rate": 3.1633333333333334e-05,
      "loss": 0.0036,
      "step": 27550
    },
    {
      "epoch": 1.4698666666666667,
      "grad_norm": 0.8033183813095093,
      "learning_rate": 3.1626666666666667e-05,
      "loss": 0.002,
      "step": 27560
    },
    {
      "epoch": 1.4704,
      "grad_norm": 0.079546719789505,
      "learning_rate": 3.162e-05,
      "loss": 0.0021,
      "step": 27570
    },
    {
      "epoch": 1.4709333333333334,
      "grad_norm": 0.42562806606292725,
      "learning_rate": 3.161333333333333e-05,
      "loss": 0.0024,
      "step": 27580
    },
    {
      "epoch": 1.4714666666666667,
      "grad_norm": 0.3311501741409302,
      "learning_rate": 3.160666666666667e-05,
      "loss": 0.0027,
      "step": 27590
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.21038079261779785,
      "learning_rate": 3.16e-05,
      "loss": 0.0028,
      "step": 27600
    },
    {
      "epoch": 1.4725333333333332,
      "grad_norm": 0.17706014215946198,
      "learning_rate": 3.1593333333333335e-05,
      "loss": 0.0017,
      "step": 27610
    },
    {
      "epoch": 1.4730666666666667,
      "grad_norm": 0.46399709582328796,
      "learning_rate": 3.158666666666667e-05,
      "loss": 0.0023,
      "step": 27620
    },
    {
      "epoch": 1.4736,
      "grad_norm": 0.1828768253326416,
      "learning_rate": 3.1580000000000006e-05,
      "loss": 0.0031,
      "step": 27630
    },
    {
      "epoch": 1.4741333333333333,
      "grad_norm": 0.14647698402404785,
      "learning_rate": 3.157333333333333e-05,
      "loss": 0.0026,
      "step": 27640
    },
    {
      "epoch": 1.4746666666666668,
      "grad_norm": 0.7055908441543579,
      "learning_rate": 3.1566666666666664e-05,
      "loss": 0.0022,
      "step": 27650
    },
    {
      "epoch": 1.4752,
      "grad_norm": 0.33451080322265625,
      "learning_rate": 3.156e-05,
      "loss": 0.0025,
      "step": 27660
    },
    {
      "epoch": 1.4757333333333333,
      "grad_norm": 0.09780887514352798,
      "learning_rate": 3.1553333333333335e-05,
      "loss": 0.0025,
      "step": 27670
    },
    {
      "epoch": 1.4762666666666666,
      "grad_norm": 0.2994285821914673,
      "learning_rate": 3.154666666666667e-05,
      "loss": 0.0031,
      "step": 27680
    },
    {
      "epoch": 1.4768,
      "grad_norm": 0.200522780418396,
      "learning_rate": 3.154e-05,
      "loss": 0.0021,
      "step": 27690
    },
    {
      "epoch": 1.4773333333333334,
      "grad_norm": 0.19877150654792786,
      "learning_rate": 3.153333333333334e-05,
      "loss": 0.0023,
      "step": 27700
    },
    {
      "epoch": 1.4778666666666667,
      "grad_norm": 0.17780527472496033,
      "learning_rate": 3.1526666666666664e-05,
      "loss": 0.0024,
      "step": 27710
    },
    {
      "epoch": 1.4784,
      "grad_norm": 0.24600018560886383,
      "learning_rate": 3.1519999999999996e-05,
      "loss": 0.0025,
      "step": 27720
    },
    {
      "epoch": 1.4789333333333334,
      "grad_norm": 0.730042576789856,
      "learning_rate": 3.1513333333333335e-05,
      "loss": 0.0027,
      "step": 27730
    },
    {
      "epoch": 1.4794666666666667,
      "grad_norm": 0.4147069454193115,
      "learning_rate": 3.150666666666667e-05,
      "loss": 0.0022,
      "step": 27740
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.8204194903373718,
      "learning_rate": 3.15e-05,
      "loss": 0.0025,
      "step": 27750
    },
    {
      "epoch": 1.4805333333333333,
      "grad_norm": 0.6610513925552368,
      "learning_rate": 3.149333333333334e-05,
      "loss": 0.0025,
      "step": 27760
    },
    {
      "epoch": 1.4810666666666665,
      "grad_norm": 0.3959384262561798,
      "learning_rate": 3.148666666666667e-05,
      "loss": 0.0037,
      "step": 27770
    },
    {
      "epoch": 1.4816,
      "grad_norm": 0.23948873579502106,
      "learning_rate": 3.1480000000000004e-05,
      "loss": 0.0032,
      "step": 27780
    },
    {
      "epoch": 1.4821333333333333,
      "grad_norm": 0.3329029083251953,
      "learning_rate": 3.1473333333333336e-05,
      "loss": 0.0025,
      "step": 27790
    },
    {
      "epoch": 1.4826666666666668,
      "grad_norm": 0.38980069756507874,
      "learning_rate": 3.146666666666667e-05,
      "loss": 0.0029,
      "step": 27800
    },
    {
      "epoch": 1.4832,
      "grad_norm": 0.5167651772499084,
      "learning_rate": 3.146e-05,
      "loss": 0.0025,
      "step": 27810
    },
    {
      "epoch": 1.4837333333333333,
      "grad_norm": 0.16503369808197021,
      "learning_rate": 3.145333333333333e-05,
      "loss": 0.0022,
      "step": 27820
    },
    {
      "epoch": 1.4842666666666666,
      "grad_norm": 0.15084750950336456,
      "learning_rate": 3.144666666666667e-05,
      "loss": 0.0033,
      "step": 27830
    },
    {
      "epoch": 1.4848,
      "grad_norm": 0.3592413663864136,
      "learning_rate": 3.1440000000000004e-05,
      "loss": 0.0027,
      "step": 27840
    },
    {
      "epoch": 1.4853333333333334,
      "grad_norm": 0.27869096398353577,
      "learning_rate": 3.1433333333333336e-05,
      "loss": 0.0021,
      "step": 27850
    },
    {
      "epoch": 1.4858666666666667,
      "grad_norm": 0.5486170649528503,
      "learning_rate": 3.142666666666667e-05,
      "loss": 0.0021,
      "step": 27860
    },
    {
      "epoch": 1.4864,
      "grad_norm": 0.5789684653282166,
      "learning_rate": 3.142e-05,
      "loss": 0.0024,
      "step": 27870
    },
    {
      "epoch": 1.4869333333333334,
      "grad_norm": 0.17016349732875824,
      "learning_rate": 3.141333333333333e-05,
      "loss": 0.0019,
      "step": 27880
    },
    {
      "epoch": 1.4874666666666667,
      "grad_norm": 0.3128493130207062,
      "learning_rate": 3.1406666666666665e-05,
      "loss": 0.0019,
      "step": 27890
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.09870431572198868,
      "learning_rate": 3.1400000000000004e-05,
      "loss": 0.0025,
      "step": 27900
    },
    {
      "epoch": 1.4885333333333333,
      "grad_norm": 0.26106345653533936,
      "learning_rate": 3.1393333333333337e-05,
      "loss": 0.0029,
      "step": 27910
    },
    {
      "epoch": 1.4890666666666665,
      "grad_norm": 0.800879716873169,
      "learning_rate": 3.138666666666667e-05,
      "loss": 0.0028,
      "step": 27920
    },
    {
      "epoch": 1.4896,
      "grad_norm": 0.615361213684082,
      "learning_rate": 3.138e-05,
      "loss": 0.0034,
      "step": 27930
    },
    {
      "epoch": 1.4901333333333333,
      "grad_norm": 0.7158251404762268,
      "learning_rate": 3.137333333333333e-05,
      "loss": 0.0019,
      "step": 27940
    },
    {
      "epoch": 1.4906666666666666,
      "grad_norm": 0.1428016722202301,
      "learning_rate": 3.1366666666666666e-05,
      "loss": 0.0031,
      "step": 27950
    },
    {
      "epoch": 1.4912,
      "grad_norm": 0.40829503536224365,
      "learning_rate": 3.136e-05,
      "loss": 0.0053,
      "step": 27960
    },
    {
      "epoch": 1.4917333333333334,
      "grad_norm": 0.09133508056402206,
      "learning_rate": 3.135333333333334e-05,
      "loss": 0.0029,
      "step": 27970
    },
    {
      "epoch": 1.4922666666666666,
      "grad_norm": 0.34631744027137756,
      "learning_rate": 3.134666666666667e-05,
      "loss": 0.0029,
      "step": 27980
    },
    {
      "epoch": 1.4928,
      "grad_norm": 0.2065131813287735,
      "learning_rate": 3.134e-05,
      "loss": 0.0024,
      "step": 27990
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.09041086584329605,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 0.0024,
      "step": 28000
    },
    {
      "epoch": 1.4938666666666667,
      "grad_norm": 0.4163984954357147,
      "learning_rate": 3.132666666666667e-05,
      "loss": 0.0019,
      "step": 28010
    },
    {
      "epoch": 1.4944,
      "grad_norm": 0.45616379380226135,
      "learning_rate": 3.132e-05,
      "loss": 0.002,
      "step": 28020
    },
    {
      "epoch": 1.4949333333333334,
      "grad_norm": 0.5027026534080505,
      "learning_rate": 3.131333333333333e-05,
      "loss": 0.0045,
      "step": 28030
    },
    {
      "epoch": 1.4954666666666667,
      "grad_norm": 0.6388409733772278,
      "learning_rate": 3.130666666666667e-05,
      "loss": 0.0029,
      "step": 28040
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.2312084287405014,
      "learning_rate": 3.13e-05,
      "loss": 0.0022,
      "step": 28050
    },
    {
      "epoch": 1.4965333333333333,
      "grad_norm": 0.3500549793243408,
      "learning_rate": 3.1293333333333334e-05,
      "loss": 0.0029,
      "step": 28060
    },
    {
      "epoch": 1.4970666666666665,
      "grad_norm": 0.35615378618240356,
      "learning_rate": 3.1286666666666666e-05,
      "loss": 0.0037,
      "step": 28070
    },
    {
      "epoch": 1.4976,
      "grad_norm": 0.3122791051864624,
      "learning_rate": 3.1280000000000005e-05,
      "loss": 0.0021,
      "step": 28080
    },
    {
      "epoch": 1.4981333333333333,
      "grad_norm": 0.44177183508872986,
      "learning_rate": 3.127333333333333e-05,
      "loss": 0.0025,
      "step": 28090
    },
    {
      "epoch": 1.4986666666666666,
      "grad_norm": 0.2914298474788666,
      "learning_rate": 3.126666666666666e-05,
      "loss": 0.0023,
      "step": 28100
    },
    {
      "epoch": 1.4992,
      "grad_norm": 0.3225886821746826,
      "learning_rate": 3.126e-05,
      "loss": 0.0022,
      "step": 28110
    },
    {
      "epoch": 1.4997333333333334,
      "grad_norm": 0.3578648269176483,
      "learning_rate": 3.1253333333333335e-05,
      "loss": 0.0027,
      "step": 28120
    },
    {
      "epoch": 1.5002666666666666,
      "grad_norm": 0.22091463208198547,
      "learning_rate": 3.124666666666667e-05,
      "loss": 0.003,
      "step": 28130
    },
    {
      "epoch": 1.5008,
      "grad_norm": 0.3569892942905426,
      "learning_rate": 3.1240000000000006e-05,
      "loss": 0.0021,
      "step": 28140
    },
    {
      "epoch": 1.5013333333333332,
      "grad_norm": 0.3529341220855713,
      "learning_rate": 3.123333333333334e-05,
      "loss": 0.0022,
      "step": 28150
    },
    {
      "epoch": 1.5018666666666667,
      "grad_norm": 0.5613965392112732,
      "learning_rate": 3.122666666666667e-05,
      "loss": 0.0024,
      "step": 28160
    },
    {
      "epoch": 1.5024,
      "grad_norm": 0.12890930473804474,
      "learning_rate": 3.122e-05,
      "loss": 0.0023,
      "step": 28170
    },
    {
      "epoch": 1.5029333333333335,
      "grad_norm": 0.11894086003303528,
      "learning_rate": 3.1213333333333335e-05,
      "loss": 0.0024,
      "step": 28180
    },
    {
      "epoch": 1.5034666666666667,
      "grad_norm": 0.15763865411281586,
      "learning_rate": 3.120666666666667e-05,
      "loss": 0.0035,
      "step": 28190
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.6096553206443787,
      "learning_rate": 3.12e-05,
      "loss": 0.0021,
      "step": 28200
    },
    {
      "epoch": 1.5045333333333333,
      "grad_norm": 0.336166650056839,
      "learning_rate": 3.119333333333334e-05,
      "loss": 0.0024,
      "step": 28210
    },
    {
      "epoch": 1.5050666666666666,
      "grad_norm": 0.28672125935554504,
      "learning_rate": 3.118666666666667e-05,
      "loss": 0.0031,
      "step": 28220
    },
    {
      "epoch": 1.5056,
      "grad_norm": 0.17578411102294922,
      "learning_rate": 3.118e-05,
      "loss": 0.0033,
      "step": 28230
    },
    {
      "epoch": 1.5061333333333333,
      "grad_norm": 0.08095123618841171,
      "learning_rate": 3.1173333333333335e-05,
      "loss": 0.0039,
      "step": 28240
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.15303358435630798,
      "learning_rate": 3.116666666666667e-05,
      "loss": 0.0023,
      "step": 28250
    },
    {
      "epoch": 1.5072,
      "grad_norm": 0.11748265475034714,
      "learning_rate": 3.116e-05,
      "loss": 0.002,
      "step": 28260
    },
    {
      "epoch": 1.5077333333333334,
      "grad_norm": 0.7961359620094299,
      "learning_rate": 3.115333333333333e-05,
      "loss": 0.0023,
      "step": 28270
    },
    {
      "epoch": 1.5082666666666666,
      "grad_norm": 0.12419426441192627,
      "learning_rate": 3.114666666666667e-05,
      "loss": 0.0022,
      "step": 28280
    },
    {
      "epoch": 1.5088,
      "grad_norm": 0.16701188683509827,
      "learning_rate": 3.1140000000000003e-05,
      "loss": 0.0027,
      "step": 28290
    },
    {
      "epoch": 1.5093333333333332,
      "grad_norm": 0.44049352407455444,
      "learning_rate": 3.1133333333333336e-05,
      "loss": 0.0033,
      "step": 28300
    },
    {
      "epoch": 1.5098666666666667,
      "grad_norm": 0.5869840383529663,
      "learning_rate": 3.112666666666667e-05,
      "loss": 0.0022,
      "step": 28310
    },
    {
      "epoch": 1.5104,
      "grad_norm": 0.3697700500488281,
      "learning_rate": 3.112e-05,
      "loss": 0.0034,
      "step": 28320
    },
    {
      "epoch": 1.5109333333333335,
      "grad_norm": 0.2793932855129242,
      "learning_rate": 3.111333333333333e-05,
      "loss": 0.0037,
      "step": 28330
    },
    {
      "epoch": 1.5114666666666667,
      "grad_norm": 0.2507961094379425,
      "learning_rate": 3.1106666666666665e-05,
      "loss": 0.0024,
      "step": 28340
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.11299384385347366,
      "learning_rate": 3.1100000000000004e-05,
      "loss": 0.003,
      "step": 28350
    },
    {
      "epoch": 1.5125333333333333,
      "grad_norm": 0.14336544275283813,
      "learning_rate": 3.1093333333333336e-05,
      "loss": 0.0034,
      "step": 28360
    },
    {
      "epoch": 1.5130666666666666,
      "grad_norm": 0.20792894065380096,
      "learning_rate": 3.108666666666667e-05,
      "loss": 0.0044,
      "step": 28370
    },
    {
      "epoch": 1.5135999999999998,
      "grad_norm": 0.7911335229873657,
      "learning_rate": 3.108e-05,
      "loss": 0.0028,
      "step": 28380
    },
    {
      "epoch": 1.5141333333333333,
      "grad_norm": 0.16434727609157562,
      "learning_rate": 3.107333333333333e-05,
      "loss": 0.0038,
      "step": 28390
    },
    {
      "epoch": 1.5146666666666668,
      "grad_norm": 0.4524942636489868,
      "learning_rate": 3.1066666666666665e-05,
      "loss": 0.0056,
      "step": 28400
    },
    {
      "epoch": 1.5152,
      "grad_norm": 0.41867995262145996,
      "learning_rate": 3.106e-05,
      "loss": 0.0018,
      "step": 28410
    },
    {
      "epoch": 1.5157333333333334,
      "grad_norm": 0.5269710421562195,
      "learning_rate": 3.1053333333333336e-05,
      "loss": 0.0023,
      "step": 28420
    },
    {
      "epoch": 1.5162666666666667,
      "grad_norm": 0.3455001413822174,
      "learning_rate": 3.104666666666667e-05,
      "loss": 0.0023,
      "step": 28430
    },
    {
      "epoch": 1.5168,
      "grad_norm": 0.49756720662117004,
      "learning_rate": 3.104e-05,
      "loss": 0.0026,
      "step": 28440
    },
    {
      "epoch": 1.5173333333333332,
      "grad_norm": 0.5405810475349426,
      "learning_rate": 3.103333333333333e-05,
      "loss": 0.0027,
      "step": 28450
    },
    {
      "epoch": 1.5178666666666667,
      "grad_norm": 0.3508369028568268,
      "learning_rate": 3.102666666666667e-05,
      "loss": 0.0029,
      "step": 28460
    },
    {
      "epoch": 1.5184,
      "grad_norm": 0.6086670160293579,
      "learning_rate": 3.102e-05,
      "loss": 0.0036,
      "step": 28470
    },
    {
      "epoch": 1.5189333333333335,
      "grad_norm": 0.5473854541778564,
      "learning_rate": 3.101333333333333e-05,
      "loss": 0.0034,
      "step": 28480
    },
    {
      "epoch": 1.5194666666666667,
      "grad_norm": 0.2368163913488388,
      "learning_rate": 3.100666666666667e-05,
      "loss": 0.0018,
      "step": 28490
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.6462506651878357,
      "learning_rate": 3.1e-05,
      "loss": 0.003,
      "step": 28500
    },
    {
      "epoch": 1.5205333333333333,
      "grad_norm": 0.46708306670188904,
      "learning_rate": 3.0993333333333334e-05,
      "loss": 0.0023,
      "step": 28510
    },
    {
      "epoch": 1.5210666666666666,
      "grad_norm": 0.10703917592763901,
      "learning_rate": 3.098666666666667e-05,
      "loss": 0.0023,
      "step": 28520
    },
    {
      "epoch": 1.5215999999999998,
      "grad_norm": 0.37260428071022034,
      "learning_rate": 3.0980000000000005e-05,
      "loss": 0.0029,
      "step": 28530
    },
    {
      "epoch": 1.5221333333333333,
      "grad_norm": 0.231035515666008,
      "learning_rate": 3.097333333333334e-05,
      "loss": 0.0024,
      "step": 28540
    },
    {
      "epoch": 1.5226666666666666,
      "grad_norm": 0.3739788234233856,
      "learning_rate": 3.096666666666666e-05,
      "loss": 0.0033,
      "step": 28550
    },
    {
      "epoch": 1.5232,
      "grad_norm": 0.5664089918136597,
      "learning_rate": 3.096e-05,
      "loss": 0.0023,
      "step": 28560
    },
    {
      "epoch": 1.5237333333333334,
      "grad_norm": 0.0870368629693985,
      "learning_rate": 3.0953333333333334e-05,
      "loss": 0.0021,
      "step": 28570
    },
    {
      "epoch": 1.5242666666666667,
      "grad_norm": 0.434644490480423,
      "learning_rate": 3.0946666666666666e-05,
      "loss": 0.0021,
      "step": 28580
    },
    {
      "epoch": 1.5248,
      "grad_norm": 0.12272163480520248,
      "learning_rate": 3.0940000000000005e-05,
      "loss": 0.0019,
      "step": 28590
    },
    {
      "epoch": 1.5253333333333332,
      "grad_norm": 0.08715149015188217,
      "learning_rate": 3.093333333333334e-05,
      "loss": 0.0019,
      "step": 28600
    },
    {
      "epoch": 1.5258666666666667,
      "grad_norm": 0.31596770882606506,
      "learning_rate": 3.092666666666667e-05,
      "loss": 0.003,
      "step": 28610
    },
    {
      "epoch": 1.5264,
      "grad_norm": 0.41964060068130493,
      "learning_rate": 3.092e-05,
      "loss": 0.0022,
      "step": 28620
    },
    {
      "epoch": 1.5269333333333335,
      "grad_norm": 0.423630952835083,
      "learning_rate": 3.0913333333333334e-05,
      "loss": 0.0023,
      "step": 28630
    },
    {
      "epoch": 1.5274666666666668,
      "grad_norm": 0.14123299717903137,
      "learning_rate": 3.090666666666667e-05,
      "loss": 0.0018,
      "step": 28640
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.11355575174093246,
      "learning_rate": 3.09e-05,
      "loss": 0.0019,
      "step": 28650
    },
    {
      "epoch": 1.5285333333333333,
      "grad_norm": 0.4710259735584259,
      "learning_rate": 3.089333333333334e-05,
      "loss": 0.0037,
      "step": 28660
    },
    {
      "epoch": 1.5290666666666666,
      "grad_norm": 0.4233431816101074,
      "learning_rate": 3.088666666666667e-05,
      "loss": 0.0028,
      "step": 28670
    },
    {
      "epoch": 1.5295999999999998,
      "grad_norm": 0.24127931892871857,
      "learning_rate": 3.088e-05,
      "loss": 0.0025,
      "step": 28680
    },
    {
      "epoch": 1.5301333333333333,
      "grad_norm": 0.16755402088165283,
      "learning_rate": 3.0873333333333335e-05,
      "loss": 0.0032,
      "step": 28690
    },
    {
      "epoch": 1.5306666666666666,
      "grad_norm": 0.1748153269290924,
      "learning_rate": 3.086666666666667e-05,
      "loss": 0.0023,
      "step": 28700
    },
    {
      "epoch": 1.5312000000000001,
      "grad_norm": 0.16133548319339752,
      "learning_rate": 3.086e-05,
      "loss": 0.0021,
      "step": 28710
    },
    {
      "epoch": 1.5317333333333334,
      "grad_norm": 0.5162587761878967,
      "learning_rate": 3.085333333333333e-05,
      "loss": 0.0029,
      "step": 28720
    },
    {
      "epoch": 1.5322666666666667,
      "grad_norm": 0.6255807876586914,
      "learning_rate": 3.084666666666667e-05,
      "loss": 0.0023,
      "step": 28730
    },
    {
      "epoch": 1.5328,
      "grad_norm": 0.8489243984222412,
      "learning_rate": 3.084e-05,
      "loss": 0.0031,
      "step": 28740
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.41802093386650085,
      "learning_rate": 3.0833333333333335e-05,
      "loss": 0.0043,
      "step": 28750
    },
    {
      "epoch": 1.5338666666666667,
      "grad_norm": 0.24309401214122772,
      "learning_rate": 3.082666666666667e-05,
      "loss": 0.002,
      "step": 28760
    },
    {
      "epoch": 1.5344,
      "grad_norm": 0.48354941606521606,
      "learning_rate": 3.082e-05,
      "loss": 0.0042,
      "step": 28770
    },
    {
      "epoch": 1.5349333333333335,
      "grad_norm": 0.6602446436882019,
      "learning_rate": 3.081333333333333e-05,
      "loss": 0.0021,
      "step": 28780
    },
    {
      "epoch": 1.5354666666666668,
      "grad_norm": 0.27428174018859863,
      "learning_rate": 3.0806666666666664e-05,
      "loss": 0.0018,
      "step": 28790
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.2883089780807495,
      "learning_rate": 3.08e-05,
      "loss": 0.0026,
      "step": 28800
    },
    {
      "epoch": 1.5365333333333333,
      "grad_norm": 0.24858640134334564,
      "learning_rate": 3.0793333333333336e-05,
      "loss": 0.0021,
      "step": 28810
    },
    {
      "epoch": 1.5370666666666666,
      "grad_norm": 0.6885708570480347,
      "learning_rate": 3.078666666666667e-05,
      "loss": 0.0021,
      "step": 28820
    },
    {
      "epoch": 1.5375999999999999,
      "grad_norm": 0.15059423446655273,
      "learning_rate": 3.078e-05,
      "loss": 0.0021,
      "step": 28830
    },
    {
      "epoch": 1.5381333333333334,
      "grad_norm": 0.5052439570426941,
      "learning_rate": 3.077333333333334e-05,
      "loss": 0.0026,
      "step": 28840
    },
    {
      "epoch": 1.5386666666666666,
      "grad_norm": 0.681016743183136,
      "learning_rate": 3.0766666666666665e-05,
      "loss": 0.0039,
      "step": 28850
    },
    {
      "epoch": 1.5392000000000001,
      "grad_norm": 0.08505257964134216,
      "learning_rate": 3.076e-05,
      "loss": 0.0021,
      "step": 28860
    },
    {
      "epoch": 1.5397333333333334,
      "grad_norm": 0.49961575865745544,
      "learning_rate": 3.0753333333333336e-05,
      "loss": 0.0024,
      "step": 28870
    },
    {
      "epoch": 1.5402666666666667,
      "grad_norm": 0.7176101207733154,
      "learning_rate": 3.074666666666667e-05,
      "loss": 0.0023,
      "step": 28880
    },
    {
      "epoch": 1.5408,
      "grad_norm": 0.06060988828539848,
      "learning_rate": 3.074e-05,
      "loss": 0.0033,
      "step": 28890
    },
    {
      "epoch": 1.5413333333333332,
      "grad_norm": 0.35825279355049133,
      "learning_rate": 3.073333333333334e-05,
      "loss": 0.0024,
      "step": 28900
    },
    {
      "epoch": 1.5418666666666667,
      "grad_norm": 0.5395958423614502,
      "learning_rate": 3.072666666666667e-05,
      "loss": 0.0028,
      "step": 28910
    },
    {
      "epoch": 1.5424,
      "grad_norm": 0.3271034061908722,
      "learning_rate": 3.072e-05,
      "loss": 0.0018,
      "step": 28920
    },
    {
      "epoch": 1.5429333333333335,
      "grad_norm": 0.44909557700157166,
      "learning_rate": 3.071333333333333e-05,
      "loss": 0.0021,
      "step": 28930
    },
    {
      "epoch": 1.5434666666666668,
      "grad_norm": 0.4381403625011444,
      "learning_rate": 3.070666666666667e-05,
      "loss": 0.0022,
      "step": 28940
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.5754733085632324,
      "learning_rate": 3.07e-05,
      "loss": 0.0025,
      "step": 28950
    },
    {
      "epoch": 1.5445333333333333,
      "grad_norm": 0.13945335149765015,
      "learning_rate": 3.069333333333333e-05,
      "loss": 0.0022,
      "step": 28960
    },
    {
      "epoch": 1.5450666666666666,
      "grad_norm": 0.18526363372802734,
      "learning_rate": 3.068666666666667e-05,
      "loss": 0.0018,
      "step": 28970
    },
    {
      "epoch": 1.5455999999999999,
      "grad_norm": 0.13682453334331512,
      "learning_rate": 3.0680000000000004e-05,
      "loss": 0.0033,
      "step": 28980
    },
    {
      "epoch": 1.5461333333333334,
      "grad_norm": 0.2768685221672058,
      "learning_rate": 3.067333333333334e-05,
      "loss": 0.0028,
      "step": 28990
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.16700200736522675,
      "learning_rate": 3.066666666666667e-05,
      "loss": 0.0026,
      "step": 29000
    },
    {
      "epoch": 1.5472000000000001,
      "grad_norm": 0.7182588577270508,
      "learning_rate": 3.066e-05,
      "loss": 0.0034,
      "step": 29010
    },
    {
      "epoch": 1.5477333333333334,
      "grad_norm": 0.09658965468406677,
      "learning_rate": 3.0653333333333333e-05,
      "loss": 0.0021,
      "step": 29020
    },
    {
      "epoch": 1.5482666666666667,
      "grad_norm": 0.42739126086235046,
      "learning_rate": 3.0646666666666666e-05,
      "loss": 0.0024,
      "step": 29030
    },
    {
      "epoch": 1.5488,
      "grad_norm": 0.5247159004211426,
      "learning_rate": 3.0640000000000005e-05,
      "loss": 0.0027,
      "step": 29040
    },
    {
      "epoch": 1.5493333333333332,
      "grad_norm": 0.4495824873447418,
      "learning_rate": 3.063333333333334e-05,
      "loss": 0.0023,
      "step": 29050
    },
    {
      "epoch": 1.5498666666666665,
      "grad_norm": 0.20181502401828766,
      "learning_rate": 3.062666666666667e-05,
      "loss": 0.0022,
      "step": 29060
    },
    {
      "epoch": 1.5504,
      "grad_norm": 0.27463456988334656,
      "learning_rate": 3.062e-05,
      "loss": 0.0027,
      "step": 29070
    },
    {
      "epoch": 1.5509333333333335,
      "grad_norm": 0.25603145360946655,
      "learning_rate": 3.0613333333333334e-05,
      "loss": 0.0035,
      "step": 29080
    },
    {
      "epoch": 1.5514666666666668,
      "grad_norm": 0.3345930576324463,
      "learning_rate": 3.0606666666666666e-05,
      "loss": 0.0037,
      "step": 29090
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.2695765495300293,
      "learning_rate": 3.06e-05,
      "loss": 0.0018,
      "step": 29100
    },
    {
      "epoch": 1.5525333333333333,
      "grad_norm": 0.3758004605770111,
      "learning_rate": 3.059333333333334e-05,
      "loss": 0.0024,
      "step": 29110
    },
    {
      "epoch": 1.5530666666666666,
      "grad_norm": 0.18831202387809753,
      "learning_rate": 3.058666666666667e-05,
      "loss": 0.002,
      "step": 29120
    },
    {
      "epoch": 1.5535999999999999,
      "grad_norm": 0.39552539587020874,
      "learning_rate": 3.058e-05,
      "loss": 0.0022,
      "step": 29130
    },
    {
      "epoch": 1.5541333333333334,
      "grad_norm": 0.19044998288154602,
      "learning_rate": 3.0573333333333334e-05,
      "loss": 0.0022,
      "step": 29140
    },
    {
      "epoch": 1.5546666666666666,
      "grad_norm": 0.4709120988845825,
      "learning_rate": 3.0566666666666667e-05,
      "loss": 0.0041,
      "step": 29150
    },
    {
      "epoch": 1.5552000000000001,
      "grad_norm": 0.1820158064365387,
      "learning_rate": 3.056e-05,
      "loss": 0.0026,
      "step": 29160
    },
    {
      "epoch": 1.5557333333333334,
      "grad_norm": 0.10951253026723862,
      "learning_rate": 3.055333333333333e-05,
      "loss": 0.0039,
      "step": 29170
    },
    {
      "epoch": 1.5562666666666667,
      "grad_norm": 0.5634427070617676,
      "learning_rate": 3.054666666666667e-05,
      "loss": 0.0032,
      "step": 29180
    },
    {
      "epoch": 1.5568,
      "grad_norm": 0.1480846107006073,
      "learning_rate": 3.054e-05,
      "loss": 0.0022,
      "step": 29190
    },
    {
      "epoch": 1.5573333333333332,
      "grad_norm": 0.14330433309078217,
      "learning_rate": 3.0533333333333335e-05,
      "loss": 0.002,
      "step": 29200
    },
    {
      "epoch": 1.5578666666666665,
      "grad_norm": 0.5111159682273865,
      "learning_rate": 3.052666666666667e-05,
      "loss": 0.0024,
      "step": 29210
    },
    {
      "epoch": 1.5584,
      "grad_norm": 0.2908651828765869,
      "learning_rate": 3.0520000000000006e-05,
      "loss": 0.002,
      "step": 29220
    },
    {
      "epoch": 1.5589333333333333,
      "grad_norm": 0.5075985193252563,
      "learning_rate": 3.051333333333333e-05,
      "loss": 0.0043,
      "step": 29230
    },
    {
      "epoch": 1.5594666666666668,
      "grad_norm": 0.08322249352931976,
      "learning_rate": 3.0506666666666667e-05,
      "loss": 0.0029,
      "step": 29240
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.2649178206920624,
      "learning_rate": 3.05e-05,
      "loss": 0.0023,
      "step": 29250
    },
    {
      "epoch": 1.5605333333333333,
      "grad_norm": 0.2914518117904663,
      "learning_rate": 3.0493333333333335e-05,
      "loss": 0.0017,
      "step": 29260
    },
    {
      "epoch": 1.5610666666666666,
      "grad_norm": 0.28553181886672974,
      "learning_rate": 3.0486666666666667e-05,
      "loss": 0.0032,
      "step": 29270
    },
    {
      "epoch": 1.5615999999999999,
      "grad_norm": 0.670691192150116,
      "learning_rate": 3.0480000000000003e-05,
      "loss": 0.0038,
      "step": 29280
    },
    {
      "epoch": 1.5621333333333334,
      "grad_norm": 0.1932247281074524,
      "learning_rate": 3.047333333333334e-05,
      "loss": 0.0027,
      "step": 29290
    },
    {
      "epoch": 1.5626666666666666,
      "grad_norm": 0.16325516998767853,
      "learning_rate": 3.0466666666666664e-05,
      "loss": 0.003,
      "step": 29300
    },
    {
      "epoch": 1.5632000000000001,
      "grad_norm": 0.2504520118236542,
      "learning_rate": 3.046e-05,
      "loss": 0.003,
      "step": 29310
    },
    {
      "epoch": 1.5637333333333334,
      "grad_norm": 0.2524321377277374,
      "learning_rate": 3.0453333333333335e-05,
      "loss": 0.0024,
      "step": 29320
    },
    {
      "epoch": 1.5642666666666667,
      "grad_norm": 0.5218052864074707,
      "learning_rate": 3.0446666666666668e-05,
      "loss": 0.0034,
      "step": 29330
    },
    {
      "epoch": 1.5648,
      "grad_norm": 0.20657901465892792,
      "learning_rate": 3.0440000000000003e-05,
      "loss": 0.0032,
      "step": 29340
    },
    {
      "epoch": 1.5653333333333332,
      "grad_norm": 0.10643815249204636,
      "learning_rate": 3.0433333333333336e-05,
      "loss": 0.0028,
      "step": 29350
    },
    {
      "epoch": 1.5658666666666665,
      "grad_norm": 0.24385122954845428,
      "learning_rate": 3.042666666666667e-05,
      "loss": 0.0021,
      "step": 29360
    },
    {
      "epoch": 1.5664,
      "grad_norm": 0.37438979744911194,
      "learning_rate": 3.0420000000000004e-05,
      "loss": 0.0028,
      "step": 29370
    },
    {
      "epoch": 1.5669333333333333,
      "grad_norm": 0.060537390410900116,
      "learning_rate": 3.0413333333333332e-05,
      "loss": 0.0033,
      "step": 29380
    },
    {
      "epoch": 1.5674666666666668,
      "grad_norm": 0.6367594003677368,
      "learning_rate": 3.0406666666666668e-05,
      "loss": 0.0031,
      "step": 29390
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.45849427580833435,
      "learning_rate": 3.04e-05,
      "loss": 0.0017,
      "step": 29400
    },
    {
      "epoch": 1.5685333333333333,
      "grad_norm": 0.1505661904811859,
      "learning_rate": 3.0393333333333336e-05,
      "loss": 0.0024,
      "step": 29410
    },
    {
      "epoch": 1.5690666666666666,
      "grad_norm": 0.10887657850980759,
      "learning_rate": 3.0386666666666668e-05,
      "loss": 0.0022,
      "step": 29420
    },
    {
      "epoch": 1.5695999999999999,
      "grad_norm": 0.07988601177930832,
      "learning_rate": 3.0380000000000004e-05,
      "loss": 0.0022,
      "step": 29430
    },
    {
      "epoch": 1.5701333333333334,
      "grad_norm": 0.20973791182041168,
      "learning_rate": 3.0373333333333336e-05,
      "loss": 0.0022,
      "step": 29440
    },
    {
      "epoch": 1.5706666666666667,
      "grad_norm": 0.7416885495185852,
      "learning_rate": 3.0366666666666665e-05,
      "loss": 0.0026,
      "step": 29450
    },
    {
      "epoch": 1.5712000000000002,
      "grad_norm": 0.06372958421707153,
      "learning_rate": 3.036e-05,
      "loss": 0.0032,
      "step": 29460
    },
    {
      "epoch": 1.5717333333333334,
      "grad_norm": 0.2959620952606201,
      "learning_rate": 3.0353333333333333e-05,
      "loss": 0.0028,
      "step": 29470
    },
    {
      "epoch": 1.5722666666666667,
      "grad_norm": 0.7164046764373779,
      "learning_rate": 3.034666666666667e-05,
      "loss": 0.0027,
      "step": 29480
    },
    {
      "epoch": 1.5728,
      "grad_norm": 0.5324406623840332,
      "learning_rate": 3.034e-05,
      "loss": 0.0029,
      "step": 29490
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.21568021178245544,
      "learning_rate": 3.0333333333333337e-05,
      "loss": 0.0017,
      "step": 29500
    },
    {
      "epoch": 1.5738666666666665,
      "grad_norm": 0.5506736636161804,
      "learning_rate": 3.032666666666667e-05,
      "loss": 0.0024,
      "step": 29510
    },
    {
      "epoch": 1.5744,
      "grad_norm": 0.4278517961502075,
      "learning_rate": 3.0320000000000004e-05,
      "loss": 0.0028,
      "step": 29520
    },
    {
      "epoch": 1.5749333333333333,
      "grad_norm": 0.8966948986053467,
      "learning_rate": 3.0313333333333333e-05,
      "loss": 0.0033,
      "step": 29530
    },
    {
      "epoch": 1.5754666666666668,
      "grad_norm": 0.383590966463089,
      "learning_rate": 3.0306666666666666e-05,
      "loss": 0.0026,
      "step": 29540
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.16169562935829163,
      "learning_rate": 3.03e-05,
      "loss": 0.0032,
      "step": 29550
    },
    {
      "epoch": 1.5765333333333333,
      "grad_norm": 0.3248479664325714,
      "learning_rate": 3.0293333333333334e-05,
      "loss": 0.0022,
      "step": 29560
    },
    {
      "epoch": 1.5770666666666666,
      "grad_norm": 0.2724652588367462,
      "learning_rate": 3.028666666666667e-05,
      "loss": 0.0021,
      "step": 29570
    },
    {
      "epoch": 1.5776,
      "grad_norm": 0.12955577671527863,
      "learning_rate": 3.028e-05,
      "loss": 0.0021,
      "step": 29580
    },
    {
      "epoch": 1.5781333333333334,
      "grad_norm": 0.3393140435218811,
      "learning_rate": 3.0273333333333337e-05,
      "loss": 0.003,
      "step": 29590
    },
    {
      "epoch": 1.5786666666666667,
      "grad_norm": 0.5862722396850586,
      "learning_rate": 3.0266666666666666e-05,
      "loss": 0.0025,
      "step": 29600
    },
    {
      "epoch": 1.5792000000000002,
      "grad_norm": 0.36066684126853943,
      "learning_rate": 3.0259999999999998e-05,
      "loss": 0.0022,
      "step": 29610
    },
    {
      "epoch": 1.5797333333333334,
      "grad_norm": 0.13290905952453613,
      "learning_rate": 3.0253333333333334e-05,
      "loss": 0.003,
      "step": 29620
    },
    {
      "epoch": 1.5802666666666667,
      "grad_norm": 0.21344955265522003,
      "learning_rate": 3.0246666666666666e-05,
      "loss": 0.0021,
      "step": 29630
    },
    {
      "epoch": 1.5808,
      "grad_norm": 0.29844292998313904,
      "learning_rate": 3.0240000000000002e-05,
      "loss": 0.0023,
      "step": 29640
    },
    {
      "epoch": 1.5813333333333333,
      "grad_norm": 0.3109407424926758,
      "learning_rate": 3.0233333333333334e-05,
      "loss": 0.0036,
      "step": 29650
    },
    {
      "epoch": 1.5818666666666665,
      "grad_norm": 0.6847842931747437,
      "learning_rate": 3.022666666666667e-05,
      "loss": 0.003,
      "step": 29660
    },
    {
      "epoch": 1.5824,
      "grad_norm": 0.41117531061172485,
      "learning_rate": 3.0220000000000005e-05,
      "loss": 0.0024,
      "step": 29670
    },
    {
      "epoch": 1.5829333333333333,
      "grad_norm": 0.16388046741485596,
      "learning_rate": 3.021333333333333e-05,
      "loss": 0.0026,
      "step": 29680
    },
    {
      "epoch": 1.5834666666666668,
      "grad_norm": 0.577515184879303,
      "learning_rate": 3.0206666666666667e-05,
      "loss": 0.0022,
      "step": 29690
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.2572808563709259,
      "learning_rate": 3.02e-05,
      "loss": 0.0026,
      "step": 29700
    },
    {
      "epoch": 1.5845333333333333,
      "grad_norm": 0.13812923431396484,
      "learning_rate": 3.0193333333333335e-05,
      "loss": 0.0036,
      "step": 29710
    },
    {
      "epoch": 1.5850666666666666,
      "grad_norm": 0.43085092306137085,
      "learning_rate": 3.018666666666667e-05,
      "loss": 0.0021,
      "step": 29720
    },
    {
      "epoch": 1.5856,
      "grad_norm": 0.30343955755233765,
      "learning_rate": 3.0180000000000002e-05,
      "loss": 0.002,
      "step": 29730
    },
    {
      "epoch": 1.5861333333333332,
      "grad_norm": 0.11544615030288696,
      "learning_rate": 3.0173333333333338e-05,
      "loss": 0.0037,
      "step": 29740
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.27843722701072693,
      "learning_rate": 3.016666666666667e-05,
      "loss": 0.0023,
      "step": 29750
    },
    {
      "epoch": 1.5872000000000002,
      "grad_norm": 0.269163578748703,
      "learning_rate": 3.016e-05,
      "loss": 0.0032,
      "step": 29760
    },
    {
      "epoch": 1.5877333333333334,
      "grad_norm": 0.4242096543312073,
      "learning_rate": 3.0153333333333335e-05,
      "loss": 0.0021,
      "step": 29770
    },
    {
      "epoch": 1.5882666666666667,
      "grad_norm": 0.11550425738096237,
      "learning_rate": 3.0146666666666667e-05,
      "loss": 0.0024,
      "step": 29780
    },
    {
      "epoch": 1.5888,
      "grad_norm": 0.5095031261444092,
      "learning_rate": 3.0140000000000003e-05,
      "loss": 0.002,
      "step": 29790
    },
    {
      "epoch": 1.5893333333333333,
      "grad_norm": 0.06281644850969315,
      "learning_rate": 3.0133333333333335e-05,
      "loss": 0.0033,
      "step": 29800
    },
    {
      "epoch": 1.5898666666666665,
      "grad_norm": 0.1738036870956421,
      "learning_rate": 3.012666666666667e-05,
      "loss": 0.0045,
      "step": 29810
    },
    {
      "epoch": 1.5904,
      "grad_norm": 0.3792835474014282,
      "learning_rate": 3.0120000000000003e-05,
      "loss": 0.002,
      "step": 29820
    },
    {
      "epoch": 1.5909333333333333,
      "grad_norm": 0.7937422394752502,
      "learning_rate": 3.0113333333333332e-05,
      "loss": 0.0023,
      "step": 29830
    },
    {
      "epoch": 1.5914666666666668,
      "grad_norm": 0.300834059715271,
      "learning_rate": 3.0106666666666668e-05,
      "loss": 0.0031,
      "step": 29840
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.2935449182987213,
      "learning_rate": 3.01e-05,
      "loss": 0.0023,
      "step": 29850
    },
    {
      "epoch": 1.5925333333333334,
      "grad_norm": 0.1550706923007965,
      "learning_rate": 3.0093333333333335e-05,
      "loss": 0.0024,
      "step": 29860
    },
    {
      "epoch": 1.5930666666666666,
      "grad_norm": 0.613617479801178,
      "learning_rate": 3.0086666666666668e-05,
      "loss": 0.0044,
      "step": 29870
    },
    {
      "epoch": 1.5936,
      "grad_norm": 0.3121948540210724,
      "learning_rate": 3.0080000000000003e-05,
      "loss": 0.0037,
      "step": 29880
    },
    {
      "epoch": 1.5941333333333332,
      "grad_norm": 0.2052682340145111,
      "learning_rate": 3.0073333333333336e-05,
      "loss": 0.0028,
      "step": 29890
    },
    {
      "epoch": 1.5946666666666667,
      "grad_norm": 0.19901862740516663,
      "learning_rate": 3.006666666666667e-05,
      "loss": 0.0022,
      "step": 29900
    },
    {
      "epoch": 1.5952,
      "grad_norm": 0.7038996815681458,
      "learning_rate": 3.006e-05,
      "loss": 0.0019,
      "step": 29910
    },
    {
      "epoch": 1.5957333333333334,
      "grad_norm": 0.14006051421165466,
      "learning_rate": 3.0053333333333332e-05,
      "loss": 0.0025,
      "step": 29920
    },
    {
      "epoch": 1.5962666666666667,
      "grad_norm": 0.47136783599853516,
      "learning_rate": 3.0046666666666668e-05,
      "loss": 0.0036,
      "step": 29930
    },
    {
      "epoch": 1.5968,
      "grad_norm": 0.07851357758045197,
      "learning_rate": 3.004e-05,
      "loss": 0.0027,
      "step": 29940
    },
    {
      "epoch": 1.5973333333333333,
      "grad_norm": 0.24893002212047577,
      "learning_rate": 3.0033333333333336e-05,
      "loss": 0.0035,
      "step": 29950
    },
    {
      "epoch": 1.5978666666666665,
      "grad_norm": 0.113926462829113,
      "learning_rate": 3.0026666666666668e-05,
      "loss": 0.0029,
      "step": 29960
    },
    {
      "epoch": 1.5984,
      "grad_norm": 0.33399277925491333,
      "learning_rate": 3.0020000000000004e-05,
      "loss": 0.0048,
      "step": 29970
    },
    {
      "epoch": 1.5989333333333333,
      "grad_norm": 0.3837868571281433,
      "learning_rate": 3.0013333333333333e-05,
      "loss": 0.0023,
      "step": 29980
    },
    {
      "epoch": 1.5994666666666668,
      "grad_norm": 0.38738009333610535,
      "learning_rate": 3.0006666666666665e-05,
      "loss": 0.0032,
      "step": 29990
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.49403008818626404,
      "learning_rate": 3e-05,
      "loss": 0.0033,
      "step": 30000
    },
    {
      "epoch": 1.6005333333333334,
      "grad_norm": 0.628075122833252,
      "learning_rate": 2.9993333333333333e-05,
      "loss": 0.0023,
      "step": 30010
    },
    {
      "epoch": 1.6010666666666666,
      "grad_norm": 0.5814446806907654,
      "learning_rate": 2.998666666666667e-05,
      "loss": 0.0029,
      "step": 30020
    },
    {
      "epoch": 1.6016,
      "grad_norm": 0.2593965232372284,
      "learning_rate": 2.998e-05,
      "loss": 0.0037,
      "step": 30030
    },
    {
      "epoch": 1.6021333333333332,
      "grad_norm": 0.4868447184562683,
      "learning_rate": 2.9973333333333337e-05,
      "loss": 0.0028,
      "step": 30040
    },
    {
      "epoch": 1.6026666666666667,
      "grad_norm": 0.5994632840156555,
      "learning_rate": 2.9966666666666672e-05,
      "loss": 0.0025,
      "step": 30050
    },
    {
      "epoch": 1.6032,
      "grad_norm": 0.2536304295063019,
      "learning_rate": 2.9959999999999998e-05,
      "loss": 0.0031,
      "step": 30060
    },
    {
      "epoch": 1.6037333333333335,
      "grad_norm": 0.10048232972621918,
      "learning_rate": 2.9953333333333333e-05,
      "loss": 0.0021,
      "step": 30070
    },
    {
      "epoch": 1.6042666666666667,
      "grad_norm": 0.41105884313583374,
      "learning_rate": 2.9946666666666666e-05,
      "loss": 0.0028,
      "step": 30080
    },
    {
      "epoch": 1.6048,
      "grad_norm": 0.12805421650409698,
      "learning_rate": 2.994e-05,
      "loss": 0.0026,
      "step": 30090
    },
    {
      "epoch": 1.6053333333333333,
      "grad_norm": 0.15998543798923492,
      "learning_rate": 2.9933333333333337e-05,
      "loss": 0.0027,
      "step": 30100
    },
    {
      "epoch": 1.6058666666666666,
      "grad_norm": 0.238020658493042,
      "learning_rate": 2.992666666666667e-05,
      "loss": 0.0019,
      "step": 30110
    },
    {
      "epoch": 1.6064,
      "grad_norm": 0.3117201030254364,
      "learning_rate": 2.9920000000000005e-05,
      "loss": 0.0025,
      "step": 30120
    },
    {
      "epoch": 1.6069333333333333,
      "grad_norm": 0.4751639664173126,
      "learning_rate": 2.991333333333333e-05,
      "loss": 0.0019,
      "step": 30130
    },
    {
      "epoch": 1.6074666666666668,
      "grad_norm": 0.18672840297222137,
      "learning_rate": 2.9906666666666666e-05,
      "loss": 0.0019,
      "step": 30140
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.36305302381515503,
      "learning_rate": 2.9900000000000002e-05,
      "loss": 0.002,
      "step": 30150
    },
    {
      "epoch": 1.6085333333333334,
      "grad_norm": 0.18947410583496094,
      "learning_rate": 2.9893333333333334e-05,
      "loss": 0.0037,
      "step": 30160
    },
    {
      "epoch": 1.6090666666666666,
      "grad_norm": 0.21607673168182373,
      "learning_rate": 2.988666666666667e-05,
      "loss": 0.0024,
      "step": 30170
    },
    {
      "epoch": 1.6096,
      "grad_norm": 0.38804492354393005,
      "learning_rate": 2.9880000000000002e-05,
      "loss": 0.0028,
      "step": 30180
    },
    {
      "epoch": 1.6101333333333332,
      "grad_norm": 0.4690963625907898,
      "learning_rate": 2.9873333333333338e-05,
      "loss": 0.0023,
      "step": 30190
    },
    {
      "epoch": 1.6106666666666667,
      "grad_norm": 0.2530061900615692,
      "learning_rate": 2.986666666666667e-05,
      "loss": 0.0036,
      "step": 30200
    },
    {
      "epoch": 1.6112,
      "grad_norm": 0.3791613280773163,
      "learning_rate": 2.986e-05,
      "loss": 0.0027,
      "step": 30210
    },
    {
      "epoch": 1.6117333333333335,
      "grad_norm": 0.42676112055778503,
      "learning_rate": 2.9853333333333334e-05,
      "loss": 0.0019,
      "step": 30220
    },
    {
      "epoch": 1.6122666666666667,
      "grad_norm": 0.49944764375686646,
      "learning_rate": 2.9846666666666667e-05,
      "loss": 0.0026,
      "step": 30230
    },
    {
      "epoch": 1.6128,
      "grad_norm": 0.15180636942386627,
      "learning_rate": 2.9840000000000002e-05,
      "loss": 0.0022,
      "step": 30240
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.1931932419538498,
      "learning_rate": 2.9833333333333335e-05,
      "loss": 0.0032,
      "step": 30250
    },
    {
      "epoch": 1.6138666666666666,
      "grad_norm": 0.4855542778968811,
      "learning_rate": 2.982666666666667e-05,
      "loss": 0.002,
      "step": 30260
    },
    {
      "epoch": 1.6143999999999998,
      "grad_norm": 0.2615501880645752,
      "learning_rate": 2.9820000000000002e-05,
      "loss": 0.0021,
      "step": 30270
    },
    {
      "epoch": 1.6149333333333333,
      "grad_norm": 0.27062177658081055,
      "learning_rate": 2.981333333333333e-05,
      "loss": 0.0023,
      "step": 30280
    },
    {
      "epoch": 1.6154666666666668,
      "grad_norm": 0.6719305515289307,
      "learning_rate": 2.9806666666666667e-05,
      "loss": 0.0025,
      "step": 30290
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.1986149400472641,
      "learning_rate": 2.98e-05,
      "loss": 0.0044,
      "step": 30300
    },
    {
      "epoch": 1.6165333333333334,
      "grad_norm": 0.12591391801834106,
      "learning_rate": 2.9793333333333335e-05,
      "loss": 0.0031,
      "step": 30310
    },
    {
      "epoch": 1.6170666666666667,
      "grad_norm": 0.4123050272464752,
      "learning_rate": 2.9786666666666667e-05,
      "loss": 0.0018,
      "step": 30320
    },
    {
      "epoch": 1.6176,
      "grad_norm": 0.40235498547554016,
      "learning_rate": 2.9780000000000003e-05,
      "loss": 0.0021,
      "step": 30330
    },
    {
      "epoch": 1.6181333333333332,
      "grad_norm": 0.3356529176235199,
      "learning_rate": 2.9773333333333335e-05,
      "loss": 0.0033,
      "step": 30340
    },
    {
      "epoch": 1.6186666666666667,
      "grad_norm": 0.562303900718689,
      "learning_rate": 2.976666666666667e-05,
      "loss": 0.0025,
      "step": 30350
    },
    {
      "epoch": 1.6192,
      "grad_norm": 0.25621315836906433,
      "learning_rate": 2.976e-05,
      "loss": 0.002,
      "step": 30360
    },
    {
      "epoch": 1.6197333333333335,
      "grad_norm": 0.07235364615917206,
      "learning_rate": 2.9753333333333332e-05,
      "loss": 0.003,
      "step": 30370
    },
    {
      "epoch": 1.6202666666666667,
      "grad_norm": 0.35592788457870483,
      "learning_rate": 2.9746666666666668e-05,
      "loss": 0.0039,
      "step": 30380
    },
    {
      "epoch": 1.6208,
      "grad_norm": 0.33211246132850647,
      "learning_rate": 2.974e-05,
      "loss": 0.0022,
      "step": 30390
    },
    {
      "epoch": 1.6213333333333333,
      "grad_norm": 0.1388968825340271,
      "learning_rate": 2.9733333333333336e-05,
      "loss": 0.0027,
      "step": 30400
    },
    {
      "epoch": 1.6218666666666666,
      "grad_norm": 0.2951313257217407,
      "learning_rate": 2.9726666666666668e-05,
      "loss": 0.002,
      "step": 30410
    },
    {
      "epoch": 1.6223999999999998,
      "grad_norm": 0.07796116918325424,
      "learning_rate": 2.9720000000000003e-05,
      "loss": 0.0022,
      "step": 30420
    },
    {
      "epoch": 1.6229333333333333,
      "grad_norm": 0.21534891426563263,
      "learning_rate": 2.971333333333334e-05,
      "loss": 0.002,
      "step": 30430
    },
    {
      "epoch": 1.6234666666666666,
      "grad_norm": 0.40912991762161255,
      "learning_rate": 2.9706666666666665e-05,
      "loss": 0.0027,
      "step": 30440
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.521003007888794,
      "learning_rate": 2.97e-05,
      "loss": 0.0025,
      "step": 30450
    },
    {
      "epoch": 1.6245333333333334,
      "grad_norm": 0.6149510741233826,
      "learning_rate": 2.9693333333333333e-05,
      "loss": 0.0035,
      "step": 30460
    },
    {
      "epoch": 1.6250666666666667,
      "grad_norm": 0.08892490714788437,
      "learning_rate": 2.9686666666666668e-05,
      "loss": 0.0022,
      "step": 30470
    },
    {
      "epoch": 1.6256,
      "grad_norm": 0.6747348308563232,
      "learning_rate": 2.9680000000000004e-05,
      "loss": 0.0022,
      "step": 30480
    },
    {
      "epoch": 1.6261333333333332,
      "grad_norm": 0.39719176292419434,
      "learning_rate": 2.9673333333333336e-05,
      "loss": 0.0033,
      "step": 30490
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.11170029640197754,
      "learning_rate": 2.9666666666666672e-05,
      "loss": 0.0026,
      "step": 30500
    },
    {
      "epoch": 1.6272,
      "grad_norm": 0.32903915643692017,
      "learning_rate": 2.9659999999999997e-05,
      "loss": 0.0025,
      "step": 30510
    },
    {
      "epoch": 1.6277333333333335,
      "grad_norm": 0.3401370346546173,
      "learning_rate": 2.9653333333333333e-05,
      "loss": 0.003,
      "step": 30520
    },
    {
      "epoch": 1.6282666666666668,
      "grad_norm": 0.142695352435112,
      "learning_rate": 2.964666666666667e-05,
      "loss": 0.003,
      "step": 30530
    },
    {
      "epoch": 1.6288,
      "grad_norm": 0.10119866579771042,
      "learning_rate": 2.964e-05,
      "loss": 0.0026,
      "step": 30540
    },
    {
      "epoch": 1.6293333333333333,
      "grad_norm": 0.2363612949848175,
      "learning_rate": 2.9633333333333336e-05,
      "loss": 0.0033,
      "step": 30550
    },
    {
      "epoch": 1.6298666666666666,
      "grad_norm": 0.38310471177101135,
      "learning_rate": 2.962666666666667e-05,
      "loss": 0.0021,
      "step": 30560
    },
    {
      "epoch": 1.6303999999999998,
      "grad_norm": 0.3978630006313324,
      "learning_rate": 2.9620000000000004e-05,
      "loss": 0.0042,
      "step": 30570
    },
    {
      "epoch": 1.6309333333333333,
      "grad_norm": 0.5849660038948059,
      "learning_rate": 2.9613333333333337e-05,
      "loss": 0.004,
      "step": 30580
    },
    {
      "epoch": 1.6314666666666666,
      "grad_norm": 0.21179601550102234,
      "learning_rate": 2.9606666666666666e-05,
      "loss": 0.0022,
      "step": 30590
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.23390890657901764,
      "learning_rate": 2.96e-05,
      "loss": 0.0023,
      "step": 30600
    },
    {
      "epoch": 1.6325333333333334,
      "grad_norm": 0.11018688231706619,
      "learning_rate": 2.9593333333333333e-05,
      "loss": 0.0019,
      "step": 30610
    },
    {
      "epoch": 1.6330666666666667,
      "grad_norm": 0.22806531190872192,
      "learning_rate": 2.958666666666667e-05,
      "loss": 0.0027,
      "step": 30620
    },
    {
      "epoch": 1.6336,
      "grad_norm": 0.4932900369167328,
      "learning_rate": 2.958e-05,
      "loss": 0.0027,
      "step": 30630
    },
    {
      "epoch": 1.6341333333333332,
      "grad_norm": 0.49825260043144226,
      "learning_rate": 2.9573333333333337e-05,
      "loss": 0.0022,
      "step": 30640
    },
    {
      "epoch": 1.6346666666666667,
      "grad_norm": 0.22573880851268768,
      "learning_rate": 2.956666666666667e-05,
      "loss": 0.0021,
      "step": 30650
    },
    {
      "epoch": 1.6352,
      "grad_norm": 0.13757842779159546,
      "learning_rate": 2.9559999999999998e-05,
      "loss": 0.0031,
      "step": 30660
    },
    {
      "epoch": 1.6357333333333335,
      "grad_norm": 0.09538397938013077,
      "learning_rate": 2.9553333333333334e-05,
      "loss": 0.0025,
      "step": 30670
    },
    {
      "epoch": 1.6362666666666668,
      "grad_norm": 0.10740616172552109,
      "learning_rate": 2.9546666666666666e-05,
      "loss": 0.0025,
      "step": 30680
    },
    {
      "epoch": 1.6368,
      "grad_norm": 0.48453977704048157,
      "learning_rate": 2.9540000000000002e-05,
      "loss": 0.0028,
      "step": 30690
    },
    {
      "epoch": 1.6373333333333333,
      "grad_norm": 0.0951918363571167,
      "learning_rate": 2.9533333333333334e-05,
      "loss": 0.0024,
      "step": 30700
    },
    {
      "epoch": 1.6378666666666666,
      "grad_norm": 0.2662928104400635,
      "learning_rate": 2.952666666666667e-05,
      "loss": 0.0023,
      "step": 30710
    },
    {
      "epoch": 1.6383999999999999,
      "grad_norm": 0.44673553109169006,
      "learning_rate": 2.9520000000000002e-05,
      "loss": 0.0028,
      "step": 30720
    },
    {
      "epoch": 1.6389333333333334,
      "grad_norm": 0.21464864909648895,
      "learning_rate": 2.9513333333333338e-05,
      "loss": 0.0031,
      "step": 30730
    },
    {
      "epoch": 1.6394666666666666,
      "grad_norm": 0.5049962997436523,
      "learning_rate": 2.9506666666666667e-05,
      "loss": 0.0025,
      "step": 30740
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.11859941482543945,
      "learning_rate": 2.95e-05,
      "loss": 0.0019,
      "step": 30750
    },
    {
      "epoch": 1.6405333333333334,
      "grad_norm": 0.47081074118614197,
      "learning_rate": 2.9493333333333334e-05,
      "loss": 0.0043,
      "step": 30760
    },
    {
      "epoch": 1.6410666666666667,
      "grad_norm": 0.3108680248260498,
      "learning_rate": 2.9486666666666667e-05,
      "loss": 0.0025,
      "step": 30770
    },
    {
      "epoch": 1.6416,
      "grad_norm": 0.14092642068862915,
      "learning_rate": 2.9480000000000002e-05,
      "loss": 0.0027,
      "step": 30780
    },
    {
      "epoch": 1.6421333333333332,
      "grad_norm": 0.10025488585233688,
      "learning_rate": 2.9473333333333335e-05,
      "loss": 0.0026,
      "step": 30790
    },
    {
      "epoch": 1.6426666666666667,
      "grad_norm": 0.36666205525398254,
      "learning_rate": 2.946666666666667e-05,
      "loss": 0.0021,
      "step": 30800
    },
    {
      "epoch": 1.6432,
      "grad_norm": 0.07977310568094254,
      "learning_rate": 2.946e-05,
      "loss": 0.0031,
      "step": 30810
    },
    {
      "epoch": 1.6437333333333335,
      "grad_norm": 0.9403289556503296,
      "learning_rate": 2.945333333333333e-05,
      "loss": 0.0029,
      "step": 30820
    },
    {
      "epoch": 1.6442666666666668,
      "grad_norm": 0.375864177942276,
      "learning_rate": 2.9446666666666667e-05,
      "loss": 0.003,
      "step": 30830
    },
    {
      "epoch": 1.6448,
      "grad_norm": 0.23600180447101593,
      "learning_rate": 2.944e-05,
      "loss": 0.002,
      "step": 30840
    },
    {
      "epoch": 1.6453333333333333,
      "grad_norm": 0.4439581334590912,
      "learning_rate": 2.9433333333333335e-05,
      "loss": 0.0019,
      "step": 30850
    },
    {
      "epoch": 1.6458666666666666,
      "grad_norm": 0.4338217079639435,
      "learning_rate": 2.942666666666667e-05,
      "loss": 0.0021,
      "step": 30860
    },
    {
      "epoch": 1.6463999999999999,
      "grad_norm": 0.06766097247600555,
      "learning_rate": 2.9420000000000003e-05,
      "loss": 0.0024,
      "step": 30870
    },
    {
      "epoch": 1.6469333333333334,
      "grad_norm": 0.19546589255332947,
      "learning_rate": 2.941333333333334e-05,
      "loss": 0.0027,
      "step": 30880
    },
    {
      "epoch": 1.6474666666666666,
      "grad_norm": 0.24057742953300476,
      "learning_rate": 2.9406666666666664e-05,
      "loss": 0.0017,
      "step": 30890
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.07107425481081009,
      "learning_rate": 2.94e-05,
      "loss": 0.0025,
      "step": 30900
    },
    {
      "epoch": 1.6485333333333334,
      "grad_norm": 0.4454789161682129,
      "learning_rate": 2.9393333333333335e-05,
      "loss": 0.0029,
      "step": 30910
    },
    {
      "epoch": 1.6490666666666667,
      "grad_norm": 0.3935193419456482,
      "learning_rate": 2.9386666666666668e-05,
      "loss": 0.0023,
      "step": 30920
    },
    {
      "epoch": 1.6496,
      "grad_norm": 0.2521756589412689,
      "learning_rate": 2.9380000000000003e-05,
      "loss": 0.0028,
      "step": 30930
    },
    {
      "epoch": 1.6501333333333332,
      "grad_norm": 0.36055824160575867,
      "learning_rate": 2.9373333333333336e-05,
      "loss": 0.0028,
      "step": 30940
    },
    {
      "epoch": 1.6506666666666665,
      "grad_norm": 0.5904647707939148,
      "learning_rate": 2.936666666666667e-05,
      "loss": 0.0039,
      "step": 30950
    },
    {
      "epoch": 1.6512,
      "grad_norm": 0.7732879519462585,
      "learning_rate": 2.9360000000000003e-05,
      "loss": 0.0025,
      "step": 30960
    },
    {
      "epoch": 1.6517333333333335,
      "grad_norm": 0.5076049566268921,
      "learning_rate": 2.9353333333333332e-05,
      "loss": 0.0022,
      "step": 30970
    },
    {
      "epoch": 1.6522666666666668,
      "grad_norm": 0.4272003769874573,
      "learning_rate": 2.9346666666666668e-05,
      "loss": 0.0023,
      "step": 30980
    },
    {
      "epoch": 1.6528,
      "grad_norm": 0.04895906150341034,
      "learning_rate": 2.934e-05,
      "loss": 0.0033,
      "step": 30990
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.41253307461738586,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 0.0029,
      "step": 31000
    },
    {
      "epoch": 1.6538666666666666,
      "grad_norm": 0.23697280883789062,
      "learning_rate": 2.9326666666666668e-05,
      "loss": 0.0025,
      "step": 31010
    },
    {
      "epoch": 1.6543999999999999,
      "grad_norm": 0.16579017043113708,
      "learning_rate": 2.9320000000000004e-05,
      "loss": 0.0026,
      "step": 31020
    },
    {
      "epoch": 1.6549333333333334,
      "grad_norm": 0.4381239116191864,
      "learning_rate": 2.9313333333333336e-05,
      "loss": 0.0022,
      "step": 31030
    },
    {
      "epoch": 1.6554666666666666,
      "grad_norm": 0.5301443338394165,
      "learning_rate": 2.9306666666666665e-05,
      "loss": 0.0018,
      "step": 31040
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.4402519464492798,
      "learning_rate": 2.93e-05,
      "loss": 0.0035,
      "step": 31050
    },
    {
      "epoch": 1.6565333333333334,
      "grad_norm": 0.28158700466156006,
      "learning_rate": 2.9293333333333333e-05,
      "loss": 0.0022,
      "step": 31060
    },
    {
      "epoch": 1.6570666666666667,
      "grad_norm": 0.2876327335834503,
      "learning_rate": 2.928666666666667e-05,
      "loss": 0.0022,
      "step": 31070
    },
    {
      "epoch": 1.6576,
      "grad_norm": 0.29951682686805725,
      "learning_rate": 2.928e-05,
      "loss": 0.003,
      "step": 31080
    },
    {
      "epoch": 1.6581333333333332,
      "grad_norm": 0.7764512896537781,
      "learning_rate": 2.9273333333333337e-05,
      "loss": 0.0023,
      "step": 31090
    },
    {
      "epoch": 1.6586666666666665,
      "grad_norm": 0.2147379368543625,
      "learning_rate": 2.926666666666667e-05,
      "loss": 0.0022,
      "step": 31100
    },
    {
      "epoch": 1.6592,
      "grad_norm": 0.14007650315761566,
      "learning_rate": 2.9260000000000004e-05,
      "loss": 0.0032,
      "step": 31110
    },
    {
      "epoch": 1.6597333333333333,
      "grad_norm": 0.37045714259147644,
      "learning_rate": 2.9253333333333333e-05,
      "loss": 0.0023,
      "step": 31120
    },
    {
      "epoch": 1.6602666666666668,
      "grad_norm": 0.5721372961997986,
      "learning_rate": 2.9246666666666666e-05,
      "loss": 0.0035,
      "step": 31130
    },
    {
      "epoch": 1.6608,
      "grad_norm": 0.31336119771003723,
      "learning_rate": 2.924e-05,
      "loss": 0.0038,
      "step": 31140
    },
    {
      "epoch": 1.6613333333333333,
      "grad_norm": 0.4944726526737213,
      "learning_rate": 2.9233333333333334e-05,
      "loss": 0.0022,
      "step": 31150
    },
    {
      "epoch": 1.6618666666666666,
      "grad_norm": 0.7502762079238892,
      "learning_rate": 2.922666666666667e-05,
      "loss": 0.002,
      "step": 31160
    },
    {
      "epoch": 1.6623999999999999,
      "grad_norm": 0.3297550678253174,
      "learning_rate": 2.922e-05,
      "loss": 0.0024,
      "step": 31170
    },
    {
      "epoch": 1.6629333333333334,
      "grad_norm": 0.06825648993253708,
      "learning_rate": 2.9213333333333337e-05,
      "loss": 0.0034,
      "step": 31180
    },
    {
      "epoch": 1.6634666666666666,
      "grad_norm": 0.5736005306243896,
      "learning_rate": 2.9206666666666666e-05,
      "loss": 0.0021,
      "step": 31190
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.5765969753265381,
      "learning_rate": 2.9199999999999998e-05,
      "loss": 0.0021,
      "step": 31200
    },
    {
      "epoch": 1.6645333333333334,
      "grad_norm": 0.33452722430229187,
      "learning_rate": 2.9193333333333334e-05,
      "loss": 0.0026,
      "step": 31210
    },
    {
      "epoch": 1.6650666666666667,
      "grad_norm": 0.43167394399642944,
      "learning_rate": 2.9186666666666666e-05,
      "loss": 0.0029,
      "step": 31220
    },
    {
      "epoch": 1.6656,
      "grad_norm": 0.41184157133102417,
      "learning_rate": 2.9180000000000002e-05,
      "loss": 0.0028,
      "step": 31230
    },
    {
      "epoch": 1.6661333333333332,
      "grad_norm": 0.19085149466991425,
      "learning_rate": 2.9173333333333337e-05,
      "loss": 0.0025,
      "step": 31240
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.11745346337556839,
      "learning_rate": 2.916666666666667e-05,
      "loss": 0.003,
      "step": 31250
    },
    {
      "epoch": 1.6672,
      "grad_norm": 0.7046366333961487,
      "learning_rate": 2.9160000000000005e-05,
      "loss": 0.0029,
      "step": 31260
    },
    {
      "epoch": 1.6677333333333333,
      "grad_norm": 0.25527653098106384,
      "learning_rate": 2.915333333333333e-05,
      "loss": 0.0027,
      "step": 31270
    },
    {
      "epoch": 1.6682666666666668,
      "grad_norm": 0.5893809199333191,
      "learning_rate": 2.9146666666666667e-05,
      "loss": 0.0023,
      "step": 31280
    },
    {
      "epoch": 1.6688,
      "grad_norm": 0.35374996066093445,
      "learning_rate": 2.9140000000000002e-05,
      "loss": 0.0024,
      "step": 31290
    },
    {
      "epoch": 1.6693333333333333,
      "grad_norm": 0.07597097009420395,
      "learning_rate": 2.9133333333333334e-05,
      "loss": 0.0036,
      "step": 31300
    },
    {
      "epoch": 1.6698666666666666,
      "grad_norm": 0.32106131315231323,
      "learning_rate": 2.912666666666667e-05,
      "loss": 0.0022,
      "step": 31310
    },
    {
      "epoch": 1.6703999999999999,
      "grad_norm": 0.7110759019851685,
      "learning_rate": 2.9120000000000002e-05,
      "loss": 0.0026,
      "step": 31320
    },
    {
      "epoch": 1.6709333333333334,
      "grad_norm": 0.26823240518569946,
      "learning_rate": 2.9113333333333338e-05,
      "loss": 0.003,
      "step": 31330
    },
    {
      "epoch": 1.6714666666666667,
      "grad_norm": 0.13421107828617096,
      "learning_rate": 2.9106666666666667e-05,
      "loss": 0.0023,
      "step": 31340
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.3034583330154419,
      "learning_rate": 2.91e-05,
      "loss": 0.003,
      "step": 31350
    },
    {
      "epoch": 1.6725333333333334,
      "grad_norm": 0.11713337153196335,
      "learning_rate": 2.9093333333333335e-05,
      "loss": 0.0018,
      "step": 31360
    },
    {
      "epoch": 1.6730666666666667,
      "grad_norm": 0.24144276976585388,
      "learning_rate": 2.9086666666666667e-05,
      "loss": 0.0036,
      "step": 31370
    },
    {
      "epoch": 1.6736,
      "grad_norm": 0.22877751290798187,
      "learning_rate": 2.9080000000000003e-05,
      "loss": 0.002,
      "step": 31380
    },
    {
      "epoch": 1.6741333333333333,
      "grad_norm": 0.47687289118766785,
      "learning_rate": 2.9073333333333335e-05,
      "loss": 0.0026,
      "step": 31390
    },
    {
      "epoch": 1.6746666666666665,
      "grad_norm": 0.3754405677318573,
      "learning_rate": 2.906666666666667e-05,
      "loss": 0.0025,
      "step": 31400
    },
    {
      "epoch": 1.6752,
      "grad_norm": 0.39769574999809265,
      "learning_rate": 2.9060000000000003e-05,
      "loss": 0.0032,
      "step": 31410
    },
    {
      "epoch": 1.6757333333333333,
      "grad_norm": 0.6094158887863159,
      "learning_rate": 2.9053333333333332e-05,
      "loss": 0.0027,
      "step": 31420
    },
    {
      "epoch": 1.6762666666666668,
      "grad_norm": 0.3505019545555115,
      "learning_rate": 2.9046666666666668e-05,
      "loss": 0.0023,
      "step": 31430
    },
    {
      "epoch": 1.6768,
      "grad_norm": 0.21490561962127686,
      "learning_rate": 2.904e-05,
      "loss": 0.0025,
      "step": 31440
    },
    {
      "epoch": 1.6773333333333333,
      "grad_norm": 0.5863192677497864,
      "learning_rate": 2.9033333333333335e-05,
      "loss": 0.0027,
      "step": 31450
    },
    {
      "epoch": 1.6778666666666666,
      "grad_norm": 0.22858624160289764,
      "learning_rate": 2.9026666666666668e-05,
      "loss": 0.0031,
      "step": 31460
    },
    {
      "epoch": 1.6784,
      "grad_norm": 0.43042072653770447,
      "learning_rate": 2.9020000000000003e-05,
      "loss": 0.0026,
      "step": 31470
    },
    {
      "epoch": 1.6789333333333334,
      "grad_norm": 0.6217260956764221,
      "learning_rate": 2.9013333333333336e-05,
      "loss": 0.0023,
      "step": 31480
    },
    {
      "epoch": 1.6794666666666667,
      "grad_norm": 0.709721028804779,
      "learning_rate": 2.9006666666666665e-05,
      "loss": 0.0022,
      "step": 31490
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.289530873298645,
      "learning_rate": 2.9e-05,
      "loss": 0.0028,
      "step": 31500
    },
    {
      "epoch": 1.6805333333333334,
      "grad_norm": 0.6159116625785828,
      "learning_rate": 2.8993333333333332e-05,
      "loss": 0.002,
      "step": 31510
    },
    {
      "epoch": 1.6810666666666667,
      "grad_norm": 0.2737707197666168,
      "learning_rate": 2.8986666666666668e-05,
      "loss": 0.0023,
      "step": 31520
    },
    {
      "epoch": 1.6816,
      "grad_norm": 0.1036486029624939,
      "learning_rate": 2.898e-05,
      "loss": 0.002,
      "step": 31530
    },
    {
      "epoch": 1.6821333333333333,
      "grad_norm": 0.4961451590061188,
      "learning_rate": 2.8973333333333336e-05,
      "loss": 0.002,
      "step": 31540
    },
    {
      "epoch": 1.6826666666666665,
      "grad_norm": 0.5993706583976746,
      "learning_rate": 2.8966666666666668e-05,
      "loss": 0.0028,
      "step": 31550
    },
    {
      "epoch": 1.6832,
      "grad_norm": 0.2574675679206848,
      "learning_rate": 2.8960000000000004e-05,
      "loss": 0.0022,
      "step": 31560
    },
    {
      "epoch": 1.6837333333333333,
      "grad_norm": 0.23272626101970673,
      "learning_rate": 2.8953333333333333e-05,
      "loss": 0.0023,
      "step": 31570
    },
    {
      "epoch": 1.6842666666666668,
      "grad_norm": 0.08533122390508652,
      "learning_rate": 2.8946666666666665e-05,
      "loss": 0.0027,
      "step": 31580
    },
    {
      "epoch": 1.6848,
      "grad_norm": 0.2864720821380615,
      "learning_rate": 2.894e-05,
      "loss": 0.0024,
      "step": 31590
    },
    {
      "epoch": 1.6853333333333333,
      "grad_norm": 0.44557875394821167,
      "learning_rate": 2.8933333333333333e-05,
      "loss": 0.0031,
      "step": 31600
    },
    {
      "epoch": 1.6858666666666666,
      "grad_norm": 0.21796812117099762,
      "learning_rate": 2.892666666666667e-05,
      "loss": 0.0026,
      "step": 31610
    },
    {
      "epoch": 1.6864,
      "grad_norm": 0.3803097903728485,
      "learning_rate": 2.8920000000000004e-05,
      "loss": 0.0022,
      "step": 31620
    },
    {
      "epoch": 1.6869333333333332,
      "grad_norm": 0.343350887298584,
      "learning_rate": 2.8913333333333337e-05,
      "loss": 0.002,
      "step": 31630
    },
    {
      "epoch": 1.6874666666666667,
      "grad_norm": 0.17280878126621246,
      "learning_rate": 2.8906666666666672e-05,
      "loss": 0.0023,
      "step": 31640
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.2896086275577545,
      "learning_rate": 2.8899999999999998e-05,
      "loss": 0.0028,
      "step": 31650
    },
    {
      "epoch": 1.6885333333333334,
      "grad_norm": 0.5600749850273132,
      "learning_rate": 2.8893333333333333e-05,
      "loss": 0.0022,
      "step": 31660
    },
    {
      "epoch": 1.6890666666666667,
      "grad_norm": 0.3225133717060089,
      "learning_rate": 2.888666666666667e-05,
      "loss": 0.0032,
      "step": 31670
    },
    {
      "epoch": 1.6896,
      "grad_norm": 0.38308051228523254,
      "learning_rate": 2.888e-05,
      "loss": 0.0026,
      "step": 31680
    },
    {
      "epoch": 1.6901333333333333,
      "grad_norm": 0.06505348533391953,
      "learning_rate": 2.8873333333333337e-05,
      "loss": 0.0029,
      "step": 31690
    },
    {
      "epoch": 1.6906666666666665,
      "grad_norm": 0.19839292764663696,
      "learning_rate": 2.886666666666667e-05,
      "loss": 0.0021,
      "step": 31700
    },
    {
      "epoch": 1.6912,
      "grad_norm": 0.1431654393672943,
      "learning_rate": 2.8860000000000005e-05,
      "loss": 0.0018,
      "step": 31710
    },
    {
      "epoch": 1.6917333333333333,
      "grad_norm": 0.3150240182876587,
      "learning_rate": 2.8853333333333334e-05,
      "loss": 0.0029,
      "step": 31720
    },
    {
      "epoch": 1.6922666666666668,
      "grad_norm": 0.10649160295724869,
      "learning_rate": 2.8846666666666666e-05,
      "loss": 0.0029,
      "step": 31730
    },
    {
      "epoch": 1.6928,
      "grad_norm": 0.7096382975578308,
      "learning_rate": 2.8840000000000002e-05,
      "loss": 0.0023,
      "step": 31740
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.20940032601356506,
      "learning_rate": 2.8833333333333334e-05,
      "loss": 0.0021,
      "step": 31750
    },
    {
      "epoch": 1.6938666666666666,
      "grad_norm": 0.19567279517650604,
      "learning_rate": 2.882666666666667e-05,
      "loss": 0.0019,
      "step": 31760
    },
    {
      "epoch": 1.6944,
      "grad_norm": 0.25427860021591187,
      "learning_rate": 2.8820000000000002e-05,
      "loss": 0.0029,
      "step": 31770
    },
    {
      "epoch": 1.6949333333333332,
      "grad_norm": 0.6384989619255066,
      "learning_rate": 2.8813333333333338e-05,
      "loss": 0.002,
      "step": 31780
    },
    {
      "epoch": 1.6954666666666667,
      "grad_norm": 0.7493933439254761,
      "learning_rate": 2.880666666666667e-05,
      "loss": 0.0026,
      "step": 31790
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.21671749651432037,
      "learning_rate": 2.88e-05,
      "loss": 0.0025,
      "step": 31800
    },
    {
      "epoch": 1.6965333333333334,
      "grad_norm": 0.44262731075286865,
      "learning_rate": 2.8793333333333334e-05,
      "loss": 0.0017,
      "step": 31810
    },
    {
      "epoch": 1.6970666666666667,
      "grad_norm": 0.33976417779922485,
      "learning_rate": 2.8786666666666667e-05,
      "loss": 0.0017,
      "step": 31820
    },
    {
      "epoch": 1.6976,
      "grad_norm": 0.5571447014808655,
      "learning_rate": 2.8780000000000002e-05,
      "loss": 0.0027,
      "step": 31830
    },
    {
      "epoch": 1.6981333333333333,
      "grad_norm": 0.08850786089897156,
      "learning_rate": 2.8773333333333335e-05,
      "loss": 0.0018,
      "step": 31840
    },
    {
      "epoch": 1.6986666666666665,
      "grad_norm": 0.41345900297164917,
      "learning_rate": 2.876666666666667e-05,
      "loss": 0.0026,
      "step": 31850
    },
    {
      "epoch": 1.6992,
      "grad_norm": 0.042147550731897354,
      "learning_rate": 2.8760000000000002e-05,
      "loss": 0.0042,
      "step": 31860
    },
    {
      "epoch": 1.6997333333333333,
      "grad_norm": 0.2400313913822174,
      "learning_rate": 2.875333333333333e-05,
      "loss": 0.0035,
      "step": 31870
    },
    {
      "epoch": 1.7002666666666668,
      "grad_norm": 0.1253378987312317,
      "learning_rate": 2.8746666666666667e-05,
      "loss": 0.0036,
      "step": 31880
    },
    {
      "epoch": 1.7008,
      "grad_norm": 0.15476186573505402,
      "learning_rate": 2.874e-05,
      "loss": 0.0028,
      "step": 31890
    },
    {
      "epoch": 1.7013333333333334,
      "grad_norm": 0.13637536764144897,
      "learning_rate": 2.8733333333333335e-05,
      "loss": 0.0045,
      "step": 31900
    },
    {
      "epoch": 1.7018666666666666,
      "grad_norm": 0.36911845207214355,
      "learning_rate": 2.8726666666666667e-05,
      "loss": 0.0025,
      "step": 31910
    },
    {
      "epoch": 1.7024,
      "grad_norm": 0.15042904019355774,
      "learning_rate": 2.8720000000000003e-05,
      "loss": 0.0021,
      "step": 31920
    },
    {
      "epoch": 1.7029333333333332,
      "grad_norm": 0.27731335163116455,
      "learning_rate": 2.8713333333333335e-05,
      "loss": 0.0019,
      "step": 31930
    },
    {
      "epoch": 1.7034666666666667,
      "grad_norm": 0.13708850741386414,
      "learning_rate": 2.870666666666667e-05,
      "loss": 0.002,
      "step": 31940
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.3517453372478485,
      "learning_rate": 2.87e-05,
      "loss": 0.0031,
      "step": 31950
    },
    {
      "epoch": 1.7045333333333335,
      "grad_norm": 0.10772484540939331,
      "learning_rate": 2.8693333333333332e-05,
      "loss": 0.0022,
      "step": 31960
    },
    {
      "epoch": 1.7050666666666667,
      "grad_norm": 0.5280653834342957,
      "learning_rate": 2.8686666666666668e-05,
      "loss": 0.0021,
      "step": 31970
    },
    {
      "epoch": 1.7056,
      "grad_norm": 0.06839855760335922,
      "learning_rate": 2.868e-05,
      "loss": 0.0037,
      "step": 31980
    },
    {
      "epoch": 1.7061333333333333,
      "grad_norm": 0.1853797286748886,
      "learning_rate": 2.8673333333333336e-05,
      "loss": 0.0032,
      "step": 31990
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.2740074396133423,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 0.0018,
      "step": 32000
    },
    {
      "epoch": 1.7072,
      "grad_norm": 0.4887523353099823,
      "learning_rate": 2.8660000000000003e-05,
      "loss": 0.0039,
      "step": 32010
    },
    {
      "epoch": 1.7077333333333333,
      "grad_norm": 0.34549376368522644,
      "learning_rate": 2.8653333333333332e-05,
      "loss": 0.0023,
      "step": 32020
    },
    {
      "epoch": 1.7082666666666668,
      "grad_norm": 0.5063981413841248,
      "learning_rate": 2.8646666666666665e-05,
      "loss": 0.0044,
      "step": 32030
    },
    {
      "epoch": 1.7088,
      "grad_norm": 0.4288908839225769,
      "learning_rate": 2.864e-05,
      "loss": 0.0027,
      "step": 32040
    },
    {
      "epoch": 1.7093333333333334,
      "grad_norm": 0.41445392370224,
      "learning_rate": 2.8633333333333336e-05,
      "loss": 0.0027,
      "step": 32050
    },
    {
      "epoch": 1.7098666666666666,
      "grad_norm": 0.08318444341421127,
      "learning_rate": 2.8626666666666668e-05,
      "loss": 0.0024,
      "step": 32060
    },
    {
      "epoch": 1.7104,
      "grad_norm": 0.4518424868583679,
      "learning_rate": 2.8620000000000004e-05,
      "loss": 0.0022,
      "step": 32070
    },
    {
      "epoch": 1.7109333333333332,
      "grad_norm": 0.5194186568260193,
      "learning_rate": 2.8613333333333336e-05,
      "loss": 0.0039,
      "step": 32080
    },
    {
      "epoch": 1.7114666666666667,
      "grad_norm": 0.32009679079055786,
      "learning_rate": 2.8606666666666672e-05,
      "loss": 0.0027,
      "step": 32090
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.502666175365448,
      "learning_rate": 2.86e-05,
      "loss": 0.0026,
      "step": 32100
    },
    {
      "epoch": 1.7125333333333335,
      "grad_norm": 0.4558212161064148,
      "learning_rate": 2.8593333333333333e-05,
      "loss": 0.0021,
      "step": 32110
    },
    {
      "epoch": 1.7130666666666667,
      "grad_norm": 0.46637192368507385,
      "learning_rate": 2.858666666666667e-05,
      "loss": 0.0026,
      "step": 32120
    },
    {
      "epoch": 1.7136,
      "grad_norm": 0.07028557360172272,
      "learning_rate": 2.858e-05,
      "loss": 0.002,
      "step": 32130
    },
    {
      "epoch": 1.7141333333333333,
      "grad_norm": 0.3625310957431793,
      "learning_rate": 2.8573333333333336e-05,
      "loss": 0.0018,
      "step": 32140
    },
    {
      "epoch": 1.7146666666666666,
      "grad_norm": 0.3821849226951599,
      "learning_rate": 2.856666666666667e-05,
      "loss": 0.0048,
      "step": 32150
    },
    {
      "epoch": 1.7151999999999998,
      "grad_norm": 0.4411105513572693,
      "learning_rate": 2.8560000000000004e-05,
      "loss": 0.0025,
      "step": 32160
    },
    {
      "epoch": 1.7157333333333333,
      "grad_norm": 0.5912158489227295,
      "learning_rate": 2.8553333333333333e-05,
      "loss": 0.0022,
      "step": 32170
    },
    {
      "epoch": 1.7162666666666668,
      "grad_norm": 0.13893075287342072,
      "learning_rate": 2.8546666666666666e-05,
      "loss": 0.0021,
      "step": 32180
    },
    {
      "epoch": 1.7168,
      "grad_norm": 0.783562183380127,
      "learning_rate": 2.854e-05,
      "loss": 0.0021,
      "step": 32190
    },
    {
      "epoch": 1.7173333333333334,
      "grad_norm": 0.31078875064849854,
      "learning_rate": 2.8533333333333333e-05,
      "loss": 0.0022,
      "step": 32200
    },
    {
      "epoch": 1.7178666666666667,
      "grad_norm": 0.4834904670715332,
      "learning_rate": 2.852666666666667e-05,
      "loss": 0.0021,
      "step": 32210
    },
    {
      "epoch": 1.7184,
      "grad_norm": 0.3963663578033447,
      "learning_rate": 2.852e-05,
      "loss": 0.0026,
      "step": 32220
    },
    {
      "epoch": 1.7189333333333332,
      "grad_norm": 0.35221999883651733,
      "learning_rate": 2.8513333333333337e-05,
      "loss": 0.0032,
      "step": 32230
    },
    {
      "epoch": 1.7194666666666667,
      "grad_norm": 0.4406082332134247,
      "learning_rate": 2.850666666666667e-05,
      "loss": 0.0023,
      "step": 32240
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.14399851858615875,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 0.0027,
      "step": 32250
    },
    {
      "epoch": 1.7205333333333335,
      "grad_norm": 0.491680771112442,
      "learning_rate": 2.8493333333333334e-05,
      "loss": 0.002,
      "step": 32260
    },
    {
      "epoch": 1.7210666666666667,
      "grad_norm": 0.403401643037796,
      "learning_rate": 2.8486666666666666e-05,
      "loss": 0.0029,
      "step": 32270
    },
    {
      "epoch": 1.7216,
      "grad_norm": 0.5224404335021973,
      "learning_rate": 2.8480000000000002e-05,
      "loss": 0.0018,
      "step": 32280
    },
    {
      "epoch": 1.7221333333333333,
      "grad_norm": 0.3132990598678589,
      "learning_rate": 2.8473333333333334e-05,
      "loss": 0.0023,
      "step": 32290
    },
    {
      "epoch": 1.7226666666666666,
      "grad_norm": 0.7387753129005432,
      "learning_rate": 2.846666666666667e-05,
      "loss": 0.0034,
      "step": 32300
    },
    {
      "epoch": 1.7231999999999998,
      "grad_norm": 0.2358076125383377,
      "learning_rate": 2.8460000000000002e-05,
      "loss": 0.0018,
      "step": 32310
    },
    {
      "epoch": 1.7237333333333333,
      "grad_norm": 0.47608307003974915,
      "learning_rate": 2.8453333333333338e-05,
      "loss": 0.0019,
      "step": 32320
    },
    {
      "epoch": 1.7242666666666666,
      "grad_norm": 0.6683905720710754,
      "learning_rate": 2.8446666666666666e-05,
      "loss": 0.0024,
      "step": 32330
    },
    {
      "epoch": 1.7248,
      "grad_norm": 0.38746947050094604,
      "learning_rate": 2.844e-05,
      "loss": 0.0029,
      "step": 32340
    },
    {
      "epoch": 1.7253333333333334,
      "grad_norm": 0.10809653252363205,
      "learning_rate": 2.8433333333333334e-05,
      "loss": 0.0024,
      "step": 32350
    },
    {
      "epoch": 1.7258666666666667,
      "grad_norm": 0.24035441875457764,
      "learning_rate": 2.8426666666666667e-05,
      "loss": 0.003,
      "step": 32360
    },
    {
      "epoch": 1.7264,
      "grad_norm": 0.10455767810344696,
      "learning_rate": 2.8420000000000002e-05,
      "loss": 0.0029,
      "step": 32370
    },
    {
      "epoch": 1.7269333333333332,
      "grad_norm": 0.13926225900650024,
      "learning_rate": 2.8413333333333335e-05,
      "loss": 0.0019,
      "step": 32380
    },
    {
      "epoch": 1.7274666666666667,
      "grad_norm": 0.3573598265647888,
      "learning_rate": 2.840666666666667e-05,
      "loss": 0.0029,
      "step": 32390
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.11615078896284103,
      "learning_rate": 2.84e-05,
      "loss": 0.0019,
      "step": 32400
    },
    {
      "epoch": 1.7285333333333335,
      "grad_norm": 0.2271745800971985,
      "learning_rate": 2.839333333333333e-05,
      "loss": 0.0023,
      "step": 32410
    },
    {
      "epoch": 1.7290666666666668,
      "grad_norm": 0.38962695002555847,
      "learning_rate": 2.8386666666666667e-05,
      "loss": 0.0023,
      "step": 32420
    },
    {
      "epoch": 1.7296,
      "grad_norm": 0.1735769808292389,
      "learning_rate": 2.8380000000000003e-05,
      "loss": 0.0043,
      "step": 32430
    },
    {
      "epoch": 1.7301333333333333,
      "grad_norm": 0.14417999982833862,
      "learning_rate": 2.8373333333333335e-05,
      "loss": 0.0021,
      "step": 32440
    },
    {
      "epoch": 1.7306666666666666,
      "grad_norm": 0.4420377314090729,
      "learning_rate": 2.836666666666667e-05,
      "loss": 0.0019,
      "step": 32450
    },
    {
      "epoch": 1.7311999999999999,
      "grad_norm": 0.5399437546730042,
      "learning_rate": 2.8360000000000003e-05,
      "loss": 0.0036,
      "step": 32460
    },
    {
      "epoch": 1.7317333333333333,
      "grad_norm": 0.26307952404022217,
      "learning_rate": 2.835333333333334e-05,
      "loss": 0.0023,
      "step": 32470
    },
    {
      "epoch": 1.7322666666666666,
      "grad_norm": 0.5793457627296448,
      "learning_rate": 2.8346666666666667e-05,
      "loss": 0.0023,
      "step": 32480
    },
    {
      "epoch": 1.7328000000000001,
      "grad_norm": 0.39866408705711365,
      "learning_rate": 2.834e-05,
      "loss": 0.0034,
      "step": 32490
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.29253673553466797,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 0.0037,
      "step": 32500
    },
    {
      "epoch": 1.7338666666666667,
      "grad_norm": 0.1332828849554062,
      "learning_rate": 2.8326666666666668e-05,
      "loss": 0.0022,
      "step": 32510
    },
    {
      "epoch": 1.7344,
      "grad_norm": 0.3686731457710266,
      "learning_rate": 2.8320000000000003e-05,
      "loss": 0.0026,
      "step": 32520
    },
    {
      "epoch": 1.7349333333333332,
      "grad_norm": 0.5164365768432617,
      "learning_rate": 2.8313333333333336e-05,
      "loss": 0.0021,
      "step": 32530
    },
    {
      "epoch": 1.7354666666666667,
      "grad_norm": 0.18908029794692993,
      "learning_rate": 2.830666666666667e-05,
      "loss": 0.0018,
      "step": 32540
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.27786341309547424,
      "learning_rate": 2.83e-05,
      "loss": 0.0024,
      "step": 32550
    },
    {
      "epoch": 1.7365333333333335,
      "grad_norm": 0.21987789869308472,
      "learning_rate": 2.8293333333333332e-05,
      "loss": 0.002,
      "step": 32560
    },
    {
      "epoch": 1.7370666666666668,
      "grad_norm": 0.41148245334625244,
      "learning_rate": 2.8286666666666668e-05,
      "loss": 0.0022,
      "step": 32570
    },
    {
      "epoch": 1.7376,
      "grad_norm": 0.2985134422779083,
      "learning_rate": 2.828e-05,
      "loss": 0.0028,
      "step": 32580
    },
    {
      "epoch": 1.7381333333333333,
      "grad_norm": 0.23244665563106537,
      "learning_rate": 2.8273333333333336e-05,
      "loss": 0.0025,
      "step": 32590
    },
    {
      "epoch": 1.7386666666666666,
      "grad_norm": 0.337566077709198,
      "learning_rate": 2.8266666666666668e-05,
      "loss": 0.0033,
      "step": 32600
    },
    {
      "epoch": 1.7391999999999999,
      "grad_norm": 0.20520690083503723,
      "learning_rate": 2.8260000000000004e-05,
      "loss": 0.0037,
      "step": 32610
    },
    {
      "epoch": 1.7397333333333334,
      "grad_norm": 0.3091171085834503,
      "learning_rate": 2.8253333333333336e-05,
      "loss": 0.002,
      "step": 32620
    },
    {
      "epoch": 1.7402666666666666,
      "grad_norm": 0.09314556419849396,
      "learning_rate": 2.8246666666666665e-05,
      "loss": 0.0026,
      "step": 32630
    },
    {
      "epoch": 1.7408000000000001,
      "grad_norm": 0.22941364347934723,
      "learning_rate": 2.824e-05,
      "loss": 0.0027,
      "step": 32640
    },
    {
      "epoch": 1.7413333333333334,
      "grad_norm": 0.11798928678035736,
      "learning_rate": 2.8233333333333333e-05,
      "loss": 0.0039,
      "step": 32650
    },
    {
      "epoch": 1.7418666666666667,
      "grad_norm": 0.5388981103897095,
      "learning_rate": 2.822666666666667e-05,
      "loss": 0.0023,
      "step": 32660
    },
    {
      "epoch": 1.7424,
      "grad_norm": 0.1878942847251892,
      "learning_rate": 2.822e-05,
      "loss": 0.0021,
      "step": 32670
    },
    {
      "epoch": 1.7429333333333332,
      "grad_norm": 0.48252955079078674,
      "learning_rate": 2.8213333333333337e-05,
      "loss": 0.0031,
      "step": 32680
    },
    {
      "epoch": 1.7434666666666667,
      "grad_norm": 0.3547218441963196,
      "learning_rate": 2.820666666666667e-05,
      "loss": 0.0029,
      "step": 32690
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.1892291009426117,
      "learning_rate": 2.8199999999999998e-05,
      "loss": 0.0033,
      "step": 32700
    },
    {
      "epoch": 1.7445333333333335,
      "grad_norm": 0.453852117061615,
      "learning_rate": 2.8193333333333333e-05,
      "loss": 0.0025,
      "step": 32710
    },
    {
      "epoch": 1.7450666666666668,
      "grad_norm": 0.4313499629497528,
      "learning_rate": 2.8186666666666666e-05,
      "loss": 0.0025,
      "step": 32720
    },
    {
      "epoch": 1.7456,
      "grad_norm": 0.6645748019218445,
      "learning_rate": 2.818e-05,
      "loss": 0.0026,
      "step": 32730
    },
    {
      "epoch": 1.7461333333333333,
      "grad_norm": 0.16990546882152557,
      "learning_rate": 2.8173333333333334e-05,
      "loss": 0.0019,
      "step": 32740
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.42225325107574463,
      "learning_rate": 2.816666666666667e-05,
      "loss": 0.002,
      "step": 32750
    },
    {
      "epoch": 1.7471999999999999,
      "grad_norm": 0.5317528247833252,
      "learning_rate": 2.816e-05,
      "loss": 0.0024,
      "step": 32760
    },
    {
      "epoch": 1.7477333333333334,
      "grad_norm": 0.3338196277618408,
      "learning_rate": 2.8153333333333337e-05,
      "loss": 0.0037,
      "step": 32770
    },
    {
      "epoch": 1.7482666666666666,
      "grad_norm": 0.4302046000957489,
      "learning_rate": 2.8146666666666666e-05,
      "loss": 0.003,
      "step": 32780
    },
    {
      "epoch": 1.7488000000000001,
      "grad_norm": 0.16611376404762268,
      "learning_rate": 2.8139999999999998e-05,
      "loss": 0.0034,
      "step": 32790
    },
    {
      "epoch": 1.7493333333333334,
      "grad_norm": 0.3089960813522339,
      "learning_rate": 2.8133333333333334e-05,
      "loss": 0.003,
      "step": 32800
    },
    {
      "epoch": 1.7498666666666667,
      "grad_norm": 0.7060614824295044,
      "learning_rate": 2.8126666666666666e-05,
      "loss": 0.003,
      "step": 32810
    },
    {
      "epoch": 1.7504,
      "grad_norm": 0.31680113077163696,
      "learning_rate": 2.8120000000000002e-05,
      "loss": 0.004,
      "step": 32820
    },
    {
      "epoch": 1.7509333333333332,
      "grad_norm": 0.8564016222953796,
      "learning_rate": 2.8113333333333337e-05,
      "loss": 0.0021,
      "step": 32830
    },
    {
      "epoch": 1.7514666666666665,
      "grad_norm": 0.5420435070991516,
      "learning_rate": 2.810666666666667e-05,
      "loss": 0.0022,
      "step": 32840
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.221565842628479,
      "learning_rate": 2.8100000000000005e-05,
      "loss": 0.0029,
      "step": 32850
    },
    {
      "epoch": 1.7525333333333335,
      "grad_norm": 0.08894519507884979,
      "learning_rate": 2.8093333333333334e-05,
      "loss": 0.0048,
      "step": 32860
    },
    {
      "epoch": 1.7530666666666668,
      "grad_norm": 0.2984660565853119,
      "learning_rate": 2.8086666666666667e-05,
      "loss": 0.0028,
      "step": 32870
    },
    {
      "epoch": 1.7536,
      "grad_norm": 0.41893383860588074,
      "learning_rate": 2.8080000000000002e-05,
      "loss": 0.0024,
      "step": 32880
    },
    {
      "epoch": 1.7541333333333333,
      "grad_norm": 0.5553134679794312,
      "learning_rate": 2.8073333333333334e-05,
      "loss": 0.0023,
      "step": 32890
    },
    {
      "epoch": 1.7546666666666666,
      "grad_norm": 0.3947223424911499,
      "learning_rate": 2.806666666666667e-05,
      "loss": 0.0029,
      "step": 32900
    },
    {
      "epoch": 1.7551999999999999,
      "grad_norm": 0.15679089725017548,
      "learning_rate": 2.8060000000000002e-05,
      "loss": 0.0025,
      "step": 32910
    },
    {
      "epoch": 1.7557333333333334,
      "grad_norm": 0.43166235089302063,
      "learning_rate": 2.8053333333333338e-05,
      "loss": 0.0023,
      "step": 32920
    },
    {
      "epoch": 1.7562666666666666,
      "grad_norm": 0.1896333247423172,
      "learning_rate": 2.8046666666666667e-05,
      "loss": 0.0019,
      "step": 32930
    },
    {
      "epoch": 1.7568000000000001,
      "grad_norm": 0.3603416979312897,
      "learning_rate": 2.804e-05,
      "loss": 0.0021,
      "step": 32940
    },
    {
      "epoch": 1.7573333333333334,
      "grad_norm": 0.1537071317434311,
      "learning_rate": 2.8033333333333335e-05,
      "loss": 0.0023,
      "step": 32950
    },
    {
      "epoch": 1.7578666666666667,
      "grad_norm": 0.31863734126091003,
      "learning_rate": 2.8026666666666667e-05,
      "loss": 0.0019,
      "step": 32960
    },
    {
      "epoch": 1.7584,
      "grad_norm": 0.18922723829746246,
      "learning_rate": 2.8020000000000003e-05,
      "loss": 0.0018,
      "step": 32970
    },
    {
      "epoch": 1.7589333333333332,
      "grad_norm": 0.38083550333976746,
      "learning_rate": 2.8013333333333335e-05,
      "loss": 0.0022,
      "step": 32980
    },
    {
      "epoch": 1.7594666666666665,
      "grad_norm": 0.10019334405660629,
      "learning_rate": 2.800666666666667e-05,
      "loss": 0.0021,
      "step": 32990
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.7917457222938538,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.002,
      "step": 33000
    },
    {
      "epoch": 1.7605333333333333,
      "grad_norm": 0.1329614371061325,
      "learning_rate": 2.7993333333333332e-05,
      "loss": 0.003,
      "step": 33010
    },
    {
      "epoch": 1.7610666666666668,
      "grad_norm": 0.49707406759262085,
      "learning_rate": 2.7986666666666668e-05,
      "loss": 0.0022,
      "step": 33020
    },
    {
      "epoch": 1.7616,
      "grad_norm": 0.10240840166807175,
      "learning_rate": 2.798e-05,
      "loss": 0.0028,
      "step": 33030
    },
    {
      "epoch": 1.7621333333333333,
      "grad_norm": 0.2817026972770691,
      "learning_rate": 2.7973333333333335e-05,
      "loss": 0.002,
      "step": 33040
    },
    {
      "epoch": 1.7626666666666666,
      "grad_norm": 0.34964314103126526,
      "learning_rate": 2.7966666666666668e-05,
      "loss": 0.0023,
      "step": 33050
    },
    {
      "epoch": 1.7631999999999999,
      "grad_norm": 0.5404939651489258,
      "learning_rate": 2.7960000000000003e-05,
      "loss": 0.0025,
      "step": 33060
    },
    {
      "epoch": 1.7637333333333334,
      "grad_norm": 0.3608757555484772,
      "learning_rate": 2.7953333333333336e-05,
      "loss": 0.0041,
      "step": 33070
    },
    {
      "epoch": 1.7642666666666666,
      "grad_norm": 0.38152819871902466,
      "learning_rate": 2.7946666666666664e-05,
      "loss": 0.002,
      "step": 33080
    },
    {
      "epoch": 1.7648000000000001,
      "grad_norm": 0.11896983534097672,
      "learning_rate": 2.794e-05,
      "loss": 0.0028,
      "step": 33090
    },
    {
      "epoch": 1.7653333333333334,
      "grad_norm": 0.07562968879938126,
      "learning_rate": 2.7933333333333332e-05,
      "loss": 0.0024,
      "step": 33100
    },
    {
      "epoch": 1.7658666666666667,
      "grad_norm": 0.13981790840625763,
      "learning_rate": 2.7926666666666668e-05,
      "loss": 0.0034,
      "step": 33110
    },
    {
      "epoch": 1.7664,
      "grad_norm": 0.5409876108169556,
      "learning_rate": 2.792e-05,
      "loss": 0.0033,
      "step": 33120
    },
    {
      "epoch": 1.7669333333333332,
      "grad_norm": 0.6310268044471741,
      "learning_rate": 2.7913333333333336e-05,
      "loss": 0.0019,
      "step": 33130
    },
    {
      "epoch": 1.7674666666666665,
      "grad_norm": 0.20362690091133118,
      "learning_rate": 2.7906666666666668e-05,
      "loss": 0.002,
      "step": 33140
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.4385519027709961,
      "learning_rate": 2.7900000000000004e-05,
      "loss": 0.0028,
      "step": 33150
    },
    {
      "epoch": 1.7685333333333333,
      "grad_norm": 0.3246743083000183,
      "learning_rate": 2.7893333333333333e-05,
      "loss": 0.0027,
      "step": 33160
    },
    {
      "epoch": 1.7690666666666668,
      "grad_norm": 0.22032327950000763,
      "learning_rate": 2.7886666666666665e-05,
      "loss": 0.0036,
      "step": 33170
    },
    {
      "epoch": 1.7696,
      "grad_norm": 0.2832615375518799,
      "learning_rate": 2.788e-05,
      "loss": 0.0023,
      "step": 33180
    },
    {
      "epoch": 1.7701333333333333,
      "grad_norm": 0.1857527792453766,
      "learning_rate": 2.7873333333333333e-05,
      "loss": 0.0022,
      "step": 33190
    },
    {
      "epoch": 1.7706666666666666,
      "grad_norm": 0.22706258296966553,
      "learning_rate": 2.786666666666667e-05,
      "loss": 0.0021,
      "step": 33200
    },
    {
      "epoch": 1.7711999999999999,
      "grad_norm": 0.48809731006622314,
      "learning_rate": 2.7860000000000004e-05,
      "loss": 0.002,
      "step": 33210
    },
    {
      "epoch": 1.7717333333333334,
      "grad_norm": 0.392411470413208,
      "learning_rate": 2.7853333333333337e-05,
      "loss": 0.0027,
      "step": 33220
    },
    {
      "epoch": 1.7722666666666667,
      "grad_norm": 0.12108397483825684,
      "learning_rate": 2.7846666666666665e-05,
      "loss": 0.0019,
      "step": 33230
    },
    {
      "epoch": 1.7728000000000002,
      "grad_norm": 0.6243584156036377,
      "learning_rate": 2.7839999999999998e-05,
      "loss": 0.0021,
      "step": 33240
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.11801305413246155,
      "learning_rate": 2.7833333333333333e-05,
      "loss": 0.004,
      "step": 33250
    },
    {
      "epoch": 1.7738666666666667,
      "grad_norm": 0.33652523159980774,
      "learning_rate": 2.782666666666667e-05,
      "loss": 0.0031,
      "step": 33260
    },
    {
      "epoch": 1.7744,
      "grad_norm": 0.3828248083591461,
      "learning_rate": 2.782e-05,
      "loss": 0.0022,
      "step": 33270
    },
    {
      "epoch": 1.7749333333333333,
      "grad_norm": 0.3040178120136261,
      "learning_rate": 2.7813333333333337e-05,
      "loss": 0.0028,
      "step": 33280
    },
    {
      "epoch": 1.7754666666666665,
      "grad_norm": 0.18008777499198914,
      "learning_rate": 2.780666666666667e-05,
      "loss": 0.0034,
      "step": 33290
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.28128138184547424,
      "learning_rate": 2.7800000000000005e-05,
      "loss": 0.0021,
      "step": 33300
    },
    {
      "epoch": 1.7765333333333333,
      "grad_norm": 0.1610552817583084,
      "learning_rate": 2.7793333333333334e-05,
      "loss": 0.0031,
      "step": 33310
    },
    {
      "epoch": 1.7770666666666668,
      "grad_norm": 0.24691161513328552,
      "learning_rate": 2.7786666666666666e-05,
      "loss": 0.0031,
      "step": 33320
    },
    {
      "epoch": 1.7776,
      "grad_norm": 0.6238081455230713,
      "learning_rate": 2.778e-05,
      "loss": 0.0021,
      "step": 33330
    },
    {
      "epoch": 1.7781333333333333,
      "grad_norm": 0.32402122020721436,
      "learning_rate": 2.7773333333333334e-05,
      "loss": 0.0022,
      "step": 33340
    },
    {
      "epoch": 1.7786666666666666,
      "grad_norm": 0.3946511745452881,
      "learning_rate": 2.776666666666667e-05,
      "loss": 0.002,
      "step": 33350
    },
    {
      "epoch": 1.7792,
      "grad_norm": 0.13457338511943817,
      "learning_rate": 2.7760000000000002e-05,
      "loss": 0.002,
      "step": 33360
    },
    {
      "epoch": 1.7797333333333332,
      "grad_norm": 0.6047199964523315,
      "learning_rate": 2.7753333333333338e-05,
      "loss": 0.0023,
      "step": 33370
    },
    {
      "epoch": 1.7802666666666667,
      "grad_norm": 0.2442713975906372,
      "learning_rate": 2.7746666666666666e-05,
      "loss": 0.0023,
      "step": 33380
    },
    {
      "epoch": 1.7808000000000002,
      "grad_norm": 0.08589077740907669,
      "learning_rate": 2.774e-05,
      "loss": 0.0025,
      "step": 33390
    },
    {
      "epoch": 1.7813333333333334,
      "grad_norm": 0.32863685488700867,
      "learning_rate": 2.7733333333333334e-05,
      "loss": 0.002,
      "step": 33400
    },
    {
      "epoch": 1.7818666666666667,
      "grad_norm": 0.10965719074010849,
      "learning_rate": 2.7726666666666667e-05,
      "loss": 0.0023,
      "step": 33410
    },
    {
      "epoch": 1.7824,
      "grad_norm": 0.32002320885658264,
      "learning_rate": 2.7720000000000002e-05,
      "loss": 0.0023,
      "step": 33420
    },
    {
      "epoch": 1.7829333333333333,
      "grad_norm": 0.4126024842262268,
      "learning_rate": 2.7713333333333335e-05,
      "loss": 0.0029,
      "step": 33430
    },
    {
      "epoch": 1.7834666666666665,
      "grad_norm": 0.4197676479816437,
      "learning_rate": 2.770666666666667e-05,
      "loss": 0.0023,
      "step": 33440
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.8850394487380981,
      "learning_rate": 2.7700000000000002e-05,
      "loss": 0.0043,
      "step": 33450
    },
    {
      "epoch": 1.7845333333333333,
      "grad_norm": 0.12810902297496796,
      "learning_rate": 2.769333333333333e-05,
      "loss": 0.003,
      "step": 33460
    },
    {
      "epoch": 1.7850666666666668,
      "grad_norm": 0.6576396822929382,
      "learning_rate": 2.7686666666666667e-05,
      "loss": 0.0025,
      "step": 33470
    },
    {
      "epoch": 1.7856,
      "grad_norm": 0.198187455534935,
      "learning_rate": 2.768e-05,
      "loss": 0.0021,
      "step": 33480
    },
    {
      "epoch": 1.7861333333333334,
      "grad_norm": 0.09740803390741348,
      "learning_rate": 2.7673333333333335e-05,
      "loss": 0.0034,
      "step": 33490
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.3536929190158844,
      "learning_rate": 2.7666666666666667e-05,
      "loss": 0.0021,
      "step": 33500
    },
    {
      "epoch": 1.7872,
      "grad_norm": 0.5555069446563721,
      "learning_rate": 2.7660000000000003e-05,
      "loss": 0.0034,
      "step": 33510
    },
    {
      "epoch": 1.7877333333333332,
      "grad_norm": 0.21948182582855225,
      "learning_rate": 2.7653333333333335e-05,
      "loss": 0.0021,
      "step": 33520
    },
    {
      "epoch": 1.7882666666666667,
      "grad_norm": 0.5897484421730042,
      "learning_rate": 2.764666666666667e-05,
      "loss": 0.0023,
      "step": 33530
    },
    {
      "epoch": 1.7888,
      "grad_norm": 0.49543654918670654,
      "learning_rate": 2.764e-05,
      "loss": 0.0021,
      "step": 33540
    },
    {
      "epoch": 1.7893333333333334,
      "grad_norm": 0.10690611600875854,
      "learning_rate": 2.7633333333333332e-05,
      "loss": 0.0019,
      "step": 33550
    },
    {
      "epoch": 1.7898666666666667,
      "grad_norm": 0.37604472041130066,
      "learning_rate": 2.7626666666666668e-05,
      "loss": 0.0026,
      "step": 33560
    },
    {
      "epoch": 1.7904,
      "grad_norm": 0.22802123427391052,
      "learning_rate": 2.762e-05,
      "loss": 0.002,
      "step": 33570
    },
    {
      "epoch": 1.7909333333333333,
      "grad_norm": 0.21884781122207642,
      "learning_rate": 2.7613333333333335e-05,
      "loss": 0.0023,
      "step": 33580
    },
    {
      "epoch": 1.7914666666666665,
      "grad_norm": 0.11865250766277313,
      "learning_rate": 2.760666666666667e-05,
      "loss": 0.0027,
      "step": 33590
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.6712441444396973,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 0.0018,
      "step": 33600
    },
    {
      "epoch": 1.7925333333333333,
      "grad_norm": 0.16141881048679352,
      "learning_rate": 2.7593333333333332e-05,
      "loss": 0.0018,
      "step": 33610
    },
    {
      "epoch": 1.7930666666666668,
      "grad_norm": 0.22385598719120026,
      "learning_rate": 2.7586666666666665e-05,
      "loss": 0.0019,
      "step": 33620
    },
    {
      "epoch": 1.7936,
      "grad_norm": 0.31244853138923645,
      "learning_rate": 2.758e-05,
      "loss": 0.0026,
      "step": 33630
    },
    {
      "epoch": 1.7941333333333334,
      "grad_norm": 0.19032904505729675,
      "learning_rate": 2.7573333333333336e-05,
      "loss": 0.0021,
      "step": 33640
    },
    {
      "epoch": 1.7946666666666666,
      "grad_norm": 0.09942899644374847,
      "learning_rate": 2.7566666666666668e-05,
      "loss": 0.0021,
      "step": 33650
    },
    {
      "epoch": 1.7952,
      "grad_norm": 0.45106515288352966,
      "learning_rate": 2.7560000000000004e-05,
      "loss": 0.0023,
      "step": 33660
    },
    {
      "epoch": 1.7957333333333332,
      "grad_norm": 0.07572215050458908,
      "learning_rate": 2.7553333333333336e-05,
      "loss": 0.0023,
      "step": 33670
    },
    {
      "epoch": 1.7962666666666667,
      "grad_norm": 0.1999550610780716,
      "learning_rate": 2.7546666666666672e-05,
      "loss": 0.003,
      "step": 33680
    },
    {
      "epoch": 1.7968,
      "grad_norm": 0.6404096484184265,
      "learning_rate": 2.754e-05,
      "loss": 0.0023,
      "step": 33690
    },
    {
      "epoch": 1.7973333333333334,
      "grad_norm": 0.12421669065952301,
      "learning_rate": 2.7533333333333333e-05,
      "loss": 0.0029,
      "step": 33700
    },
    {
      "epoch": 1.7978666666666667,
      "grad_norm": 0.07721780240535736,
      "learning_rate": 2.752666666666667e-05,
      "loss": 0.0033,
      "step": 33710
    },
    {
      "epoch": 1.7984,
      "grad_norm": 0.7100762128829956,
      "learning_rate": 2.752e-05,
      "loss": 0.0023,
      "step": 33720
    },
    {
      "epoch": 1.7989333333333333,
      "grad_norm": 0.16931606829166412,
      "learning_rate": 2.7513333333333336e-05,
      "loss": 0.0039,
      "step": 33730
    },
    {
      "epoch": 1.7994666666666665,
      "grad_norm": 0.4481014609336853,
      "learning_rate": 2.750666666666667e-05,
      "loss": 0.0027,
      "step": 33740
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.34695249795913696,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 0.002,
      "step": 33750
    },
    {
      "epoch": 1.8005333333333333,
      "grad_norm": 0.1203114315867424,
      "learning_rate": 2.7493333333333333e-05,
      "loss": 0.0019,
      "step": 33760
    },
    {
      "epoch": 1.8010666666666668,
      "grad_norm": 0.46751126646995544,
      "learning_rate": 2.7486666666666666e-05,
      "loss": 0.003,
      "step": 33770
    },
    {
      "epoch": 1.8016,
      "grad_norm": 0.4493754506111145,
      "learning_rate": 2.748e-05,
      "loss": 0.0035,
      "step": 33780
    },
    {
      "epoch": 1.8021333333333334,
      "grad_norm": 0.2875211834907532,
      "learning_rate": 2.7473333333333333e-05,
      "loss": 0.003,
      "step": 33790
    },
    {
      "epoch": 1.8026666666666666,
      "grad_norm": 0.18117313086986542,
      "learning_rate": 2.746666666666667e-05,
      "loss": 0.0027,
      "step": 33800
    },
    {
      "epoch": 1.8032,
      "grad_norm": 0.37413424253463745,
      "learning_rate": 2.746e-05,
      "loss": 0.0021,
      "step": 33810
    },
    {
      "epoch": 1.8037333333333332,
      "grad_norm": 0.8952678442001343,
      "learning_rate": 2.7453333333333337e-05,
      "loss": 0.0028,
      "step": 33820
    },
    {
      "epoch": 1.8042666666666667,
      "grad_norm": 0.1042085662484169,
      "learning_rate": 2.744666666666667e-05,
      "loss": 0.0034,
      "step": 33830
    },
    {
      "epoch": 1.8048,
      "grad_norm": 0.6221544146537781,
      "learning_rate": 2.7439999999999998e-05,
      "loss": 0.0021,
      "step": 33840
    },
    {
      "epoch": 1.8053333333333335,
      "grad_norm": 0.3408866226673126,
      "learning_rate": 2.7433333333333334e-05,
      "loss": 0.0028,
      "step": 33850
    },
    {
      "epoch": 1.8058666666666667,
      "grad_norm": 0.22273503243923187,
      "learning_rate": 2.7426666666666666e-05,
      "loss": 0.0031,
      "step": 33860
    },
    {
      "epoch": 1.8064,
      "grad_norm": 0.18756689131259918,
      "learning_rate": 2.7420000000000002e-05,
      "loss": 0.0029,
      "step": 33870
    },
    {
      "epoch": 1.8069333333333333,
      "grad_norm": 0.5006664395332336,
      "learning_rate": 2.7413333333333334e-05,
      "loss": 0.0019,
      "step": 33880
    },
    {
      "epoch": 1.8074666666666666,
      "grad_norm": 0.5740720629692078,
      "learning_rate": 2.740666666666667e-05,
      "loss": 0.0019,
      "step": 33890
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.706026554107666,
      "learning_rate": 2.7400000000000002e-05,
      "loss": 0.0023,
      "step": 33900
    },
    {
      "epoch": 1.8085333333333333,
      "grad_norm": 0.1834525614976883,
      "learning_rate": 2.739333333333333e-05,
      "loss": 0.0034,
      "step": 33910
    },
    {
      "epoch": 1.8090666666666668,
      "grad_norm": 0.3854375183582306,
      "learning_rate": 2.7386666666666666e-05,
      "loss": 0.0018,
      "step": 33920
    },
    {
      "epoch": 1.8096,
      "grad_norm": 0.7378861308097839,
      "learning_rate": 2.738e-05,
      "loss": 0.0033,
      "step": 33930
    },
    {
      "epoch": 1.8101333333333334,
      "grad_norm": 0.3010040819644928,
      "learning_rate": 2.7373333333333334e-05,
      "loss": 0.0021,
      "step": 33940
    },
    {
      "epoch": 1.8106666666666666,
      "grad_norm": 0.7122950553894043,
      "learning_rate": 2.7366666666666667e-05,
      "loss": 0.0025,
      "step": 33950
    },
    {
      "epoch": 1.8112,
      "grad_norm": 0.17873312532901764,
      "learning_rate": 2.7360000000000002e-05,
      "loss": 0.0022,
      "step": 33960
    },
    {
      "epoch": 1.8117333333333332,
      "grad_norm": 0.1305653303861618,
      "learning_rate": 2.7353333333333338e-05,
      "loss": 0.0031,
      "step": 33970
    },
    {
      "epoch": 1.8122666666666667,
      "grad_norm": 0.3689820170402527,
      "learning_rate": 2.734666666666667e-05,
      "loss": 0.002,
      "step": 33980
    },
    {
      "epoch": 1.8128,
      "grad_norm": 0.35226520895957947,
      "learning_rate": 2.734e-05,
      "loss": 0.0041,
      "step": 33990
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.09876879304647446,
      "learning_rate": 2.733333333333333e-05,
      "loss": 0.0019,
      "step": 34000
    },
    {
      "epoch": 1.8138666666666667,
      "grad_norm": 0.29370155930519104,
      "learning_rate": 2.7326666666666667e-05,
      "loss": 0.0029,
      "step": 34010
    },
    {
      "epoch": 1.8144,
      "grad_norm": 0.06855054944753647,
      "learning_rate": 2.7320000000000003e-05,
      "loss": 0.0034,
      "step": 34020
    },
    {
      "epoch": 1.8149333333333333,
      "grad_norm": 0.49988409876823425,
      "learning_rate": 2.7313333333333335e-05,
      "loss": 0.0018,
      "step": 34030
    },
    {
      "epoch": 1.8154666666666666,
      "grad_norm": 0.10646160691976547,
      "learning_rate": 2.730666666666667e-05,
      "loss": 0.0041,
      "step": 34040
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.484761118888855,
      "learning_rate": 2.7300000000000003e-05,
      "loss": 0.0017,
      "step": 34050
    },
    {
      "epoch": 1.8165333333333333,
      "grad_norm": 0.36194950342178345,
      "learning_rate": 2.7293333333333332e-05,
      "loss": 0.0036,
      "step": 34060
    },
    {
      "epoch": 1.8170666666666668,
      "grad_norm": 0.33207711577415466,
      "learning_rate": 2.7286666666666667e-05,
      "loss": 0.0019,
      "step": 34070
    },
    {
      "epoch": 1.8176,
      "grad_norm": 0.6021108031272888,
      "learning_rate": 2.728e-05,
      "loss": 0.0027,
      "step": 34080
    },
    {
      "epoch": 1.8181333333333334,
      "grad_norm": 0.05337783694267273,
      "learning_rate": 2.7273333333333335e-05,
      "loss": 0.0031,
      "step": 34090
    },
    {
      "epoch": 1.8186666666666667,
      "grad_norm": 0.4201134145259857,
      "learning_rate": 2.7266666666666668e-05,
      "loss": 0.0028,
      "step": 34100
    },
    {
      "epoch": 1.8192,
      "grad_norm": 0.2115824818611145,
      "learning_rate": 2.7260000000000003e-05,
      "loss": 0.0021,
      "step": 34110
    },
    {
      "epoch": 1.8197333333333332,
      "grad_norm": 0.16734634339809418,
      "learning_rate": 2.7253333333333336e-05,
      "loss": 0.005,
      "step": 34120
    },
    {
      "epoch": 1.8202666666666667,
      "grad_norm": 0.18714310228824615,
      "learning_rate": 2.724666666666667e-05,
      "loss": 0.0024,
      "step": 34130
    },
    {
      "epoch": 1.8208,
      "grad_norm": 0.09089676290750504,
      "learning_rate": 2.724e-05,
      "loss": 0.0038,
      "step": 34140
    },
    {
      "epoch": 1.8213333333333335,
      "grad_norm": 0.292360782623291,
      "learning_rate": 2.7233333333333332e-05,
      "loss": 0.0025,
      "step": 34150
    },
    {
      "epoch": 1.8218666666666667,
      "grad_norm": 0.25107428431510925,
      "learning_rate": 2.7226666666666668e-05,
      "loss": 0.0028,
      "step": 34160
    },
    {
      "epoch": 1.8224,
      "grad_norm": 0.2692777216434479,
      "learning_rate": 2.722e-05,
      "loss": 0.002,
      "step": 34170
    },
    {
      "epoch": 1.8229333333333333,
      "grad_norm": 0.07706758379936218,
      "learning_rate": 2.7213333333333336e-05,
      "loss": 0.0019,
      "step": 34180
    },
    {
      "epoch": 1.8234666666666666,
      "grad_norm": 0.28552281856536865,
      "learning_rate": 2.7206666666666668e-05,
      "loss": 0.0028,
      "step": 34190
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.4863731861114502,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 0.0019,
      "step": 34200
    },
    {
      "epoch": 1.8245333333333333,
      "grad_norm": 0.13614194095134735,
      "learning_rate": 2.7193333333333336e-05,
      "loss": 0.0024,
      "step": 34210
    },
    {
      "epoch": 1.8250666666666666,
      "grad_norm": 0.0949375182390213,
      "learning_rate": 2.7186666666666665e-05,
      "loss": 0.0025,
      "step": 34220
    },
    {
      "epoch": 1.8256000000000001,
      "grad_norm": 0.1579645425081253,
      "learning_rate": 2.718e-05,
      "loss": 0.002,
      "step": 34230
    },
    {
      "epoch": 1.8261333333333334,
      "grad_norm": 0.5561189651489258,
      "learning_rate": 2.7173333333333333e-05,
      "loss": 0.003,
      "step": 34240
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.5370168089866638,
      "learning_rate": 2.716666666666667e-05,
      "loss": 0.0021,
      "step": 34250
    },
    {
      "epoch": 1.8272,
      "grad_norm": 0.21251419186592102,
      "learning_rate": 2.716e-05,
      "loss": 0.0025,
      "step": 34260
    },
    {
      "epoch": 1.8277333333333332,
      "grad_norm": 0.7260526418685913,
      "learning_rate": 2.7153333333333337e-05,
      "loss": 0.0035,
      "step": 34270
    },
    {
      "epoch": 1.8282666666666667,
      "grad_norm": 0.4087485074996948,
      "learning_rate": 2.714666666666667e-05,
      "loss": 0.0032,
      "step": 34280
    },
    {
      "epoch": 1.8288,
      "grad_norm": 0.20181533694267273,
      "learning_rate": 2.7139999999999998e-05,
      "loss": 0.0025,
      "step": 34290
    },
    {
      "epoch": 1.8293333333333335,
      "grad_norm": 0.3527716398239136,
      "learning_rate": 2.7133333333333333e-05,
      "loss": 0.0025,
      "step": 34300
    },
    {
      "epoch": 1.8298666666666668,
      "grad_norm": 0.38176241517066956,
      "learning_rate": 2.7126666666666666e-05,
      "loss": 0.002,
      "step": 34310
    },
    {
      "epoch": 1.8304,
      "grad_norm": 0.2852567732334137,
      "learning_rate": 2.712e-05,
      "loss": 0.0022,
      "step": 34320
    },
    {
      "epoch": 1.8309333333333333,
      "grad_norm": 0.30233442783355713,
      "learning_rate": 2.7113333333333333e-05,
      "loss": 0.0044,
      "step": 34330
    },
    {
      "epoch": 1.8314666666666666,
      "grad_norm": 0.3142736852169037,
      "learning_rate": 2.710666666666667e-05,
      "loss": 0.0025,
      "step": 34340
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.23178468644618988,
      "learning_rate": 2.7100000000000005e-05,
      "loss": 0.0019,
      "step": 34350
    },
    {
      "epoch": 1.8325333333333333,
      "grad_norm": 0.07750872522592545,
      "learning_rate": 2.7093333333333337e-05,
      "loss": 0.0025,
      "step": 34360
    },
    {
      "epoch": 1.8330666666666666,
      "grad_norm": 0.17086580395698547,
      "learning_rate": 2.7086666666666666e-05,
      "loss": 0.0031,
      "step": 34370
    },
    {
      "epoch": 1.8336000000000001,
      "grad_norm": 0.43295547366142273,
      "learning_rate": 2.7079999999999998e-05,
      "loss": 0.003,
      "step": 34380
    },
    {
      "epoch": 1.8341333333333334,
      "grad_norm": 0.5143896341323853,
      "learning_rate": 2.7073333333333334e-05,
      "loss": 0.0023,
      "step": 34390
    },
    {
      "epoch": 1.8346666666666667,
      "grad_norm": 0.7727704644203186,
      "learning_rate": 2.706666666666667e-05,
      "loss": 0.0021,
      "step": 34400
    },
    {
      "epoch": 1.8352,
      "grad_norm": 0.09785431623458862,
      "learning_rate": 2.7060000000000002e-05,
      "loss": 0.0019,
      "step": 34410
    },
    {
      "epoch": 1.8357333333333332,
      "grad_norm": 0.2018221914768219,
      "learning_rate": 2.7053333333333337e-05,
      "loss": 0.0021,
      "step": 34420
    },
    {
      "epoch": 1.8362666666666667,
      "grad_norm": 0.7013764977455139,
      "learning_rate": 2.704666666666667e-05,
      "loss": 0.0019,
      "step": 34430
    },
    {
      "epoch": 1.8368,
      "grad_norm": 0.6784619092941284,
      "learning_rate": 2.704e-05,
      "loss": 0.0025,
      "step": 34440
    },
    {
      "epoch": 1.8373333333333335,
      "grad_norm": 0.19019679725170135,
      "learning_rate": 2.7033333333333334e-05,
      "loss": 0.0019,
      "step": 34450
    },
    {
      "epoch": 1.8378666666666668,
      "grad_norm": 0.12915892899036407,
      "learning_rate": 2.7026666666666667e-05,
      "loss": 0.0033,
      "step": 34460
    },
    {
      "epoch": 1.8384,
      "grad_norm": 0.7784175872802734,
      "learning_rate": 2.7020000000000002e-05,
      "loss": 0.0018,
      "step": 34470
    },
    {
      "epoch": 1.8389333333333333,
      "grad_norm": 0.3004603683948517,
      "learning_rate": 2.7013333333333334e-05,
      "loss": 0.0025,
      "step": 34480
    },
    {
      "epoch": 1.8394666666666666,
      "grad_norm": 0.4165302813053131,
      "learning_rate": 2.700666666666667e-05,
      "loss": 0.0027,
      "step": 34490
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.1613471657037735,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.0025,
      "step": 34500
    },
    {
      "epoch": 1.8405333333333334,
      "grad_norm": 0.3691033124923706,
      "learning_rate": 2.6993333333333338e-05,
      "loss": 0.0035,
      "step": 34510
    },
    {
      "epoch": 1.8410666666666666,
      "grad_norm": 0.3656514585018158,
      "learning_rate": 2.6986666666666667e-05,
      "loss": 0.0028,
      "step": 34520
    },
    {
      "epoch": 1.8416000000000001,
      "grad_norm": 0.24993130564689636,
      "learning_rate": 2.698e-05,
      "loss": 0.0019,
      "step": 34530
    },
    {
      "epoch": 1.8421333333333334,
      "grad_norm": 0.18129239976406097,
      "learning_rate": 2.6973333333333335e-05,
      "loss": 0.0036,
      "step": 34540
    },
    {
      "epoch": 1.8426666666666667,
      "grad_norm": 0.2031940072774887,
      "learning_rate": 2.6966666666666667e-05,
      "loss": 0.002,
      "step": 34550
    },
    {
      "epoch": 1.8432,
      "grad_norm": 0.29998016357421875,
      "learning_rate": 2.6960000000000003e-05,
      "loss": 0.0023,
      "step": 34560
    },
    {
      "epoch": 1.8437333333333332,
      "grad_norm": 0.22619813680648804,
      "learning_rate": 2.6953333333333335e-05,
      "loss": 0.0022,
      "step": 34570
    },
    {
      "epoch": 1.8442666666666667,
      "grad_norm": 0.48938339948654175,
      "learning_rate": 2.694666666666667e-05,
      "loss": 0.0016,
      "step": 34580
    },
    {
      "epoch": 1.8448,
      "grad_norm": 0.1477285921573639,
      "learning_rate": 2.694e-05,
      "loss": 0.0026,
      "step": 34590
    },
    {
      "epoch": 1.8453333333333335,
      "grad_norm": 0.31257927417755127,
      "learning_rate": 2.6933333333333332e-05,
      "loss": 0.0023,
      "step": 34600
    },
    {
      "epoch": 1.8458666666666668,
      "grad_norm": 0.3599952161312103,
      "learning_rate": 2.6926666666666667e-05,
      "loss": 0.0021,
      "step": 34610
    },
    {
      "epoch": 1.8464,
      "grad_norm": 0.23123249411582947,
      "learning_rate": 2.692e-05,
      "loss": 0.0027,
      "step": 34620
    },
    {
      "epoch": 1.8469333333333333,
      "grad_norm": 0.25646862387657166,
      "learning_rate": 2.6913333333333335e-05,
      "loss": 0.0023,
      "step": 34630
    },
    {
      "epoch": 1.8474666666666666,
      "grad_norm": 0.2829040288925171,
      "learning_rate": 2.6906666666666668e-05,
      "loss": 0.0029,
      "step": 34640
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.19068332016468048,
      "learning_rate": 2.6900000000000003e-05,
      "loss": 0.0028,
      "step": 34650
    },
    {
      "epoch": 1.8485333333333334,
      "grad_norm": 0.7207956314086914,
      "learning_rate": 2.6893333333333336e-05,
      "loss": 0.0028,
      "step": 34660
    },
    {
      "epoch": 1.8490666666666666,
      "grad_norm": 0.23427416384220123,
      "learning_rate": 2.6886666666666664e-05,
      "loss": 0.0023,
      "step": 34670
    },
    {
      "epoch": 1.8496000000000001,
      "grad_norm": 0.420186847448349,
      "learning_rate": 2.688e-05,
      "loss": 0.0036,
      "step": 34680
    },
    {
      "epoch": 1.8501333333333334,
      "grad_norm": 0.14864495396614075,
      "learning_rate": 2.6873333333333332e-05,
      "loss": 0.0021,
      "step": 34690
    },
    {
      "epoch": 1.8506666666666667,
      "grad_norm": 0.22378404438495636,
      "learning_rate": 2.6866666666666668e-05,
      "loss": 0.0034,
      "step": 34700
    },
    {
      "epoch": 1.8512,
      "grad_norm": 0.2843843996524811,
      "learning_rate": 2.686e-05,
      "loss": 0.004,
      "step": 34710
    },
    {
      "epoch": 1.8517333333333332,
      "grad_norm": 0.3945196270942688,
      "learning_rate": 2.6853333333333336e-05,
      "loss": 0.0021,
      "step": 34720
    },
    {
      "epoch": 1.8522666666666665,
      "grad_norm": 0.27575910091400146,
      "learning_rate": 2.684666666666667e-05,
      "loss": 0.0027,
      "step": 34730
    },
    {
      "epoch": 1.8528,
      "grad_norm": 0.14777903258800507,
      "learning_rate": 2.6840000000000004e-05,
      "loss": 0.0029,
      "step": 34740
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.24156025052070618,
      "learning_rate": 2.6833333333333333e-05,
      "loss": 0.0029,
      "step": 34750
    },
    {
      "epoch": 1.8538666666666668,
      "grad_norm": 0.5696569085121155,
      "learning_rate": 2.6826666666666665e-05,
      "loss": 0.0024,
      "step": 34760
    },
    {
      "epoch": 1.8544,
      "grad_norm": 0.414177268743515,
      "learning_rate": 2.682e-05,
      "loss": 0.0017,
      "step": 34770
    },
    {
      "epoch": 1.8549333333333333,
      "grad_norm": 0.13970308005809784,
      "learning_rate": 2.6813333333333336e-05,
      "loss": 0.0019,
      "step": 34780
    },
    {
      "epoch": 1.8554666666666666,
      "grad_norm": 0.2449702024459839,
      "learning_rate": 2.680666666666667e-05,
      "loss": 0.0022,
      "step": 34790
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.11534670740365982,
      "learning_rate": 2.6800000000000004e-05,
      "loss": 0.0019,
      "step": 34800
    },
    {
      "epoch": 1.8565333333333334,
      "grad_norm": 0.18357710540294647,
      "learning_rate": 2.6793333333333337e-05,
      "loss": 0.0024,
      "step": 34810
    },
    {
      "epoch": 1.8570666666666666,
      "grad_norm": 0.5942721962928772,
      "learning_rate": 2.6786666666666665e-05,
      "loss": 0.0034,
      "step": 34820
    },
    {
      "epoch": 1.8576000000000001,
      "grad_norm": 0.2033536583185196,
      "learning_rate": 2.678e-05,
      "loss": 0.0022,
      "step": 34830
    },
    {
      "epoch": 1.8581333333333334,
      "grad_norm": 0.3240707516670227,
      "learning_rate": 2.6773333333333333e-05,
      "loss": 0.003,
      "step": 34840
    },
    {
      "epoch": 1.8586666666666667,
      "grad_norm": 0.4626157283782959,
      "learning_rate": 2.676666666666667e-05,
      "loss": 0.0037,
      "step": 34850
    },
    {
      "epoch": 1.8592,
      "grad_norm": 0.32548660039901733,
      "learning_rate": 2.676e-05,
      "loss": 0.0024,
      "step": 34860
    },
    {
      "epoch": 1.8597333333333332,
      "grad_norm": 0.3523024618625641,
      "learning_rate": 2.6753333333333337e-05,
      "loss": 0.0026,
      "step": 34870
    },
    {
      "epoch": 1.8602666666666665,
      "grad_norm": 0.5485492944717407,
      "learning_rate": 2.674666666666667e-05,
      "loss": 0.0032,
      "step": 34880
    },
    {
      "epoch": 1.8608,
      "grad_norm": 0.32459455728530884,
      "learning_rate": 2.6740000000000005e-05,
      "loss": 0.002,
      "step": 34890
    },
    {
      "epoch": 1.8613333333333333,
      "grad_norm": 0.5477375388145447,
      "learning_rate": 2.6733333333333334e-05,
      "loss": 0.0032,
      "step": 34900
    },
    {
      "epoch": 1.8618666666666668,
      "grad_norm": 0.5284065008163452,
      "learning_rate": 2.6726666666666666e-05,
      "loss": 0.0027,
      "step": 34910
    },
    {
      "epoch": 1.8624,
      "grad_norm": 0.1917508840560913,
      "learning_rate": 2.672e-05,
      "loss": 0.002,
      "step": 34920
    },
    {
      "epoch": 1.8629333333333333,
      "grad_norm": 0.09351310133934021,
      "learning_rate": 2.6713333333333334e-05,
      "loss": 0.0026,
      "step": 34930
    },
    {
      "epoch": 1.8634666666666666,
      "grad_norm": 0.48127350211143494,
      "learning_rate": 2.670666666666667e-05,
      "loss": 0.0041,
      "step": 34940
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.7671904563903809,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 0.0035,
      "step": 34950
    },
    {
      "epoch": 1.8645333333333334,
      "grad_norm": 0.2443312108516693,
      "learning_rate": 2.6693333333333338e-05,
      "loss": 0.0022,
      "step": 34960
    },
    {
      "epoch": 1.8650666666666667,
      "grad_norm": 0.07455932348966599,
      "learning_rate": 2.6686666666666666e-05,
      "loss": 0.0026,
      "step": 34970
    },
    {
      "epoch": 1.8656000000000001,
      "grad_norm": 0.24654912948608398,
      "learning_rate": 2.668e-05,
      "loss": 0.0032,
      "step": 34980
    },
    {
      "epoch": 1.8661333333333334,
      "grad_norm": 0.06067556515336037,
      "learning_rate": 2.6673333333333334e-05,
      "loss": 0.0028,
      "step": 34990
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.20834319293498993,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.003,
      "step": 35000
    },
    {
      "epoch": 1.8672,
      "grad_norm": 0.29109475016593933,
      "learning_rate": 2.6660000000000002e-05,
      "loss": 0.0025,
      "step": 35010
    },
    {
      "epoch": 1.8677333333333332,
      "grad_norm": 0.22939760982990265,
      "learning_rate": 2.6653333333333335e-05,
      "loss": 0.0023,
      "step": 35020
    },
    {
      "epoch": 1.8682666666666665,
      "grad_norm": 0.6157238483428955,
      "learning_rate": 2.664666666666667e-05,
      "loss": 0.0034,
      "step": 35030
    },
    {
      "epoch": 1.8688,
      "grad_norm": 0.41421961784362793,
      "learning_rate": 2.6640000000000002e-05,
      "loss": 0.002,
      "step": 35040
    },
    {
      "epoch": 1.8693333333333333,
      "grad_norm": 0.41050466895103455,
      "learning_rate": 2.663333333333333e-05,
      "loss": 0.0026,
      "step": 35050
    },
    {
      "epoch": 1.8698666666666668,
      "grad_norm": 0.8775705695152283,
      "learning_rate": 2.6626666666666667e-05,
      "loss": 0.002,
      "step": 35060
    },
    {
      "epoch": 1.8704,
      "grad_norm": 0.2565370798110962,
      "learning_rate": 2.662e-05,
      "loss": 0.002,
      "step": 35070
    },
    {
      "epoch": 1.8709333333333333,
      "grad_norm": 0.11586422473192215,
      "learning_rate": 2.6613333333333335e-05,
      "loss": 0.0042,
      "step": 35080
    },
    {
      "epoch": 1.8714666666666666,
      "grad_norm": 0.1498040407896042,
      "learning_rate": 2.6606666666666667e-05,
      "loss": 0.0027,
      "step": 35090
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.22983106970787048,
      "learning_rate": 2.6600000000000003e-05,
      "loss": 0.0024,
      "step": 35100
    },
    {
      "epoch": 1.8725333333333334,
      "grad_norm": 0.5083332657814026,
      "learning_rate": 2.6593333333333335e-05,
      "loss": 0.0019,
      "step": 35110
    },
    {
      "epoch": 1.8730666666666667,
      "grad_norm": 0.2745671570301056,
      "learning_rate": 2.6586666666666664e-05,
      "loss": 0.0034,
      "step": 35120
    },
    {
      "epoch": 1.8736000000000002,
      "grad_norm": 0.45119741559028625,
      "learning_rate": 2.658e-05,
      "loss": 0.0036,
      "step": 35130
    },
    {
      "epoch": 1.8741333333333334,
      "grad_norm": 0.3557850122451782,
      "learning_rate": 2.6573333333333332e-05,
      "loss": 0.0028,
      "step": 35140
    },
    {
      "epoch": 1.8746666666666667,
      "grad_norm": 0.38767120242118835,
      "learning_rate": 2.6566666666666668e-05,
      "loss": 0.0021,
      "step": 35150
    },
    {
      "epoch": 1.8752,
      "grad_norm": 0.27792835235595703,
      "learning_rate": 2.6560000000000003e-05,
      "loss": 0.0019,
      "step": 35160
    },
    {
      "epoch": 1.8757333333333333,
      "grad_norm": 0.08645107597112656,
      "learning_rate": 2.6553333333333335e-05,
      "loss": 0.0033,
      "step": 35170
    },
    {
      "epoch": 1.8762666666666665,
      "grad_norm": 0.31132787466049194,
      "learning_rate": 2.654666666666667e-05,
      "loss": 0.0025,
      "step": 35180
    },
    {
      "epoch": 1.8768,
      "grad_norm": 0.24898435175418854,
      "learning_rate": 2.6540000000000003e-05,
      "loss": 0.0026,
      "step": 35190
    },
    {
      "epoch": 1.8773333333333333,
      "grad_norm": 0.3881375789642334,
      "learning_rate": 2.6533333333333332e-05,
      "loss": 0.0022,
      "step": 35200
    },
    {
      "epoch": 1.8778666666666668,
      "grad_norm": 0.5620467662811279,
      "learning_rate": 2.6526666666666668e-05,
      "loss": 0.0027,
      "step": 35210
    },
    {
      "epoch": 1.8784,
      "grad_norm": 0.6370152235031128,
      "learning_rate": 2.652e-05,
      "loss": 0.0016,
      "step": 35220
    },
    {
      "epoch": 1.8789333333333333,
      "grad_norm": 0.19982652366161346,
      "learning_rate": 2.6513333333333336e-05,
      "loss": 0.0034,
      "step": 35230
    },
    {
      "epoch": 1.8794666666666666,
      "grad_norm": 0.1867910772562027,
      "learning_rate": 2.6506666666666668e-05,
      "loss": 0.0038,
      "step": 35240
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.264594167470932,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 0.0024,
      "step": 35250
    },
    {
      "epoch": 1.8805333333333332,
      "grad_norm": 0.12390701472759247,
      "learning_rate": 2.6493333333333336e-05,
      "loss": 0.0023,
      "step": 35260
    },
    {
      "epoch": 1.8810666666666667,
      "grad_norm": 0.26535099744796753,
      "learning_rate": 2.6486666666666665e-05,
      "loss": 0.0024,
      "step": 35270
    },
    {
      "epoch": 1.8816000000000002,
      "grad_norm": 0.1432730108499527,
      "learning_rate": 2.648e-05,
      "loss": 0.002,
      "step": 35280
    },
    {
      "epoch": 1.8821333333333334,
      "grad_norm": 0.36617133021354675,
      "learning_rate": 2.6473333333333333e-05,
      "loss": 0.0033,
      "step": 35290
    },
    {
      "epoch": 1.8826666666666667,
      "grad_norm": 0.18390807509422302,
      "learning_rate": 2.646666666666667e-05,
      "loss": 0.0026,
      "step": 35300
    },
    {
      "epoch": 1.8832,
      "grad_norm": 0.3453320264816284,
      "learning_rate": 2.646e-05,
      "loss": 0.0034,
      "step": 35310
    },
    {
      "epoch": 1.8837333333333333,
      "grad_norm": 0.5409010648727417,
      "learning_rate": 2.6453333333333336e-05,
      "loss": 0.0023,
      "step": 35320
    },
    {
      "epoch": 1.8842666666666665,
      "grad_norm": 0.465808242559433,
      "learning_rate": 2.644666666666667e-05,
      "loss": 0.0034,
      "step": 35330
    },
    {
      "epoch": 1.8848,
      "grad_norm": 0.1294543743133545,
      "learning_rate": 2.6440000000000004e-05,
      "loss": 0.0026,
      "step": 35340
    },
    {
      "epoch": 1.8853333333333333,
      "grad_norm": 0.18879733979701996,
      "learning_rate": 2.6433333333333333e-05,
      "loss": 0.0034,
      "step": 35350
    },
    {
      "epoch": 1.8858666666666668,
      "grad_norm": 0.10721056163311005,
      "learning_rate": 2.6426666666666665e-05,
      "loss": 0.0022,
      "step": 35360
    },
    {
      "epoch": 1.8864,
      "grad_norm": 0.7311365604400635,
      "learning_rate": 2.642e-05,
      "loss": 0.0021,
      "step": 35370
    },
    {
      "epoch": 1.8869333333333334,
      "grad_norm": 0.5935441851615906,
      "learning_rate": 2.6413333333333333e-05,
      "loss": 0.0021,
      "step": 35380
    },
    {
      "epoch": 1.8874666666666666,
      "grad_norm": 0.08911757171154022,
      "learning_rate": 2.640666666666667e-05,
      "loss": 0.0032,
      "step": 35390
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.17012639343738556,
      "learning_rate": 2.64e-05,
      "loss": 0.0033,
      "step": 35400
    },
    {
      "epoch": 1.8885333333333332,
      "grad_norm": 0.1380568891763687,
      "learning_rate": 2.6393333333333337e-05,
      "loss": 0.002,
      "step": 35410
    },
    {
      "epoch": 1.8890666666666667,
      "grad_norm": 0.1145760789513588,
      "learning_rate": 2.638666666666667e-05,
      "loss": 0.0027,
      "step": 35420
    },
    {
      "epoch": 1.8896,
      "grad_norm": 0.14753597974777222,
      "learning_rate": 2.6379999999999998e-05,
      "loss": 0.0023,
      "step": 35430
    },
    {
      "epoch": 1.8901333333333334,
      "grad_norm": 0.6737158894538879,
      "learning_rate": 2.6373333333333334e-05,
      "loss": 0.0025,
      "step": 35440
    },
    {
      "epoch": 1.8906666666666667,
      "grad_norm": 0.35340017080307007,
      "learning_rate": 2.6366666666666666e-05,
      "loss": 0.0024,
      "step": 35450
    },
    {
      "epoch": 1.8912,
      "grad_norm": 0.07497730106115341,
      "learning_rate": 2.6360000000000002e-05,
      "loss": 0.002,
      "step": 35460
    },
    {
      "epoch": 1.8917333333333333,
      "grad_norm": 0.6834841370582581,
      "learning_rate": 2.6353333333333334e-05,
      "loss": 0.0021,
      "step": 35470
    },
    {
      "epoch": 1.8922666666666665,
      "grad_norm": 0.4405871331691742,
      "learning_rate": 2.634666666666667e-05,
      "loss": 0.0026,
      "step": 35480
    },
    {
      "epoch": 1.8928,
      "grad_norm": 0.17896749079227448,
      "learning_rate": 2.6340000000000002e-05,
      "loss": 0.0029,
      "step": 35490
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.2413972169160843,
      "learning_rate": 2.633333333333333e-05,
      "loss": 0.0018,
      "step": 35500
    },
    {
      "epoch": 1.8938666666666668,
      "grad_norm": 0.33335575461387634,
      "learning_rate": 2.6326666666666666e-05,
      "loss": 0.0021,
      "step": 35510
    },
    {
      "epoch": 1.8944,
      "grad_norm": 0.22607696056365967,
      "learning_rate": 2.632e-05,
      "loss": 0.0019,
      "step": 35520
    },
    {
      "epoch": 1.8949333333333334,
      "grad_norm": 0.11409606784582138,
      "learning_rate": 2.6313333333333334e-05,
      "loss": 0.0022,
      "step": 35530
    },
    {
      "epoch": 1.8954666666666666,
      "grad_norm": 0.46649736166000366,
      "learning_rate": 2.630666666666667e-05,
      "loss": 0.0022,
      "step": 35540
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.30140221118927,
      "learning_rate": 2.6300000000000002e-05,
      "loss": 0.0031,
      "step": 35550
    },
    {
      "epoch": 1.8965333333333332,
      "grad_norm": 0.6048506498336792,
      "learning_rate": 2.6293333333333338e-05,
      "loss": 0.002,
      "step": 35560
    },
    {
      "epoch": 1.8970666666666667,
      "grad_norm": 0.7152786254882812,
      "learning_rate": 2.628666666666667e-05,
      "loss": 0.0028,
      "step": 35570
    },
    {
      "epoch": 1.8976,
      "grad_norm": 0.12484338879585266,
      "learning_rate": 2.628e-05,
      "loss": 0.0028,
      "step": 35580
    },
    {
      "epoch": 1.8981333333333335,
      "grad_norm": 0.32670527696609497,
      "learning_rate": 2.6273333333333335e-05,
      "loss": 0.002,
      "step": 35590
    },
    {
      "epoch": 1.8986666666666667,
      "grad_norm": 0.39871707558631897,
      "learning_rate": 2.6266666666666667e-05,
      "loss": 0.0027,
      "step": 35600
    },
    {
      "epoch": 1.8992,
      "grad_norm": 0.3558795750141144,
      "learning_rate": 2.6260000000000003e-05,
      "loss": 0.0033,
      "step": 35610
    },
    {
      "epoch": 1.8997333333333333,
      "grad_norm": 0.43350744247436523,
      "learning_rate": 2.6253333333333335e-05,
      "loss": 0.0022,
      "step": 35620
    },
    {
      "epoch": 1.9002666666666665,
      "grad_norm": 0.19673961400985718,
      "learning_rate": 2.624666666666667e-05,
      "loss": 0.0031,
      "step": 35630
    },
    {
      "epoch": 1.9008,
      "grad_norm": 0.263200968503952,
      "learning_rate": 2.6240000000000003e-05,
      "loss": 0.0026,
      "step": 35640
    },
    {
      "epoch": 1.9013333333333333,
      "grad_norm": 0.1816454976797104,
      "learning_rate": 2.6233333333333332e-05,
      "loss": 0.0023,
      "step": 35650
    },
    {
      "epoch": 1.9018666666666668,
      "grad_norm": 0.14588409662246704,
      "learning_rate": 2.6226666666666667e-05,
      "loss": 0.0025,
      "step": 35660
    },
    {
      "epoch": 1.9024,
      "grad_norm": 0.05065717548131943,
      "learning_rate": 2.622e-05,
      "loss": 0.0018,
      "step": 35670
    },
    {
      "epoch": 1.9029333333333334,
      "grad_norm": 0.28969043493270874,
      "learning_rate": 2.6213333333333335e-05,
      "loss": 0.0027,
      "step": 35680
    },
    {
      "epoch": 1.9034666666666666,
      "grad_norm": 0.40767642855644226,
      "learning_rate": 2.6206666666666668e-05,
      "loss": 0.002,
      "step": 35690
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.6341366767883301,
      "learning_rate": 2.6200000000000003e-05,
      "loss": 0.0023,
      "step": 35700
    },
    {
      "epoch": 1.9045333333333332,
      "grad_norm": 0.4616241157054901,
      "learning_rate": 2.6193333333333336e-05,
      "loss": 0.0028,
      "step": 35710
    },
    {
      "epoch": 1.9050666666666667,
      "grad_norm": 0.6745427846908569,
      "learning_rate": 2.618666666666667e-05,
      "loss": 0.0022,
      "step": 35720
    },
    {
      "epoch": 1.9056,
      "grad_norm": 0.20172756910324097,
      "learning_rate": 2.618e-05,
      "loss": 0.0027,
      "step": 35730
    },
    {
      "epoch": 1.9061333333333335,
      "grad_norm": 0.13812269270420074,
      "learning_rate": 2.6173333333333332e-05,
      "loss": 0.0018,
      "step": 35740
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.1298617422580719,
      "learning_rate": 2.6166666666666668e-05,
      "loss": 0.0023,
      "step": 35750
    },
    {
      "epoch": 1.9072,
      "grad_norm": 0.37680137157440186,
      "learning_rate": 2.616e-05,
      "loss": 0.0029,
      "step": 35760
    },
    {
      "epoch": 1.9077333333333333,
      "grad_norm": 0.29591572284698486,
      "learning_rate": 2.6153333333333336e-05,
      "loss": 0.0023,
      "step": 35770
    },
    {
      "epoch": 1.9082666666666666,
      "grad_norm": 0.08212757110595703,
      "learning_rate": 2.6146666666666668e-05,
      "loss": 0.0026,
      "step": 35780
    },
    {
      "epoch": 1.9088,
      "grad_norm": 0.2369837760925293,
      "learning_rate": 2.6140000000000004e-05,
      "loss": 0.0021,
      "step": 35790
    },
    {
      "epoch": 1.9093333333333333,
      "grad_norm": 0.13093732297420502,
      "learning_rate": 2.6133333333333333e-05,
      "loss": 0.0042,
      "step": 35800
    },
    {
      "epoch": 1.9098666666666668,
      "grad_norm": 0.13156068325042725,
      "learning_rate": 2.6126666666666665e-05,
      "loss": 0.0029,
      "step": 35810
    },
    {
      "epoch": 1.9104,
      "grad_norm": 0.4775419533252716,
      "learning_rate": 2.612e-05,
      "loss": 0.0029,
      "step": 35820
    },
    {
      "epoch": 1.9109333333333334,
      "grad_norm": 0.10689989477396011,
      "learning_rate": 2.6113333333333333e-05,
      "loss": 0.0036,
      "step": 35830
    },
    {
      "epoch": 1.9114666666666666,
      "grad_norm": 0.22443169355392456,
      "learning_rate": 2.610666666666667e-05,
      "loss": 0.0036,
      "step": 35840
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.8603376150131226,
      "learning_rate": 2.61e-05,
      "loss": 0.0025,
      "step": 35850
    },
    {
      "epoch": 1.9125333333333332,
      "grad_norm": 0.6081827878952026,
      "learning_rate": 2.6093333333333336e-05,
      "loss": 0.0028,
      "step": 35860
    },
    {
      "epoch": 1.9130666666666667,
      "grad_norm": 0.3412422835826874,
      "learning_rate": 2.608666666666667e-05,
      "loss": 0.0018,
      "step": 35870
    },
    {
      "epoch": 1.9136,
      "grad_norm": 0.25470462441444397,
      "learning_rate": 2.6079999999999998e-05,
      "loss": 0.0033,
      "step": 35880
    },
    {
      "epoch": 1.9141333333333335,
      "grad_norm": 0.33147066831588745,
      "learning_rate": 2.6073333333333333e-05,
      "loss": 0.0028,
      "step": 35890
    },
    {
      "epoch": 1.9146666666666667,
      "grad_norm": 0.392729252576828,
      "learning_rate": 2.6066666666666666e-05,
      "loss": 0.0031,
      "step": 35900
    },
    {
      "epoch": 1.9152,
      "grad_norm": 0.5787444114685059,
      "learning_rate": 2.606e-05,
      "loss": 0.0022,
      "step": 35910
    },
    {
      "epoch": 1.9157333333333333,
      "grad_norm": 0.22737190127372742,
      "learning_rate": 2.6053333333333333e-05,
      "loss": 0.0019,
      "step": 35920
    },
    {
      "epoch": 1.9162666666666666,
      "grad_norm": 0.39399462938308716,
      "learning_rate": 2.604666666666667e-05,
      "loss": 0.0037,
      "step": 35930
    },
    {
      "epoch": 1.9167999999999998,
      "grad_norm": 0.3273220956325531,
      "learning_rate": 2.6040000000000005e-05,
      "loss": 0.0025,
      "step": 35940
    },
    {
      "epoch": 1.9173333333333333,
      "grad_norm": 0.2112330049276352,
      "learning_rate": 2.6033333333333337e-05,
      "loss": 0.0028,
      "step": 35950
    },
    {
      "epoch": 1.9178666666666668,
      "grad_norm": 0.1939823478460312,
      "learning_rate": 2.6026666666666666e-05,
      "loss": 0.0021,
      "step": 35960
    },
    {
      "epoch": 1.9184,
      "grad_norm": 0.34899038076400757,
      "learning_rate": 2.602e-05,
      "loss": 0.0031,
      "step": 35970
    },
    {
      "epoch": 1.9189333333333334,
      "grad_norm": 0.3789737820625305,
      "learning_rate": 2.6013333333333334e-05,
      "loss": 0.002,
      "step": 35980
    },
    {
      "epoch": 1.9194666666666667,
      "grad_norm": 0.16216933727264404,
      "learning_rate": 2.600666666666667e-05,
      "loss": 0.0022,
      "step": 35990
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.4875975251197815,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.0031,
      "step": 36000
    },
    {
      "epoch": 1.9205333333333332,
      "grad_norm": 0.15336106717586517,
      "learning_rate": 2.5993333333333337e-05,
      "loss": 0.0023,
      "step": 36010
    },
    {
      "epoch": 1.9210666666666667,
      "grad_norm": 0.779720664024353,
      "learning_rate": 2.598666666666667e-05,
      "loss": 0.0027,
      "step": 36020
    },
    {
      "epoch": 1.9216,
      "grad_norm": 0.3569154739379883,
      "learning_rate": 2.598e-05,
      "loss": 0.0037,
      "step": 36030
    },
    {
      "epoch": 1.9221333333333335,
      "grad_norm": 0.4893627166748047,
      "learning_rate": 2.5973333333333334e-05,
      "loss": 0.0031,
      "step": 36040
    },
    {
      "epoch": 1.9226666666666667,
      "grad_norm": 0.5073156356811523,
      "learning_rate": 2.5966666666666667e-05,
      "loss": 0.0029,
      "step": 36050
    },
    {
      "epoch": 1.9232,
      "grad_norm": 0.20605328679084778,
      "learning_rate": 2.5960000000000002e-05,
      "loss": 0.0027,
      "step": 36060
    },
    {
      "epoch": 1.9237333333333333,
      "grad_norm": 0.21355657279491425,
      "learning_rate": 2.5953333333333334e-05,
      "loss": 0.0031,
      "step": 36070
    },
    {
      "epoch": 1.9242666666666666,
      "grad_norm": 0.595745861530304,
      "learning_rate": 2.594666666666667e-05,
      "loss": 0.0031,
      "step": 36080
    },
    {
      "epoch": 1.9247999999999998,
      "grad_norm": 0.3205644488334656,
      "learning_rate": 2.5940000000000002e-05,
      "loss": 0.0029,
      "step": 36090
    },
    {
      "epoch": 1.9253333333333333,
      "grad_norm": 0.17009185254573822,
      "learning_rate": 2.5933333333333338e-05,
      "loss": 0.0028,
      "step": 36100
    },
    {
      "epoch": 1.9258666666666666,
      "grad_norm": 0.2670191824436188,
      "learning_rate": 2.5926666666666667e-05,
      "loss": 0.0024,
      "step": 36110
    },
    {
      "epoch": 1.9264000000000001,
      "grad_norm": 0.37657392024993896,
      "learning_rate": 2.592e-05,
      "loss": 0.0025,
      "step": 36120
    },
    {
      "epoch": 1.9269333333333334,
      "grad_norm": 0.3845888376235962,
      "learning_rate": 2.5913333333333335e-05,
      "loss": 0.0032,
      "step": 36130
    },
    {
      "epoch": 1.9274666666666667,
      "grad_norm": 0.44428834319114685,
      "learning_rate": 2.5906666666666667e-05,
      "loss": 0.0026,
      "step": 36140
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.18053124845027924,
      "learning_rate": 2.5900000000000003e-05,
      "loss": 0.0036,
      "step": 36150
    },
    {
      "epoch": 1.9285333333333332,
      "grad_norm": 0.19699297845363617,
      "learning_rate": 2.5893333333333335e-05,
      "loss": 0.0026,
      "step": 36160
    },
    {
      "epoch": 1.9290666666666667,
      "grad_norm": 0.15353558957576752,
      "learning_rate": 2.588666666666667e-05,
      "loss": 0.0027,
      "step": 36170
    },
    {
      "epoch": 1.9296,
      "grad_norm": 0.32167866826057434,
      "learning_rate": 2.588e-05,
      "loss": 0.0019,
      "step": 36180
    },
    {
      "epoch": 1.9301333333333335,
      "grad_norm": 0.15963362157344818,
      "learning_rate": 2.5873333333333332e-05,
      "loss": 0.0031,
      "step": 36190
    },
    {
      "epoch": 1.9306666666666668,
      "grad_norm": 0.4226320683956146,
      "learning_rate": 2.5866666666666667e-05,
      "loss": 0.0031,
      "step": 36200
    },
    {
      "epoch": 1.9312,
      "grad_norm": 0.07731004804372787,
      "learning_rate": 2.586e-05,
      "loss": 0.0036,
      "step": 36210
    },
    {
      "epoch": 1.9317333333333333,
      "grad_norm": 0.33582803606987,
      "learning_rate": 2.5853333333333335e-05,
      "loss": 0.0021,
      "step": 36220
    },
    {
      "epoch": 1.9322666666666666,
      "grad_norm": 0.2834585905075073,
      "learning_rate": 2.5846666666666668e-05,
      "loss": 0.0031,
      "step": 36230
    },
    {
      "epoch": 1.9327999999999999,
      "grad_norm": 0.11011569947004318,
      "learning_rate": 2.5840000000000003e-05,
      "loss": 0.0037,
      "step": 36240
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.08732527494430542,
      "learning_rate": 2.5833333333333336e-05,
      "loss": 0.003,
      "step": 36250
    },
    {
      "epoch": 1.9338666666666666,
      "grad_norm": 0.12904903292655945,
      "learning_rate": 2.5826666666666664e-05,
      "loss": 0.0021,
      "step": 36260
    },
    {
      "epoch": 1.9344000000000001,
      "grad_norm": 0.2578901946544647,
      "learning_rate": 2.582e-05,
      "loss": 0.0027,
      "step": 36270
    },
    {
      "epoch": 1.9349333333333334,
      "grad_norm": 0.5218847990036011,
      "learning_rate": 2.5813333333333332e-05,
      "loss": 0.0033,
      "step": 36280
    },
    {
      "epoch": 1.9354666666666667,
      "grad_norm": 0.1798287183046341,
      "learning_rate": 2.5806666666666668e-05,
      "loss": 0.0031,
      "step": 36290
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.2293207347393036,
      "learning_rate": 2.58e-05,
      "loss": 0.0025,
      "step": 36300
    },
    {
      "epoch": 1.9365333333333332,
      "grad_norm": 0.5675688982009888,
      "learning_rate": 2.5793333333333336e-05,
      "loss": 0.0022,
      "step": 36310
    },
    {
      "epoch": 1.9370666666666667,
      "grad_norm": 0.09991945326328278,
      "learning_rate": 2.578666666666667e-05,
      "loss": 0.0021,
      "step": 36320
    },
    {
      "epoch": 1.9376,
      "grad_norm": 0.24824318289756775,
      "learning_rate": 2.5779999999999997e-05,
      "loss": 0.0018,
      "step": 36330
    },
    {
      "epoch": 1.9381333333333335,
      "grad_norm": 0.1537666916847229,
      "learning_rate": 2.5773333333333333e-05,
      "loss": 0.0022,
      "step": 36340
    },
    {
      "epoch": 1.9386666666666668,
      "grad_norm": 0.09269625693559647,
      "learning_rate": 2.5766666666666665e-05,
      "loss": 0.0024,
      "step": 36350
    },
    {
      "epoch": 1.9392,
      "grad_norm": 0.30906185507774353,
      "learning_rate": 2.576e-05,
      "loss": 0.0023,
      "step": 36360
    },
    {
      "epoch": 1.9397333333333333,
      "grad_norm": 0.6202979683876038,
      "learning_rate": 2.5753333333333336e-05,
      "loss": 0.0021,
      "step": 36370
    },
    {
      "epoch": 1.9402666666666666,
      "grad_norm": 0.10316102206707001,
      "learning_rate": 2.574666666666667e-05,
      "loss": 0.0026,
      "step": 36380
    },
    {
      "epoch": 1.9407999999999999,
      "grad_norm": 0.918312132358551,
      "learning_rate": 2.5740000000000004e-05,
      "loss": 0.0026,
      "step": 36390
    },
    {
      "epoch": 1.9413333333333334,
      "grad_norm": 0.4323878288269043,
      "learning_rate": 2.5733333333333337e-05,
      "loss": 0.003,
      "step": 36400
    },
    {
      "epoch": 1.9418666666666666,
      "grad_norm": 0.35061147809028625,
      "learning_rate": 2.5726666666666665e-05,
      "loss": 0.0022,
      "step": 36410
    },
    {
      "epoch": 1.9424000000000001,
      "grad_norm": 0.6241084933280945,
      "learning_rate": 2.572e-05,
      "loss": 0.0024,
      "step": 36420
    },
    {
      "epoch": 1.9429333333333334,
      "grad_norm": 0.14733316004276276,
      "learning_rate": 2.5713333333333333e-05,
      "loss": 0.0018,
      "step": 36430
    },
    {
      "epoch": 1.9434666666666667,
      "grad_norm": 0.5391585230827332,
      "learning_rate": 2.570666666666667e-05,
      "loss": 0.0027,
      "step": 36440
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.1770963817834854,
      "learning_rate": 2.57e-05,
      "loss": 0.0045,
      "step": 36450
    },
    {
      "epoch": 1.9445333333333332,
      "grad_norm": 0.23931865394115448,
      "learning_rate": 2.5693333333333337e-05,
      "loss": 0.0019,
      "step": 36460
    },
    {
      "epoch": 1.9450666666666667,
      "grad_norm": 0.5299458503723145,
      "learning_rate": 2.568666666666667e-05,
      "loss": 0.0018,
      "step": 36470
    },
    {
      "epoch": 1.9456,
      "grad_norm": 0.3921869397163391,
      "learning_rate": 2.5679999999999998e-05,
      "loss": 0.0023,
      "step": 36480
    },
    {
      "epoch": 1.9461333333333335,
      "grad_norm": 0.17132510244846344,
      "learning_rate": 2.5673333333333334e-05,
      "loss": 0.0028,
      "step": 36490
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.17762170732021332,
      "learning_rate": 2.5666666666666666e-05,
      "loss": 0.0027,
      "step": 36500
    },
    {
      "epoch": 1.9472,
      "grad_norm": 0.14285627007484436,
      "learning_rate": 2.566e-05,
      "loss": 0.003,
      "step": 36510
    },
    {
      "epoch": 1.9477333333333333,
      "grad_norm": 0.5921395421028137,
      "learning_rate": 2.5653333333333334e-05,
      "loss": 0.0022,
      "step": 36520
    },
    {
      "epoch": 1.9482666666666666,
      "grad_norm": 0.4231814742088318,
      "learning_rate": 2.564666666666667e-05,
      "loss": 0.0018,
      "step": 36530
    },
    {
      "epoch": 1.9487999999999999,
      "grad_norm": 0.3460598886013031,
      "learning_rate": 2.5640000000000002e-05,
      "loss": 0.0035,
      "step": 36540
    },
    {
      "epoch": 1.9493333333333334,
      "grad_norm": 0.5105390548706055,
      "learning_rate": 2.5633333333333338e-05,
      "loss": 0.0018,
      "step": 36550
    },
    {
      "epoch": 1.9498666666666666,
      "grad_norm": 0.769108772277832,
      "learning_rate": 2.5626666666666666e-05,
      "loss": 0.0022,
      "step": 36560
    },
    {
      "epoch": 1.9504000000000001,
      "grad_norm": 0.07828103005886078,
      "learning_rate": 2.562e-05,
      "loss": 0.002,
      "step": 36570
    },
    {
      "epoch": 1.9509333333333334,
      "grad_norm": 0.6690573692321777,
      "learning_rate": 2.5613333333333334e-05,
      "loss": 0.0024,
      "step": 36580
    },
    {
      "epoch": 1.9514666666666667,
      "grad_norm": 0.2570880949497223,
      "learning_rate": 2.5606666666666667e-05,
      "loss": 0.0036,
      "step": 36590
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.1716717630624771,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 0.0029,
      "step": 36600
    },
    {
      "epoch": 1.9525333333333332,
      "grad_norm": 0.11728591471910477,
      "learning_rate": 2.5593333333333334e-05,
      "loss": 0.0029,
      "step": 36610
    },
    {
      "epoch": 1.9530666666666665,
      "grad_norm": 0.25291532278060913,
      "learning_rate": 2.558666666666667e-05,
      "loss": 0.0028,
      "step": 36620
    },
    {
      "epoch": 1.9536,
      "grad_norm": 0.2529899775981903,
      "learning_rate": 2.5580000000000002e-05,
      "loss": 0.0019,
      "step": 36630
    },
    {
      "epoch": 1.9541333333333335,
      "grad_norm": 0.1802235245704651,
      "learning_rate": 2.557333333333333e-05,
      "loss": 0.0034,
      "step": 36640
    },
    {
      "epoch": 1.9546666666666668,
      "grad_norm": 0.08234725892543793,
      "learning_rate": 2.5566666666666667e-05,
      "loss": 0.0026,
      "step": 36650
    },
    {
      "epoch": 1.9552,
      "grad_norm": 0.3076990842819214,
      "learning_rate": 2.556e-05,
      "loss": 0.0018,
      "step": 36660
    },
    {
      "epoch": 1.9557333333333333,
      "grad_norm": 0.27576151490211487,
      "learning_rate": 2.5553333333333335e-05,
      "loss": 0.0022,
      "step": 36670
    },
    {
      "epoch": 1.9562666666666666,
      "grad_norm": 0.1119261234998703,
      "learning_rate": 2.5546666666666667e-05,
      "loss": 0.0024,
      "step": 36680
    },
    {
      "epoch": 1.9567999999999999,
      "grad_norm": 0.10425280779600143,
      "learning_rate": 2.5540000000000003e-05,
      "loss": 0.0026,
      "step": 36690
    },
    {
      "epoch": 1.9573333333333334,
      "grad_norm": 0.14076629281044006,
      "learning_rate": 2.553333333333334e-05,
      "loss": 0.0021,
      "step": 36700
    },
    {
      "epoch": 1.9578666666666666,
      "grad_norm": 0.09800110757350922,
      "learning_rate": 2.5526666666666664e-05,
      "loss": 0.0026,
      "step": 36710
    },
    {
      "epoch": 1.9584000000000001,
      "grad_norm": 0.10361190140247345,
      "learning_rate": 2.552e-05,
      "loss": 0.0022,
      "step": 36720
    },
    {
      "epoch": 1.9589333333333334,
      "grad_norm": 0.250543475151062,
      "learning_rate": 2.5513333333333332e-05,
      "loss": 0.0038,
      "step": 36730
    },
    {
      "epoch": 1.9594666666666667,
      "grad_norm": 0.2931140959262848,
      "learning_rate": 2.5506666666666668e-05,
      "loss": 0.002,
      "step": 36740
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.5284260511398315,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 0.0045,
      "step": 36750
    },
    {
      "epoch": 1.9605333333333332,
      "grad_norm": 0.31392353773117065,
      "learning_rate": 2.5493333333333335e-05,
      "loss": 0.0049,
      "step": 36760
    },
    {
      "epoch": 1.9610666666666665,
      "grad_norm": 0.2153313010931015,
      "learning_rate": 2.548666666666667e-05,
      "loss": 0.0025,
      "step": 36770
    },
    {
      "epoch": 1.9616,
      "grad_norm": 0.15237677097320557,
      "learning_rate": 2.5480000000000003e-05,
      "loss": 0.0018,
      "step": 36780
    },
    {
      "epoch": 1.9621333333333333,
      "grad_norm": 0.15597744286060333,
      "learning_rate": 2.5473333333333332e-05,
      "loss": 0.0028,
      "step": 36790
    },
    {
      "epoch": 1.9626666666666668,
      "grad_norm": 0.08554042875766754,
      "learning_rate": 2.5466666666666668e-05,
      "loss": 0.0018,
      "step": 36800
    },
    {
      "epoch": 1.9632,
      "grad_norm": 0.09178972989320755,
      "learning_rate": 2.546e-05,
      "loss": 0.0032,
      "step": 36810
    },
    {
      "epoch": 1.9637333333333333,
      "grad_norm": 0.7522554397583008,
      "learning_rate": 2.5453333333333336e-05,
      "loss": 0.0022,
      "step": 36820
    },
    {
      "epoch": 1.9642666666666666,
      "grad_norm": 0.17690914869308472,
      "learning_rate": 2.5446666666666668e-05,
      "loss": 0.0027,
      "step": 36830
    },
    {
      "epoch": 1.9647999999999999,
      "grad_norm": 0.3078884780406952,
      "learning_rate": 2.5440000000000004e-05,
      "loss": 0.0025,
      "step": 36840
    },
    {
      "epoch": 1.9653333333333334,
      "grad_norm": 0.22020073235034943,
      "learning_rate": 2.5433333333333336e-05,
      "loss": 0.0021,
      "step": 36850
    },
    {
      "epoch": 1.9658666666666667,
      "grad_norm": 0.16211317479610443,
      "learning_rate": 2.5426666666666665e-05,
      "loss": 0.0023,
      "step": 36860
    },
    {
      "epoch": 1.9664000000000001,
      "grad_norm": 0.15813776850700378,
      "learning_rate": 2.542e-05,
      "loss": 0.0019,
      "step": 36870
    },
    {
      "epoch": 1.9669333333333334,
      "grad_norm": 0.36068665981292725,
      "learning_rate": 2.5413333333333333e-05,
      "loss": 0.0018,
      "step": 36880
    },
    {
      "epoch": 1.9674666666666667,
      "grad_norm": 0.7079975605010986,
      "learning_rate": 2.540666666666667e-05,
      "loss": 0.0019,
      "step": 36890
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.9570286273956299,
      "learning_rate": 2.54e-05,
      "loss": 0.0024,
      "step": 36900
    },
    {
      "epoch": 1.9685333333333332,
      "grad_norm": 0.4633627235889435,
      "learning_rate": 2.5393333333333336e-05,
      "loss": 0.0019,
      "step": 36910
    },
    {
      "epoch": 1.9690666666666665,
      "grad_norm": 0.22175034880638123,
      "learning_rate": 2.538666666666667e-05,
      "loss": 0.0026,
      "step": 36920
    },
    {
      "epoch": 1.9696,
      "grad_norm": 0.35955747961997986,
      "learning_rate": 2.5380000000000004e-05,
      "loss": 0.002,
      "step": 36930
    },
    {
      "epoch": 1.9701333333333333,
      "grad_norm": 0.3479015529155731,
      "learning_rate": 2.5373333333333333e-05,
      "loss": 0.0029,
      "step": 36940
    },
    {
      "epoch": 1.9706666666666668,
      "grad_norm": 0.14795184135437012,
      "learning_rate": 2.5366666666666665e-05,
      "loss": 0.0019,
      "step": 36950
    },
    {
      "epoch": 1.9712,
      "grad_norm": 0.34072911739349365,
      "learning_rate": 2.536e-05,
      "loss": 0.0018,
      "step": 36960
    },
    {
      "epoch": 1.9717333333333333,
      "grad_norm": 0.18289119005203247,
      "learning_rate": 2.5353333333333333e-05,
      "loss": 0.0028,
      "step": 36970
    },
    {
      "epoch": 1.9722666666666666,
      "grad_norm": 0.14696495234966278,
      "learning_rate": 2.534666666666667e-05,
      "loss": 0.003,
      "step": 36980
    },
    {
      "epoch": 1.9727999999999999,
      "grad_norm": 0.6775984168052673,
      "learning_rate": 2.534e-05,
      "loss": 0.003,
      "step": 36990
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.38249754905700684,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 0.0038,
      "step": 37000
    },
    {
      "epoch": 1.9738666666666667,
      "grad_norm": 0.24155807495117188,
      "learning_rate": 2.5326666666666666e-05,
      "loss": 0.0019,
      "step": 37010
    },
    {
      "epoch": 1.9744000000000002,
      "grad_norm": 0.26561057567596436,
      "learning_rate": 2.5319999999999998e-05,
      "loss": 0.0024,
      "step": 37020
    },
    {
      "epoch": 1.9749333333333334,
      "grad_norm": 1.0283997058868408,
      "learning_rate": 2.5313333333333334e-05,
      "loss": 0.003,
      "step": 37030
    },
    {
      "epoch": 1.9754666666666667,
      "grad_norm": 0.12839564681053162,
      "learning_rate": 2.5306666666666666e-05,
      "loss": 0.0027,
      "step": 37040
    },
    {
      "epoch": 1.976,
      "grad_norm": 0.2125430554151535,
      "learning_rate": 2.5300000000000002e-05,
      "loss": 0.002,
      "step": 37050
    },
    {
      "epoch": 1.9765333333333333,
      "grad_norm": 0.32270705699920654,
      "learning_rate": 2.5293333333333334e-05,
      "loss": 0.0021,
      "step": 37060
    },
    {
      "epoch": 1.9770666666666665,
      "grad_norm": 0.11506491899490356,
      "learning_rate": 2.528666666666667e-05,
      "loss": 0.0036,
      "step": 37070
    },
    {
      "epoch": 1.9776,
      "grad_norm": 0.27197128534317017,
      "learning_rate": 2.5280000000000005e-05,
      "loss": 0.0025,
      "step": 37080
    },
    {
      "epoch": 1.9781333333333333,
      "grad_norm": 0.5095589756965637,
      "learning_rate": 2.527333333333333e-05,
      "loss": 0.0029,
      "step": 37090
    },
    {
      "epoch": 1.9786666666666668,
      "grad_norm": 0.1457192301750183,
      "learning_rate": 2.5266666666666666e-05,
      "loss": 0.0026,
      "step": 37100
    },
    {
      "epoch": 1.9792,
      "grad_norm": 0.06184423342347145,
      "learning_rate": 2.526e-05,
      "loss": 0.0019,
      "step": 37110
    },
    {
      "epoch": 1.9797333333333333,
      "grad_norm": 0.48813891410827637,
      "learning_rate": 2.5253333333333334e-05,
      "loss": 0.0028,
      "step": 37120
    },
    {
      "epoch": 1.9802666666666666,
      "grad_norm": 0.1092800498008728,
      "learning_rate": 2.524666666666667e-05,
      "loss": 0.003,
      "step": 37130
    },
    {
      "epoch": 1.9808,
      "grad_norm": 0.0843355655670166,
      "learning_rate": 2.5240000000000002e-05,
      "loss": 0.0038,
      "step": 37140
    },
    {
      "epoch": 1.9813333333333332,
      "grad_norm": 0.3319934904575348,
      "learning_rate": 2.5233333333333338e-05,
      "loss": 0.0023,
      "step": 37150
    },
    {
      "epoch": 1.9818666666666667,
      "grad_norm": 0.22438129782676697,
      "learning_rate": 2.5226666666666663e-05,
      "loss": 0.0021,
      "step": 37160
    },
    {
      "epoch": 1.9824000000000002,
      "grad_norm": 0.38756242394447327,
      "learning_rate": 2.522e-05,
      "loss": 0.0023,
      "step": 37170
    },
    {
      "epoch": 1.9829333333333334,
      "grad_norm": 0.0666353777050972,
      "learning_rate": 2.5213333333333335e-05,
      "loss": 0.0016,
      "step": 37180
    },
    {
      "epoch": 1.9834666666666667,
      "grad_norm": 0.25374236702919006,
      "learning_rate": 2.5206666666666667e-05,
      "loss": 0.0024,
      "step": 37190
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.07223383337259293,
      "learning_rate": 2.5200000000000003e-05,
      "loss": 0.0028,
      "step": 37200
    },
    {
      "epoch": 1.9845333333333333,
      "grad_norm": 0.23057927191257477,
      "learning_rate": 2.5193333333333335e-05,
      "loss": 0.0027,
      "step": 37210
    },
    {
      "epoch": 1.9850666666666665,
      "grad_norm": 0.32879048585891724,
      "learning_rate": 2.518666666666667e-05,
      "loss": 0.0035,
      "step": 37220
    },
    {
      "epoch": 1.9856,
      "grad_norm": 0.2813030183315277,
      "learning_rate": 2.5180000000000003e-05,
      "loss": 0.0043,
      "step": 37230
    },
    {
      "epoch": 1.9861333333333333,
      "grad_norm": 0.2185654640197754,
      "learning_rate": 2.5173333333333332e-05,
      "loss": 0.0023,
      "step": 37240
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 0.11157353222370148,
      "learning_rate": 2.5166666666666667e-05,
      "loss": 0.0021,
      "step": 37250
    },
    {
      "epoch": 1.9872,
      "grad_norm": 0.3178280293941498,
      "learning_rate": 2.516e-05,
      "loss": 0.0021,
      "step": 37260
    },
    {
      "epoch": 1.9877333333333334,
      "grad_norm": 0.15867507457733154,
      "learning_rate": 2.5153333333333335e-05,
      "loss": 0.002,
      "step": 37270
    },
    {
      "epoch": 1.9882666666666666,
      "grad_norm": 0.13368909060955048,
      "learning_rate": 2.5146666666666668e-05,
      "loss": 0.002,
      "step": 37280
    },
    {
      "epoch": 1.9888,
      "grad_norm": 0.4307767450809479,
      "learning_rate": 2.5140000000000003e-05,
      "loss": 0.003,
      "step": 37290
    },
    {
      "epoch": 1.9893333333333332,
      "grad_norm": 0.3373062312602997,
      "learning_rate": 2.5133333333333336e-05,
      "loss": 0.0021,
      "step": 37300
    },
    {
      "epoch": 1.9898666666666667,
      "grad_norm": 0.5328824520111084,
      "learning_rate": 2.512666666666667e-05,
      "loss": 0.0025,
      "step": 37310
    },
    {
      "epoch": 1.9904,
      "grad_norm": 0.39980417490005493,
      "learning_rate": 2.512e-05,
      "loss": 0.0023,
      "step": 37320
    },
    {
      "epoch": 1.9909333333333334,
      "grad_norm": 0.24579505622386932,
      "learning_rate": 2.5113333333333332e-05,
      "loss": 0.0029,
      "step": 37330
    },
    {
      "epoch": 1.9914666666666667,
      "grad_norm": 0.4470576047897339,
      "learning_rate": 2.5106666666666668e-05,
      "loss": 0.0019,
      "step": 37340
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.10700380802154541,
      "learning_rate": 2.51e-05,
      "loss": 0.0022,
      "step": 37350
    },
    {
      "epoch": 1.9925333333333333,
      "grad_norm": 0.3206985890865326,
      "learning_rate": 2.5093333333333336e-05,
      "loss": 0.0019,
      "step": 37360
    },
    {
      "epoch": 1.9930666666666665,
      "grad_norm": 0.37219759821891785,
      "learning_rate": 2.5086666666666668e-05,
      "loss": 0.002,
      "step": 37370
    },
    {
      "epoch": 1.9936,
      "grad_norm": 0.09632551670074463,
      "learning_rate": 2.5080000000000004e-05,
      "loss": 0.0029,
      "step": 37380
    },
    {
      "epoch": 1.9941333333333333,
      "grad_norm": 0.101593017578125,
      "learning_rate": 2.5073333333333333e-05,
      "loss": 0.0024,
      "step": 37390
    },
    {
      "epoch": 1.9946666666666668,
      "grad_norm": 0.4819096326828003,
      "learning_rate": 2.5066666666666665e-05,
      "loss": 0.0021,
      "step": 37400
    },
    {
      "epoch": 1.9952,
      "grad_norm": 0.1977730095386505,
      "learning_rate": 2.506e-05,
      "loss": 0.0037,
      "step": 37410
    },
    {
      "epoch": 1.9957333333333334,
      "grad_norm": 0.11678615212440491,
      "learning_rate": 2.5053333333333333e-05,
      "loss": 0.0021,
      "step": 37420
    },
    {
      "epoch": 1.9962666666666666,
      "grad_norm": 0.23832647502422333,
      "learning_rate": 2.504666666666667e-05,
      "loss": 0.0021,
      "step": 37430
    },
    {
      "epoch": 1.9968,
      "grad_norm": 0.6099564433097839,
      "learning_rate": 2.504e-05,
      "loss": 0.003,
      "step": 37440
    },
    {
      "epoch": 1.9973333333333332,
      "grad_norm": 0.39615917205810547,
      "learning_rate": 2.5033333333333336e-05,
      "loss": 0.0022,
      "step": 37450
    },
    {
      "epoch": 1.9978666666666667,
      "grad_norm": 0.1350545585155487,
      "learning_rate": 2.5026666666666672e-05,
      "loss": 0.0033,
      "step": 37460
    },
    {
      "epoch": 1.9984,
      "grad_norm": 0.13194327056407928,
      "learning_rate": 2.5019999999999998e-05,
      "loss": 0.0027,
      "step": 37470
    },
    {
      "epoch": 1.9989333333333335,
      "grad_norm": 0.4071575403213501,
      "learning_rate": 2.5013333333333333e-05,
      "loss": 0.0022,
      "step": 37480
    },
    {
      "epoch": 1.9994666666666667,
      "grad_norm": 0.35797202587127686,
      "learning_rate": 2.5006666666666666e-05,
      "loss": 0.0024,
      "step": 37490
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.44137755036354065,
      "learning_rate": 2.5e-05,
      "loss": 0.0019,
      "step": 37500
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.002582649001851678,
      "eval_runtime": 162.8822,
      "eval_samples_per_second": 1534.852,
      "eval_steps_per_second": 38.371,
      "step": 37500
    },
    {
      "epoch": 2.0005333333333333,
      "grad_norm": 0.19941940903663635,
      "learning_rate": 2.4993333333333337e-05,
      "loss": 0.0023,
      "step": 37510
    },
    {
      "epoch": 2.0010666666666665,
      "grad_norm": 0.10767145454883575,
      "learning_rate": 2.4986666666666666e-05,
      "loss": 0.0024,
      "step": 37520
    },
    {
      "epoch": 2.0016,
      "grad_norm": 0.16026972234249115,
      "learning_rate": 2.498e-05,
      "loss": 0.0025,
      "step": 37530
    },
    {
      "epoch": 2.0021333333333335,
      "grad_norm": 0.4556356370449066,
      "learning_rate": 2.4973333333333334e-05,
      "loss": 0.0033,
      "step": 37540
    },
    {
      "epoch": 2.002666666666667,
      "grad_norm": 0.5685112476348877,
      "learning_rate": 2.496666666666667e-05,
      "loss": 0.0035,
      "step": 37550
    },
    {
      "epoch": 2.0032,
      "grad_norm": 0.5143622159957886,
      "learning_rate": 2.496e-05,
      "loss": 0.0037,
      "step": 37560
    },
    {
      "epoch": 2.0037333333333334,
      "grad_norm": 0.29424139857292175,
      "learning_rate": 2.4953333333333334e-05,
      "loss": 0.0032,
      "step": 37570
    },
    {
      "epoch": 2.0042666666666666,
      "grad_norm": 0.6358069181442261,
      "learning_rate": 2.494666666666667e-05,
      "loss": 0.0021,
      "step": 37580
    },
    {
      "epoch": 2.0048,
      "grad_norm": 0.39616432785987854,
      "learning_rate": 2.4940000000000002e-05,
      "loss": 0.0052,
      "step": 37590
    },
    {
      "epoch": 2.005333333333333,
      "grad_norm": 0.46313536167144775,
      "learning_rate": 2.4933333333333334e-05,
      "loss": 0.0021,
      "step": 37600
    },
    {
      "epoch": 2.0058666666666665,
      "grad_norm": 0.15472137928009033,
      "learning_rate": 2.4926666666666666e-05,
      "loss": 0.0031,
      "step": 37610
    },
    {
      "epoch": 2.0064,
      "grad_norm": 0.3052045404911041,
      "learning_rate": 2.4920000000000002e-05,
      "loss": 0.002,
      "step": 37620
    },
    {
      "epoch": 2.0069333333333335,
      "grad_norm": 0.6041815280914307,
      "learning_rate": 2.4913333333333334e-05,
      "loss": 0.0022,
      "step": 37630
    },
    {
      "epoch": 2.0074666666666667,
      "grad_norm": 0.13627313077449799,
      "learning_rate": 2.4906666666666666e-05,
      "loss": 0.0026,
      "step": 37640
    },
    {
      "epoch": 2.008,
      "grad_norm": 0.4275848865509033,
      "learning_rate": 2.4900000000000002e-05,
      "loss": 0.0026,
      "step": 37650
    },
    {
      "epoch": 2.0085333333333333,
      "grad_norm": 0.2718125283718109,
      "learning_rate": 2.4893333333333334e-05,
      "loss": 0.0022,
      "step": 37660
    },
    {
      "epoch": 2.0090666666666666,
      "grad_norm": 0.22027835249900818,
      "learning_rate": 2.488666666666667e-05,
      "loss": 0.0018,
      "step": 37670
    },
    {
      "epoch": 2.0096,
      "grad_norm": 0.12267433106899261,
      "learning_rate": 2.488e-05,
      "loss": 0.003,
      "step": 37680
    },
    {
      "epoch": 2.0101333333333335,
      "grad_norm": 0.1381511092185974,
      "learning_rate": 2.4873333333333335e-05,
      "loss": 0.0021,
      "step": 37690
    },
    {
      "epoch": 2.010666666666667,
      "grad_norm": 0.2832929790019989,
      "learning_rate": 2.486666666666667e-05,
      "loss": 0.0022,
      "step": 37700
    },
    {
      "epoch": 2.0112,
      "grad_norm": 0.10395660251379013,
      "learning_rate": 2.486e-05,
      "loss": 0.003,
      "step": 37710
    },
    {
      "epoch": 2.0117333333333334,
      "grad_norm": 0.20876671373844147,
      "learning_rate": 2.4853333333333335e-05,
      "loss": 0.0017,
      "step": 37720
    },
    {
      "epoch": 2.0122666666666666,
      "grad_norm": 0.24549801647663116,
      "learning_rate": 2.4846666666666667e-05,
      "loss": 0.0049,
      "step": 37730
    },
    {
      "epoch": 2.0128,
      "grad_norm": 0.24610385298728943,
      "learning_rate": 2.4840000000000003e-05,
      "loss": 0.0025,
      "step": 37740
    },
    {
      "epoch": 2.013333333333333,
      "grad_norm": 0.18515126407146454,
      "learning_rate": 2.4833333333333335e-05,
      "loss": 0.0022,
      "step": 37750
    },
    {
      "epoch": 2.0138666666666665,
      "grad_norm": 0.26658716797828674,
      "learning_rate": 2.4826666666666667e-05,
      "loss": 0.0022,
      "step": 37760
    },
    {
      "epoch": 2.0144,
      "grad_norm": 0.11018684506416321,
      "learning_rate": 2.4820000000000003e-05,
      "loss": 0.0025,
      "step": 37770
    },
    {
      "epoch": 2.0149333333333335,
      "grad_norm": 0.5035569667816162,
      "learning_rate": 2.4813333333333335e-05,
      "loss": 0.0019,
      "step": 37780
    },
    {
      "epoch": 2.0154666666666667,
      "grad_norm": 0.32436659932136536,
      "learning_rate": 2.4806666666666667e-05,
      "loss": 0.0023,
      "step": 37790
    },
    {
      "epoch": 2.016,
      "grad_norm": 0.13602355122566223,
      "learning_rate": 2.48e-05,
      "loss": 0.0027,
      "step": 37800
    },
    {
      "epoch": 2.0165333333333333,
      "grad_norm": 0.3457300662994385,
      "learning_rate": 2.4793333333333335e-05,
      "loss": 0.0019,
      "step": 37810
    },
    {
      "epoch": 2.0170666666666666,
      "grad_norm": 0.7920458912849426,
      "learning_rate": 2.4786666666666668e-05,
      "loss": 0.0027,
      "step": 37820
    },
    {
      "epoch": 2.0176,
      "grad_norm": 0.3788003921508789,
      "learning_rate": 2.478e-05,
      "loss": 0.0034,
      "step": 37830
    },
    {
      "epoch": 2.018133333333333,
      "grad_norm": 0.3231269121170044,
      "learning_rate": 2.4773333333333336e-05,
      "loss": 0.0019,
      "step": 37840
    },
    {
      "epoch": 2.018666666666667,
      "grad_norm": 0.29049035906791687,
      "learning_rate": 2.4766666666666668e-05,
      "loss": 0.0026,
      "step": 37850
    },
    {
      "epoch": 2.0192,
      "grad_norm": 0.14600521326065063,
      "learning_rate": 2.476e-05,
      "loss": 0.0054,
      "step": 37860
    },
    {
      "epoch": 2.0197333333333334,
      "grad_norm": 0.191494882106781,
      "learning_rate": 2.4753333333333332e-05,
      "loss": 0.0023,
      "step": 37870
    },
    {
      "epoch": 2.0202666666666667,
      "grad_norm": 0.2869347333908081,
      "learning_rate": 2.4746666666666668e-05,
      "loss": 0.0029,
      "step": 37880
    },
    {
      "epoch": 2.0208,
      "grad_norm": 0.08863228559494019,
      "learning_rate": 2.4740000000000004e-05,
      "loss": 0.0025,
      "step": 37890
    },
    {
      "epoch": 2.021333333333333,
      "grad_norm": 0.19601322710514069,
      "learning_rate": 2.4733333333333333e-05,
      "loss": 0.0021,
      "step": 37900
    },
    {
      "epoch": 2.0218666666666665,
      "grad_norm": 0.45421266555786133,
      "learning_rate": 2.4726666666666668e-05,
      "loss": 0.0038,
      "step": 37910
    },
    {
      "epoch": 2.0224,
      "grad_norm": 0.21885991096496582,
      "learning_rate": 2.472e-05,
      "loss": 0.0023,
      "step": 37920
    },
    {
      "epoch": 2.0229333333333335,
      "grad_norm": 0.2388083040714264,
      "learning_rate": 2.4713333333333336e-05,
      "loss": 0.002,
      "step": 37930
    },
    {
      "epoch": 2.0234666666666667,
      "grad_norm": 0.1628682166337967,
      "learning_rate": 2.470666666666667e-05,
      "loss": 0.0027,
      "step": 37940
    },
    {
      "epoch": 2.024,
      "grad_norm": 0.08236575871706009,
      "learning_rate": 2.47e-05,
      "loss": 0.0026,
      "step": 37950
    },
    {
      "epoch": 2.0245333333333333,
      "grad_norm": 0.17789813876152039,
      "learning_rate": 2.4693333333333336e-05,
      "loss": 0.002,
      "step": 37960
    },
    {
      "epoch": 2.0250666666666666,
      "grad_norm": 0.47646722197532654,
      "learning_rate": 2.468666666666667e-05,
      "loss": 0.0022,
      "step": 37970
    },
    {
      "epoch": 2.0256,
      "grad_norm": 0.68342524766922,
      "learning_rate": 2.468e-05,
      "loss": 0.0017,
      "step": 37980
    },
    {
      "epoch": 2.026133333333333,
      "grad_norm": 0.407402366399765,
      "learning_rate": 2.4673333333333333e-05,
      "loss": 0.0025,
      "step": 37990
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 0.3819200396537781,
      "learning_rate": 2.466666666666667e-05,
      "loss": 0.0026,
      "step": 38000
    },
    {
      "epoch": 2.0272,
      "grad_norm": 0.1147472932934761,
      "learning_rate": 2.466e-05,
      "loss": 0.0019,
      "step": 38010
    },
    {
      "epoch": 2.0277333333333334,
      "grad_norm": 0.2642018496990204,
      "learning_rate": 2.4653333333333333e-05,
      "loss": 0.0034,
      "step": 38020
    },
    {
      "epoch": 2.0282666666666667,
      "grad_norm": 0.2658350467681885,
      "learning_rate": 2.464666666666667e-05,
      "loss": 0.0034,
      "step": 38030
    },
    {
      "epoch": 2.0288,
      "grad_norm": 0.3166249394416809,
      "learning_rate": 2.464e-05,
      "loss": 0.0023,
      "step": 38040
    },
    {
      "epoch": 2.029333333333333,
      "grad_norm": 0.23455464839935303,
      "learning_rate": 2.4633333333333334e-05,
      "loss": 0.003,
      "step": 38050
    },
    {
      "epoch": 2.0298666666666665,
      "grad_norm": 0.4710618853569031,
      "learning_rate": 2.4626666666666666e-05,
      "loss": 0.002,
      "step": 38060
    },
    {
      "epoch": 2.0304,
      "grad_norm": 0.10425816476345062,
      "learning_rate": 2.462e-05,
      "loss": 0.002,
      "step": 38070
    },
    {
      "epoch": 2.0309333333333335,
      "grad_norm": 0.2605457305908203,
      "learning_rate": 2.4613333333333337e-05,
      "loss": 0.0028,
      "step": 38080
    },
    {
      "epoch": 2.0314666666666668,
      "grad_norm": 0.09683103859424591,
      "learning_rate": 2.4606666666666666e-05,
      "loss": 0.0021,
      "step": 38090
    },
    {
      "epoch": 2.032,
      "grad_norm": 0.20747414231300354,
      "learning_rate": 2.46e-05,
      "loss": 0.0022,
      "step": 38100
    },
    {
      "epoch": 2.0325333333333333,
      "grad_norm": 0.16322828829288483,
      "learning_rate": 2.4593333333333334e-05,
      "loss": 0.0029,
      "step": 38110
    },
    {
      "epoch": 2.0330666666666666,
      "grad_norm": 0.7299560308456421,
      "learning_rate": 2.458666666666667e-05,
      "loss": 0.0031,
      "step": 38120
    },
    {
      "epoch": 2.0336,
      "grad_norm": 0.41004157066345215,
      "learning_rate": 2.4580000000000002e-05,
      "loss": 0.003,
      "step": 38130
    },
    {
      "epoch": 2.034133333333333,
      "grad_norm": 0.20761500298976898,
      "learning_rate": 2.4573333333333334e-05,
      "loss": 0.0019,
      "step": 38140
    },
    {
      "epoch": 2.034666666666667,
      "grad_norm": 0.3081373870372772,
      "learning_rate": 2.456666666666667e-05,
      "loss": 0.0029,
      "step": 38150
    },
    {
      "epoch": 2.0352,
      "grad_norm": 0.2960904538631439,
      "learning_rate": 2.4560000000000002e-05,
      "loss": 0.0046,
      "step": 38160
    },
    {
      "epoch": 2.0357333333333334,
      "grad_norm": 0.224924236536026,
      "learning_rate": 2.4553333333333334e-05,
      "loss": 0.0023,
      "step": 38170
    },
    {
      "epoch": 2.0362666666666667,
      "grad_norm": 0.12197903543710709,
      "learning_rate": 2.4546666666666667e-05,
      "loss": 0.0022,
      "step": 38180
    },
    {
      "epoch": 2.0368,
      "grad_norm": 0.2090141624212265,
      "learning_rate": 2.4540000000000002e-05,
      "loss": 0.002,
      "step": 38190
    },
    {
      "epoch": 2.037333333333333,
      "grad_norm": 0.18124334514141083,
      "learning_rate": 2.4533333333333334e-05,
      "loss": 0.002,
      "step": 38200
    },
    {
      "epoch": 2.0378666666666665,
      "grad_norm": 0.22548578679561615,
      "learning_rate": 2.4526666666666667e-05,
      "loss": 0.0036,
      "step": 38210
    },
    {
      "epoch": 2.0384,
      "grad_norm": 0.12322913855314255,
      "learning_rate": 2.4520000000000002e-05,
      "loss": 0.0023,
      "step": 38220
    },
    {
      "epoch": 2.0389333333333335,
      "grad_norm": 0.2952258884906769,
      "learning_rate": 2.4513333333333335e-05,
      "loss": 0.0024,
      "step": 38230
    },
    {
      "epoch": 2.0394666666666668,
      "grad_norm": 0.2086944729089737,
      "learning_rate": 2.4506666666666667e-05,
      "loss": 0.0025,
      "step": 38240
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.24412785470485687,
      "learning_rate": 2.45e-05,
      "loss": 0.0025,
      "step": 38250
    },
    {
      "epoch": 2.0405333333333333,
      "grad_norm": 0.32276883721351624,
      "learning_rate": 2.4493333333333335e-05,
      "loss": 0.0023,
      "step": 38260
    },
    {
      "epoch": 2.0410666666666666,
      "grad_norm": 0.20028014481067657,
      "learning_rate": 2.448666666666667e-05,
      "loss": 0.0021,
      "step": 38270
    },
    {
      "epoch": 2.0416,
      "grad_norm": 0.06801515817642212,
      "learning_rate": 2.448e-05,
      "loss": 0.002,
      "step": 38280
    },
    {
      "epoch": 2.042133333333333,
      "grad_norm": 0.5775279998779297,
      "learning_rate": 2.4473333333333335e-05,
      "loss": 0.0037,
      "step": 38290
    },
    {
      "epoch": 2.042666666666667,
      "grad_norm": 0.4117133915424347,
      "learning_rate": 2.4466666666666667e-05,
      "loss": 0.0027,
      "step": 38300
    },
    {
      "epoch": 2.0432,
      "grad_norm": 0.5841996669769287,
      "learning_rate": 2.4460000000000003e-05,
      "loss": 0.003,
      "step": 38310
    },
    {
      "epoch": 2.0437333333333334,
      "grad_norm": 0.3074633479118347,
      "learning_rate": 2.4453333333333335e-05,
      "loss": 0.0019,
      "step": 38320
    },
    {
      "epoch": 2.0442666666666667,
      "grad_norm": 0.37594449520111084,
      "learning_rate": 2.4446666666666668e-05,
      "loss": 0.0023,
      "step": 38330
    },
    {
      "epoch": 2.0448,
      "grad_norm": 0.47090065479278564,
      "learning_rate": 2.4440000000000003e-05,
      "loss": 0.0023,
      "step": 38340
    },
    {
      "epoch": 2.0453333333333332,
      "grad_norm": 0.13320915400981903,
      "learning_rate": 2.4433333333333335e-05,
      "loss": 0.0025,
      "step": 38350
    },
    {
      "epoch": 2.0458666666666665,
      "grad_norm": 0.4163450598716736,
      "learning_rate": 2.4426666666666668e-05,
      "loss": 0.0032,
      "step": 38360
    },
    {
      "epoch": 2.0464,
      "grad_norm": 0.10449902713298798,
      "learning_rate": 2.442e-05,
      "loss": 0.0018,
      "step": 38370
    },
    {
      "epoch": 2.0469333333333335,
      "grad_norm": 0.42206427454948425,
      "learning_rate": 2.4413333333333336e-05,
      "loss": 0.0025,
      "step": 38380
    },
    {
      "epoch": 2.0474666666666668,
      "grad_norm": 0.21016551554203033,
      "learning_rate": 2.4406666666666668e-05,
      "loss": 0.002,
      "step": 38390
    },
    {
      "epoch": 2.048,
      "grad_norm": 0.12474808841943741,
      "learning_rate": 2.44e-05,
      "loss": 0.0021,
      "step": 38400
    },
    {
      "epoch": 2.0485333333333333,
      "grad_norm": 0.8045287728309631,
      "learning_rate": 2.4393333333333336e-05,
      "loss": 0.0023,
      "step": 38410
    },
    {
      "epoch": 2.0490666666666666,
      "grad_norm": 0.10800062865018845,
      "learning_rate": 2.4386666666666668e-05,
      "loss": 0.0029,
      "step": 38420
    },
    {
      "epoch": 2.0496,
      "grad_norm": 0.45761916041374207,
      "learning_rate": 2.438e-05,
      "loss": 0.0025,
      "step": 38430
    },
    {
      "epoch": 2.050133333333333,
      "grad_norm": 0.26228126883506775,
      "learning_rate": 2.4373333333333333e-05,
      "loss": 0.0044,
      "step": 38440
    },
    {
      "epoch": 2.050666666666667,
      "grad_norm": 0.5181933641433716,
      "learning_rate": 2.4366666666666668e-05,
      "loss": 0.0027,
      "step": 38450
    },
    {
      "epoch": 2.0512,
      "grad_norm": 0.8665773868560791,
      "learning_rate": 2.4360000000000004e-05,
      "loss": 0.0024,
      "step": 38460
    },
    {
      "epoch": 2.0517333333333334,
      "grad_norm": 0.6201019287109375,
      "learning_rate": 2.4353333333333333e-05,
      "loss": 0.003,
      "step": 38470
    },
    {
      "epoch": 2.0522666666666667,
      "grad_norm": 0.6760374903678894,
      "learning_rate": 2.434666666666667e-05,
      "loss": 0.0022,
      "step": 38480
    },
    {
      "epoch": 2.0528,
      "grad_norm": 0.5215662121772766,
      "learning_rate": 2.434e-05,
      "loss": 0.0021,
      "step": 38490
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 0.10184728354215622,
      "learning_rate": 2.4333333333333336e-05,
      "loss": 0.003,
      "step": 38500
    },
    {
      "epoch": 2.0538666666666665,
      "grad_norm": 0.37021902203559875,
      "learning_rate": 2.432666666666667e-05,
      "loss": 0.0021,
      "step": 38510
    },
    {
      "epoch": 2.0544,
      "grad_norm": 0.2414139360189438,
      "learning_rate": 2.432e-05,
      "loss": 0.0024,
      "step": 38520
    },
    {
      "epoch": 2.0549333333333335,
      "grad_norm": 0.17277394235134125,
      "learning_rate": 2.4313333333333337e-05,
      "loss": 0.0027,
      "step": 38530
    },
    {
      "epoch": 2.0554666666666668,
      "grad_norm": 0.2778882384300232,
      "learning_rate": 2.4306666666666665e-05,
      "loss": 0.0026,
      "step": 38540
    },
    {
      "epoch": 2.056,
      "grad_norm": 0.6836854815483093,
      "learning_rate": 2.43e-05,
      "loss": 0.002,
      "step": 38550
    },
    {
      "epoch": 2.0565333333333333,
      "grad_norm": 0.5238195657730103,
      "learning_rate": 2.4293333333333333e-05,
      "loss": 0.0032,
      "step": 38560
    },
    {
      "epoch": 2.0570666666666666,
      "grad_norm": 0.3531886041164398,
      "learning_rate": 2.428666666666667e-05,
      "loss": 0.0024,
      "step": 38570
    },
    {
      "epoch": 2.0576,
      "grad_norm": 0.1295899897813797,
      "learning_rate": 2.428e-05,
      "loss": 0.0028,
      "step": 38580
    },
    {
      "epoch": 2.058133333333333,
      "grad_norm": 0.31197550892829895,
      "learning_rate": 2.4273333333333334e-05,
      "loss": 0.002,
      "step": 38590
    },
    {
      "epoch": 2.058666666666667,
      "grad_norm": 0.3118383288383484,
      "learning_rate": 2.426666666666667e-05,
      "loss": 0.0025,
      "step": 38600
    },
    {
      "epoch": 2.0592,
      "grad_norm": 0.2967860698699951,
      "learning_rate": 2.426e-05,
      "loss": 0.0031,
      "step": 38610
    },
    {
      "epoch": 2.0597333333333334,
      "grad_norm": 0.6906458735466003,
      "learning_rate": 2.4253333333333334e-05,
      "loss": 0.0035,
      "step": 38620
    },
    {
      "epoch": 2.0602666666666667,
      "grad_norm": 0.19986897706985474,
      "learning_rate": 2.4246666666666666e-05,
      "loss": 0.002,
      "step": 38630
    },
    {
      "epoch": 2.0608,
      "grad_norm": 0.17538686096668243,
      "learning_rate": 2.4240000000000002e-05,
      "loss": 0.0026,
      "step": 38640
    },
    {
      "epoch": 2.0613333333333332,
      "grad_norm": 0.20863854885101318,
      "learning_rate": 2.4233333333333337e-05,
      "loss": 0.0021,
      "step": 38650
    },
    {
      "epoch": 2.0618666666666665,
      "grad_norm": 0.08272653818130493,
      "learning_rate": 2.4226666666666666e-05,
      "loss": 0.0021,
      "step": 38660
    },
    {
      "epoch": 2.0624,
      "grad_norm": 0.21040935814380646,
      "learning_rate": 2.4220000000000002e-05,
      "loss": 0.002,
      "step": 38670
    },
    {
      "epoch": 2.0629333333333335,
      "grad_norm": 0.15015333890914917,
      "learning_rate": 2.4213333333333334e-05,
      "loss": 0.0025,
      "step": 38680
    },
    {
      "epoch": 2.063466666666667,
      "grad_norm": 0.4770054221153259,
      "learning_rate": 2.420666666666667e-05,
      "loss": 0.002,
      "step": 38690
    },
    {
      "epoch": 2.064,
      "grad_norm": 0.2102777659893036,
      "learning_rate": 2.4200000000000002e-05,
      "loss": 0.0038,
      "step": 38700
    },
    {
      "epoch": 2.0645333333333333,
      "grad_norm": 0.21395492553710938,
      "learning_rate": 2.4193333333333334e-05,
      "loss": 0.0022,
      "step": 38710
    },
    {
      "epoch": 2.0650666666666666,
      "grad_norm": 0.3487868010997772,
      "learning_rate": 2.418666666666667e-05,
      "loss": 0.0039,
      "step": 38720
    },
    {
      "epoch": 2.0656,
      "grad_norm": 0.6651550531387329,
      "learning_rate": 2.418e-05,
      "loss": 0.0032,
      "step": 38730
    },
    {
      "epoch": 2.066133333333333,
      "grad_norm": 0.6724461913108826,
      "learning_rate": 2.4173333333333335e-05,
      "loss": 0.0023,
      "step": 38740
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.1271030455827713,
      "learning_rate": 2.4166666666666667e-05,
      "loss": 0.0021,
      "step": 38750
    },
    {
      "epoch": 2.0672,
      "grad_norm": 0.07731880992650986,
      "learning_rate": 2.4160000000000002e-05,
      "loss": 0.0032,
      "step": 38760
    },
    {
      "epoch": 2.0677333333333334,
      "grad_norm": 0.39253127574920654,
      "learning_rate": 2.4153333333333335e-05,
      "loss": 0.0042,
      "step": 38770
    },
    {
      "epoch": 2.0682666666666667,
      "grad_norm": 0.24751102924346924,
      "learning_rate": 2.4146666666666667e-05,
      "loss": 0.0026,
      "step": 38780
    },
    {
      "epoch": 2.0688,
      "grad_norm": 0.35722148418426514,
      "learning_rate": 2.4140000000000003e-05,
      "loss": 0.0019,
      "step": 38790
    },
    {
      "epoch": 2.0693333333333332,
      "grad_norm": 0.1537378430366516,
      "learning_rate": 2.4133333333333335e-05,
      "loss": 0.0032,
      "step": 38800
    },
    {
      "epoch": 2.0698666666666665,
      "grad_norm": 0.3788856863975525,
      "learning_rate": 2.4126666666666667e-05,
      "loss": 0.0026,
      "step": 38810
    },
    {
      "epoch": 2.0704,
      "grad_norm": 0.29511258006095886,
      "learning_rate": 2.412e-05,
      "loss": 0.0022,
      "step": 38820
    },
    {
      "epoch": 2.0709333333333335,
      "grad_norm": 0.1459328979253769,
      "learning_rate": 2.4113333333333335e-05,
      "loss": 0.0043,
      "step": 38830
    },
    {
      "epoch": 2.071466666666667,
      "grad_norm": 0.1185079887509346,
      "learning_rate": 2.4106666666666667e-05,
      "loss": 0.002,
      "step": 38840
    },
    {
      "epoch": 2.072,
      "grad_norm": 0.6371874213218689,
      "learning_rate": 2.41e-05,
      "loss": 0.0029,
      "step": 38850
    },
    {
      "epoch": 2.0725333333333333,
      "grad_norm": 0.2510072588920593,
      "learning_rate": 2.4093333333333335e-05,
      "loss": 0.003,
      "step": 38860
    },
    {
      "epoch": 2.0730666666666666,
      "grad_norm": 0.07701174914836884,
      "learning_rate": 2.4086666666666668e-05,
      "loss": 0.0017,
      "step": 38870
    },
    {
      "epoch": 2.0736,
      "grad_norm": 0.19334226846694946,
      "learning_rate": 2.408e-05,
      "loss": 0.0023,
      "step": 38880
    },
    {
      "epoch": 2.074133333333333,
      "grad_norm": 0.435925155878067,
      "learning_rate": 2.4073333333333335e-05,
      "loss": 0.0023,
      "step": 38890
    },
    {
      "epoch": 2.074666666666667,
      "grad_norm": 0.3267897963523865,
      "learning_rate": 2.4066666666666668e-05,
      "loss": 0.0019,
      "step": 38900
    },
    {
      "epoch": 2.0752,
      "grad_norm": 0.12282741069793701,
      "learning_rate": 2.4060000000000003e-05,
      "loss": 0.0033,
      "step": 38910
    },
    {
      "epoch": 2.0757333333333334,
      "grad_norm": 0.27883055806159973,
      "learning_rate": 2.4053333333333332e-05,
      "loss": 0.003,
      "step": 38920
    },
    {
      "epoch": 2.0762666666666667,
      "grad_norm": 0.0867081955075264,
      "learning_rate": 2.4046666666666668e-05,
      "loss": 0.0019,
      "step": 38930
    },
    {
      "epoch": 2.0768,
      "grad_norm": 0.16836300492286682,
      "learning_rate": 2.404e-05,
      "loss": 0.0035,
      "step": 38940
    },
    {
      "epoch": 2.0773333333333333,
      "grad_norm": 0.5222991108894348,
      "learning_rate": 2.4033333333333336e-05,
      "loss": 0.0018,
      "step": 38950
    },
    {
      "epoch": 2.0778666666666665,
      "grad_norm": 0.46667739748954773,
      "learning_rate": 2.4026666666666668e-05,
      "loss": 0.0023,
      "step": 38960
    },
    {
      "epoch": 2.0784,
      "grad_norm": 0.14820070564746857,
      "learning_rate": 2.402e-05,
      "loss": 0.0024,
      "step": 38970
    },
    {
      "epoch": 2.0789333333333335,
      "grad_norm": 0.09643836319446564,
      "learning_rate": 2.4013333333333336e-05,
      "loss": 0.0024,
      "step": 38980
    },
    {
      "epoch": 2.079466666666667,
      "grad_norm": 0.13623987138271332,
      "learning_rate": 2.400666666666667e-05,
      "loss": 0.0023,
      "step": 38990
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.1917552947998047,
      "learning_rate": 2.4e-05,
      "loss": 0.0022,
      "step": 39000
    },
    {
      "epoch": 2.0805333333333333,
      "grad_norm": 0.1367359608411789,
      "learning_rate": 2.3993333333333333e-05,
      "loss": 0.0017,
      "step": 39010
    },
    {
      "epoch": 2.0810666666666666,
      "grad_norm": 0.19118192791938782,
      "learning_rate": 2.398666666666667e-05,
      "loss": 0.0027,
      "step": 39020
    },
    {
      "epoch": 2.0816,
      "grad_norm": 0.2069791555404663,
      "learning_rate": 2.398e-05,
      "loss": 0.0025,
      "step": 39030
    },
    {
      "epoch": 2.082133333333333,
      "grad_norm": 0.2781929671764374,
      "learning_rate": 2.3973333333333333e-05,
      "loss": 0.0019,
      "step": 39040
    },
    {
      "epoch": 2.0826666666666664,
      "grad_norm": 0.2331760674715042,
      "learning_rate": 2.396666666666667e-05,
      "loss": 0.0022,
      "step": 39050
    },
    {
      "epoch": 2.0832,
      "grad_norm": 0.4217124581336975,
      "learning_rate": 2.396e-05,
      "loss": 0.0021,
      "step": 39060
    },
    {
      "epoch": 2.0837333333333334,
      "grad_norm": 0.07244721800088882,
      "learning_rate": 2.3953333333333333e-05,
      "loss": 0.0023,
      "step": 39070
    },
    {
      "epoch": 2.0842666666666667,
      "grad_norm": 0.34274548292160034,
      "learning_rate": 2.394666666666667e-05,
      "loss": 0.0021,
      "step": 39080
    },
    {
      "epoch": 2.0848,
      "grad_norm": 0.19007591903209686,
      "learning_rate": 2.394e-05,
      "loss": 0.0027,
      "step": 39090
    },
    {
      "epoch": 2.0853333333333333,
      "grad_norm": 0.3539706766605377,
      "learning_rate": 2.3933333333333337e-05,
      "loss": 0.0022,
      "step": 39100
    },
    {
      "epoch": 2.0858666666666665,
      "grad_norm": 0.25954869389533997,
      "learning_rate": 2.3926666666666666e-05,
      "loss": 0.0018,
      "step": 39110
    },
    {
      "epoch": 2.0864,
      "grad_norm": 0.09291046857833862,
      "learning_rate": 2.392e-05,
      "loss": 0.0023,
      "step": 39120
    },
    {
      "epoch": 2.0869333333333335,
      "grad_norm": 0.6166231036186218,
      "learning_rate": 2.3913333333333334e-05,
      "loss": 0.002,
      "step": 39130
    },
    {
      "epoch": 2.087466666666667,
      "grad_norm": 0.06849435716867447,
      "learning_rate": 2.390666666666667e-05,
      "loss": 0.0024,
      "step": 39140
    },
    {
      "epoch": 2.088,
      "grad_norm": 0.4063936769962311,
      "learning_rate": 2.39e-05,
      "loss": 0.0036,
      "step": 39150
    },
    {
      "epoch": 2.0885333333333334,
      "grad_norm": 0.4265972375869751,
      "learning_rate": 2.3893333333333334e-05,
      "loss": 0.0026,
      "step": 39160
    },
    {
      "epoch": 2.0890666666666666,
      "grad_norm": 0.24740828573703766,
      "learning_rate": 2.388666666666667e-05,
      "loss": 0.0035,
      "step": 39170
    },
    {
      "epoch": 2.0896,
      "grad_norm": 0.9389817118644714,
      "learning_rate": 2.3880000000000002e-05,
      "loss": 0.0029,
      "step": 39180
    },
    {
      "epoch": 2.090133333333333,
      "grad_norm": 0.11437508463859558,
      "learning_rate": 2.3873333333333334e-05,
      "loss": 0.0028,
      "step": 39190
    },
    {
      "epoch": 2.0906666666666665,
      "grad_norm": 0.25549229979515076,
      "learning_rate": 2.3866666666666666e-05,
      "loss": 0.0022,
      "step": 39200
    },
    {
      "epoch": 2.0912,
      "grad_norm": 0.13178153336048126,
      "learning_rate": 2.3860000000000002e-05,
      "loss": 0.0023,
      "step": 39210
    },
    {
      "epoch": 2.0917333333333334,
      "grad_norm": 0.11109233647584915,
      "learning_rate": 2.3853333333333334e-05,
      "loss": 0.0023,
      "step": 39220
    },
    {
      "epoch": 2.0922666666666667,
      "grad_norm": 0.07344342023134232,
      "learning_rate": 2.3846666666666666e-05,
      "loss": 0.0017,
      "step": 39230
    },
    {
      "epoch": 2.0928,
      "grad_norm": 0.1330546736717224,
      "learning_rate": 2.3840000000000002e-05,
      "loss": 0.0026,
      "step": 39240
    },
    {
      "epoch": 2.0933333333333333,
      "grad_norm": 0.09630436450242996,
      "learning_rate": 2.3833333333333334e-05,
      "loss": 0.0025,
      "step": 39250
    },
    {
      "epoch": 2.0938666666666665,
      "grad_norm": 0.14046518504619598,
      "learning_rate": 2.3826666666666667e-05,
      "loss": 0.0021,
      "step": 39260
    },
    {
      "epoch": 2.0944,
      "grad_norm": 0.5994095802307129,
      "learning_rate": 2.3820000000000002e-05,
      "loss": 0.0019,
      "step": 39270
    },
    {
      "epoch": 2.0949333333333335,
      "grad_norm": 0.21005210280418396,
      "learning_rate": 2.3813333333333335e-05,
      "loss": 0.0019,
      "step": 39280
    },
    {
      "epoch": 2.095466666666667,
      "grad_norm": 0.16855520009994507,
      "learning_rate": 2.380666666666667e-05,
      "loss": 0.0019,
      "step": 39290
    },
    {
      "epoch": 2.096,
      "grad_norm": 0.6471937298774719,
      "learning_rate": 2.38e-05,
      "loss": 0.002,
      "step": 39300
    },
    {
      "epoch": 2.0965333333333334,
      "grad_norm": 0.14534270763397217,
      "learning_rate": 2.3793333333333335e-05,
      "loss": 0.0023,
      "step": 39310
    },
    {
      "epoch": 2.0970666666666666,
      "grad_norm": 0.3434380888938904,
      "learning_rate": 2.3786666666666667e-05,
      "loss": 0.0026,
      "step": 39320
    },
    {
      "epoch": 2.0976,
      "grad_norm": 0.1644599288702011,
      "learning_rate": 2.3780000000000003e-05,
      "loss": 0.0021,
      "step": 39330
    },
    {
      "epoch": 2.098133333333333,
      "grad_norm": 0.20320983231067657,
      "learning_rate": 2.3773333333333335e-05,
      "loss": 0.0028,
      "step": 39340
    },
    {
      "epoch": 2.0986666666666665,
      "grad_norm": 0.39729568362236023,
      "learning_rate": 2.3766666666666667e-05,
      "loss": 0.0029,
      "step": 39350
    },
    {
      "epoch": 2.0992,
      "grad_norm": 0.4023357927799225,
      "learning_rate": 2.3760000000000003e-05,
      "loss": 0.0028,
      "step": 39360
    },
    {
      "epoch": 2.0997333333333335,
      "grad_norm": 0.40276074409484863,
      "learning_rate": 2.3753333333333335e-05,
      "loss": 0.0023,
      "step": 39370
    },
    {
      "epoch": 2.1002666666666667,
      "grad_norm": 0.35521459579467773,
      "learning_rate": 2.3746666666666667e-05,
      "loss": 0.002,
      "step": 39380
    },
    {
      "epoch": 2.1008,
      "grad_norm": 0.17367590963840485,
      "learning_rate": 2.374e-05,
      "loss": 0.0025,
      "step": 39390
    },
    {
      "epoch": 2.1013333333333333,
      "grad_norm": 0.34435558319091797,
      "learning_rate": 2.3733333333333335e-05,
      "loss": 0.0021,
      "step": 39400
    },
    {
      "epoch": 2.1018666666666665,
      "grad_norm": 0.23889228701591492,
      "learning_rate": 2.3726666666666668e-05,
      "loss": 0.0023,
      "step": 39410
    },
    {
      "epoch": 2.1024,
      "grad_norm": 0.18807704746723175,
      "learning_rate": 2.372e-05,
      "loss": 0.0052,
      "step": 39420
    },
    {
      "epoch": 2.1029333333333335,
      "grad_norm": 0.18950513005256653,
      "learning_rate": 2.3713333333333336e-05,
      "loss": 0.0021,
      "step": 39430
    },
    {
      "epoch": 2.103466666666667,
      "grad_norm": 0.3492392599582672,
      "learning_rate": 2.3706666666666668e-05,
      "loss": 0.0025,
      "step": 39440
    },
    {
      "epoch": 2.104,
      "grad_norm": 0.44354256987571716,
      "learning_rate": 2.37e-05,
      "loss": 0.0024,
      "step": 39450
    },
    {
      "epoch": 2.1045333333333334,
      "grad_norm": 0.1981676071882248,
      "learning_rate": 2.3693333333333332e-05,
      "loss": 0.0028,
      "step": 39460
    },
    {
      "epoch": 2.1050666666666666,
      "grad_norm": 0.5253176093101501,
      "learning_rate": 2.3686666666666668e-05,
      "loss": 0.0022,
      "step": 39470
    },
    {
      "epoch": 2.1056,
      "grad_norm": 0.19422486424446106,
      "learning_rate": 2.3680000000000004e-05,
      "loss": 0.0019,
      "step": 39480
    },
    {
      "epoch": 2.106133333333333,
      "grad_norm": 0.26555174589157104,
      "learning_rate": 2.3673333333333333e-05,
      "loss": 0.0029,
      "step": 39490
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 0.3009662628173828,
      "learning_rate": 2.3666666666666668e-05,
      "loss": 0.0022,
      "step": 39500
    },
    {
      "epoch": 2.1072,
      "grad_norm": 0.4143320918083191,
      "learning_rate": 2.366e-05,
      "loss": 0.0019,
      "step": 39510
    },
    {
      "epoch": 2.1077333333333335,
      "grad_norm": 0.27913808822631836,
      "learning_rate": 2.3653333333333336e-05,
      "loss": 0.0024,
      "step": 39520
    },
    {
      "epoch": 2.1082666666666667,
      "grad_norm": 0.22721560299396515,
      "learning_rate": 2.364666666666667e-05,
      "loss": 0.003,
      "step": 39530
    },
    {
      "epoch": 2.1088,
      "grad_norm": 0.2433135062456131,
      "learning_rate": 2.364e-05,
      "loss": 0.002,
      "step": 39540
    },
    {
      "epoch": 2.1093333333333333,
      "grad_norm": 0.339342325925827,
      "learning_rate": 2.3633333333333336e-05,
      "loss": 0.0036,
      "step": 39550
    },
    {
      "epoch": 2.1098666666666666,
      "grad_norm": 0.0715775266289711,
      "learning_rate": 2.362666666666667e-05,
      "loss": 0.0018,
      "step": 39560
    },
    {
      "epoch": 2.1104,
      "grad_norm": 0.24175502359867096,
      "learning_rate": 2.362e-05,
      "loss": 0.0021,
      "step": 39570
    },
    {
      "epoch": 2.1109333333333336,
      "grad_norm": 0.6132416129112244,
      "learning_rate": 2.3613333333333333e-05,
      "loss": 0.0018,
      "step": 39580
    },
    {
      "epoch": 2.111466666666667,
      "grad_norm": 0.2384217381477356,
      "learning_rate": 2.360666666666667e-05,
      "loss": 0.0021,
      "step": 39590
    },
    {
      "epoch": 2.112,
      "grad_norm": 0.28132015466690063,
      "learning_rate": 2.36e-05,
      "loss": 0.0017,
      "step": 39600
    },
    {
      "epoch": 2.1125333333333334,
      "grad_norm": 0.30175817012786865,
      "learning_rate": 2.3593333333333333e-05,
      "loss": 0.0018,
      "step": 39610
    },
    {
      "epoch": 2.1130666666666666,
      "grad_norm": 0.1689915955066681,
      "learning_rate": 2.358666666666667e-05,
      "loss": 0.0031,
      "step": 39620
    },
    {
      "epoch": 2.1136,
      "grad_norm": 0.24606814980506897,
      "learning_rate": 2.358e-05,
      "loss": 0.0028,
      "step": 39630
    },
    {
      "epoch": 2.114133333333333,
      "grad_norm": 0.4576368033885956,
      "learning_rate": 2.3573333333333334e-05,
      "loss": 0.0022,
      "step": 39640
    },
    {
      "epoch": 2.1146666666666665,
      "grad_norm": 0.22591635584831238,
      "learning_rate": 2.3566666666666666e-05,
      "loss": 0.0018,
      "step": 39650
    },
    {
      "epoch": 2.1152,
      "grad_norm": 0.19820471107959747,
      "learning_rate": 2.356e-05,
      "loss": 0.0039,
      "step": 39660
    },
    {
      "epoch": 2.1157333333333335,
      "grad_norm": 0.2195747047662735,
      "learning_rate": 2.3553333333333337e-05,
      "loss": 0.0021,
      "step": 39670
    },
    {
      "epoch": 2.1162666666666667,
      "grad_norm": 0.2946784794330597,
      "learning_rate": 2.3546666666666666e-05,
      "loss": 0.0023,
      "step": 39680
    },
    {
      "epoch": 2.1168,
      "grad_norm": 0.29781582951545715,
      "learning_rate": 2.354e-05,
      "loss": 0.0028,
      "step": 39690
    },
    {
      "epoch": 2.1173333333333333,
      "grad_norm": 0.1286272257566452,
      "learning_rate": 2.3533333333333334e-05,
      "loss": 0.0031,
      "step": 39700
    },
    {
      "epoch": 2.1178666666666666,
      "grad_norm": 0.5222994089126587,
      "learning_rate": 2.352666666666667e-05,
      "loss": 0.0033,
      "step": 39710
    },
    {
      "epoch": 2.1184,
      "grad_norm": 0.12313690781593323,
      "learning_rate": 2.3520000000000002e-05,
      "loss": 0.0024,
      "step": 39720
    },
    {
      "epoch": 2.1189333333333336,
      "grad_norm": 0.35066139698028564,
      "learning_rate": 2.3513333333333334e-05,
      "loss": 0.0023,
      "step": 39730
    },
    {
      "epoch": 2.119466666666667,
      "grad_norm": 0.134759321808815,
      "learning_rate": 2.350666666666667e-05,
      "loss": 0.002,
      "step": 39740
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.3640138506889343,
      "learning_rate": 2.35e-05,
      "loss": 0.0018,
      "step": 39750
    },
    {
      "epoch": 2.1205333333333334,
      "grad_norm": 0.30285733938217163,
      "learning_rate": 2.3493333333333334e-05,
      "loss": 0.0027,
      "step": 39760
    },
    {
      "epoch": 2.1210666666666667,
      "grad_norm": 0.6040064692497253,
      "learning_rate": 2.3486666666666667e-05,
      "loss": 0.0022,
      "step": 39770
    },
    {
      "epoch": 2.1216,
      "grad_norm": 0.15348361432552338,
      "learning_rate": 2.3480000000000002e-05,
      "loss": 0.0021,
      "step": 39780
    },
    {
      "epoch": 2.122133333333333,
      "grad_norm": 0.14229434728622437,
      "learning_rate": 2.3473333333333334e-05,
      "loss": 0.0033,
      "step": 39790
    },
    {
      "epoch": 2.1226666666666665,
      "grad_norm": 0.4914843440055847,
      "learning_rate": 2.3466666666666667e-05,
      "loss": 0.0026,
      "step": 39800
    },
    {
      "epoch": 2.1232,
      "grad_norm": 0.19776900112628937,
      "learning_rate": 2.3460000000000002e-05,
      "loss": 0.0029,
      "step": 39810
    },
    {
      "epoch": 2.1237333333333335,
      "grad_norm": 0.21270455420017242,
      "learning_rate": 2.3453333333333335e-05,
      "loss": 0.0032,
      "step": 39820
    },
    {
      "epoch": 2.1242666666666667,
      "grad_norm": 0.2295953780412674,
      "learning_rate": 2.3446666666666667e-05,
      "loss": 0.002,
      "step": 39830
    },
    {
      "epoch": 2.1248,
      "grad_norm": 0.26464152336120605,
      "learning_rate": 2.344e-05,
      "loss": 0.0029,
      "step": 39840
    },
    {
      "epoch": 2.1253333333333333,
      "grad_norm": 0.2070242464542389,
      "learning_rate": 2.3433333333333335e-05,
      "loss": 0.0027,
      "step": 39850
    },
    {
      "epoch": 2.1258666666666666,
      "grad_norm": 0.2844746708869934,
      "learning_rate": 2.342666666666667e-05,
      "loss": 0.003,
      "step": 39860
    },
    {
      "epoch": 2.1264,
      "grad_norm": 0.41865479946136475,
      "learning_rate": 2.342e-05,
      "loss": 0.0033,
      "step": 39870
    },
    {
      "epoch": 2.1269333333333336,
      "grad_norm": 0.548721969127655,
      "learning_rate": 2.3413333333333335e-05,
      "loss": 0.0021,
      "step": 39880
    },
    {
      "epoch": 2.127466666666667,
      "grad_norm": 0.1883733570575714,
      "learning_rate": 2.3406666666666667e-05,
      "loss": 0.0022,
      "step": 39890
    },
    {
      "epoch": 2.128,
      "grad_norm": 0.25878164172172546,
      "learning_rate": 2.3400000000000003e-05,
      "loss": 0.0034,
      "step": 39900
    },
    {
      "epoch": 2.1285333333333334,
      "grad_norm": 0.14033102989196777,
      "learning_rate": 2.3393333333333335e-05,
      "loss": 0.0026,
      "step": 39910
    },
    {
      "epoch": 2.1290666666666667,
      "grad_norm": 0.32670488953590393,
      "learning_rate": 2.3386666666666668e-05,
      "loss": 0.0033,
      "step": 39920
    },
    {
      "epoch": 2.1296,
      "grad_norm": 0.11191551387310028,
      "learning_rate": 2.3380000000000003e-05,
      "loss": 0.0032,
      "step": 39930
    },
    {
      "epoch": 2.130133333333333,
      "grad_norm": 0.08532112091779709,
      "learning_rate": 2.3373333333333332e-05,
      "loss": 0.0019,
      "step": 39940
    },
    {
      "epoch": 2.1306666666666665,
      "grad_norm": 0.571679949760437,
      "learning_rate": 2.3366666666666668e-05,
      "loss": 0.0022,
      "step": 39950
    },
    {
      "epoch": 2.1312,
      "grad_norm": 0.19712497293949127,
      "learning_rate": 2.336e-05,
      "loss": 0.0019,
      "step": 39960
    },
    {
      "epoch": 2.1317333333333335,
      "grad_norm": 0.23880720138549805,
      "learning_rate": 2.3353333333333336e-05,
      "loss": 0.002,
      "step": 39970
    },
    {
      "epoch": 2.1322666666666668,
      "grad_norm": 0.39655396342277527,
      "learning_rate": 2.3346666666666668e-05,
      "loss": 0.0018,
      "step": 39980
    },
    {
      "epoch": 2.1328,
      "grad_norm": 0.10944990068674088,
      "learning_rate": 2.334e-05,
      "loss": 0.002,
      "step": 39990
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.35258597135543823,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.0026,
      "step": 40000
    },
    {
      "epoch": 2.1338666666666666,
      "grad_norm": 0.19387409090995789,
      "learning_rate": 2.3326666666666668e-05,
      "loss": 0.0019,
      "step": 40010
    },
    {
      "epoch": 2.1344,
      "grad_norm": 0.3368983268737793,
      "learning_rate": 2.332e-05,
      "loss": 0.0022,
      "step": 40020
    },
    {
      "epoch": 2.134933333333333,
      "grad_norm": 0.16231396794319153,
      "learning_rate": 2.3313333333333333e-05,
      "loss": 0.0033,
      "step": 40030
    },
    {
      "epoch": 2.135466666666667,
      "grad_norm": 0.08455951511859894,
      "learning_rate": 2.3306666666666668e-05,
      "loss": 0.0028,
      "step": 40040
    },
    {
      "epoch": 2.136,
      "grad_norm": 0.09835489839315414,
      "learning_rate": 2.3300000000000004e-05,
      "loss": 0.0019,
      "step": 40050
    },
    {
      "epoch": 2.1365333333333334,
      "grad_norm": 0.174353688955307,
      "learning_rate": 2.3293333333333333e-05,
      "loss": 0.003,
      "step": 40060
    },
    {
      "epoch": 2.1370666666666667,
      "grad_norm": 0.5456147789955139,
      "learning_rate": 2.328666666666667e-05,
      "loss": 0.0018,
      "step": 40070
    },
    {
      "epoch": 2.1376,
      "grad_norm": 0.27117443084716797,
      "learning_rate": 2.328e-05,
      "loss": 0.003,
      "step": 40080
    },
    {
      "epoch": 2.138133333333333,
      "grad_norm": 0.23103730380535126,
      "learning_rate": 2.3273333333333333e-05,
      "loss": 0.0027,
      "step": 40090
    },
    {
      "epoch": 2.1386666666666665,
      "grad_norm": 0.3280813694000244,
      "learning_rate": 2.326666666666667e-05,
      "loss": 0.0034,
      "step": 40100
    },
    {
      "epoch": 2.1391999999999998,
      "grad_norm": 0.4495723843574524,
      "learning_rate": 2.326e-05,
      "loss": 0.003,
      "step": 40110
    },
    {
      "epoch": 2.1397333333333335,
      "grad_norm": 0.06664443761110306,
      "learning_rate": 2.3253333333333337e-05,
      "loss": 0.002,
      "step": 40120
    },
    {
      "epoch": 2.1402666666666668,
      "grad_norm": 0.09846418350934982,
      "learning_rate": 2.3246666666666665e-05,
      "loss": 0.0031,
      "step": 40130
    },
    {
      "epoch": 2.1408,
      "grad_norm": 0.3140758275985718,
      "learning_rate": 2.324e-05,
      "loss": 0.0024,
      "step": 40140
    },
    {
      "epoch": 2.1413333333333333,
      "grad_norm": 0.13419710099697113,
      "learning_rate": 2.3233333333333333e-05,
      "loss": 0.002,
      "step": 40150
    },
    {
      "epoch": 2.1418666666666666,
      "grad_norm": 0.3409457802772522,
      "learning_rate": 2.322666666666667e-05,
      "loss": 0.0026,
      "step": 40160
    },
    {
      "epoch": 2.1424,
      "grad_norm": 0.319991797208786,
      "learning_rate": 2.322e-05,
      "loss": 0.002,
      "step": 40170
    },
    {
      "epoch": 2.142933333333333,
      "grad_norm": 0.16567829251289368,
      "learning_rate": 2.3213333333333334e-05,
      "loss": 0.0036,
      "step": 40180
    },
    {
      "epoch": 2.143466666666667,
      "grad_norm": 0.20784173905849457,
      "learning_rate": 2.320666666666667e-05,
      "loss": 0.0023,
      "step": 40190
    },
    {
      "epoch": 2.144,
      "grad_norm": 0.33619189262390137,
      "learning_rate": 2.32e-05,
      "loss": 0.0021,
      "step": 40200
    },
    {
      "epoch": 2.1445333333333334,
      "grad_norm": 0.14179754257202148,
      "learning_rate": 2.3193333333333334e-05,
      "loss": 0.0028,
      "step": 40210
    },
    {
      "epoch": 2.1450666666666667,
      "grad_norm": 0.37559929490089417,
      "learning_rate": 2.3186666666666666e-05,
      "loss": 0.0024,
      "step": 40220
    },
    {
      "epoch": 2.1456,
      "grad_norm": 0.3773103952407837,
      "learning_rate": 2.318e-05,
      "loss": 0.0029,
      "step": 40230
    },
    {
      "epoch": 2.1461333333333332,
      "grad_norm": 0.7112975716590881,
      "learning_rate": 2.3173333333333337e-05,
      "loss": 0.0028,
      "step": 40240
    },
    {
      "epoch": 2.1466666666666665,
      "grad_norm": 0.3982582092285156,
      "learning_rate": 2.3166666666666666e-05,
      "loss": 0.0029,
      "step": 40250
    },
    {
      "epoch": 2.1471999999999998,
      "grad_norm": 0.2442103773355484,
      "learning_rate": 2.3160000000000002e-05,
      "loss": 0.0029,
      "step": 40260
    },
    {
      "epoch": 2.1477333333333335,
      "grad_norm": 0.12180199474096298,
      "learning_rate": 2.3153333333333334e-05,
      "loss": 0.0027,
      "step": 40270
    },
    {
      "epoch": 2.1482666666666668,
      "grad_norm": 0.07123035192489624,
      "learning_rate": 2.3146666666666666e-05,
      "loss": 0.0018,
      "step": 40280
    },
    {
      "epoch": 2.1488,
      "grad_norm": 0.14445368945598602,
      "learning_rate": 2.3140000000000002e-05,
      "loss": 0.0021,
      "step": 40290
    },
    {
      "epoch": 2.1493333333333333,
      "grad_norm": 0.44714123010635376,
      "learning_rate": 2.3133333333333334e-05,
      "loss": 0.0018,
      "step": 40300
    },
    {
      "epoch": 2.1498666666666666,
      "grad_norm": 0.7408584952354431,
      "learning_rate": 2.312666666666667e-05,
      "loss": 0.0022,
      "step": 40310
    },
    {
      "epoch": 2.1504,
      "grad_norm": 0.45657315850257874,
      "learning_rate": 2.312e-05,
      "loss": 0.0019,
      "step": 40320
    },
    {
      "epoch": 2.150933333333333,
      "grad_norm": 0.4721394181251526,
      "learning_rate": 2.3113333333333335e-05,
      "loss": 0.0035,
      "step": 40330
    },
    {
      "epoch": 2.151466666666667,
      "grad_norm": 0.27748480439186096,
      "learning_rate": 2.3106666666666667e-05,
      "loss": 0.0017,
      "step": 40340
    },
    {
      "epoch": 2.152,
      "grad_norm": 0.09348508715629578,
      "learning_rate": 2.3100000000000002e-05,
      "loss": 0.0027,
      "step": 40350
    },
    {
      "epoch": 2.1525333333333334,
      "grad_norm": 0.21728405356407166,
      "learning_rate": 2.3093333333333335e-05,
      "loss": 0.0022,
      "step": 40360
    },
    {
      "epoch": 2.1530666666666667,
      "grad_norm": 0.10500975698232651,
      "learning_rate": 2.3086666666666667e-05,
      "loss": 0.0027,
      "step": 40370
    },
    {
      "epoch": 2.1536,
      "grad_norm": 0.18061602115631104,
      "learning_rate": 2.3080000000000003e-05,
      "loss": 0.0017,
      "step": 40380
    },
    {
      "epoch": 2.1541333333333332,
      "grad_norm": 0.1763215959072113,
      "learning_rate": 2.3073333333333335e-05,
      "loss": 0.0023,
      "step": 40390
    },
    {
      "epoch": 2.1546666666666665,
      "grad_norm": 0.45051807165145874,
      "learning_rate": 2.3066666666666667e-05,
      "loss": 0.0017,
      "step": 40400
    },
    {
      "epoch": 2.1552,
      "grad_norm": 0.12298182398080826,
      "learning_rate": 2.306e-05,
      "loss": 0.0022,
      "step": 40410
    },
    {
      "epoch": 2.1557333333333335,
      "grad_norm": 0.4460509419441223,
      "learning_rate": 2.3053333333333335e-05,
      "loss": 0.002,
      "step": 40420
    },
    {
      "epoch": 2.1562666666666668,
      "grad_norm": 0.45027968287467957,
      "learning_rate": 2.3046666666666667e-05,
      "loss": 0.0025,
      "step": 40430
    },
    {
      "epoch": 2.1568,
      "grad_norm": 0.3005901277065277,
      "learning_rate": 2.304e-05,
      "loss": 0.0018,
      "step": 40440
    },
    {
      "epoch": 2.1573333333333333,
      "grad_norm": 0.5758880376815796,
      "learning_rate": 2.3033333333333335e-05,
      "loss": 0.0031,
      "step": 40450
    },
    {
      "epoch": 2.1578666666666666,
      "grad_norm": 0.32754069566726685,
      "learning_rate": 2.3026666666666668e-05,
      "loss": 0.0031,
      "step": 40460
    },
    {
      "epoch": 2.1584,
      "grad_norm": 0.36646559834480286,
      "learning_rate": 2.302e-05,
      "loss": 0.0017,
      "step": 40470
    },
    {
      "epoch": 2.158933333333333,
      "grad_norm": 0.19653436541557312,
      "learning_rate": 2.3013333333333335e-05,
      "loss": 0.0022,
      "step": 40480
    },
    {
      "epoch": 2.159466666666667,
      "grad_norm": 0.50611412525177,
      "learning_rate": 2.3006666666666668e-05,
      "loss": 0.0027,
      "step": 40490
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.2325788289308548,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.0027,
      "step": 40500
    },
    {
      "epoch": 2.1605333333333334,
      "grad_norm": 0.7309768795967102,
      "learning_rate": 2.2993333333333332e-05,
      "loss": 0.0028,
      "step": 40510
    },
    {
      "epoch": 2.1610666666666667,
      "grad_norm": 0.39732998609542847,
      "learning_rate": 2.2986666666666668e-05,
      "loss": 0.0016,
      "step": 40520
    },
    {
      "epoch": 2.1616,
      "grad_norm": 0.35163208842277527,
      "learning_rate": 2.298e-05,
      "loss": 0.0022,
      "step": 40530
    },
    {
      "epoch": 2.1621333333333332,
      "grad_norm": 0.3201206624507904,
      "learning_rate": 2.2973333333333336e-05,
      "loss": 0.0036,
      "step": 40540
    },
    {
      "epoch": 2.1626666666666665,
      "grad_norm": 0.40548598766326904,
      "learning_rate": 2.2966666666666668e-05,
      "loss": 0.002,
      "step": 40550
    },
    {
      "epoch": 2.1632,
      "grad_norm": 0.40853676199913025,
      "learning_rate": 2.296e-05,
      "loss": 0.0021,
      "step": 40560
    },
    {
      "epoch": 2.1637333333333335,
      "grad_norm": 0.2929813265800476,
      "learning_rate": 2.2953333333333336e-05,
      "loss": 0.0031,
      "step": 40570
    },
    {
      "epoch": 2.164266666666667,
      "grad_norm": 0.25353938341140747,
      "learning_rate": 2.294666666666667e-05,
      "loss": 0.0019,
      "step": 40580
    },
    {
      "epoch": 2.1648,
      "grad_norm": 0.09294818341732025,
      "learning_rate": 2.294e-05,
      "loss": 0.0025,
      "step": 40590
    },
    {
      "epoch": 2.1653333333333333,
      "grad_norm": 0.17322415113449097,
      "learning_rate": 2.2933333333333333e-05,
      "loss": 0.0023,
      "step": 40600
    },
    {
      "epoch": 2.1658666666666666,
      "grad_norm": 0.4082993268966675,
      "learning_rate": 2.292666666666667e-05,
      "loss": 0.0032,
      "step": 40610
    },
    {
      "epoch": 2.1664,
      "grad_norm": 0.4208868741989136,
      "learning_rate": 2.292e-05,
      "loss": 0.0028,
      "step": 40620
    },
    {
      "epoch": 2.166933333333333,
      "grad_norm": 0.059355009347200394,
      "learning_rate": 2.2913333333333333e-05,
      "loss": 0.0035,
      "step": 40630
    },
    {
      "epoch": 2.167466666666667,
      "grad_norm": 0.8279513120651245,
      "learning_rate": 2.290666666666667e-05,
      "loss": 0.0023,
      "step": 40640
    },
    {
      "epoch": 2.168,
      "grad_norm": 0.4179052412509918,
      "learning_rate": 2.29e-05,
      "loss": 0.0023,
      "step": 40650
    },
    {
      "epoch": 2.1685333333333334,
      "grad_norm": 0.16939884424209595,
      "learning_rate": 2.2893333333333333e-05,
      "loss": 0.0021,
      "step": 40660
    },
    {
      "epoch": 2.1690666666666667,
      "grad_norm": 0.31439366936683655,
      "learning_rate": 2.288666666666667e-05,
      "loss": 0.0032,
      "step": 40670
    },
    {
      "epoch": 2.1696,
      "grad_norm": 0.12960410118103027,
      "learning_rate": 2.288e-05,
      "loss": 0.0046,
      "step": 40680
    },
    {
      "epoch": 2.1701333333333332,
      "grad_norm": 0.187310129404068,
      "learning_rate": 2.2873333333333337e-05,
      "loss": 0.0024,
      "step": 40690
    },
    {
      "epoch": 2.1706666666666665,
      "grad_norm": 0.27862682938575745,
      "learning_rate": 2.2866666666666666e-05,
      "loss": 0.003,
      "step": 40700
    },
    {
      "epoch": 2.1712,
      "grad_norm": 0.32671549916267395,
      "learning_rate": 2.286e-05,
      "loss": 0.0019,
      "step": 40710
    },
    {
      "epoch": 2.1717333333333335,
      "grad_norm": 0.37876492738723755,
      "learning_rate": 2.2853333333333334e-05,
      "loss": 0.004,
      "step": 40720
    },
    {
      "epoch": 2.172266666666667,
      "grad_norm": 0.193796306848526,
      "learning_rate": 2.284666666666667e-05,
      "loss": 0.0022,
      "step": 40730
    },
    {
      "epoch": 2.1728,
      "grad_norm": 0.3108975887298584,
      "learning_rate": 2.284e-05,
      "loss": 0.002,
      "step": 40740
    },
    {
      "epoch": 2.1733333333333333,
      "grad_norm": 0.5708919167518616,
      "learning_rate": 2.2833333333333334e-05,
      "loss": 0.0017,
      "step": 40750
    },
    {
      "epoch": 2.1738666666666666,
      "grad_norm": 0.10531137883663177,
      "learning_rate": 2.282666666666667e-05,
      "loss": 0.0025,
      "step": 40760
    },
    {
      "epoch": 2.1744,
      "grad_norm": 0.13179191946983337,
      "learning_rate": 2.282e-05,
      "loss": 0.0017,
      "step": 40770
    },
    {
      "epoch": 2.174933333333333,
      "grad_norm": 0.17061075568199158,
      "learning_rate": 2.2813333333333334e-05,
      "loss": 0.0017,
      "step": 40780
    },
    {
      "epoch": 2.175466666666667,
      "grad_norm": 0.41950559616088867,
      "learning_rate": 2.2806666666666666e-05,
      "loss": 0.0037,
      "step": 40790
    },
    {
      "epoch": 2.176,
      "grad_norm": 0.1854896992444992,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 0.0023,
      "step": 40800
    },
    {
      "epoch": 2.1765333333333334,
      "grad_norm": 0.309575617313385,
      "learning_rate": 2.2793333333333334e-05,
      "loss": 0.0023,
      "step": 40810
    },
    {
      "epoch": 2.1770666666666667,
      "grad_norm": 0.6373306512832642,
      "learning_rate": 2.2786666666666666e-05,
      "loss": 0.0029,
      "step": 40820
    },
    {
      "epoch": 2.1776,
      "grad_norm": 0.559356689453125,
      "learning_rate": 2.2780000000000002e-05,
      "loss": 0.0021,
      "step": 40830
    },
    {
      "epoch": 2.1781333333333333,
      "grad_norm": 0.08441156148910522,
      "learning_rate": 2.2773333333333334e-05,
      "loss": 0.0024,
      "step": 40840
    },
    {
      "epoch": 2.1786666666666665,
      "grad_norm": 0.3320143520832062,
      "learning_rate": 2.2766666666666667e-05,
      "loss": 0.0025,
      "step": 40850
    },
    {
      "epoch": 2.1792,
      "grad_norm": 0.17400135099887848,
      "learning_rate": 2.2760000000000002e-05,
      "loss": 0.0024,
      "step": 40860
    },
    {
      "epoch": 2.1797333333333335,
      "grad_norm": 0.23001500964164734,
      "learning_rate": 2.2753333333333335e-05,
      "loss": 0.0037,
      "step": 40870
    },
    {
      "epoch": 2.180266666666667,
      "grad_norm": 0.11562858521938324,
      "learning_rate": 2.274666666666667e-05,
      "loss": 0.002,
      "step": 40880
    },
    {
      "epoch": 2.1808,
      "grad_norm": 0.1094849705696106,
      "learning_rate": 2.274e-05,
      "loss": 0.0022,
      "step": 40890
    },
    {
      "epoch": 2.1813333333333333,
      "grad_norm": 0.30084848403930664,
      "learning_rate": 2.2733333333333335e-05,
      "loss": 0.0023,
      "step": 40900
    },
    {
      "epoch": 2.1818666666666666,
      "grad_norm": 0.6159718036651611,
      "learning_rate": 2.2726666666666667e-05,
      "loss": 0.0026,
      "step": 40910
    },
    {
      "epoch": 2.1824,
      "grad_norm": 0.18906539678573608,
      "learning_rate": 2.2720000000000003e-05,
      "loss": 0.0029,
      "step": 40920
    },
    {
      "epoch": 2.182933333333333,
      "grad_norm": 0.22495073080062866,
      "learning_rate": 2.2713333333333335e-05,
      "loss": 0.003,
      "step": 40930
    },
    {
      "epoch": 2.183466666666667,
      "grad_norm": 0.32065364718437195,
      "learning_rate": 2.2706666666666667e-05,
      "loss": 0.0033,
      "step": 40940
    },
    {
      "epoch": 2.184,
      "grad_norm": 0.28125491738319397,
      "learning_rate": 2.2700000000000003e-05,
      "loss": 0.0025,
      "step": 40950
    },
    {
      "epoch": 2.1845333333333334,
      "grad_norm": 0.35431283712387085,
      "learning_rate": 2.2693333333333332e-05,
      "loss": 0.0034,
      "step": 40960
    },
    {
      "epoch": 2.1850666666666667,
      "grad_norm": 0.7606768608093262,
      "learning_rate": 2.2686666666666667e-05,
      "loss": 0.0038,
      "step": 40970
    },
    {
      "epoch": 2.1856,
      "grad_norm": 0.11976440250873566,
      "learning_rate": 2.268e-05,
      "loss": 0.0021,
      "step": 40980
    },
    {
      "epoch": 2.1861333333333333,
      "grad_norm": 0.47904640436172485,
      "learning_rate": 2.2673333333333335e-05,
      "loss": 0.0029,
      "step": 40990
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 0.12302584946155548,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 0.0031,
      "step": 41000
    },
    {
      "epoch": 2.1872,
      "grad_norm": 0.5734213590621948,
      "learning_rate": 2.266e-05,
      "loss": 0.0035,
      "step": 41010
    },
    {
      "epoch": 2.1877333333333335,
      "grad_norm": 0.9870907068252563,
      "learning_rate": 2.2653333333333336e-05,
      "loss": 0.0041,
      "step": 41020
    },
    {
      "epoch": 2.188266666666667,
      "grad_norm": 0.2771896719932556,
      "learning_rate": 2.2646666666666668e-05,
      "loss": 0.0025,
      "step": 41030
    },
    {
      "epoch": 2.1888,
      "grad_norm": 0.15845300257205963,
      "learning_rate": 2.264e-05,
      "loss": 0.0028,
      "step": 41040
    },
    {
      "epoch": 2.1893333333333334,
      "grad_norm": 0.24674873054027557,
      "learning_rate": 2.2633333333333336e-05,
      "loss": 0.0024,
      "step": 41050
    },
    {
      "epoch": 2.1898666666666666,
      "grad_norm": 0.11517074704170227,
      "learning_rate": 2.2626666666666668e-05,
      "loss": 0.0021,
      "step": 41060
    },
    {
      "epoch": 2.1904,
      "grad_norm": 0.4959738254547119,
      "learning_rate": 2.2620000000000004e-05,
      "loss": 0.0032,
      "step": 41070
    },
    {
      "epoch": 2.190933333333333,
      "grad_norm": 0.2616862952709198,
      "learning_rate": 2.2613333333333333e-05,
      "loss": 0.0032,
      "step": 41080
    },
    {
      "epoch": 2.191466666666667,
      "grad_norm": 0.5311612486839294,
      "learning_rate": 2.2606666666666668e-05,
      "loss": 0.0034,
      "step": 41090
    },
    {
      "epoch": 2.192,
      "grad_norm": 0.267290860414505,
      "learning_rate": 2.26e-05,
      "loss": 0.0021,
      "step": 41100
    },
    {
      "epoch": 2.1925333333333334,
      "grad_norm": 0.1186782494187355,
      "learning_rate": 2.2593333333333336e-05,
      "loss": 0.0037,
      "step": 41110
    },
    {
      "epoch": 2.1930666666666667,
      "grad_norm": 0.7307263016700745,
      "learning_rate": 2.258666666666667e-05,
      "loss": 0.003,
      "step": 41120
    },
    {
      "epoch": 2.1936,
      "grad_norm": 0.1818217933177948,
      "learning_rate": 2.258e-05,
      "loss": 0.0021,
      "step": 41130
    },
    {
      "epoch": 2.1941333333333333,
      "grad_norm": 0.13801397383213043,
      "learning_rate": 2.2573333333333336e-05,
      "loss": 0.0028,
      "step": 41140
    },
    {
      "epoch": 2.1946666666666665,
      "grad_norm": 0.435126394033432,
      "learning_rate": 2.2566666666666665e-05,
      "loss": 0.0023,
      "step": 41150
    },
    {
      "epoch": 2.1952,
      "grad_norm": 0.3994690775871277,
      "learning_rate": 2.256e-05,
      "loss": 0.0027,
      "step": 41160
    },
    {
      "epoch": 2.1957333333333335,
      "grad_norm": 0.10017996281385422,
      "learning_rate": 2.2553333333333333e-05,
      "loss": 0.0028,
      "step": 41170
    },
    {
      "epoch": 2.196266666666667,
      "grad_norm": 0.09059092402458191,
      "learning_rate": 2.254666666666667e-05,
      "loss": 0.0019,
      "step": 41180
    },
    {
      "epoch": 2.1968,
      "grad_norm": 0.16148121654987335,
      "learning_rate": 2.254e-05,
      "loss": 0.0022,
      "step": 41190
    },
    {
      "epoch": 2.1973333333333334,
      "grad_norm": 0.6658868789672852,
      "learning_rate": 2.2533333333333333e-05,
      "loss": 0.002,
      "step": 41200
    },
    {
      "epoch": 2.1978666666666666,
      "grad_norm": 0.6476572751998901,
      "learning_rate": 2.252666666666667e-05,
      "loss": 0.0019,
      "step": 41210
    },
    {
      "epoch": 2.1984,
      "grad_norm": 0.14812800288200378,
      "learning_rate": 2.252e-05,
      "loss": 0.0026,
      "step": 41220
    },
    {
      "epoch": 2.198933333333333,
      "grad_norm": 0.09977904707193375,
      "learning_rate": 2.2513333333333333e-05,
      "loss": 0.002,
      "step": 41230
    },
    {
      "epoch": 2.1994666666666665,
      "grad_norm": 0.15839941799640656,
      "learning_rate": 2.250666666666667e-05,
      "loss": 0.0031,
      "step": 41240
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.10477940738201141,
      "learning_rate": 2.25e-05,
      "loss": 0.0021,
      "step": 41250
    },
    {
      "epoch": 2.2005333333333335,
      "grad_norm": 0.3535097539424896,
      "learning_rate": 2.2493333333333337e-05,
      "loss": 0.0029,
      "step": 41260
    },
    {
      "epoch": 2.2010666666666667,
      "grad_norm": 0.26164722442626953,
      "learning_rate": 2.2486666666666666e-05,
      "loss": 0.003,
      "step": 41270
    },
    {
      "epoch": 2.2016,
      "grad_norm": 0.28439414501190186,
      "learning_rate": 2.248e-05,
      "loss": 0.0028,
      "step": 41280
    },
    {
      "epoch": 2.2021333333333333,
      "grad_norm": 0.24330861866474152,
      "learning_rate": 2.2473333333333334e-05,
      "loss": 0.0024,
      "step": 41290
    },
    {
      "epoch": 2.2026666666666666,
      "grad_norm": 0.41797322034835815,
      "learning_rate": 2.2466666666666666e-05,
      "loss": 0.0018,
      "step": 41300
    },
    {
      "epoch": 2.2032,
      "grad_norm": 0.4679798483848572,
      "learning_rate": 2.2460000000000002e-05,
      "loss": 0.0033,
      "step": 41310
    },
    {
      "epoch": 2.203733333333333,
      "grad_norm": 0.2015051692724228,
      "learning_rate": 2.2453333333333334e-05,
      "loss": 0.0023,
      "step": 41320
    },
    {
      "epoch": 2.204266666666667,
      "grad_norm": 0.18491262197494507,
      "learning_rate": 2.244666666666667e-05,
      "loss": 0.0021,
      "step": 41330
    },
    {
      "epoch": 2.2048,
      "grad_norm": 0.2262965589761734,
      "learning_rate": 2.244e-05,
      "loss": 0.0025,
      "step": 41340
    },
    {
      "epoch": 2.2053333333333334,
      "grad_norm": 0.41937392950057983,
      "learning_rate": 2.2433333333333334e-05,
      "loss": 0.0027,
      "step": 41350
    },
    {
      "epoch": 2.2058666666666666,
      "grad_norm": 0.4775264263153076,
      "learning_rate": 2.2426666666666667e-05,
      "loss": 0.0029,
      "step": 41360
    },
    {
      "epoch": 2.2064,
      "grad_norm": 0.798352062702179,
      "learning_rate": 2.2420000000000002e-05,
      "loss": 0.0024,
      "step": 41370
    },
    {
      "epoch": 2.206933333333333,
      "grad_norm": 0.09340780973434448,
      "learning_rate": 2.2413333333333334e-05,
      "loss": 0.0029,
      "step": 41380
    },
    {
      "epoch": 2.2074666666666665,
      "grad_norm": 0.08673784136772156,
      "learning_rate": 2.2406666666666667e-05,
      "loss": 0.002,
      "step": 41390
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.3479337990283966,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 0.0024,
      "step": 41400
    },
    {
      "epoch": 2.2085333333333335,
      "grad_norm": 0.5159127712249756,
      "learning_rate": 2.2393333333333335e-05,
      "loss": 0.0019,
      "step": 41410
    },
    {
      "epoch": 2.2090666666666667,
      "grad_norm": 0.06271471083164215,
      "learning_rate": 2.2386666666666667e-05,
      "loss": 0.0032,
      "step": 41420
    },
    {
      "epoch": 2.2096,
      "grad_norm": 0.24022766947746277,
      "learning_rate": 2.2380000000000003e-05,
      "loss": 0.0017,
      "step": 41430
    },
    {
      "epoch": 2.2101333333333333,
      "grad_norm": 0.1542094200849533,
      "learning_rate": 2.2373333333333335e-05,
      "loss": 0.002,
      "step": 41440
    },
    {
      "epoch": 2.2106666666666666,
      "grad_norm": 0.28201019763946533,
      "learning_rate": 2.236666666666667e-05,
      "loss": 0.0028,
      "step": 41450
    },
    {
      "epoch": 2.2112,
      "grad_norm": 0.35532039403915405,
      "learning_rate": 2.236e-05,
      "loss": 0.0026,
      "step": 41460
    },
    {
      "epoch": 2.211733333333333,
      "grad_norm": 0.2372882217168808,
      "learning_rate": 2.2353333333333335e-05,
      "loss": 0.0022,
      "step": 41470
    },
    {
      "epoch": 2.212266666666667,
      "grad_norm": 0.1378633826971054,
      "learning_rate": 2.2346666666666667e-05,
      "loss": 0.0021,
      "step": 41480
    },
    {
      "epoch": 2.2128,
      "grad_norm": 0.12032660096883774,
      "learning_rate": 2.234e-05,
      "loss": 0.0023,
      "step": 41490
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 0.40121233463287354,
      "learning_rate": 2.2333333333333335e-05,
      "loss": 0.0023,
      "step": 41500
    },
    {
      "epoch": 2.2138666666666666,
      "grad_norm": 0.47119906544685364,
      "learning_rate": 2.2326666666666667e-05,
      "loss": 0.0017,
      "step": 41510
    },
    {
      "epoch": 2.2144,
      "grad_norm": 0.17834660410881042,
      "learning_rate": 2.2320000000000003e-05,
      "loss": 0.003,
      "step": 41520
    },
    {
      "epoch": 2.214933333333333,
      "grad_norm": 0.2651260197162628,
      "learning_rate": 2.2313333333333332e-05,
      "loss": 0.0023,
      "step": 41530
    },
    {
      "epoch": 2.2154666666666665,
      "grad_norm": 0.29016855359077454,
      "learning_rate": 2.2306666666666668e-05,
      "loss": 0.003,
      "step": 41540
    },
    {
      "epoch": 2.216,
      "grad_norm": 0.07228060811758041,
      "learning_rate": 2.23e-05,
      "loss": 0.002,
      "step": 41550
    },
    {
      "epoch": 2.2165333333333335,
      "grad_norm": 0.2596912980079651,
      "learning_rate": 2.2293333333333336e-05,
      "loss": 0.0022,
      "step": 41560
    },
    {
      "epoch": 2.2170666666666667,
      "grad_norm": 0.17312748730182648,
      "learning_rate": 2.2286666666666668e-05,
      "loss": 0.0038,
      "step": 41570
    },
    {
      "epoch": 2.2176,
      "grad_norm": 0.3220105767250061,
      "learning_rate": 2.228e-05,
      "loss": 0.0032,
      "step": 41580
    },
    {
      "epoch": 2.2181333333333333,
      "grad_norm": 0.11129318177700043,
      "learning_rate": 2.2273333333333336e-05,
      "loss": 0.0018,
      "step": 41590
    },
    {
      "epoch": 2.2186666666666666,
      "grad_norm": 0.5357166528701782,
      "learning_rate": 2.2266666666666668e-05,
      "loss": 0.0018,
      "step": 41600
    },
    {
      "epoch": 2.2192,
      "grad_norm": 0.44809219241142273,
      "learning_rate": 2.226e-05,
      "loss": 0.0025,
      "step": 41610
    },
    {
      "epoch": 2.219733333333333,
      "grad_norm": 0.4054310917854309,
      "learning_rate": 2.2253333333333336e-05,
      "loss": 0.0022,
      "step": 41620
    },
    {
      "epoch": 2.220266666666667,
      "grad_norm": 0.10077705979347229,
      "learning_rate": 2.2246666666666668e-05,
      "loss": 0.0045,
      "step": 41630
    },
    {
      "epoch": 2.2208,
      "grad_norm": 0.6373829245567322,
      "learning_rate": 2.224e-05,
      "loss": 0.0027,
      "step": 41640
    },
    {
      "epoch": 2.2213333333333334,
      "grad_norm": 0.5866521596908569,
      "learning_rate": 2.2233333333333333e-05,
      "loss": 0.0027,
      "step": 41650
    },
    {
      "epoch": 2.2218666666666667,
      "grad_norm": 0.4631076455116272,
      "learning_rate": 2.222666666666667e-05,
      "loss": 0.0032,
      "step": 41660
    },
    {
      "epoch": 2.2224,
      "grad_norm": 0.2696983516216278,
      "learning_rate": 2.222e-05,
      "loss": 0.0031,
      "step": 41670
    },
    {
      "epoch": 2.222933333333333,
      "grad_norm": 0.21416421234607697,
      "learning_rate": 2.2213333333333333e-05,
      "loss": 0.003,
      "step": 41680
    },
    {
      "epoch": 2.2234666666666665,
      "grad_norm": 0.3041006326675415,
      "learning_rate": 2.220666666666667e-05,
      "loss": 0.0025,
      "step": 41690
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.27758145332336426,
      "learning_rate": 2.22e-05,
      "loss": 0.0019,
      "step": 41700
    },
    {
      "epoch": 2.2245333333333335,
      "grad_norm": 0.09869212657213211,
      "learning_rate": 2.2193333333333337e-05,
      "loss": 0.0033,
      "step": 41710
    },
    {
      "epoch": 2.2250666666666667,
      "grad_norm": 0.29025885462760925,
      "learning_rate": 2.2186666666666665e-05,
      "loss": 0.0026,
      "step": 41720
    },
    {
      "epoch": 2.2256,
      "grad_norm": 0.42297935485839844,
      "learning_rate": 2.218e-05,
      "loss": 0.0036,
      "step": 41730
    },
    {
      "epoch": 2.2261333333333333,
      "grad_norm": 0.13288429379463196,
      "learning_rate": 2.2173333333333333e-05,
      "loss": 0.0027,
      "step": 41740
    },
    {
      "epoch": 2.2266666666666666,
      "grad_norm": 0.4381319582462311,
      "learning_rate": 2.216666666666667e-05,
      "loss": 0.0024,
      "step": 41750
    },
    {
      "epoch": 2.2272,
      "grad_norm": 0.2793566584587097,
      "learning_rate": 2.216e-05,
      "loss": 0.0024,
      "step": 41760
    },
    {
      "epoch": 2.227733333333333,
      "grad_norm": 0.16845054924488068,
      "learning_rate": 2.2153333333333334e-05,
      "loss": 0.0026,
      "step": 41770
    },
    {
      "epoch": 2.228266666666667,
      "grad_norm": 0.5049049258232117,
      "learning_rate": 2.214666666666667e-05,
      "loss": 0.0028,
      "step": 41780
    },
    {
      "epoch": 2.2288,
      "grad_norm": 0.17494136095046997,
      "learning_rate": 2.214e-05,
      "loss": 0.0017,
      "step": 41790
    },
    {
      "epoch": 2.2293333333333334,
      "grad_norm": 0.5824812054634094,
      "learning_rate": 2.2133333333333334e-05,
      "loss": 0.0021,
      "step": 41800
    },
    {
      "epoch": 2.2298666666666667,
      "grad_norm": 0.4120887517929077,
      "learning_rate": 2.212666666666667e-05,
      "loss": 0.0028,
      "step": 41810
    },
    {
      "epoch": 2.2304,
      "grad_norm": 0.20641493797302246,
      "learning_rate": 2.212e-05,
      "loss": 0.0018,
      "step": 41820
    },
    {
      "epoch": 2.230933333333333,
      "grad_norm": 0.21112684905529022,
      "learning_rate": 2.2113333333333334e-05,
      "loss": 0.0021,
      "step": 41830
    },
    {
      "epoch": 2.2314666666666665,
      "grad_norm": 0.2552359998226166,
      "learning_rate": 2.2106666666666666e-05,
      "loss": 0.0022,
      "step": 41840
    },
    {
      "epoch": 2.232,
      "grad_norm": 0.4528062641620636,
      "learning_rate": 2.2100000000000002e-05,
      "loss": 0.0035,
      "step": 41850
    },
    {
      "epoch": 2.2325333333333335,
      "grad_norm": 0.2505490779876709,
      "learning_rate": 2.2093333333333334e-05,
      "loss": 0.0035,
      "step": 41860
    },
    {
      "epoch": 2.2330666666666668,
      "grad_norm": 0.4639870524406433,
      "learning_rate": 2.2086666666666666e-05,
      "loss": 0.002,
      "step": 41870
    },
    {
      "epoch": 2.2336,
      "grad_norm": 0.17255254089832306,
      "learning_rate": 2.2080000000000002e-05,
      "loss": 0.0025,
      "step": 41880
    },
    {
      "epoch": 2.2341333333333333,
      "grad_norm": 0.12788712978363037,
      "learning_rate": 2.2073333333333334e-05,
      "loss": 0.0027,
      "step": 41890
    },
    {
      "epoch": 2.2346666666666666,
      "grad_norm": 0.3382030725479126,
      "learning_rate": 2.206666666666667e-05,
      "loss": 0.003,
      "step": 41900
    },
    {
      "epoch": 2.2352,
      "grad_norm": 0.47742825746536255,
      "learning_rate": 2.206e-05,
      "loss": 0.0022,
      "step": 41910
    },
    {
      "epoch": 2.235733333333333,
      "grad_norm": 0.22843532264232635,
      "learning_rate": 2.2053333333333335e-05,
      "loss": 0.0027,
      "step": 41920
    },
    {
      "epoch": 2.236266666666667,
      "grad_norm": 0.2914084494113922,
      "learning_rate": 2.2046666666666667e-05,
      "loss": 0.0029,
      "step": 41930
    },
    {
      "epoch": 2.2368,
      "grad_norm": 0.2597320079803467,
      "learning_rate": 2.2040000000000002e-05,
      "loss": 0.0019,
      "step": 41940
    },
    {
      "epoch": 2.2373333333333334,
      "grad_norm": 0.1625165045261383,
      "learning_rate": 2.2033333333333335e-05,
      "loss": 0.0026,
      "step": 41950
    },
    {
      "epoch": 2.2378666666666667,
      "grad_norm": 0.1738293468952179,
      "learning_rate": 2.2026666666666667e-05,
      "loss": 0.0021,
      "step": 41960
    },
    {
      "epoch": 2.2384,
      "grad_norm": 0.26508358120918274,
      "learning_rate": 2.2020000000000003e-05,
      "loss": 0.002,
      "step": 41970
    },
    {
      "epoch": 2.238933333333333,
      "grad_norm": 0.16732823848724365,
      "learning_rate": 2.201333333333333e-05,
      "loss": 0.002,
      "step": 41980
    },
    {
      "epoch": 2.2394666666666665,
      "grad_norm": 0.4594966173171997,
      "learning_rate": 2.2006666666666667e-05,
      "loss": 0.0021,
      "step": 41990
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.33404871821403503,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.0023,
      "step": 42000
    },
    {
      "epoch": 2.2405333333333335,
      "grad_norm": 0.12396441400051117,
      "learning_rate": 2.1993333333333335e-05,
      "loss": 0.002,
      "step": 42010
    },
    {
      "epoch": 2.2410666666666668,
      "grad_norm": 0.4109102189540863,
      "learning_rate": 2.1986666666666667e-05,
      "loss": 0.0029,
      "step": 42020
    },
    {
      "epoch": 2.2416,
      "grad_norm": 0.48763489723205566,
      "learning_rate": 2.198e-05,
      "loss": 0.0028,
      "step": 42030
    },
    {
      "epoch": 2.2421333333333333,
      "grad_norm": 0.1607256829738617,
      "learning_rate": 2.1973333333333335e-05,
      "loss": 0.0019,
      "step": 42040
    },
    {
      "epoch": 2.2426666666666666,
      "grad_norm": 0.19851860404014587,
      "learning_rate": 2.1966666666666668e-05,
      "loss": 0.0021,
      "step": 42050
    },
    {
      "epoch": 2.2432,
      "grad_norm": 0.47712552547454834,
      "learning_rate": 2.196e-05,
      "loss": 0.003,
      "step": 42060
    },
    {
      "epoch": 2.243733333333333,
      "grad_norm": 0.6857925057411194,
      "learning_rate": 2.1953333333333335e-05,
      "loss": 0.0019,
      "step": 42070
    },
    {
      "epoch": 2.244266666666667,
      "grad_norm": 0.32236555218696594,
      "learning_rate": 2.1946666666666668e-05,
      "loss": 0.0025,
      "step": 42080
    },
    {
      "epoch": 2.2448,
      "grad_norm": 0.22495786845684052,
      "learning_rate": 2.1940000000000003e-05,
      "loss": 0.0035,
      "step": 42090
    },
    {
      "epoch": 2.2453333333333334,
      "grad_norm": 0.15221020579338074,
      "learning_rate": 2.1933333333333332e-05,
      "loss": 0.003,
      "step": 42100
    },
    {
      "epoch": 2.2458666666666667,
      "grad_norm": 0.4354093074798584,
      "learning_rate": 2.1926666666666668e-05,
      "loss": 0.0032,
      "step": 42110
    },
    {
      "epoch": 2.2464,
      "grad_norm": 0.33874210715293884,
      "learning_rate": 2.192e-05,
      "loss": 0.0026,
      "step": 42120
    },
    {
      "epoch": 2.2469333333333332,
      "grad_norm": 0.4635898470878601,
      "learning_rate": 2.1913333333333336e-05,
      "loss": 0.0029,
      "step": 42130
    },
    {
      "epoch": 2.2474666666666665,
      "grad_norm": 0.5069018602371216,
      "learning_rate": 2.1906666666666668e-05,
      "loss": 0.0029,
      "step": 42140
    },
    {
      "epoch": 2.248,
      "grad_norm": 0.3883923590183258,
      "learning_rate": 2.19e-05,
      "loss": 0.0033,
      "step": 42150
    },
    {
      "epoch": 2.2485333333333335,
      "grad_norm": 0.10376685112714767,
      "learning_rate": 2.1893333333333336e-05,
      "loss": 0.0054,
      "step": 42160
    },
    {
      "epoch": 2.2490666666666668,
      "grad_norm": 0.2526509761810303,
      "learning_rate": 2.1886666666666665e-05,
      "loss": 0.0026,
      "step": 42170
    },
    {
      "epoch": 2.2496,
      "grad_norm": 0.25787684321403503,
      "learning_rate": 2.188e-05,
      "loss": 0.0021,
      "step": 42180
    },
    {
      "epoch": 2.2501333333333333,
      "grad_norm": 0.7338590025901794,
      "learning_rate": 2.1873333333333336e-05,
      "loss": 0.0021,
      "step": 42190
    },
    {
      "epoch": 2.2506666666666666,
      "grad_norm": 0.4044937789440155,
      "learning_rate": 2.186666666666667e-05,
      "loss": 0.0021,
      "step": 42200
    },
    {
      "epoch": 2.2512,
      "grad_norm": 0.44077348709106445,
      "learning_rate": 2.186e-05,
      "loss": 0.003,
      "step": 42210
    },
    {
      "epoch": 2.251733333333333,
      "grad_norm": 0.220033198595047,
      "learning_rate": 2.1853333333333333e-05,
      "loss": 0.0019,
      "step": 42220
    },
    {
      "epoch": 2.2522666666666664,
      "grad_norm": 0.36525872349739075,
      "learning_rate": 2.184666666666667e-05,
      "loss": 0.0023,
      "step": 42230
    },
    {
      "epoch": 2.2528,
      "grad_norm": 0.368156373500824,
      "learning_rate": 2.184e-05,
      "loss": 0.002,
      "step": 42240
    },
    {
      "epoch": 2.2533333333333334,
      "grad_norm": 0.21836379170417786,
      "learning_rate": 2.1833333333333333e-05,
      "loss": 0.0024,
      "step": 42250
    },
    {
      "epoch": 2.2538666666666667,
      "grad_norm": 0.0879802331328392,
      "learning_rate": 2.182666666666667e-05,
      "loss": 0.0017,
      "step": 42260
    },
    {
      "epoch": 2.2544,
      "grad_norm": 0.4401406943798065,
      "learning_rate": 2.182e-05,
      "loss": 0.0019,
      "step": 42270
    },
    {
      "epoch": 2.2549333333333332,
      "grad_norm": 0.3941251039505005,
      "learning_rate": 2.1813333333333337e-05,
      "loss": 0.003,
      "step": 42280
    },
    {
      "epoch": 2.2554666666666665,
      "grad_norm": 0.13587641716003418,
      "learning_rate": 2.1806666666666666e-05,
      "loss": 0.004,
      "step": 42290
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.3244060277938843,
      "learning_rate": 2.18e-05,
      "loss": 0.0034,
      "step": 42300
    },
    {
      "epoch": 2.2565333333333335,
      "grad_norm": 0.6340371966362,
      "learning_rate": 2.1793333333333334e-05,
      "loss": 0.0036,
      "step": 42310
    },
    {
      "epoch": 2.2570666666666668,
      "grad_norm": 0.3215164542198181,
      "learning_rate": 2.1786666666666666e-05,
      "loss": 0.0018,
      "step": 42320
    },
    {
      "epoch": 2.2576,
      "grad_norm": 0.314896821975708,
      "learning_rate": 2.178e-05,
      "loss": 0.0025,
      "step": 42330
    },
    {
      "epoch": 2.2581333333333333,
      "grad_norm": 0.09030035138130188,
      "learning_rate": 2.1773333333333334e-05,
      "loss": 0.0019,
      "step": 42340
    },
    {
      "epoch": 2.2586666666666666,
      "grad_norm": 0.4259921908378601,
      "learning_rate": 2.176666666666667e-05,
      "loss": 0.0027,
      "step": 42350
    },
    {
      "epoch": 2.2592,
      "grad_norm": 0.27519017457962036,
      "learning_rate": 2.176e-05,
      "loss": 0.0035,
      "step": 42360
    },
    {
      "epoch": 2.259733333333333,
      "grad_norm": 0.1794288158416748,
      "learning_rate": 2.1753333333333334e-05,
      "loss": 0.0034,
      "step": 42370
    },
    {
      "epoch": 2.2602666666666664,
      "grad_norm": 0.16203118860721588,
      "learning_rate": 2.174666666666667e-05,
      "loss": 0.0022,
      "step": 42380
    },
    {
      "epoch": 2.2608,
      "grad_norm": 0.14541217684745789,
      "learning_rate": 2.1740000000000002e-05,
      "loss": 0.0027,
      "step": 42390
    },
    {
      "epoch": 2.2613333333333334,
      "grad_norm": 0.12166932970285416,
      "learning_rate": 2.1733333333333334e-05,
      "loss": 0.005,
      "step": 42400
    },
    {
      "epoch": 2.2618666666666667,
      "grad_norm": 0.1392882913351059,
      "learning_rate": 2.1726666666666666e-05,
      "loss": 0.0026,
      "step": 42410
    },
    {
      "epoch": 2.2624,
      "grad_norm": 0.0990431010723114,
      "learning_rate": 2.1720000000000002e-05,
      "loss": 0.0038,
      "step": 42420
    },
    {
      "epoch": 2.2629333333333332,
      "grad_norm": 0.5653038024902344,
      "learning_rate": 2.1713333333333334e-05,
      "loss": 0.0028,
      "step": 42430
    },
    {
      "epoch": 2.2634666666666665,
      "grad_norm": 0.39592617750167847,
      "learning_rate": 2.1706666666666667e-05,
      "loss": 0.0023,
      "step": 42440
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 0.8370068669319153,
      "learning_rate": 2.1700000000000002e-05,
      "loss": 0.0023,
      "step": 42450
    },
    {
      "epoch": 2.2645333333333335,
      "grad_norm": 0.3725288212299347,
      "learning_rate": 2.1693333333333335e-05,
      "loss": 0.0018,
      "step": 42460
    },
    {
      "epoch": 2.265066666666667,
      "grad_norm": 0.45694658160209656,
      "learning_rate": 2.168666666666667e-05,
      "loss": 0.003,
      "step": 42470
    },
    {
      "epoch": 2.2656,
      "grad_norm": 0.2692183256149292,
      "learning_rate": 2.168e-05,
      "loss": 0.0027,
      "step": 42480
    },
    {
      "epoch": 2.2661333333333333,
      "grad_norm": 0.5112624168395996,
      "learning_rate": 2.1673333333333335e-05,
      "loss": 0.0033,
      "step": 42490
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.6345463991165161,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 0.0028,
      "step": 42500
    },
    {
      "epoch": 2.2672,
      "grad_norm": 0.27123335003852844,
      "learning_rate": 2.166e-05,
      "loss": 0.0032,
      "step": 42510
    },
    {
      "epoch": 2.267733333333333,
      "grad_norm": 0.24170993268489838,
      "learning_rate": 2.1653333333333335e-05,
      "loss": 0.0026,
      "step": 42520
    },
    {
      "epoch": 2.2682666666666664,
      "grad_norm": 0.13264039158821106,
      "learning_rate": 2.1646666666666667e-05,
      "loss": 0.0033,
      "step": 42530
    },
    {
      "epoch": 2.2688,
      "grad_norm": 0.33608242869377136,
      "learning_rate": 2.1640000000000003e-05,
      "loss": 0.0025,
      "step": 42540
    },
    {
      "epoch": 2.2693333333333334,
      "grad_norm": 0.13475900888442993,
      "learning_rate": 2.1633333333333332e-05,
      "loss": 0.0021,
      "step": 42550
    },
    {
      "epoch": 2.2698666666666667,
      "grad_norm": 0.7530732154846191,
      "learning_rate": 2.1626666666666667e-05,
      "loss": 0.003,
      "step": 42560
    },
    {
      "epoch": 2.2704,
      "grad_norm": 0.646528422832489,
      "learning_rate": 2.162e-05,
      "loss": 0.0023,
      "step": 42570
    },
    {
      "epoch": 2.2709333333333332,
      "grad_norm": 0.11760817468166351,
      "learning_rate": 2.1613333333333335e-05,
      "loss": 0.0023,
      "step": 42580
    },
    {
      "epoch": 2.2714666666666665,
      "grad_norm": 0.20233270525932312,
      "learning_rate": 2.1606666666666668e-05,
      "loss": 0.0022,
      "step": 42590
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 0.4050235152244568,
      "learning_rate": 2.16e-05,
      "loss": 0.0019,
      "step": 42600
    },
    {
      "epoch": 2.2725333333333335,
      "grad_norm": 0.6829659938812256,
      "learning_rate": 2.1593333333333336e-05,
      "loss": 0.0018,
      "step": 42610
    },
    {
      "epoch": 2.273066666666667,
      "grad_norm": 0.11188427358865738,
      "learning_rate": 2.1586666666666668e-05,
      "loss": 0.0029,
      "step": 42620
    },
    {
      "epoch": 2.2736,
      "grad_norm": 0.4015181362628937,
      "learning_rate": 2.158e-05,
      "loss": 0.0017,
      "step": 42630
    },
    {
      "epoch": 2.2741333333333333,
      "grad_norm": 0.20267412066459656,
      "learning_rate": 2.1573333333333336e-05,
      "loss": 0.0021,
      "step": 42640
    },
    {
      "epoch": 2.2746666666666666,
      "grad_norm": 0.12403671443462372,
      "learning_rate": 2.1566666666666668e-05,
      "loss": 0.0018,
      "step": 42650
    },
    {
      "epoch": 2.2752,
      "grad_norm": 0.3194389343261719,
      "learning_rate": 2.1560000000000004e-05,
      "loss": 0.0023,
      "step": 42660
    },
    {
      "epoch": 2.275733333333333,
      "grad_norm": 0.17118848860263824,
      "learning_rate": 2.1553333333333333e-05,
      "loss": 0.002,
      "step": 42670
    },
    {
      "epoch": 2.2762666666666664,
      "grad_norm": 0.34106090664863586,
      "learning_rate": 2.1546666666666668e-05,
      "loss": 0.0018,
      "step": 42680
    },
    {
      "epoch": 2.2768,
      "grad_norm": 0.38413870334625244,
      "learning_rate": 2.154e-05,
      "loss": 0.0019,
      "step": 42690
    },
    {
      "epoch": 2.2773333333333334,
      "grad_norm": 0.385479599237442,
      "learning_rate": 2.1533333333333333e-05,
      "loss": 0.0028,
      "step": 42700
    },
    {
      "epoch": 2.2778666666666667,
      "grad_norm": 0.07402095198631287,
      "learning_rate": 2.152666666666667e-05,
      "loss": 0.004,
      "step": 42710
    },
    {
      "epoch": 2.2784,
      "grad_norm": 0.41952353715896606,
      "learning_rate": 2.152e-05,
      "loss": 0.0022,
      "step": 42720
    },
    {
      "epoch": 2.2789333333333333,
      "grad_norm": 0.425558477640152,
      "learning_rate": 2.1513333333333336e-05,
      "loss": 0.0025,
      "step": 42730
    },
    {
      "epoch": 2.2794666666666665,
      "grad_norm": 0.6032502055168152,
      "learning_rate": 2.1506666666666665e-05,
      "loss": 0.0021,
      "step": 42740
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.6559741497039795,
      "learning_rate": 2.15e-05,
      "loss": 0.0031,
      "step": 42750
    },
    {
      "epoch": 2.2805333333333335,
      "grad_norm": 0.1589532047510147,
      "learning_rate": 2.1493333333333333e-05,
      "loss": 0.0033,
      "step": 42760
    },
    {
      "epoch": 2.281066666666667,
      "grad_norm": 0.2426522672176361,
      "learning_rate": 2.148666666666667e-05,
      "loss": 0.0047,
      "step": 42770
    },
    {
      "epoch": 2.2816,
      "grad_norm": 0.3638922870159149,
      "learning_rate": 2.148e-05,
      "loss": 0.0025,
      "step": 42780
    },
    {
      "epoch": 2.2821333333333333,
      "grad_norm": 0.1970897912979126,
      "learning_rate": 2.1473333333333333e-05,
      "loss": 0.0031,
      "step": 42790
    },
    {
      "epoch": 2.2826666666666666,
      "grad_norm": 0.6357124447822571,
      "learning_rate": 2.146666666666667e-05,
      "loss": 0.0047,
      "step": 42800
    },
    {
      "epoch": 2.2832,
      "grad_norm": 0.11290018260478973,
      "learning_rate": 2.146e-05,
      "loss": 0.0026,
      "step": 42810
    },
    {
      "epoch": 2.283733333333333,
      "grad_norm": 0.4318348467350006,
      "learning_rate": 2.1453333333333333e-05,
      "loss": 0.0027,
      "step": 42820
    },
    {
      "epoch": 2.2842666666666664,
      "grad_norm": 0.29511559009552,
      "learning_rate": 2.144666666666667e-05,
      "loss": 0.0028,
      "step": 42830
    },
    {
      "epoch": 2.2848,
      "grad_norm": 0.6349931359291077,
      "learning_rate": 2.144e-05,
      "loss": 0.0026,
      "step": 42840
    },
    {
      "epoch": 2.2853333333333334,
      "grad_norm": 0.36430883407592773,
      "learning_rate": 2.1433333333333334e-05,
      "loss": 0.0023,
      "step": 42850
    },
    {
      "epoch": 2.2858666666666667,
      "grad_norm": 0.5372632741928101,
      "learning_rate": 2.1426666666666666e-05,
      "loss": 0.0027,
      "step": 42860
    },
    {
      "epoch": 2.2864,
      "grad_norm": 0.3615688979625702,
      "learning_rate": 2.142e-05,
      "loss": 0.0029,
      "step": 42870
    },
    {
      "epoch": 2.2869333333333333,
      "grad_norm": 0.09325319528579712,
      "learning_rate": 2.1413333333333334e-05,
      "loss": 0.0028,
      "step": 42880
    },
    {
      "epoch": 2.2874666666666665,
      "grad_norm": 0.4750123620033264,
      "learning_rate": 2.1406666666666666e-05,
      "loss": 0.0023,
      "step": 42890
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.5739715695381165,
      "learning_rate": 2.1400000000000002e-05,
      "loss": 0.0029,
      "step": 42900
    },
    {
      "epoch": 2.2885333333333335,
      "grad_norm": 0.12501414120197296,
      "learning_rate": 2.1393333333333334e-05,
      "loss": 0.0019,
      "step": 42910
    },
    {
      "epoch": 2.289066666666667,
      "grad_norm": 0.37959858775138855,
      "learning_rate": 2.138666666666667e-05,
      "loss": 0.0018,
      "step": 42920
    },
    {
      "epoch": 2.2896,
      "grad_norm": 0.297607958316803,
      "learning_rate": 2.138e-05,
      "loss": 0.0023,
      "step": 42930
    },
    {
      "epoch": 2.2901333333333334,
      "grad_norm": 0.5737553238868713,
      "learning_rate": 2.1373333333333334e-05,
      "loss": 0.0028,
      "step": 42940
    },
    {
      "epoch": 2.2906666666666666,
      "grad_norm": 0.4662724733352661,
      "learning_rate": 2.1366666666666667e-05,
      "loss": 0.0029,
      "step": 42950
    },
    {
      "epoch": 2.2912,
      "grad_norm": 0.19440321624279022,
      "learning_rate": 2.1360000000000002e-05,
      "loss": 0.0019,
      "step": 42960
    },
    {
      "epoch": 2.291733333333333,
      "grad_norm": 0.10577835887670517,
      "learning_rate": 2.1353333333333334e-05,
      "loss": 0.0021,
      "step": 42970
    },
    {
      "epoch": 2.2922666666666665,
      "grad_norm": 0.1522737592458725,
      "learning_rate": 2.1346666666666667e-05,
      "loss": 0.0018,
      "step": 42980
    },
    {
      "epoch": 2.2928,
      "grad_norm": 0.17501479387283325,
      "learning_rate": 2.1340000000000002e-05,
      "loss": 0.0021,
      "step": 42990
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 0.3099152147769928,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 0.0029,
      "step": 43000
    },
    {
      "epoch": 2.2938666666666667,
      "grad_norm": 0.33398136496543884,
      "learning_rate": 2.1326666666666667e-05,
      "loss": 0.0028,
      "step": 43010
    },
    {
      "epoch": 2.2944,
      "grad_norm": 0.17502033710479736,
      "learning_rate": 2.1320000000000003e-05,
      "loss": 0.002,
      "step": 43020
    },
    {
      "epoch": 2.2949333333333333,
      "grad_norm": 0.557141125202179,
      "learning_rate": 2.1313333333333335e-05,
      "loss": 0.0021,
      "step": 43030
    },
    {
      "epoch": 2.2954666666666665,
      "grad_norm": 0.5317451357841492,
      "learning_rate": 2.1306666666666667e-05,
      "loss": 0.0036,
      "step": 43040
    },
    {
      "epoch": 2.296,
      "grad_norm": 0.4043377935886383,
      "learning_rate": 2.13e-05,
      "loss": 0.0034,
      "step": 43050
    },
    {
      "epoch": 2.2965333333333335,
      "grad_norm": 0.2684434652328491,
      "learning_rate": 2.1293333333333335e-05,
      "loss": 0.0022,
      "step": 43060
    },
    {
      "epoch": 2.297066666666667,
      "grad_norm": 0.4320951998233795,
      "learning_rate": 2.1286666666666667e-05,
      "loss": 0.0032,
      "step": 43070
    },
    {
      "epoch": 2.2976,
      "grad_norm": 0.3226300776004791,
      "learning_rate": 2.128e-05,
      "loss": 0.002,
      "step": 43080
    },
    {
      "epoch": 2.2981333333333334,
      "grad_norm": 0.16992828249931335,
      "learning_rate": 2.1273333333333335e-05,
      "loss": 0.003,
      "step": 43090
    },
    {
      "epoch": 2.2986666666666666,
      "grad_norm": 0.19626066088676453,
      "learning_rate": 2.1266666666666667e-05,
      "loss": 0.002,
      "step": 43100
    },
    {
      "epoch": 2.2992,
      "grad_norm": 0.2384946197271347,
      "learning_rate": 2.1260000000000003e-05,
      "loss": 0.0025,
      "step": 43110
    },
    {
      "epoch": 2.299733333333333,
      "grad_norm": 0.40139463543891907,
      "learning_rate": 2.1253333333333332e-05,
      "loss": 0.0028,
      "step": 43120
    },
    {
      "epoch": 2.3002666666666665,
      "grad_norm": 0.12683936953544617,
      "learning_rate": 2.1246666666666668e-05,
      "loss": 0.0025,
      "step": 43130
    },
    {
      "epoch": 2.3008,
      "grad_norm": 0.4462811350822449,
      "learning_rate": 2.124e-05,
      "loss": 0.0023,
      "step": 43140
    },
    {
      "epoch": 2.3013333333333335,
      "grad_norm": 0.127161905169487,
      "learning_rate": 2.1233333333333336e-05,
      "loss": 0.0027,
      "step": 43150
    },
    {
      "epoch": 2.3018666666666667,
      "grad_norm": 0.2706219255924225,
      "learning_rate": 2.1226666666666668e-05,
      "loss": 0.004,
      "step": 43160
    },
    {
      "epoch": 2.3024,
      "grad_norm": 0.22522202134132385,
      "learning_rate": 2.122e-05,
      "loss": 0.0038,
      "step": 43170
    },
    {
      "epoch": 2.3029333333333333,
      "grad_norm": 0.0928061231970787,
      "learning_rate": 2.1213333333333336e-05,
      "loss": 0.0017,
      "step": 43180
    },
    {
      "epoch": 2.3034666666666666,
      "grad_norm": 0.3570053279399872,
      "learning_rate": 2.1206666666666665e-05,
      "loss": 0.0024,
      "step": 43190
    },
    {
      "epoch": 2.304,
      "grad_norm": 0.29964426159858704,
      "learning_rate": 2.12e-05,
      "loss": 0.0019,
      "step": 43200
    },
    {
      "epoch": 2.3045333333333335,
      "grad_norm": 0.28925368189811707,
      "learning_rate": 2.1193333333333336e-05,
      "loss": 0.0028,
      "step": 43210
    },
    {
      "epoch": 2.305066666666667,
      "grad_norm": 0.13483861088752747,
      "learning_rate": 2.1186666666666668e-05,
      "loss": 0.0021,
      "step": 43220
    },
    {
      "epoch": 2.3056,
      "grad_norm": 0.1554626226425171,
      "learning_rate": 2.118e-05,
      "loss": 0.0019,
      "step": 43230
    },
    {
      "epoch": 2.3061333333333334,
      "grad_norm": 0.3583104908466339,
      "learning_rate": 2.1173333333333333e-05,
      "loss": 0.0019,
      "step": 43240
    },
    {
      "epoch": 2.3066666666666666,
      "grad_norm": 0.4592694342136383,
      "learning_rate": 2.116666666666667e-05,
      "loss": 0.003,
      "step": 43250
    },
    {
      "epoch": 2.3072,
      "grad_norm": 0.4785846173763275,
      "learning_rate": 2.116e-05,
      "loss": 0.0027,
      "step": 43260
    },
    {
      "epoch": 2.307733333333333,
      "grad_norm": 0.4922933280467987,
      "learning_rate": 2.1153333333333333e-05,
      "loss": 0.0021,
      "step": 43270
    },
    {
      "epoch": 2.3082666666666665,
      "grad_norm": 0.22870635986328125,
      "learning_rate": 2.114666666666667e-05,
      "loss": 0.002,
      "step": 43280
    },
    {
      "epoch": 2.3088,
      "grad_norm": 0.0796300619840622,
      "learning_rate": 2.114e-05,
      "loss": 0.0016,
      "step": 43290
    },
    {
      "epoch": 2.3093333333333335,
      "grad_norm": 0.05512407422065735,
      "learning_rate": 2.1133333333333337e-05,
      "loss": 0.0032,
      "step": 43300
    },
    {
      "epoch": 2.3098666666666667,
      "grad_norm": 0.471121609210968,
      "learning_rate": 2.1126666666666665e-05,
      "loss": 0.0026,
      "step": 43310
    },
    {
      "epoch": 2.3104,
      "grad_norm": 0.15240752696990967,
      "learning_rate": 2.112e-05,
      "loss": 0.0017,
      "step": 43320
    },
    {
      "epoch": 2.3109333333333333,
      "grad_norm": 0.13478416204452515,
      "learning_rate": 2.1113333333333333e-05,
      "loss": 0.0019,
      "step": 43330
    },
    {
      "epoch": 2.3114666666666666,
      "grad_norm": 0.19614538550376892,
      "learning_rate": 2.110666666666667e-05,
      "loss": 0.0028,
      "step": 43340
    },
    {
      "epoch": 2.312,
      "grad_norm": 0.28661608695983887,
      "learning_rate": 2.11e-05,
      "loss": 0.0023,
      "step": 43350
    },
    {
      "epoch": 2.3125333333333336,
      "grad_norm": 0.2833132743835449,
      "learning_rate": 2.1093333333333334e-05,
      "loss": 0.0022,
      "step": 43360
    },
    {
      "epoch": 2.313066666666667,
      "grad_norm": 0.40263208746910095,
      "learning_rate": 2.108666666666667e-05,
      "loss": 0.0021,
      "step": 43370
    },
    {
      "epoch": 2.3136,
      "grad_norm": 0.2583308219909668,
      "learning_rate": 2.1079999999999998e-05,
      "loss": 0.0021,
      "step": 43380
    },
    {
      "epoch": 2.3141333333333334,
      "grad_norm": 0.07148203998804092,
      "learning_rate": 2.1073333333333334e-05,
      "loss": 0.002,
      "step": 43390
    },
    {
      "epoch": 2.3146666666666667,
      "grad_norm": 0.32929423451423645,
      "learning_rate": 2.106666666666667e-05,
      "loss": 0.0026,
      "step": 43400
    },
    {
      "epoch": 2.3152,
      "grad_norm": 0.18873287737369537,
      "learning_rate": 2.106e-05,
      "loss": 0.0032,
      "step": 43410
    },
    {
      "epoch": 2.315733333333333,
      "grad_norm": 0.1264164000749588,
      "learning_rate": 2.1053333333333334e-05,
      "loss": 0.0022,
      "step": 43420
    },
    {
      "epoch": 2.3162666666666665,
      "grad_norm": 0.19984275102615356,
      "learning_rate": 2.1046666666666666e-05,
      "loss": 0.0029,
      "step": 43430
    },
    {
      "epoch": 2.3168,
      "grad_norm": 0.40503165125846863,
      "learning_rate": 2.1040000000000002e-05,
      "loss": 0.0041,
      "step": 43440
    },
    {
      "epoch": 2.3173333333333335,
      "grad_norm": 0.373196005821228,
      "learning_rate": 2.1033333333333334e-05,
      "loss": 0.0025,
      "step": 43450
    },
    {
      "epoch": 2.3178666666666667,
      "grad_norm": 0.09632466733455658,
      "learning_rate": 2.1026666666666666e-05,
      "loss": 0.0037,
      "step": 43460
    },
    {
      "epoch": 2.3184,
      "grad_norm": 0.16645732522010803,
      "learning_rate": 2.1020000000000002e-05,
      "loss": 0.003,
      "step": 43470
    },
    {
      "epoch": 2.3189333333333333,
      "grad_norm": 0.10704931616783142,
      "learning_rate": 2.1013333333333334e-05,
      "loss": 0.0024,
      "step": 43480
    },
    {
      "epoch": 2.3194666666666666,
      "grad_norm": 0.19426603615283966,
      "learning_rate": 2.100666666666667e-05,
      "loss": 0.0028,
      "step": 43490
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.33084020018577576,
      "learning_rate": 2.1e-05,
      "loss": 0.0033,
      "step": 43500
    },
    {
      "epoch": 2.3205333333333336,
      "grad_norm": 0.15987712144851685,
      "learning_rate": 2.0993333333333334e-05,
      "loss": 0.0018,
      "step": 43510
    },
    {
      "epoch": 2.321066666666667,
      "grad_norm": 0.49686896800994873,
      "learning_rate": 2.0986666666666667e-05,
      "loss": 0.0022,
      "step": 43520
    },
    {
      "epoch": 2.3216,
      "grad_norm": 0.145940899848938,
      "learning_rate": 2.098e-05,
      "loss": 0.0022,
      "step": 43530
    },
    {
      "epoch": 2.3221333333333334,
      "grad_norm": 0.15035861730575562,
      "learning_rate": 2.0973333333333335e-05,
      "loss": 0.0027,
      "step": 43540
    },
    {
      "epoch": 2.3226666666666667,
      "grad_norm": 0.29329800605773926,
      "learning_rate": 2.0966666666666667e-05,
      "loss": 0.0028,
      "step": 43550
    },
    {
      "epoch": 2.3232,
      "grad_norm": 0.12293028086423874,
      "learning_rate": 2.0960000000000003e-05,
      "loss": 0.0019,
      "step": 43560
    },
    {
      "epoch": 2.323733333333333,
      "grad_norm": 0.19502457976341248,
      "learning_rate": 2.095333333333333e-05,
      "loss": 0.004,
      "step": 43570
    },
    {
      "epoch": 2.3242666666666665,
      "grad_norm": 0.25570863485336304,
      "learning_rate": 2.0946666666666667e-05,
      "loss": 0.0029,
      "step": 43580
    },
    {
      "epoch": 2.3247999999999998,
      "grad_norm": 0.4223752021789551,
      "learning_rate": 2.0940000000000003e-05,
      "loss": 0.0023,
      "step": 43590
    },
    {
      "epoch": 2.3253333333333335,
      "grad_norm": 0.2410869151353836,
      "learning_rate": 2.0933333333333335e-05,
      "loss": 0.0019,
      "step": 43600
    },
    {
      "epoch": 2.3258666666666667,
      "grad_norm": 0.3386983871459961,
      "learning_rate": 2.0926666666666667e-05,
      "loss": 0.0026,
      "step": 43610
    },
    {
      "epoch": 2.3264,
      "grad_norm": 0.3994039297103882,
      "learning_rate": 2.092e-05,
      "loss": 0.0036,
      "step": 43620
    },
    {
      "epoch": 2.3269333333333333,
      "grad_norm": 0.4178535044193268,
      "learning_rate": 2.0913333333333335e-05,
      "loss": 0.0021,
      "step": 43630
    },
    {
      "epoch": 2.3274666666666666,
      "grad_norm": 0.37185952067375183,
      "learning_rate": 2.0906666666666668e-05,
      "loss": 0.0032,
      "step": 43640
    },
    {
      "epoch": 2.328,
      "grad_norm": 0.532880961894989,
      "learning_rate": 2.09e-05,
      "loss": 0.0028,
      "step": 43650
    },
    {
      "epoch": 2.3285333333333336,
      "grad_norm": 0.31404566764831543,
      "learning_rate": 2.0893333333333335e-05,
      "loss": 0.0016,
      "step": 43660
    },
    {
      "epoch": 2.329066666666667,
      "grad_norm": 0.5616986751556396,
      "learning_rate": 2.0886666666666668e-05,
      "loss": 0.0019,
      "step": 43670
    },
    {
      "epoch": 2.3296,
      "grad_norm": 0.12215947359800339,
      "learning_rate": 2.0880000000000003e-05,
      "loss": 0.0017,
      "step": 43680
    },
    {
      "epoch": 2.3301333333333334,
      "grad_norm": 0.5533604025840759,
      "learning_rate": 2.0873333333333332e-05,
      "loss": 0.0025,
      "step": 43690
    },
    {
      "epoch": 2.3306666666666667,
      "grad_norm": 0.5589070916175842,
      "learning_rate": 2.0866666666666668e-05,
      "loss": 0.0024,
      "step": 43700
    },
    {
      "epoch": 2.3312,
      "grad_norm": 0.040418848395347595,
      "learning_rate": 2.086e-05,
      "loss": 0.0036,
      "step": 43710
    },
    {
      "epoch": 2.331733333333333,
      "grad_norm": 0.30966266989707947,
      "learning_rate": 2.0853333333333332e-05,
      "loss": 0.0019,
      "step": 43720
    },
    {
      "epoch": 2.3322666666666665,
      "grad_norm": 0.447587251663208,
      "learning_rate": 2.0846666666666668e-05,
      "loss": 0.0027,
      "step": 43730
    },
    {
      "epoch": 2.3327999999999998,
      "grad_norm": 0.40547648072242737,
      "learning_rate": 2.084e-05,
      "loss": 0.0017,
      "step": 43740
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.3760773837566376,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 0.0021,
      "step": 43750
    },
    {
      "epoch": 2.3338666666666668,
      "grad_norm": 0.18574300408363342,
      "learning_rate": 2.0826666666666665e-05,
      "loss": 0.0025,
      "step": 43760
    },
    {
      "epoch": 2.3344,
      "grad_norm": 0.3135085105895996,
      "learning_rate": 2.082e-05,
      "loss": 0.0027,
      "step": 43770
    },
    {
      "epoch": 2.3349333333333333,
      "grad_norm": 0.12676863372325897,
      "learning_rate": 2.0813333333333336e-05,
      "loss": 0.0026,
      "step": 43780
    },
    {
      "epoch": 2.3354666666666666,
      "grad_norm": 0.1210915744304657,
      "learning_rate": 2.080666666666667e-05,
      "loss": 0.0029,
      "step": 43790
    },
    {
      "epoch": 2.336,
      "grad_norm": 0.27494314312934875,
      "learning_rate": 2.08e-05,
      "loss": 0.0025,
      "step": 43800
    },
    {
      "epoch": 2.3365333333333336,
      "grad_norm": 0.5083160400390625,
      "learning_rate": 2.0793333333333333e-05,
      "loss": 0.0019,
      "step": 43810
    },
    {
      "epoch": 2.337066666666667,
      "grad_norm": 0.1468822956085205,
      "learning_rate": 2.078666666666667e-05,
      "loss": 0.0022,
      "step": 43820
    },
    {
      "epoch": 2.3376,
      "grad_norm": 0.4445256292819977,
      "learning_rate": 2.078e-05,
      "loss": 0.0026,
      "step": 43830
    },
    {
      "epoch": 2.3381333333333334,
      "grad_norm": 0.08543116599321365,
      "learning_rate": 2.0773333333333333e-05,
      "loss": 0.003,
      "step": 43840
    },
    {
      "epoch": 2.3386666666666667,
      "grad_norm": 0.0707164853811264,
      "learning_rate": 2.076666666666667e-05,
      "loss": 0.002,
      "step": 43850
    },
    {
      "epoch": 2.3392,
      "grad_norm": 0.4662210941314697,
      "learning_rate": 2.076e-05,
      "loss": 0.0026,
      "step": 43860
    },
    {
      "epoch": 2.339733333333333,
      "grad_norm": 0.267553448677063,
      "learning_rate": 2.0753333333333333e-05,
      "loss": 0.0032,
      "step": 43870
    },
    {
      "epoch": 2.3402666666666665,
      "grad_norm": 0.45581263303756714,
      "learning_rate": 2.0746666666666666e-05,
      "loss": 0.0022,
      "step": 43880
    },
    {
      "epoch": 2.3407999999999998,
      "grad_norm": 0.1438659429550171,
      "learning_rate": 2.074e-05,
      "loss": 0.0032,
      "step": 43890
    },
    {
      "epoch": 2.3413333333333335,
      "grad_norm": 0.27960845828056335,
      "learning_rate": 2.0733333333333334e-05,
      "loss": 0.0025,
      "step": 43900
    },
    {
      "epoch": 2.3418666666666668,
      "grad_norm": 0.14707867801189423,
      "learning_rate": 2.0726666666666666e-05,
      "loss": 0.0025,
      "step": 43910
    },
    {
      "epoch": 2.3424,
      "grad_norm": 0.33857420086860657,
      "learning_rate": 2.072e-05,
      "loss": 0.0026,
      "step": 43920
    },
    {
      "epoch": 2.3429333333333333,
      "grad_norm": 0.2580678462982178,
      "learning_rate": 2.0713333333333334e-05,
      "loss": 0.0024,
      "step": 43930
    },
    {
      "epoch": 2.3434666666666666,
      "grad_norm": 0.08839332312345505,
      "learning_rate": 2.070666666666667e-05,
      "loss": 0.0033,
      "step": 43940
    },
    {
      "epoch": 2.344,
      "grad_norm": 0.2222720831632614,
      "learning_rate": 2.07e-05,
      "loss": 0.003,
      "step": 43950
    },
    {
      "epoch": 2.3445333333333336,
      "grad_norm": 0.21019767224788666,
      "learning_rate": 2.0693333333333334e-05,
      "loss": 0.0019,
      "step": 43960
    },
    {
      "epoch": 2.345066666666667,
      "grad_norm": 0.37268444895744324,
      "learning_rate": 2.068666666666667e-05,
      "loss": 0.0023,
      "step": 43970
    },
    {
      "epoch": 2.3456,
      "grad_norm": 0.5663424134254456,
      "learning_rate": 2.0680000000000002e-05,
      "loss": 0.0016,
      "step": 43980
    },
    {
      "epoch": 2.3461333333333334,
      "grad_norm": 0.08479513227939606,
      "learning_rate": 2.0673333333333334e-05,
      "loss": 0.0029,
      "step": 43990
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 0.21536089479923248,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 0.002,
      "step": 44000
    },
    {
      "epoch": 2.3472,
      "grad_norm": 0.09460907429456711,
      "learning_rate": 2.0660000000000002e-05,
      "loss": 0.0034,
      "step": 44010
    },
    {
      "epoch": 2.3477333333333332,
      "grad_norm": 0.08869537711143494,
      "learning_rate": 2.0653333333333334e-05,
      "loss": 0.0027,
      "step": 44020
    },
    {
      "epoch": 2.3482666666666665,
      "grad_norm": 0.2510916292667389,
      "learning_rate": 2.0646666666666667e-05,
      "loss": 0.0024,
      "step": 44030
    },
    {
      "epoch": 2.3487999999999998,
      "grad_norm": 0.5071398615837097,
      "learning_rate": 2.0640000000000002e-05,
      "loss": 0.0021,
      "step": 44040
    },
    {
      "epoch": 2.3493333333333335,
      "grad_norm": 0.19275635480880737,
      "learning_rate": 2.0633333333333335e-05,
      "loss": 0.0025,
      "step": 44050
    },
    {
      "epoch": 2.3498666666666668,
      "grad_norm": 0.28669047355651855,
      "learning_rate": 2.0626666666666667e-05,
      "loss": 0.0029,
      "step": 44060
    },
    {
      "epoch": 2.3504,
      "grad_norm": 0.143628790974617,
      "learning_rate": 2.062e-05,
      "loss": 0.0025,
      "step": 44070
    },
    {
      "epoch": 2.3509333333333333,
      "grad_norm": 0.06289031356573105,
      "learning_rate": 2.0613333333333335e-05,
      "loss": 0.0027,
      "step": 44080
    },
    {
      "epoch": 2.3514666666666666,
      "grad_norm": 0.670457124710083,
      "learning_rate": 2.0606666666666667e-05,
      "loss": 0.0033,
      "step": 44090
    },
    {
      "epoch": 2.352,
      "grad_norm": 0.15374673902988434,
      "learning_rate": 2.06e-05,
      "loss": 0.002,
      "step": 44100
    },
    {
      "epoch": 2.352533333333333,
      "grad_norm": 0.26180562376976013,
      "learning_rate": 2.0593333333333335e-05,
      "loss": 0.0029,
      "step": 44110
    },
    {
      "epoch": 2.353066666666667,
      "grad_norm": 0.3443273603916168,
      "learning_rate": 2.0586666666666667e-05,
      "loss": 0.0037,
      "step": 44120
    },
    {
      "epoch": 2.3536,
      "grad_norm": 0.15244172513484955,
      "learning_rate": 2.0580000000000003e-05,
      "loss": 0.0021,
      "step": 44130
    },
    {
      "epoch": 2.3541333333333334,
      "grad_norm": 0.3566964268684387,
      "learning_rate": 2.0573333333333332e-05,
      "loss": 0.0021,
      "step": 44140
    },
    {
      "epoch": 2.3546666666666667,
      "grad_norm": 0.19283804297447205,
      "learning_rate": 2.0566666666666667e-05,
      "loss": 0.002,
      "step": 44150
    },
    {
      "epoch": 2.3552,
      "grad_norm": 0.10146436840295792,
      "learning_rate": 2.0560000000000003e-05,
      "loss": 0.0021,
      "step": 44160
    },
    {
      "epoch": 2.3557333333333332,
      "grad_norm": 0.08775462210178375,
      "learning_rate": 2.0553333333333335e-05,
      "loss": 0.0021,
      "step": 44170
    },
    {
      "epoch": 2.3562666666666665,
      "grad_norm": 0.48552677035331726,
      "learning_rate": 2.0546666666666668e-05,
      "loss": 0.0028,
      "step": 44180
    },
    {
      "epoch": 2.3568,
      "grad_norm": 0.9336785078048706,
      "learning_rate": 2.054e-05,
      "loss": 0.0033,
      "step": 44190
    },
    {
      "epoch": 2.3573333333333335,
      "grad_norm": 0.2563968896865845,
      "learning_rate": 2.0533333333333336e-05,
      "loss": 0.002,
      "step": 44200
    },
    {
      "epoch": 2.3578666666666668,
      "grad_norm": 0.46158236265182495,
      "learning_rate": 2.0526666666666668e-05,
      "loss": 0.0027,
      "step": 44210
    },
    {
      "epoch": 2.3584,
      "grad_norm": 0.0944032147526741,
      "learning_rate": 2.052e-05,
      "loss": 0.0023,
      "step": 44220
    },
    {
      "epoch": 2.3589333333333333,
      "grad_norm": 0.3139764964580536,
      "learning_rate": 2.0513333333333336e-05,
      "loss": 0.0025,
      "step": 44230
    },
    {
      "epoch": 2.3594666666666666,
      "grad_norm": 0.31817495822906494,
      "learning_rate": 2.0506666666666668e-05,
      "loss": 0.002,
      "step": 44240
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.3353452682495117,
      "learning_rate": 2.05e-05,
      "loss": 0.0021,
      "step": 44250
    },
    {
      "epoch": 2.360533333333333,
      "grad_norm": 0.44226711988449097,
      "learning_rate": 2.0493333333333333e-05,
      "loss": 0.0021,
      "step": 44260
    },
    {
      "epoch": 2.361066666666667,
      "grad_norm": 0.2280493527650833,
      "learning_rate": 2.0486666666666668e-05,
      "loss": 0.0033,
      "step": 44270
    },
    {
      "epoch": 2.3616,
      "grad_norm": 0.5306673645973206,
      "learning_rate": 2.048e-05,
      "loss": 0.0021,
      "step": 44280
    },
    {
      "epoch": 2.3621333333333334,
      "grad_norm": 0.11987843364477158,
      "learning_rate": 2.0473333333333333e-05,
      "loss": 0.0025,
      "step": 44290
    },
    {
      "epoch": 2.3626666666666667,
      "grad_norm": 0.05748606100678444,
      "learning_rate": 2.046666666666667e-05,
      "loss": 0.0018,
      "step": 44300
    },
    {
      "epoch": 2.3632,
      "grad_norm": 0.3322405219078064,
      "learning_rate": 2.046e-05,
      "loss": 0.0024,
      "step": 44310
    },
    {
      "epoch": 2.3637333333333332,
      "grad_norm": 0.15710507333278656,
      "learning_rate": 2.0453333333333336e-05,
      "loss": 0.0033,
      "step": 44320
    },
    {
      "epoch": 2.3642666666666665,
      "grad_norm": 0.16696617007255554,
      "learning_rate": 2.0446666666666665e-05,
      "loss": 0.0027,
      "step": 44330
    },
    {
      "epoch": 2.3648,
      "grad_norm": 0.6516968011856079,
      "learning_rate": 2.044e-05,
      "loss": 0.0024,
      "step": 44340
    },
    {
      "epoch": 2.3653333333333335,
      "grad_norm": 0.30750808119773865,
      "learning_rate": 2.0433333333333336e-05,
      "loss": 0.002,
      "step": 44350
    },
    {
      "epoch": 2.365866666666667,
      "grad_norm": 0.20893782377243042,
      "learning_rate": 2.042666666666667e-05,
      "loss": 0.0027,
      "step": 44360
    },
    {
      "epoch": 2.3664,
      "grad_norm": 0.06531746685504913,
      "learning_rate": 2.042e-05,
      "loss": 0.0021,
      "step": 44370
    },
    {
      "epoch": 2.3669333333333333,
      "grad_norm": 0.3429420292377472,
      "learning_rate": 2.0413333333333333e-05,
      "loss": 0.0033,
      "step": 44380
    },
    {
      "epoch": 2.3674666666666666,
      "grad_norm": 0.12273448705673218,
      "learning_rate": 2.040666666666667e-05,
      "loss": 0.0021,
      "step": 44390
    },
    {
      "epoch": 2.368,
      "grad_norm": 0.27716541290283203,
      "learning_rate": 2.04e-05,
      "loss": 0.0022,
      "step": 44400
    },
    {
      "epoch": 2.368533333333333,
      "grad_norm": 0.26519089937210083,
      "learning_rate": 2.0393333333333333e-05,
      "loss": 0.0028,
      "step": 44410
    },
    {
      "epoch": 2.369066666666667,
      "grad_norm": 0.4582870602607727,
      "learning_rate": 2.038666666666667e-05,
      "loss": 0.0035,
      "step": 44420
    },
    {
      "epoch": 2.3696,
      "grad_norm": 0.39017537236213684,
      "learning_rate": 2.038e-05,
      "loss": 0.0028,
      "step": 44430
    },
    {
      "epoch": 2.3701333333333334,
      "grad_norm": 0.19492265582084656,
      "learning_rate": 2.0373333333333334e-05,
      "loss": 0.0021,
      "step": 44440
    },
    {
      "epoch": 2.3706666666666667,
      "grad_norm": 0.2671552002429962,
      "learning_rate": 2.0366666666666666e-05,
      "loss": 0.0029,
      "step": 44450
    },
    {
      "epoch": 2.3712,
      "grad_norm": 0.41004541516304016,
      "learning_rate": 2.036e-05,
      "loss": 0.0028,
      "step": 44460
    },
    {
      "epoch": 2.3717333333333332,
      "grad_norm": 0.3369942605495453,
      "learning_rate": 2.0353333333333334e-05,
      "loss": 0.0031,
      "step": 44470
    },
    {
      "epoch": 2.3722666666666665,
      "grad_norm": 0.1449548751115799,
      "learning_rate": 2.0346666666666666e-05,
      "loss": 0.0028,
      "step": 44480
    },
    {
      "epoch": 2.3728,
      "grad_norm": 0.07101326435804367,
      "learning_rate": 2.0340000000000002e-05,
      "loss": 0.0021,
      "step": 44490
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 0.39772745966911316,
      "learning_rate": 2.0333333333333334e-05,
      "loss": 0.002,
      "step": 44500
    },
    {
      "epoch": 2.373866666666667,
      "grad_norm": 0.14392238855361938,
      "learning_rate": 2.032666666666667e-05,
      "loss": 0.0032,
      "step": 44510
    },
    {
      "epoch": 2.3744,
      "grad_norm": 0.12617099285125732,
      "learning_rate": 2.032e-05,
      "loss": 0.0031,
      "step": 44520
    },
    {
      "epoch": 2.3749333333333333,
      "grad_norm": 0.14357542991638184,
      "learning_rate": 2.0313333333333334e-05,
      "loss": 0.0021,
      "step": 44530
    },
    {
      "epoch": 2.3754666666666666,
      "grad_norm": 0.10991807281970978,
      "learning_rate": 2.030666666666667e-05,
      "loss": 0.0027,
      "step": 44540
    },
    {
      "epoch": 2.376,
      "grad_norm": 0.2679885923862457,
      "learning_rate": 2.0300000000000002e-05,
      "loss": 0.0034,
      "step": 44550
    },
    {
      "epoch": 2.376533333333333,
      "grad_norm": 0.7386162281036377,
      "learning_rate": 2.0293333333333334e-05,
      "loss": 0.0023,
      "step": 44560
    },
    {
      "epoch": 2.377066666666667,
      "grad_norm": 0.5962510108947754,
      "learning_rate": 2.0286666666666667e-05,
      "loss": 0.0027,
      "step": 44570
    },
    {
      "epoch": 2.3776,
      "grad_norm": 0.4071663022041321,
      "learning_rate": 2.0280000000000002e-05,
      "loss": 0.0021,
      "step": 44580
    },
    {
      "epoch": 2.3781333333333334,
      "grad_norm": 0.24143171310424805,
      "learning_rate": 2.0273333333333335e-05,
      "loss": 0.0035,
      "step": 44590
    },
    {
      "epoch": 2.3786666666666667,
      "grad_norm": 0.5371053218841553,
      "learning_rate": 2.0266666666666667e-05,
      "loss": 0.0033,
      "step": 44600
    },
    {
      "epoch": 2.3792,
      "grad_norm": 0.12642739713191986,
      "learning_rate": 2.0260000000000003e-05,
      "loss": 0.0027,
      "step": 44610
    },
    {
      "epoch": 2.3797333333333333,
      "grad_norm": 0.253670871257782,
      "learning_rate": 2.0253333333333335e-05,
      "loss": 0.0024,
      "step": 44620
    },
    {
      "epoch": 2.3802666666666665,
      "grad_norm": 0.1494068205356598,
      "learning_rate": 2.0246666666666667e-05,
      "loss": 0.002,
      "step": 44630
    },
    {
      "epoch": 2.3808,
      "grad_norm": 0.23733246326446533,
      "learning_rate": 2.024e-05,
      "loss": 0.002,
      "step": 44640
    },
    {
      "epoch": 2.3813333333333335,
      "grad_norm": 0.21274787187576294,
      "learning_rate": 2.0233333333333335e-05,
      "loss": 0.0017,
      "step": 44650
    },
    {
      "epoch": 2.381866666666667,
      "grad_norm": 0.20352788269519806,
      "learning_rate": 2.0226666666666667e-05,
      "loss": 0.0021,
      "step": 44660
    },
    {
      "epoch": 2.3824,
      "grad_norm": 0.38827434182167053,
      "learning_rate": 2.022e-05,
      "loss": 0.0027,
      "step": 44670
    },
    {
      "epoch": 2.3829333333333333,
      "grad_norm": 0.20805723965168,
      "learning_rate": 2.0213333333333335e-05,
      "loss": 0.0034,
      "step": 44680
    },
    {
      "epoch": 2.3834666666666666,
      "grad_norm": 0.442851722240448,
      "learning_rate": 2.0206666666666667e-05,
      "loss": 0.0036,
      "step": 44690
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.3661527931690216,
      "learning_rate": 2.0200000000000003e-05,
      "loss": 0.0021,
      "step": 44700
    },
    {
      "epoch": 2.384533333333333,
      "grad_norm": 0.10836867988109589,
      "learning_rate": 2.0193333333333332e-05,
      "loss": 0.0022,
      "step": 44710
    },
    {
      "epoch": 2.385066666666667,
      "grad_norm": 0.3185679018497467,
      "learning_rate": 2.0186666666666668e-05,
      "loss": 0.0039,
      "step": 44720
    },
    {
      "epoch": 2.3856,
      "grad_norm": 0.42715486884117126,
      "learning_rate": 2.0180000000000003e-05,
      "loss": 0.002,
      "step": 44730
    },
    {
      "epoch": 2.3861333333333334,
      "grad_norm": 0.40881335735321045,
      "learning_rate": 2.0173333333333332e-05,
      "loss": 0.0025,
      "step": 44740
    },
    {
      "epoch": 2.3866666666666667,
      "grad_norm": 0.177569180727005,
      "learning_rate": 2.0166666666666668e-05,
      "loss": 0.0016,
      "step": 44750
    },
    {
      "epoch": 2.3872,
      "grad_norm": 0.37666189670562744,
      "learning_rate": 2.016e-05,
      "loss": 0.0019,
      "step": 44760
    },
    {
      "epoch": 2.3877333333333333,
      "grad_norm": 0.06791434437036514,
      "learning_rate": 2.0153333333333336e-05,
      "loss": 0.0026,
      "step": 44770
    },
    {
      "epoch": 2.3882666666666665,
      "grad_norm": 0.2890331447124481,
      "learning_rate": 2.0146666666666668e-05,
      "loss": 0.0021,
      "step": 44780
    },
    {
      "epoch": 2.3888,
      "grad_norm": 0.26352164149284363,
      "learning_rate": 2.014e-05,
      "loss": 0.0019,
      "step": 44790
    },
    {
      "epoch": 2.389333333333333,
      "grad_norm": 0.41654691100120544,
      "learning_rate": 2.0133333333333336e-05,
      "loss": 0.0018,
      "step": 44800
    },
    {
      "epoch": 2.389866666666667,
      "grad_norm": 0.20310403406620026,
      "learning_rate": 2.0126666666666668e-05,
      "loss": 0.0022,
      "step": 44810
    },
    {
      "epoch": 2.3904,
      "grad_norm": 0.0655345693230629,
      "learning_rate": 2.012e-05,
      "loss": 0.0025,
      "step": 44820
    },
    {
      "epoch": 2.3909333333333334,
      "grad_norm": 0.31394118070602417,
      "learning_rate": 2.0113333333333333e-05,
      "loss": 0.0019,
      "step": 44830
    },
    {
      "epoch": 2.3914666666666666,
      "grad_norm": 0.10374251008033752,
      "learning_rate": 2.010666666666667e-05,
      "loss": 0.0027,
      "step": 44840
    },
    {
      "epoch": 2.392,
      "grad_norm": 0.44311580061912537,
      "learning_rate": 2.01e-05,
      "loss": 0.002,
      "step": 44850
    },
    {
      "epoch": 2.392533333333333,
      "grad_norm": 0.11264733970165253,
      "learning_rate": 2.0093333333333333e-05,
      "loss": 0.0025,
      "step": 44860
    },
    {
      "epoch": 2.393066666666667,
      "grad_norm": 0.23756292462348938,
      "learning_rate": 2.008666666666667e-05,
      "loss": 0.0029,
      "step": 44870
    },
    {
      "epoch": 2.3936,
      "grad_norm": 0.12048868089914322,
      "learning_rate": 2.008e-05,
      "loss": 0.0035,
      "step": 44880
    },
    {
      "epoch": 2.3941333333333334,
      "grad_norm": 0.12752775847911835,
      "learning_rate": 2.0073333333333337e-05,
      "loss": 0.0022,
      "step": 44890
    },
    {
      "epoch": 2.3946666666666667,
      "grad_norm": 0.3062136173248291,
      "learning_rate": 2.0066666666666665e-05,
      "loss": 0.003,
      "step": 44900
    },
    {
      "epoch": 2.3952,
      "grad_norm": 0.2766220271587372,
      "learning_rate": 2.006e-05,
      "loss": 0.0018,
      "step": 44910
    },
    {
      "epoch": 2.3957333333333333,
      "grad_norm": 0.17266105115413666,
      "learning_rate": 2.0053333333333337e-05,
      "loss": 0.0038,
      "step": 44920
    },
    {
      "epoch": 2.3962666666666665,
      "grad_norm": 0.14581263065338135,
      "learning_rate": 2.0046666666666666e-05,
      "loss": 0.0029,
      "step": 44930
    },
    {
      "epoch": 2.3968,
      "grad_norm": 0.24176867306232452,
      "learning_rate": 2.004e-05,
      "loss": 0.0019,
      "step": 44940
    },
    {
      "epoch": 2.397333333333333,
      "grad_norm": 0.21357274055480957,
      "learning_rate": 2.0033333333333334e-05,
      "loss": 0.0021,
      "step": 44950
    },
    {
      "epoch": 2.397866666666667,
      "grad_norm": 0.3324563801288605,
      "learning_rate": 2.002666666666667e-05,
      "loss": 0.0023,
      "step": 44960
    },
    {
      "epoch": 2.3984,
      "grad_norm": 0.22465623915195465,
      "learning_rate": 2.002e-05,
      "loss": 0.002,
      "step": 44970
    },
    {
      "epoch": 2.3989333333333334,
      "grad_norm": 0.36979246139526367,
      "learning_rate": 2.0013333333333334e-05,
      "loss": 0.002,
      "step": 44980
    },
    {
      "epoch": 2.3994666666666666,
      "grad_norm": 0.3439801335334778,
      "learning_rate": 2.000666666666667e-05,
      "loss": 0.0029,
      "step": 44990
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.21218077838420868,
      "learning_rate": 2e-05,
      "loss": 0.0025,
      "step": 45000
    },
    {
      "epoch": 2.400533333333333,
      "grad_norm": 0.15982091426849365,
      "learning_rate": 1.9993333333333334e-05,
      "loss": 0.0024,
      "step": 45010
    },
    {
      "epoch": 2.401066666666667,
      "grad_norm": 0.3823808431625366,
      "learning_rate": 1.9986666666666666e-05,
      "loss": 0.0025,
      "step": 45020
    },
    {
      "epoch": 2.4016,
      "grad_norm": 0.11504052579402924,
      "learning_rate": 1.9980000000000002e-05,
      "loss": 0.0033,
      "step": 45030
    },
    {
      "epoch": 2.4021333333333335,
      "grad_norm": 0.32634249329566956,
      "learning_rate": 1.9973333333333334e-05,
      "loss": 0.0018,
      "step": 45040
    },
    {
      "epoch": 2.4026666666666667,
      "grad_norm": 0.18194745481014252,
      "learning_rate": 1.9966666666666666e-05,
      "loss": 0.0018,
      "step": 45050
    },
    {
      "epoch": 2.4032,
      "grad_norm": 0.4205496311187744,
      "learning_rate": 1.9960000000000002e-05,
      "loss": 0.0023,
      "step": 45060
    },
    {
      "epoch": 2.4037333333333333,
      "grad_norm": 0.5883291959762573,
      "learning_rate": 1.9953333333333334e-05,
      "loss": 0.0017,
      "step": 45070
    },
    {
      "epoch": 2.4042666666666666,
      "grad_norm": 0.4347338080406189,
      "learning_rate": 1.9946666666666667e-05,
      "loss": 0.002,
      "step": 45080
    },
    {
      "epoch": 2.4048,
      "grad_norm": 0.10646086186170578,
      "learning_rate": 1.994e-05,
      "loss": 0.0028,
      "step": 45090
    },
    {
      "epoch": 2.405333333333333,
      "grad_norm": 0.10553965717554092,
      "learning_rate": 1.9933333333333334e-05,
      "loss": 0.0028,
      "step": 45100
    },
    {
      "epoch": 2.405866666666667,
      "grad_norm": 0.15878988802433014,
      "learning_rate": 1.992666666666667e-05,
      "loss": 0.0035,
      "step": 45110
    },
    {
      "epoch": 2.4064,
      "grad_norm": 0.28028008341789246,
      "learning_rate": 1.992e-05,
      "loss": 0.0033,
      "step": 45120
    },
    {
      "epoch": 2.4069333333333334,
      "grad_norm": 0.4024428129196167,
      "learning_rate": 1.9913333333333335e-05,
      "loss": 0.003,
      "step": 45130
    },
    {
      "epoch": 2.4074666666666666,
      "grad_norm": 0.2612716257572174,
      "learning_rate": 1.9906666666666667e-05,
      "loss": 0.0019,
      "step": 45140
    },
    {
      "epoch": 2.408,
      "grad_norm": 0.5018306970596313,
      "learning_rate": 1.9900000000000003e-05,
      "loss": 0.0025,
      "step": 45150
    },
    {
      "epoch": 2.408533333333333,
      "grad_norm": 0.2828780710697174,
      "learning_rate": 1.9893333333333335e-05,
      "loss": 0.0027,
      "step": 45160
    },
    {
      "epoch": 2.409066666666667,
      "grad_norm": 0.20846892893314362,
      "learning_rate": 1.9886666666666667e-05,
      "loss": 0.002,
      "step": 45170
    },
    {
      "epoch": 2.4096,
      "grad_norm": 0.07615659385919571,
      "learning_rate": 1.9880000000000003e-05,
      "loss": 0.0016,
      "step": 45180
    },
    {
      "epoch": 2.4101333333333335,
      "grad_norm": 0.42753928899765015,
      "learning_rate": 1.9873333333333335e-05,
      "loss": 0.0026,
      "step": 45190
    },
    {
      "epoch": 2.4106666666666667,
      "grad_norm": 0.20122602581977844,
      "learning_rate": 1.9866666666666667e-05,
      "loss": 0.0027,
      "step": 45200
    },
    {
      "epoch": 2.4112,
      "grad_norm": 0.25987568497657776,
      "learning_rate": 1.986e-05,
      "loss": 0.0022,
      "step": 45210
    },
    {
      "epoch": 2.4117333333333333,
      "grad_norm": 0.10647381097078323,
      "learning_rate": 1.9853333333333335e-05,
      "loss": 0.0026,
      "step": 45220
    },
    {
      "epoch": 2.4122666666666666,
      "grad_norm": 0.3707244098186493,
      "learning_rate": 1.9846666666666668e-05,
      "loss": 0.002,
      "step": 45230
    },
    {
      "epoch": 2.4128,
      "grad_norm": 0.2995523512363434,
      "learning_rate": 1.984e-05,
      "loss": 0.0021,
      "step": 45240
    },
    {
      "epoch": 2.413333333333333,
      "grad_norm": 0.7051252126693726,
      "learning_rate": 1.9833333333333335e-05,
      "loss": 0.0034,
      "step": 45250
    },
    {
      "epoch": 2.413866666666667,
      "grad_norm": 0.7604220509529114,
      "learning_rate": 1.9826666666666668e-05,
      "loss": 0.0029,
      "step": 45260
    },
    {
      "epoch": 2.4144,
      "grad_norm": 0.06730294227600098,
      "learning_rate": 1.982e-05,
      "loss": 0.0022,
      "step": 45270
    },
    {
      "epoch": 2.4149333333333334,
      "grad_norm": 0.4872055649757385,
      "learning_rate": 1.9813333333333332e-05,
      "loss": 0.0022,
      "step": 45280
    },
    {
      "epoch": 2.4154666666666667,
      "grad_norm": 0.22376421093940735,
      "learning_rate": 1.9806666666666668e-05,
      "loss": 0.0034,
      "step": 45290
    },
    {
      "epoch": 2.416,
      "grad_norm": 0.09559743106365204,
      "learning_rate": 1.9800000000000004e-05,
      "loss": 0.0028,
      "step": 45300
    },
    {
      "epoch": 2.416533333333333,
      "grad_norm": 0.09359566867351532,
      "learning_rate": 1.9793333333333332e-05,
      "loss": 0.0022,
      "step": 45310
    },
    {
      "epoch": 2.4170666666666665,
      "grad_norm": 0.20918259024620056,
      "learning_rate": 1.9786666666666668e-05,
      "loss": 0.002,
      "step": 45320
    },
    {
      "epoch": 2.4176,
      "grad_norm": 0.2368166446685791,
      "learning_rate": 1.978e-05,
      "loss": 0.0022,
      "step": 45330
    },
    {
      "epoch": 2.4181333333333335,
      "grad_norm": 0.20789818465709686,
      "learning_rate": 1.9773333333333336e-05,
      "loss": 0.002,
      "step": 45340
    },
    {
      "epoch": 2.4186666666666667,
      "grad_norm": 0.4685029983520508,
      "learning_rate": 1.9766666666666668e-05,
      "loss": 0.0031,
      "step": 45350
    },
    {
      "epoch": 2.4192,
      "grad_norm": 0.5085847973823547,
      "learning_rate": 1.976e-05,
      "loss": 0.0038,
      "step": 45360
    },
    {
      "epoch": 2.4197333333333333,
      "grad_norm": 0.1434689313173294,
      "learning_rate": 1.9753333333333336e-05,
      "loss": 0.002,
      "step": 45370
    },
    {
      "epoch": 2.4202666666666666,
      "grad_norm": 0.22192931175231934,
      "learning_rate": 1.974666666666667e-05,
      "loss": 0.0029,
      "step": 45380
    },
    {
      "epoch": 2.4208,
      "grad_norm": 0.33566078543663025,
      "learning_rate": 1.974e-05,
      "loss": 0.0021,
      "step": 45390
    },
    {
      "epoch": 2.421333333333333,
      "grad_norm": 0.2629926800727844,
      "learning_rate": 1.9733333333333333e-05,
      "loss": 0.0027,
      "step": 45400
    },
    {
      "epoch": 2.421866666666667,
      "grad_norm": 0.07424231618642807,
      "learning_rate": 1.972666666666667e-05,
      "loss": 0.0019,
      "step": 45410
    },
    {
      "epoch": 2.4224,
      "grad_norm": 0.23690441250801086,
      "learning_rate": 1.972e-05,
      "loss": 0.0032,
      "step": 45420
    },
    {
      "epoch": 2.4229333333333334,
      "grad_norm": 0.6588745713233948,
      "learning_rate": 1.9713333333333333e-05,
      "loss": 0.0021,
      "step": 45430
    },
    {
      "epoch": 2.4234666666666667,
      "grad_norm": 0.3685433864593506,
      "learning_rate": 1.970666666666667e-05,
      "loss": 0.003,
      "step": 45440
    },
    {
      "epoch": 2.424,
      "grad_norm": 0.2768724262714386,
      "learning_rate": 1.97e-05,
      "loss": 0.0022,
      "step": 45450
    },
    {
      "epoch": 2.424533333333333,
      "grad_norm": 0.3143351674079895,
      "learning_rate": 1.9693333333333333e-05,
      "loss": 0.0017,
      "step": 45460
    },
    {
      "epoch": 2.4250666666666665,
      "grad_norm": 0.06513291597366333,
      "learning_rate": 1.9686666666666666e-05,
      "loss": 0.0029,
      "step": 45470
    },
    {
      "epoch": 2.4256,
      "grad_norm": 0.08651452511548996,
      "learning_rate": 1.968e-05,
      "loss": 0.0027,
      "step": 45480
    },
    {
      "epoch": 2.4261333333333335,
      "grad_norm": 0.22899477183818817,
      "learning_rate": 1.9673333333333337e-05,
      "loss": 0.0022,
      "step": 45490
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 0.27558496594429016,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 0.0025,
      "step": 45500
    },
    {
      "epoch": 2.4272,
      "grad_norm": 0.21000251173973083,
      "learning_rate": 1.966e-05,
      "loss": 0.0019,
      "step": 45510
    },
    {
      "epoch": 2.4277333333333333,
      "grad_norm": 0.20354364812374115,
      "learning_rate": 1.9653333333333334e-05,
      "loss": 0.0021,
      "step": 45520
    },
    {
      "epoch": 2.4282666666666666,
      "grad_norm": 0.21872247755527496,
      "learning_rate": 1.964666666666667e-05,
      "loss": 0.0017,
      "step": 45530
    },
    {
      "epoch": 2.4288,
      "grad_norm": 0.10925693064928055,
      "learning_rate": 1.9640000000000002e-05,
      "loss": 0.0019,
      "step": 45540
    },
    {
      "epoch": 2.429333333333333,
      "grad_norm": 0.14576783776283264,
      "learning_rate": 1.9633333333333334e-05,
      "loss": 0.002,
      "step": 45550
    },
    {
      "epoch": 2.429866666666667,
      "grad_norm": 0.4527260959148407,
      "learning_rate": 1.962666666666667e-05,
      "loss": 0.0021,
      "step": 45560
    },
    {
      "epoch": 2.4304,
      "grad_norm": 0.0997266173362732,
      "learning_rate": 1.9620000000000002e-05,
      "loss": 0.0031,
      "step": 45570
    },
    {
      "epoch": 2.4309333333333334,
      "grad_norm": 0.42913195490837097,
      "learning_rate": 1.9613333333333334e-05,
      "loss": 0.0018,
      "step": 45580
    },
    {
      "epoch": 2.4314666666666667,
      "grad_norm": 0.15099994838237762,
      "learning_rate": 1.9606666666666666e-05,
      "loss": 0.0021,
      "step": 45590
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.08784321695566177,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.0031,
      "step": 45600
    },
    {
      "epoch": 2.432533333333333,
      "grad_norm": 0.1370735764503479,
      "learning_rate": 1.9593333333333334e-05,
      "loss": 0.0018,
      "step": 45610
    },
    {
      "epoch": 2.4330666666666665,
      "grad_norm": 0.19965998828411102,
      "learning_rate": 1.9586666666666667e-05,
      "loss": 0.0019,
      "step": 45620
    },
    {
      "epoch": 2.4336,
      "grad_norm": 0.36668699979782104,
      "learning_rate": 1.9580000000000002e-05,
      "loss": 0.0026,
      "step": 45630
    },
    {
      "epoch": 2.4341333333333335,
      "grad_norm": 0.3670274019241333,
      "learning_rate": 1.9573333333333335e-05,
      "loss": 0.0023,
      "step": 45640
    },
    {
      "epoch": 2.4346666666666668,
      "grad_norm": 0.3879649043083191,
      "learning_rate": 1.9566666666666667e-05,
      "loss": 0.0023,
      "step": 45650
    },
    {
      "epoch": 2.4352,
      "grad_norm": 0.6498740315437317,
      "learning_rate": 1.956e-05,
      "loss": 0.0023,
      "step": 45660
    },
    {
      "epoch": 2.4357333333333333,
      "grad_norm": 0.17850637435913086,
      "learning_rate": 1.9553333333333335e-05,
      "loss": 0.0023,
      "step": 45670
    },
    {
      "epoch": 2.4362666666666666,
      "grad_norm": 0.39616841077804565,
      "learning_rate": 1.9546666666666667e-05,
      "loss": 0.0038,
      "step": 45680
    },
    {
      "epoch": 2.4368,
      "grad_norm": 0.30361050367355347,
      "learning_rate": 1.954e-05,
      "loss": 0.0024,
      "step": 45690
    },
    {
      "epoch": 2.437333333333333,
      "grad_norm": 0.15581859648227692,
      "learning_rate": 1.9533333333333335e-05,
      "loss": 0.0019,
      "step": 45700
    },
    {
      "epoch": 2.437866666666667,
      "grad_norm": 0.44200649857521057,
      "learning_rate": 1.9526666666666667e-05,
      "loss": 0.002,
      "step": 45710
    },
    {
      "epoch": 2.4384,
      "grad_norm": 0.44635871052742004,
      "learning_rate": 1.9520000000000003e-05,
      "loss": 0.0024,
      "step": 45720
    },
    {
      "epoch": 2.4389333333333334,
      "grad_norm": 0.2811426520347595,
      "learning_rate": 1.9513333333333335e-05,
      "loss": 0.0031,
      "step": 45730
    },
    {
      "epoch": 2.4394666666666667,
      "grad_norm": 0.22711236774921417,
      "learning_rate": 1.9506666666666667e-05,
      "loss": 0.0036,
      "step": 45740
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.1251574158668518,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 0.0021,
      "step": 45750
    },
    {
      "epoch": 2.440533333333333,
      "grad_norm": 0.38185060024261475,
      "learning_rate": 1.9493333333333332e-05,
      "loss": 0.0019,
      "step": 45760
    },
    {
      "epoch": 2.4410666666666665,
      "grad_norm": 0.22038815915584564,
      "learning_rate": 1.9486666666666668e-05,
      "loss": 0.002,
      "step": 45770
    },
    {
      "epoch": 2.4416,
      "grad_norm": 0.2851119637489319,
      "learning_rate": 1.948e-05,
      "loss": 0.0019,
      "step": 45780
    },
    {
      "epoch": 2.4421333333333335,
      "grad_norm": 0.2893970012664795,
      "learning_rate": 1.9473333333333335e-05,
      "loss": 0.0025,
      "step": 45790
    },
    {
      "epoch": 2.4426666666666668,
      "grad_norm": 0.640627384185791,
      "learning_rate": 1.9466666666666668e-05,
      "loss": 0.0028,
      "step": 45800
    },
    {
      "epoch": 2.4432,
      "grad_norm": 0.7432531118392944,
      "learning_rate": 1.946e-05,
      "loss": 0.0028,
      "step": 45810
    },
    {
      "epoch": 2.4437333333333333,
      "grad_norm": 0.13441208004951477,
      "learning_rate": 1.9453333333333336e-05,
      "loss": 0.0024,
      "step": 45820
    },
    {
      "epoch": 2.4442666666666666,
      "grad_norm": 0.07220085710287094,
      "learning_rate": 1.9446666666666668e-05,
      "loss": 0.0019,
      "step": 45830
    },
    {
      "epoch": 2.4448,
      "grad_norm": 0.18602454662322998,
      "learning_rate": 1.944e-05,
      "loss": 0.0018,
      "step": 45840
    },
    {
      "epoch": 2.445333333333333,
      "grad_norm": 0.18526622653007507,
      "learning_rate": 1.9433333333333332e-05,
      "loss": 0.0022,
      "step": 45850
    },
    {
      "epoch": 2.445866666666667,
      "grad_norm": 0.14023791253566742,
      "learning_rate": 1.9426666666666668e-05,
      "loss": 0.0033,
      "step": 45860
    },
    {
      "epoch": 2.4464,
      "grad_norm": 0.11375778913497925,
      "learning_rate": 1.942e-05,
      "loss": 0.0023,
      "step": 45870
    },
    {
      "epoch": 2.4469333333333334,
      "grad_norm": 0.4517810046672821,
      "learning_rate": 1.9413333333333333e-05,
      "loss": 0.0025,
      "step": 45880
    },
    {
      "epoch": 2.4474666666666667,
      "grad_norm": 0.3845561742782593,
      "learning_rate": 1.940666666666667e-05,
      "loss": 0.0029,
      "step": 45890
    },
    {
      "epoch": 2.448,
      "grad_norm": 0.41099920868873596,
      "learning_rate": 1.94e-05,
      "loss": 0.0029,
      "step": 45900
    },
    {
      "epoch": 2.4485333333333332,
      "grad_norm": 0.2233893871307373,
      "learning_rate": 1.9393333333333336e-05,
      "loss": 0.0021,
      "step": 45910
    },
    {
      "epoch": 2.4490666666666665,
      "grad_norm": 0.07588369399309158,
      "learning_rate": 1.938666666666667e-05,
      "loss": 0.0022,
      "step": 45920
    },
    {
      "epoch": 2.4496,
      "grad_norm": 0.18364273011684418,
      "learning_rate": 1.938e-05,
      "loss": 0.0019,
      "step": 45930
    },
    {
      "epoch": 2.4501333333333335,
      "grad_norm": 0.15083755552768707,
      "learning_rate": 1.9373333333333336e-05,
      "loss": 0.0026,
      "step": 45940
    },
    {
      "epoch": 2.4506666666666668,
      "grad_norm": 0.37287822365760803,
      "learning_rate": 1.9366666666666665e-05,
      "loss": 0.0019,
      "step": 45950
    },
    {
      "epoch": 2.4512,
      "grad_norm": 0.09231129288673401,
      "learning_rate": 1.936e-05,
      "loss": 0.0024,
      "step": 45960
    },
    {
      "epoch": 2.4517333333333333,
      "grad_norm": 0.48021265864372253,
      "learning_rate": 1.9353333333333333e-05,
      "loss": 0.0018,
      "step": 45970
    },
    {
      "epoch": 2.4522666666666666,
      "grad_norm": 0.41554784774780273,
      "learning_rate": 1.934666666666667e-05,
      "loss": 0.0028,
      "step": 45980
    },
    {
      "epoch": 2.4528,
      "grad_norm": 0.5733084082603455,
      "learning_rate": 1.934e-05,
      "loss": 0.0026,
      "step": 45990
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 0.06957672536373138,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 0.0018,
      "step": 46000
    },
    {
      "epoch": 2.4538666666666664,
      "grad_norm": 0.6641995906829834,
      "learning_rate": 1.932666666666667e-05,
      "loss": 0.0024,
      "step": 46010
    },
    {
      "epoch": 2.4544,
      "grad_norm": 0.449156790971756,
      "learning_rate": 1.932e-05,
      "loss": 0.0023,
      "step": 46020
    },
    {
      "epoch": 2.4549333333333334,
      "grad_norm": 0.2944958508014679,
      "learning_rate": 1.9313333333333334e-05,
      "loss": 0.0025,
      "step": 46030
    },
    {
      "epoch": 2.4554666666666667,
      "grad_norm": 0.33972305059432983,
      "learning_rate": 1.9306666666666666e-05,
      "loss": 0.0025,
      "step": 46040
    },
    {
      "epoch": 2.456,
      "grad_norm": 0.5862957835197449,
      "learning_rate": 1.93e-05,
      "loss": 0.0018,
      "step": 46050
    },
    {
      "epoch": 2.4565333333333332,
      "grad_norm": 0.16765643656253815,
      "learning_rate": 1.9293333333333334e-05,
      "loss": 0.0023,
      "step": 46060
    },
    {
      "epoch": 2.4570666666666665,
      "grad_norm": 0.6207280158996582,
      "learning_rate": 1.9286666666666666e-05,
      "loss": 0.0027,
      "step": 46070
    },
    {
      "epoch": 2.4576000000000002,
      "grad_norm": 0.36068376898765564,
      "learning_rate": 1.9280000000000002e-05,
      "loss": 0.0024,
      "step": 46080
    },
    {
      "epoch": 2.4581333333333335,
      "grad_norm": 0.1447380632162094,
      "learning_rate": 1.9273333333333334e-05,
      "loss": 0.0017,
      "step": 46090
    },
    {
      "epoch": 2.458666666666667,
      "grad_norm": 0.2365928590297699,
      "learning_rate": 1.926666666666667e-05,
      "loss": 0.002,
      "step": 46100
    },
    {
      "epoch": 2.4592,
      "grad_norm": 0.1734723597764969,
      "learning_rate": 1.9260000000000002e-05,
      "loss": 0.0029,
      "step": 46110
    },
    {
      "epoch": 2.4597333333333333,
      "grad_norm": 0.10439327359199524,
      "learning_rate": 1.9253333333333334e-05,
      "loss": 0.0026,
      "step": 46120
    },
    {
      "epoch": 2.4602666666666666,
      "grad_norm": 0.2468327432870865,
      "learning_rate": 1.924666666666667e-05,
      "loss": 0.0017,
      "step": 46130
    },
    {
      "epoch": 2.4608,
      "grad_norm": 0.19602109491825104,
      "learning_rate": 1.924e-05,
      "loss": 0.0043,
      "step": 46140
    },
    {
      "epoch": 2.461333333333333,
      "grad_norm": 0.18896809220314026,
      "learning_rate": 1.9233333333333334e-05,
      "loss": 0.0019,
      "step": 46150
    },
    {
      "epoch": 2.4618666666666664,
      "grad_norm": 0.44245243072509766,
      "learning_rate": 1.9226666666666667e-05,
      "loss": 0.0023,
      "step": 46160
    },
    {
      "epoch": 2.4624,
      "grad_norm": 0.1442028284072876,
      "learning_rate": 1.9220000000000002e-05,
      "loss": 0.0023,
      "step": 46170
    },
    {
      "epoch": 2.4629333333333334,
      "grad_norm": 0.1892242282629013,
      "learning_rate": 1.9213333333333335e-05,
      "loss": 0.0026,
      "step": 46180
    },
    {
      "epoch": 2.4634666666666667,
      "grad_norm": 0.11196873337030411,
      "learning_rate": 1.9206666666666667e-05,
      "loss": 0.0032,
      "step": 46190
    },
    {
      "epoch": 2.464,
      "grad_norm": 0.1664467751979828,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.0035,
      "step": 46200
    },
    {
      "epoch": 2.4645333333333332,
      "grad_norm": 0.4608759880065918,
      "learning_rate": 1.9193333333333335e-05,
      "loss": 0.0027,
      "step": 46210
    },
    {
      "epoch": 2.4650666666666665,
      "grad_norm": 0.2549649477005005,
      "learning_rate": 1.9186666666666667e-05,
      "loss": 0.002,
      "step": 46220
    },
    {
      "epoch": 2.4656000000000002,
      "grad_norm": 0.05018114298582077,
      "learning_rate": 1.918e-05,
      "loss": 0.0024,
      "step": 46230
    },
    {
      "epoch": 2.4661333333333335,
      "grad_norm": 0.07953297346830368,
      "learning_rate": 1.9173333333333335e-05,
      "loss": 0.0018,
      "step": 46240
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 0.1066121906042099,
      "learning_rate": 1.9166666666666667e-05,
      "loss": 0.0023,
      "step": 46250
    },
    {
      "epoch": 2.4672,
      "grad_norm": 0.24236448109149933,
      "learning_rate": 1.916e-05,
      "loss": 0.002,
      "step": 46260
    },
    {
      "epoch": 2.4677333333333333,
      "grad_norm": 0.15075334906578064,
      "learning_rate": 1.9153333333333335e-05,
      "loss": 0.0025,
      "step": 46270
    },
    {
      "epoch": 2.4682666666666666,
      "grad_norm": 0.2327563613653183,
      "learning_rate": 1.9146666666666667e-05,
      "loss": 0.0023,
      "step": 46280
    },
    {
      "epoch": 2.4688,
      "grad_norm": 0.2377622127532959,
      "learning_rate": 1.914e-05,
      "loss": 0.0028,
      "step": 46290
    },
    {
      "epoch": 2.469333333333333,
      "grad_norm": 0.35080981254577637,
      "learning_rate": 1.9133333333333332e-05,
      "loss": 0.0029,
      "step": 46300
    },
    {
      "epoch": 2.4698666666666664,
      "grad_norm": 0.29381170868873596,
      "learning_rate": 1.9126666666666668e-05,
      "loss": 0.0019,
      "step": 46310
    },
    {
      "epoch": 2.4704,
      "grad_norm": 0.6843248605728149,
      "learning_rate": 1.9120000000000003e-05,
      "loss": 0.0025,
      "step": 46320
    },
    {
      "epoch": 2.4709333333333334,
      "grad_norm": 0.3248766362667084,
      "learning_rate": 1.9113333333333332e-05,
      "loss": 0.0019,
      "step": 46330
    },
    {
      "epoch": 2.4714666666666667,
      "grad_norm": 0.11643310636281967,
      "learning_rate": 1.9106666666666668e-05,
      "loss": 0.002,
      "step": 46340
    },
    {
      "epoch": 2.472,
      "grad_norm": 0.1263371855020523,
      "learning_rate": 1.91e-05,
      "loss": 0.0026,
      "step": 46350
    },
    {
      "epoch": 2.4725333333333332,
      "grad_norm": 0.10016623139381409,
      "learning_rate": 1.9093333333333336e-05,
      "loss": 0.0026,
      "step": 46360
    },
    {
      "epoch": 2.4730666666666665,
      "grad_norm": 0.4146610498428345,
      "learning_rate": 1.9086666666666668e-05,
      "loss": 0.0021,
      "step": 46370
    },
    {
      "epoch": 2.4736000000000002,
      "grad_norm": 0.4272198975086212,
      "learning_rate": 1.908e-05,
      "loss": 0.0024,
      "step": 46380
    },
    {
      "epoch": 2.4741333333333335,
      "grad_norm": 0.14809353649616241,
      "learning_rate": 1.9073333333333336e-05,
      "loss": 0.0022,
      "step": 46390
    },
    {
      "epoch": 2.474666666666667,
      "grad_norm": 0.07908602058887482,
      "learning_rate": 1.9066666666666668e-05,
      "loss": 0.0022,
      "step": 46400
    },
    {
      "epoch": 2.4752,
      "grad_norm": 0.5397439002990723,
      "learning_rate": 1.906e-05,
      "loss": 0.002,
      "step": 46410
    },
    {
      "epoch": 2.4757333333333333,
      "grad_norm": 0.39381715655326843,
      "learning_rate": 1.9053333333333333e-05,
      "loss": 0.0029,
      "step": 46420
    },
    {
      "epoch": 2.4762666666666666,
      "grad_norm": 0.7769196033477783,
      "learning_rate": 1.904666666666667e-05,
      "loss": 0.0018,
      "step": 46430
    },
    {
      "epoch": 2.4768,
      "grad_norm": 0.34894099831581116,
      "learning_rate": 1.904e-05,
      "loss": 0.0029,
      "step": 46440
    },
    {
      "epoch": 2.477333333333333,
      "grad_norm": 0.39230090379714966,
      "learning_rate": 1.9033333333333333e-05,
      "loss": 0.0029,
      "step": 46450
    },
    {
      "epoch": 2.4778666666666664,
      "grad_norm": 0.5653259754180908,
      "learning_rate": 1.902666666666667e-05,
      "loss": 0.0019,
      "step": 46460
    },
    {
      "epoch": 2.4784,
      "grad_norm": 0.13006632030010223,
      "learning_rate": 1.902e-05,
      "loss": 0.0041,
      "step": 46470
    },
    {
      "epoch": 2.4789333333333334,
      "grad_norm": 0.3500051200389862,
      "learning_rate": 1.9013333333333333e-05,
      "loss": 0.0023,
      "step": 46480
    },
    {
      "epoch": 2.4794666666666667,
      "grad_norm": 0.44901394844055176,
      "learning_rate": 1.9006666666666665e-05,
      "loss": 0.0029,
      "step": 46490
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.3432410955429077,
      "learning_rate": 1.9e-05,
      "loss": 0.0021,
      "step": 46500
    },
    {
      "epoch": 2.4805333333333333,
      "grad_norm": 0.17481942474842072,
      "learning_rate": 1.8993333333333337e-05,
      "loss": 0.0019,
      "step": 46510
    },
    {
      "epoch": 2.4810666666666665,
      "grad_norm": 0.44992971420288086,
      "learning_rate": 1.8986666666666666e-05,
      "loss": 0.0023,
      "step": 46520
    },
    {
      "epoch": 2.4816,
      "grad_norm": 0.22395004332065582,
      "learning_rate": 1.898e-05,
      "loss": 0.0028,
      "step": 46530
    },
    {
      "epoch": 2.4821333333333335,
      "grad_norm": 0.22708415985107422,
      "learning_rate": 1.8973333333333334e-05,
      "loss": 0.0018,
      "step": 46540
    },
    {
      "epoch": 2.482666666666667,
      "grad_norm": 0.10121229290962219,
      "learning_rate": 1.896666666666667e-05,
      "loss": 0.0026,
      "step": 46550
    },
    {
      "epoch": 2.4832,
      "grad_norm": 0.2584552466869354,
      "learning_rate": 1.896e-05,
      "loss": 0.0024,
      "step": 46560
    },
    {
      "epoch": 2.4837333333333333,
      "grad_norm": 0.20961053669452667,
      "learning_rate": 1.8953333333333334e-05,
      "loss": 0.0035,
      "step": 46570
    },
    {
      "epoch": 2.4842666666666666,
      "grad_norm": 0.4809335470199585,
      "learning_rate": 1.894666666666667e-05,
      "loss": 0.0019,
      "step": 46580
    },
    {
      "epoch": 2.4848,
      "grad_norm": 0.1326964795589447,
      "learning_rate": 1.894e-05,
      "loss": 0.0031,
      "step": 46590
    },
    {
      "epoch": 2.485333333333333,
      "grad_norm": 0.1376986801624298,
      "learning_rate": 1.8933333333333334e-05,
      "loss": 0.0028,
      "step": 46600
    },
    {
      "epoch": 2.4858666666666664,
      "grad_norm": 0.5478836297988892,
      "learning_rate": 1.8926666666666666e-05,
      "loss": 0.0028,
      "step": 46610
    },
    {
      "epoch": 2.4864,
      "grad_norm": 0.5354353189468384,
      "learning_rate": 1.8920000000000002e-05,
      "loss": 0.0022,
      "step": 46620
    },
    {
      "epoch": 2.4869333333333334,
      "grad_norm": 0.2931739091873169,
      "learning_rate": 1.8913333333333334e-05,
      "loss": 0.0033,
      "step": 46630
    },
    {
      "epoch": 2.4874666666666667,
      "grad_norm": 0.3400709331035614,
      "learning_rate": 1.8906666666666666e-05,
      "loss": 0.0029,
      "step": 46640
    },
    {
      "epoch": 2.488,
      "grad_norm": 0.13666993379592896,
      "learning_rate": 1.8900000000000002e-05,
      "loss": 0.0018,
      "step": 46650
    },
    {
      "epoch": 2.4885333333333333,
      "grad_norm": 0.5371705889701843,
      "learning_rate": 1.8893333333333334e-05,
      "loss": 0.0024,
      "step": 46660
    },
    {
      "epoch": 2.4890666666666665,
      "grad_norm": 0.6688348054885864,
      "learning_rate": 1.8886666666666667e-05,
      "loss": 0.0019,
      "step": 46670
    },
    {
      "epoch": 2.4896,
      "grad_norm": 0.05040809139609337,
      "learning_rate": 1.888e-05,
      "loss": 0.0031,
      "step": 46680
    },
    {
      "epoch": 2.4901333333333335,
      "grad_norm": 0.12117331475019455,
      "learning_rate": 1.8873333333333334e-05,
      "loss": 0.0019,
      "step": 46690
    },
    {
      "epoch": 2.490666666666667,
      "grad_norm": 0.6431753635406494,
      "learning_rate": 1.886666666666667e-05,
      "loss": 0.0024,
      "step": 46700
    },
    {
      "epoch": 2.4912,
      "grad_norm": 0.14617888629436493,
      "learning_rate": 1.886e-05,
      "loss": 0.0023,
      "step": 46710
    },
    {
      "epoch": 2.4917333333333334,
      "grad_norm": 0.3591725826263428,
      "learning_rate": 1.8853333333333335e-05,
      "loss": 0.0029,
      "step": 46720
    },
    {
      "epoch": 2.4922666666666666,
      "grad_norm": 0.241367369890213,
      "learning_rate": 1.8846666666666667e-05,
      "loss": 0.0021,
      "step": 46730
    },
    {
      "epoch": 2.4928,
      "grad_norm": 0.0910133644938469,
      "learning_rate": 1.8840000000000003e-05,
      "loss": 0.0025,
      "step": 46740
    },
    {
      "epoch": 2.493333333333333,
      "grad_norm": 0.24215231835842133,
      "learning_rate": 1.8833333333333335e-05,
      "loss": 0.0028,
      "step": 46750
    },
    {
      "epoch": 2.4938666666666665,
      "grad_norm": 0.2674050033092499,
      "learning_rate": 1.8826666666666667e-05,
      "loss": 0.003,
      "step": 46760
    },
    {
      "epoch": 2.4944,
      "grad_norm": 0.09470564126968384,
      "learning_rate": 1.8820000000000003e-05,
      "loss": 0.0023,
      "step": 46770
    },
    {
      "epoch": 2.4949333333333334,
      "grad_norm": 0.2587887644767761,
      "learning_rate": 1.8813333333333335e-05,
      "loss": 0.003,
      "step": 46780
    },
    {
      "epoch": 2.4954666666666667,
      "grad_norm": 0.33814355731010437,
      "learning_rate": 1.8806666666666667e-05,
      "loss": 0.0026,
      "step": 46790
    },
    {
      "epoch": 2.496,
      "grad_norm": 0.1926068365573883,
      "learning_rate": 1.88e-05,
      "loss": 0.0027,
      "step": 46800
    },
    {
      "epoch": 2.4965333333333333,
      "grad_norm": 0.10985477268695831,
      "learning_rate": 1.8793333333333335e-05,
      "loss": 0.0029,
      "step": 46810
    },
    {
      "epoch": 2.4970666666666665,
      "grad_norm": 0.16567957401275635,
      "learning_rate": 1.8786666666666667e-05,
      "loss": 0.0022,
      "step": 46820
    },
    {
      "epoch": 2.4976,
      "grad_norm": 0.3662114143371582,
      "learning_rate": 1.878e-05,
      "loss": 0.0027,
      "step": 46830
    },
    {
      "epoch": 2.4981333333333335,
      "grad_norm": 0.1629125475883484,
      "learning_rate": 1.8773333333333335e-05,
      "loss": 0.0025,
      "step": 46840
    },
    {
      "epoch": 2.498666666666667,
      "grad_norm": 0.3789527416229248,
      "learning_rate": 1.8766666666666668e-05,
      "loss": 0.0028,
      "step": 46850
    },
    {
      "epoch": 2.4992,
      "grad_norm": 0.22635093331336975,
      "learning_rate": 1.876e-05,
      "loss": 0.0035,
      "step": 46860
    },
    {
      "epoch": 2.4997333333333334,
      "grad_norm": 0.3228910267353058,
      "learning_rate": 1.8753333333333332e-05,
      "loss": 0.0029,
      "step": 46870
    },
    {
      "epoch": 2.5002666666666666,
      "grad_norm": 0.1083105206489563,
      "learning_rate": 1.8746666666666668e-05,
      "loss": 0.0027,
      "step": 46880
    },
    {
      "epoch": 2.5008,
      "grad_norm": 0.16429848968982697,
      "learning_rate": 1.8740000000000004e-05,
      "loss": 0.0025,
      "step": 46890
    },
    {
      "epoch": 2.501333333333333,
      "grad_norm": 0.13798220455646515,
      "learning_rate": 1.8733333333333332e-05,
      "loss": 0.0018,
      "step": 46900
    },
    {
      "epoch": 2.5018666666666665,
      "grad_norm": 0.14346151053905487,
      "learning_rate": 1.8726666666666668e-05,
      "loss": 0.0027,
      "step": 46910
    },
    {
      "epoch": 2.5023999999999997,
      "grad_norm": 0.5754536390304565,
      "learning_rate": 1.872e-05,
      "loss": 0.0023,
      "step": 46920
    },
    {
      "epoch": 2.5029333333333335,
      "grad_norm": 0.4908747375011444,
      "learning_rate": 1.8713333333333336e-05,
      "loss": 0.0019,
      "step": 46930
    },
    {
      "epoch": 2.5034666666666667,
      "grad_norm": 0.16584554314613342,
      "learning_rate": 1.8706666666666668e-05,
      "loss": 0.0024,
      "step": 46940
    },
    {
      "epoch": 2.504,
      "grad_norm": 0.33278027176856995,
      "learning_rate": 1.87e-05,
      "loss": 0.0028,
      "step": 46950
    },
    {
      "epoch": 2.5045333333333333,
      "grad_norm": 0.48983216285705566,
      "learning_rate": 1.8693333333333336e-05,
      "loss": 0.0026,
      "step": 46960
    },
    {
      "epoch": 2.5050666666666666,
      "grad_norm": 0.19693268835544586,
      "learning_rate": 1.8686666666666665e-05,
      "loss": 0.0035,
      "step": 46970
    },
    {
      "epoch": 2.5056000000000003,
      "grad_norm": 0.734214723110199,
      "learning_rate": 1.868e-05,
      "loss": 0.0023,
      "step": 46980
    },
    {
      "epoch": 2.5061333333333335,
      "grad_norm": 0.35891297459602356,
      "learning_rate": 1.8673333333333333e-05,
      "loss": 0.0036,
      "step": 46990
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 0.18445776402950287,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.0028,
      "step": 47000
    },
    {
      "epoch": 2.5072,
      "grad_norm": 0.2905529737472534,
      "learning_rate": 1.866e-05,
      "loss": 0.0031,
      "step": 47010
    },
    {
      "epoch": 2.5077333333333334,
      "grad_norm": 0.19977794587612152,
      "learning_rate": 1.8653333333333333e-05,
      "loss": 0.0035,
      "step": 47020
    },
    {
      "epoch": 2.5082666666666666,
      "grad_norm": 0.09790569543838501,
      "learning_rate": 1.864666666666667e-05,
      "loss": 0.0036,
      "step": 47030
    },
    {
      "epoch": 2.5088,
      "grad_norm": 0.17122027277946472,
      "learning_rate": 1.864e-05,
      "loss": 0.0028,
      "step": 47040
    },
    {
      "epoch": 2.509333333333333,
      "grad_norm": 0.09662279486656189,
      "learning_rate": 1.8633333333333333e-05,
      "loss": 0.0021,
      "step": 47050
    },
    {
      "epoch": 2.5098666666666665,
      "grad_norm": 0.18582791090011597,
      "learning_rate": 1.8626666666666666e-05,
      "loss": 0.0024,
      "step": 47060
    },
    {
      "epoch": 2.5103999999999997,
      "grad_norm": 0.07925356924533844,
      "learning_rate": 1.862e-05,
      "loss": 0.0031,
      "step": 47070
    },
    {
      "epoch": 2.5109333333333335,
      "grad_norm": 0.16928154230117798,
      "learning_rate": 1.8613333333333337e-05,
      "loss": 0.0055,
      "step": 47080
    },
    {
      "epoch": 2.5114666666666667,
      "grad_norm": 0.36433884501457214,
      "learning_rate": 1.8606666666666666e-05,
      "loss": 0.0028,
      "step": 47090
    },
    {
      "epoch": 2.512,
      "grad_norm": 0.42033636569976807,
      "learning_rate": 1.86e-05,
      "loss": 0.0022,
      "step": 47100
    },
    {
      "epoch": 2.5125333333333333,
      "grad_norm": 0.11057176440954208,
      "learning_rate": 1.8593333333333334e-05,
      "loss": 0.0027,
      "step": 47110
    },
    {
      "epoch": 2.5130666666666666,
      "grad_norm": 0.36818283796310425,
      "learning_rate": 1.858666666666667e-05,
      "loss": 0.0024,
      "step": 47120
    },
    {
      "epoch": 2.5136,
      "grad_norm": 0.10908398032188416,
      "learning_rate": 1.858e-05,
      "loss": 0.0035,
      "step": 47130
    },
    {
      "epoch": 2.5141333333333336,
      "grad_norm": 0.14465120434761047,
      "learning_rate": 1.8573333333333334e-05,
      "loss": 0.002,
      "step": 47140
    },
    {
      "epoch": 2.514666666666667,
      "grad_norm": 0.126949742436409,
      "learning_rate": 1.856666666666667e-05,
      "loss": 0.0021,
      "step": 47150
    },
    {
      "epoch": 2.5152,
      "grad_norm": 0.12544409930706024,
      "learning_rate": 1.856e-05,
      "loss": 0.0028,
      "step": 47160
    },
    {
      "epoch": 2.5157333333333334,
      "grad_norm": 0.399644672870636,
      "learning_rate": 1.8553333333333334e-05,
      "loss": 0.0026,
      "step": 47170
    },
    {
      "epoch": 2.5162666666666667,
      "grad_norm": 0.09049239754676819,
      "learning_rate": 1.8546666666666666e-05,
      "loss": 0.0019,
      "step": 47180
    },
    {
      "epoch": 2.5168,
      "grad_norm": 0.513331413269043,
      "learning_rate": 1.8540000000000002e-05,
      "loss": 0.0036,
      "step": 47190
    },
    {
      "epoch": 2.517333333333333,
      "grad_norm": 0.6245317459106445,
      "learning_rate": 1.8533333333333334e-05,
      "loss": 0.0022,
      "step": 47200
    },
    {
      "epoch": 2.5178666666666665,
      "grad_norm": 0.29959821701049805,
      "learning_rate": 1.8526666666666667e-05,
      "loss": 0.0024,
      "step": 47210
    },
    {
      "epoch": 2.5183999999999997,
      "grad_norm": 0.2505888044834137,
      "learning_rate": 1.8520000000000002e-05,
      "loss": 0.0025,
      "step": 47220
    },
    {
      "epoch": 2.5189333333333335,
      "grad_norm": 0.32136180996894836,
      "learning_rate": 1.8513333333333335e-05,
      "loss": 0.002,
      "step": 47230
    },
    {
      "epoch": 2.5194666666666667,
      "grad_norm": 0.12686792016029358,
      "learning_rate": 1.8506666666666667e-05,
      "loss": 0.003,
      "step": 47240
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.13394348323345184,
      "learning_rate": 1.85e-05,
      "loss": 0.0026,
      "step": 47250
    },
    {
      "epoch": 2.5205333333333333,
      "grad_norm": 0.344845175743103,
      "learning_rate": 1.8493333333333335e-05,
      "loss": 0.0044,
      "step": 47260
    },
    {
      "epoch": 2.5210666666666666,
      "grad_norm": 0.07611687481403351,
      "learning_rate": 1.848666666666667e-05,
      "loss": 0.0023,
      "step": 47270
    },
    {
      "epoch": 2.5216,
      "grad_norm": 0.4307920038700104,
      "learning_rate": 1.848e-05,
      "loss": 0.0041,
      "step": 47280
    },
    {
      "epoch": 2.5221333333333336,
      "grad_norm": 0.38922742009162903,
      "learning_rate": 1.8473333333333335e-05,
      "loss": 0.0021,
      "step": 47290
    },
    {
      "epoch": 2.522666666666667,
      "grad_norm": 0.26863062381744385,
      "learning_rate": 1.8466666666666667e-05,
      "loss": 0.0022,
      "step": 47300
    },
    {
      "epoch": 2.5232,
      "grad_norm": 0.3445117473602295,
      "learning_rate": 1.846e-05,
      "loss": 0.002,
      "step": 47310
    },
    {
      "epoch": 2.5237333333333334,
      "grad_norm": 0.42997288703918457,
      "learning_rate": 1.8453333333333335e-05,
      "loss": 0.0039,
      "step": 47320
    },
    {
      "epoch": 2.5242666666666667,
      "grad_norm": 0.26072975993156433,
      "learning_rate": 1.8446666666666667e-05,
      "loss": 0.002,
      "step": 47330
    },
    {
      "epoch": 2.5248,
      "grad_norm": 0.16490362584590912,
      "learning_rate": 1.8440000000000003e-05,
      "loss": 0.0021,
      "step": 47340
    },
    {
      "epoch": 2.525333333333333,
      "grad_norm": 0.5217890739440918,
      "learning_rate": 1.8433333333333332e-05,
      "loss": 0.0029,
      "step": 47350
    },
    {
      "epoch": 2.5258666666666665,
      "grad_norm": 0.6455861926078796,
      "learning_rate": 1.8426666666666668e-05,
      "loss": 0.0027,
      "step": 47360
    },
    {
      "epoch": 2.5263999999999998,
      "grad_norm": 0.38253358006477356,
      "learning_rate": 1.842e-05,
      "loss": 0.0018,
      "step": 47370
    },
    {
      "epoch": 2.5269333333333335,
      "grad_norm": 0.17415010929107666,
      "learning_rate": 1.8413333333333335e-05,
      "loss": 0.0022,
      "step": 47380
    },
    {
      "epoch": 2.5274666666666668,
      "grad_norm": 0.3088938295841217,
      "learning_rate": 1.8406666666666668e-05,
      "loss": 0.002,
      "step": 47390
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.11727739125490189,
      "learning_rate": 1.84e-05,
      "loss": 0.0029,
      "step": 47400
    },
    {
      "epoch": 2.5285333333333333,
      "grad_norm": 0.40668439865112305,
      "learning_rate": 1.8393333333333336e-05,
      "loss": 0.003,
      "step": 47410
    },
    {
      "epoch": 2.5290666666666666,
      "grad_norm": 0.17398996651172638,
      "learning_rate": 1.8386666666666668e-05,
      "loss": 0.0019,
      "step": 47420
    },
    {
      "epoch": 2.5296,
      "grad_norm": 0.06772001087665558,
      "learning_rate": 1.838e-05,
      "loss": 0.0022,
      "step": 47430
    },
    {
      "epoch": 2.5301333333333336,
      "grad_norm": 0.0885537788271904,
      "learning_rate": 1.8373333333333332e-05,
      "loss": 0.0026,
      "step": 47440
    },
    {
      "epoch": 2.530666666666667,
      "grad_norm": 0.09442569315433502,
      "learning_rate": 1.8366666666666668e-05,
      "loss": 0.0027,
      "step": 47450
    },
    {
      "epoch": 2.5312,
      "grad_norm": 0.2522728443145752,
      "learning_rate": 1.8360000000000004e-05,
      "loss": 0.0021,
      "step": 47460
    },
    {
      "epoch": 2.5317333333333334,
      "grad_norm": 0.10848341137170792,
      "learning_rate": 1.8353333333333333e-05,
      "loss": 0.0032,
      "step": 47470
    },
    {
      "epoch": 2.5322666666666667,
      "grad_norm": 0.46647804975509644,
      "learning_rate": 1.834666666666667e-05,
      "loss": 0.0024,
      "step": 47480
    },
    {
      "epoch": 2.5328,
      "grad_norm": 0.17262214422225952,
      "learning_rate": 1.834e-05,
      "loss": 0.0032,
      "step": 47490
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.11136310547590256,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.0021,
      "step": 47500
    },
    {
      "epoch": 2.5338666666666665,
      "grad_norm": 0.5975279808044434,
      "learning_rate": 1.832666666666667e-05,
      "loss": 0.002,
      "step": 47510
    },
    {
      "epoch": 2.5343999999999998,
      "grad_norm": 1.05196213722229,
      "learning_rate": 1.832e-05,
      "loss": 0.0022,
      "step": 47520
    },
    {
      "epoch": 2.5349333333333335,
      "grad_norm": 0.2853170931339264,
      "learning_rate": 1.8313333333333336e-05,
      "loss": 0.002,
      "step": 47530
    },
    {
      "epoch": 2.5354666666666668,
      "grad_norm": 0.07337553799152374,
      "learning_rate": 1.8306666666666665e-05,
      "loss": 0.0025,
      "step": 47540
    },
    {
      "epoch": 2.536,
      "grad_norm": 0.18390890955924988,
      "learning_rate": 1.83e-05,
      "loss": 0.0026,
      "step": 47550
    },
    {
      "epoch": 2.5365333333333333,
      "grad_norm": 0.3372899889945984,
      "learning_rate": 1.8293333333333333e-05,
      "loss": 0.0027,
      "step": 47560
    },
    {
      "epoch": 2.5370666666666666,
      "grad_norm": 0.3846137821674347,
      "learning_rate": 1.828666666666667e-05,
      "loss": 0.0029,
      "step": 47570
    },
    {
      "epoch": 2.5376,
      "grad_norm": 0.23975183069705963,
      "learning_rate": 1.828e-05,
      "loss": 0.0029,
      "step": 47580
    },
    {
      "epoch": 2.5381333333333336,
      "grad_norm": 0.13844534754753113,
      "learning_rate": 1.8273333333333333e-05,
      "loss": 0.0028,
      "step": 47590
    },
    {
      "epoch": 2.538666666666667,
      "grad_norm": 0.3629785180091858,
      "learning_rate": 1.826666666666667e-05,
      "loss": 0.0028,
      "step": 47600
    },
    {
      "epoch": 2.5392,
      "grad_norm": 0.264406681060791,
      "learning_rate": 1.826e-05,
      "loss": 0.002,
      "step": 47610
    },
    {
      "epoch": 2.5397333333333334,
      "grad_norm": 0.36857327818870544,
      "learning_rate": 1.8253333333333334e-05,
      "loss": 0.0022,
      "step": 47620
    },
    {
      "epoch": 2.5402666666666667,
      "grad_norm": 0.2219696044921875,
      "learning_rate": 1.8246666666666666e-05,
      "loss": 0.0015,
      "step": 47630
    },
    {
      "epoch": 2.5408,
      "grad_norm": 0.4863850474357605,
      "learning_rate": 1.824e-05,
      "loss": 0.0021,
      "step": 47640
    },
    {
      "epoch": 2.541333333333333,
      "grad_norm": 0.4121992290019989,
      "learning_rate": 1.8233333333333334e-05,
      "loss": 0.002,
      "step": 47650
    },
    {
      "epoch": 2.5418666666666665,
      "grad_norm": 0.23588113486766815,
      "learning_rate": 1.8226666666666666e-05,
      "loss": 0.002,
      "step": 47660
    },
    {
      "epoch": 2.5423999999999998,
      "grad_norm": 0.3041912019252777,
      "learning_rate": 1.8220000000000002e-05,
      "loss": 0.0017,
      "step": 47670
    },
    {
      "epoch": 2.5429333333333335,
      "grad_norm": 0.14150851964950562,
      "learning_rate": 1.8213333333333334e-05,
      "loss": 0.003,
      "step": 47680
    },
    {
      "epoch": 2.5434666666666668,
      "grad_norm": 0.3114163875579834,
      "learning_rate": 1.8206666666666666e-05,
      "loss": 0.0043,
      "step": 47690
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.4332493543624878,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 0.003,
      "step": 47700
    },
    {
      "epoch": 2.5445333333333333,
      "grad_norm": 0.1053604856133461,
      "learning_rate": 1.8193333333333334e-05,
      "loss": 0.0019,
      "step": 47710
    },
    {
      "epoch": 2.5450666666666666,
      "grad_norm": 0.5305508375167847,
      "learning_rate": 1.818666666666667e-05,
      "loss": 0.0034,
      "step": 47720
    },
    {
      "epoch": 2.5456,
      "grad_norm": 0.45085006952285767,
      "learning_rate": 1.818e-05,
      "loss": 0.0028,
      "step": 47730
    },
    {
      "epoch": 2.5461333333333336,
      "grad_norm": 0.3460889160633087,
      "learning_rate": 1.8173333333333334e-05,
      "loss": 0.0034,
      "step": 47740
    },
    {
      "epoch": 2.546666666666667,
      "grad_norm": 0.18922844529151917,
      "learning_rate": 1.8166666666666667e-05,
      "loss": 0.0017,
      "step": 47750
    },
    {
      "epoch": 2.5472,
      "grad_norm": 0.1745244413614273,
      "learning_rate": 1.8160000000000002e-05,
      "loss": 0.0032,
      "step": 47760
    },
    {
      "epoch": 2.5477333333333334,
      "grad_norm": 0.12403834611177444,
      "learning_rate": 1.8153333333333335e-05,
      "loss": 0.0023,
      "step": 47770
    },
    {
      "epoch": 2.5482666666666667,
      "grad_norm": 0.49597325921058655,
      "learning_rate": 1.8146666666666667e-05,
      "loss": 0.0026,
      "step": 47780
    },
    {
      "epoch": 2.5488,
      "grad_norm": 0.1303347945213318,
      "learning_rate": 1.8140000000000003e-05,
      "loss": 0.0033,
      "step": 47790
    },
    {
      "epoch": 2.5493333333333332,
      "grad_norm": 0.14780844748020172,
      "learning_rate": 1.8133333333333335e-05,
      "loss": 0.0032,
      "step": 47800
    },
    {
      "epoch": 2.5498666666666665,
      "grad_norm": 0.5089303255081177,
      "learning_rate": 1.8126666666666667e-05,
      "loss": 0.0029,
      "step": 47810
    },
    {
      "epoch": 2.5504,
      "grad_norm": 1.0297313928604126,
      "learning_rate": 1.812e-05,
      "loss": 0.0022,
      "step": 47820
    },
    {
      "epoch": 2.5509333333333335,
      "grad_norm": 0.13364160060882568,
      "learning_rate": 1.8113333333333335e-05,
      "loss": 0.0032,
      "step": 47830
    },
    {
      "epoch": 2.5514666666666668,
      "grad_norm": 0.15598070621490479,
      "learning_rate": 1.8106666666666667e-05,
      "loss": 0.0022,
      "step": 47840
    },
    {
      "epoch": 2.552,
      "grad_norm": 0.1398550271987915,
      "learning_rate": 1.81e-05,
      "loss": 0.002,
      "step": 47850
    },
    {
      "epoch": 2.5525333333333333,
      "grad_norm": 0.35750553011894226,
      "learning_rate": 1.8093333333333335e-05,
      "loss": 0.0019,
      "step": 47860
    },
    {
      "epoch": 2.5530666666666666,
      "grad_norm": 0.14269709587097168,
      "learning_rate": 1.8086666666666667e-05,
      "loss": 0.0027,
      "step": 47870
    },
    {
      "epoch": 2.5536,
      "grad_norm": 0.06273861229419708,
      "learning_rate": 1.808e-05,
      "loss": 0.0024,
      "step": 47880
    },
    {
      "epoch": 2.5541333333333336,
      "grad_norm": 0.40763363242149353,
      "learning_rate": 1.8073333333333335e-05,
      "loss": 0.0019,
      "step": 47890
    },
    {
      "epoch": 2.554666666666667,
      "grad_norm": 0.1835339218378067,
      "learning_rate": 1.8066666666666668e-05,
      "loss": 0.002,
      "step": 47900
    },
    {
      "epoch": 2.5552,
      "grad_norm": 0.30673640966415405,
      "learning_rate": 1.8060000000000003e-05,
      "loss": 0.0035,
      "step": 47910
    },
    {
      "epoch": 2.5557333333333334,
      "grad_norm": 0.259952187538147,
      "learning_rate": 1.8053333333333332e-05,
      "loss": 0.0032,
      "step": 47920
    },
    {
      "epoch": 2.5562666666666667,
      "grad_norm": 0.0846586599946022,
      "learning_rate": 1.8046666666666668e-05,
      "loss": 0.0032,
      "step": 47930
    },
    {
      "epoch": 2.5568,
      "grad_norm": 0.3670401871204376,
      "learning_rate": 1.804e-05,
      "loss": 0.0025,
      "step": 47940
    },
    {
      "epoch": 2.5573333333333332,
      "grad_norm": 0.3620867133140564,
      "learning_rate": 1.8033333333333336e-05,
      "loss": 0.002,
      "step": 47950
    },
    {
      "epoch": 2.5578666666666665,
      "grad_norm": 0.07377412170171738,
      "learning_rate": 1.8026666666666668e-05,
      "loss": 0.0023,
      "step": 47960
    },
    {
      "epoch": 2.5584,
      "grad_norm": 0.22277875244617462,
      "learning_rate": 1.802e-05,
      "loss": 0.002,
      "step": 47970
    },
    {
      "epoch": 2.558933333333333,
      "grad_norm": 0.2236696183681488,
      "learning_rate": 1.8013333333333336e-05,
      "loss": 0.0017,
      "step": 47980
    },
    {
      "epoch": 2.559466666666667,
      "grad_norm": 1.017252802848816,
      "learning_rate": 1.8006666666666668e-05,
      "loss": 0.0029,
      "step": 47990
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.13304351270198822,
      "learning_rate": 1.8e-05,
      "loss": 0.0032,
      "step": 48000
    },
    {
      "epoch": 2.5605333333333333,
      "grad_norm": 0.4433426558971405,
      "learning_rate": 1.7993333333333333e-05,
      "loss": 0.0025,
      "step": 48010
    },
    {
      "epoch": 2.5610666666666666,
      "grad_norm": 0.3502154052257538,
      "learning_rate": 1.798666666666667e-05,
      "loss": 0.0033,
      "step": 48020
    },
    {
      "epoch": 2.5616,
      "grad_norm": 0.21407882869243622,
      "learning_rate": 1.798e-05,
      "loss": 0.002,
      "step": 48030
    },
    {
      "epoch": 2.5621333333333336,
      "grad_norm": 0.36321157217025757,
      "learning_rate": 1.7973333333333333e-05,
      "loss": 0.0025,
      "step": 48040
    },
    {
      "epoch": 2.562666666666667,
      "grad_norm": 0.2684967815876007,
      "learning_rate": 1.796666666666667e-05,
      "loss": 0.0019,
      "step": 48050
    },
    {
      "epoch": 2.5632,
      "grad_norm": 0.3789415657520294,
      "learning_rate": 1.796e-05,
      "loss": 0.0021,
      "step": 48060
    },
    {
      "epoch": 2.5637333333333334,
      "grad_norm": 0.4719043970108032,
      "learning_rate": 1.7953333333333333e-05,
      "loss": 0.0021,
      "step": 48070
    },
    {
      "epoch": 2.5642666666666667,
      "grad_norm": 0.22788269817829132,
      "learning_rate": 1.794666666666667e-05,
      "loss": 0.0035,
      "step": 48080
    },
    {
      "epoch": 2.5648,
      "grad_norm": 0.2576398253440857,
      "learning_rate": 1.794e-05,
      "loss": 0.0017,
      "step": 48090
    },
    {
      "epoch": 2.5653333333333332,
      "grad_norm": 0.11272913962602615,
      "learning_rate": 1.7933333333333337e-05,
      "loss": 0.0031,
      "step": 48100
    },
    {
      "epoch": 2.5658666666666665,
      "grad_norm": 0.1794230192899704,
      "learning_rate": 1.7926666666666666e-05,
      "loss": 0.0023,
      "step": 48110
    },
    {
      "epoch": 2.5664,
      "grad_norm": 0.43182721734046936,
      "learning_rate": 1.792e-05,
      "loss": 0.0039,
      "step": 48120
    },
    {
      "epoch": 2.566933333333333,
      "grad_norm": 0.4156314432621002,
      "learning_rate": 1.7913333333333333e-05,
      "loss": 0.0033,
      "step": 48130
    },
    {
      "epoch": 2.567466666666667,
      "grad_norm": 0.3498777449131012,
      "learning_rate": 1.790666666666667e-05,
      "loss": 0.0029,
      "step": 48140
    },
    {
      "epoch": 2.568,
      "grad_norm": 0.3666040301322937,
      "learning_rate": 1.79e-05,
      "loss": 0.0019,
      "step": 48150
    },
    {
      "epoch": 2.5685333333333333,
      "grad_norm": 0.2655048966407776,
      "learning_rate": 1.7893333333333334e-05,
      "loss": 0.0018,
      "step": 48160
    },
    {
      "epoch": 2.5690666666666666,
      "grad_norm": 0.28724291920661926,
      "learning_rate": 1.788666666666667e-05,
      "loss": 0.002,
      "step": 48170
    },
    {
      "epoch": 2.5696,
      "grad_norm": 0.38831984996795654,
      "learning_rate": 1.7879999999999998e-05,
      "loss": 0.0016,
      "step": 48180
    },
    {
      "epoch": 2.5701333333333336,
      "grad_norm": 0.39282217621803284,
      "learning_rate": 1.7873333333333334e-05,
      "loss": 0.004,
      "step": 48190
    },
    {
      "epoch": 2.570666666666667,
      "grad_norm": 0.16737104952335358,
      "learning_rate": 1.7866666666666666e-05,
      "loss": 0.0025,
      "step": 48200
    },
    {
      "epoch": 2.5712,
      "grad_norm": 0.26425230503082275,
      "learning_rate": 1.7860000000000002e-05,
      "loss": 0.0034,
      "step": 48210
    },
    {
      "epoch": 2.5717333333333334,
      "grad_norm": 0.355377197265625,
      "learning_rate": 1.7853333333333334e-05,
      "loss": 0.0026,
      "step": 48220
    },
    {
      "epoch": 2.5722666666666667,
      "grad_norm": 0.15552496910095215,
      "learning_rate": 1.7846666666666666e-05,
      "loss": 0.0027,
      "step": 48230
    },
    {
      "epoch": 2.5728,
      "grad_norm": 0.07075493782758713,
      "learning_rate": 1.7840000000000002e-05,
      "loss": 0.0024,
      "step": 48240
    },
    {
      "epoch": 2.5733333333333333,
      "grad_norm": 0.10898026823997498,
      "learning_rate": 1.7833333333333334e-05,
      "loss": 0.0021,
      "step": 48250
    },
    {
      "epoch": 2.5738666666666665,
      "grad_norm": 0.2355770319700241,
      "learning_rate": 1.7826666666666667e-05,
      "loss": 0.0039,
      "step": 48260
    },
    {
      "epoch": 2.5744,
      "grad_norm": 0.16445143520832062,
      "learning_rate": 1.7820000000000002e-05,
      "loss": 0.002,
      "step": 48270
    },
    {
      "epoch": 2.574933333333333,
      "grad_norm": 0.3329926133155823,
      "learning_rate": 1.7813333333333334e-05,
      "loss": 0.0021,
      "step": 48280
    },
    {
      "epoch": 2.575466666666667,
      "grad_norm": 0.4453074336051941,
      "learning_rate": 1.780666666666667e-05,
      "loss": 0.0019,
      "step": 48290
    },
    {
      "epoch": 2.576,
      "grad_norm": 0.45171821117401123,
      "learning_rate": 1.78e-05,
      "loss": 0.0023,
      "step": 48300
    },
    {
      "epoch": 2.5765333333333333,
      "grad_norm": 0.3082869052886963,
      "learning_rate": 1.7793333333333335e-05,
      "loss": 0.0032,
      "step": 48310
    },
    {
      "epoch": 2.5770666666666666,
      "grad_norm": 0.17286227643489838,
      "learning_rate": 1.7786666666666667e-05,
      "loss": 0.0016,
      "step": 48320
    },
    {
      "epoch": 2.5776,
      "grad_norm": 0.25503894686698914,
      "learning_rate": 1.7780000000000003e-05,
      "loss": 0.0023,
      "step": 48330
    },
    {
      "epoch": 2.5781333333333336,
      "grad_norm": 0.36420997977256775,
      "learning_rate": 1.7773333333333335e-05,
      "loss": 0.0025,
      "step": 48340
    },
    {
      "epoch": 2.578666666666667,
      "grad_norm": 0.1645202338695526,
      "learning_rate": 1.7766666666666667e-05,
      "loss": 0.0027,
      "step": 48350
    },
    {
      "epoch": 2.5792,
      "grad_norm": 0.21491317451000214,
      "learning_rate": 1.7760000000000003e-05,
      "loss": 0.0036,
      "step": 48360
    },
    {
      "epoch": 2.5797333333333334,
      "grad_norm": 0.7931025624275208,
      "learning_rate": 1.775333333333333e-05,
      "loss": 0.0024,
      "step": 48370
    },
    {
      "epoch": 2.5802666666666667,
      "grad_norm": 0.36728203296661377,
      "learning_rate": 1.7746666666666667e-05,
      "loss": 0.0022,
      "step": 48380
    },
    {
      "epoch": 2.5808,
      "grad_norm": 0.09643521159887314,
      "learning_rate": 1.774e-05,
      "loss": 0.0022,
      "step": 48390
    },
    {
      "epoch": 2.5813333333333333,
      "grad_norm": 0.2020353227853775,
      "learning_rate": 1.7733333333333335e-05,
      "loss": 0.0031,
      "step": 48400
    },
    {
      "epoch": 2.5818666666666665,
      "grad_norm": 0.09483815729618073,
      "learning_rate": 1.7726666666666667e-05,
      "loss": 0.0024,
      "step": 48410
    },
    {
      "epoch": 2.5824,
      "grad_norm": 0.25275716185569763,
      "learning_rate": 1.772e-05,
      "loss": 0.003,
      "step": 48420
    },
    {
      "epoch": 2.582933333333333,
      "grad_norm": 0.1983184516429901,
      "learning_rate": 1.7713333333333335e-05,
      "loss": 0.0024,
      "step": 48430
    },
    {
      "epoch": 2.583466666666667,
      "grad_norm": 0.08576902002096176,
      "learning_rate": 1.7706666666666668e-05,
      "loss": 0.0037,
      "step": 48440
    },
    {
      "epoch": 2.584,
      "grad_norm": 0.2343941032886505,
      "learning_rate": 1.77e-05,
      "loss": 0.0016,
      "step": 48450
    },
    {
      "epoch": 2.5845333333333333,
      "grad_norm": 0.06678718328475952,
      "learning_rate": 1.7693333333333336e-05,
      "loss": 0.0021,
      "step": 48460
    },
    {
      "epoch": 2.5850666666666666,
      "grad_norm": 0.14537759125232697,
      "learning_rate": 1.7686666666666668e-05,
      "loss": 0.0019,
      "step": 48470
    },
    {
      "epoch": 2.5856,
      "grad_norm": 0.0790371373295784,
      "learning_rate": 1.7680000000000004e-05,
      "loss": 0.0033,
      "step": 48480
    },
    {
      "epoch": 2.586133333333333,
      "grad_norm": 0.32812216877937317,
      "learning_rate": 1.7673333333333332e-05,
      "loss": 0.0028,
      "step": 48490
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 0.7422248125076294,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 0.0027,
      "step": 48500
    },
    {
      "epoch": 2.5872,
      "grad_norm": 0.2348274290561676,
      "learning_rate": 1.766e-05,
      "loss": 0.003,
      "step": 48510
    },
    {
      "epoch": 2.5877333333333334,
      "grad_norm": 0.06351209431886673,
      "learning_rate": 1.7653333333333333e-05,
      "loss": 0.0026,
      "step": 48520
    },
    {
      "epoch": 2.5882666666666667,
      "grad_norm": 0.3383127450942993,
      "learning_rate": 1.7646666666666668e-05,
      "loss": 0.0029,
      "step": 48530
    },
    {
      "epoch": 2.5888,
      "grad_norm": 0.23514236509799957,
      "learning_rate": 1.764e-05,
      "loss": 0.0019,
      "step": 48540
    },
    {
      "epoch": 2.5893333333333333,
      "grad_norm": 0.5072552561759949,
      "learning_rate": 1.7633333333333336e-05,
      "loss": 0.0033,
      "step": 48550
    },
    {
      "epoch": 2.5898666666666665,
      "grad_norm": 0.45215487480163574,
      "learning_rate": 1.7626666666666665e-05,
      "loss": 0.0032,
      "step": 48560
    },
    {
      "epoch": 2.5904,
      "grad_norm": 0.2669430375099182,
      "learning_rate": 1.762e-05,
      "loss": 0.0019,
      "step": 48570
    },
    {
      "epoch": 2.590933333333333,
      "grad_norm": 0.5884156823158264,
      "learning_rate": 1.7613333333333333e-05,
      "loss": 0.0023,
      "step": 48580
    },
    {
      "epoch": 2.591466666666667,
      "grad_norm": 0.468212753534317,
      "learning_rate": 1.760666666666667e-05,
      "loss": 0.0019,
      "step": 48590
    },
    {
      "epoch": 2.592,
      "grad_norm": 0.7599213719367981,
      "learning_rate": 1.76e-05,
      "loss": 0.0024,
      "step": 48600
    },
    {
      "epoch": 2.5925333333333334,
      "grad_norm": 0.3187991976737976,
      "learning_rate": 1.7593333333333333e-05,
      "loss": 0.003,
      "step": 48610
    },
    {
      "epoch": 2.5930666666666666,
      "grad_norm": 0.14179471135139465,
      "learning_rate": 1.758666666666667e-05,
      "loss": 0.0026,
      "step": 48620
    },
    {
      "epoch": 2.5936,
      "grad_norm": 0.1621088832616806,
      "learning_rate": 1.758e-05,
      "loss": 0.0025,
      "step": 48630
    },
    {
      "epoch": 2.594133333333333,
      "grad_norm": 0.3483431935310364,
      "learning_rate": 1.7573333333333333e-05,
      "loss": 0.0026,
      "step": 48640
    },
    {
      "epoch": 2.594666666666667,
      "grad_norm": 0.32822734117507935,
      "learning_rate": 1.756666666666667e-05,
      "loss": 0.0021,
      "step": 48650
    },
    {
      "epoch": 2.5952,
      "grad_norm": 0.11575406044721603,
      "learning_rate": 1.756e-05,
      "loss": 0.003,
      "step": 48660
    },
    {
      "epoch": 2.5957333333333334,
      "grad_norm": 0.19397248327732086,
      "learning_rate": 1.7553333333333337e-05,
      "loss": 0.002,
      "step": 48670
    },
    {
      "epoch": 2.5962666666666667,
      "grad_norm": 0.2496318519115448,
      "learning_rate": 1.7546666666666666e-05,
      "loss": 0.0036,
      "step": 48680
    },
    {
      "epoch": 2.5968,
      "grad_norm": 0.07840119302272797,
      "learning_rate": 1.754e-05,
      "loss": 0.0025,
      "step": 48690
    },
    {
      "epoch": 2.5973333333333333,
      "grad_norm": 0.2060772180557251,
      "learning_rate": 1.7533333333333334e-05,
      "loss": 0.0019,
      "step": 48700
    },
    {
      "epoch": 2.5978666666666665,
      "grad_norm": 0.3506222665309906,
      "learning_rate": 1.7526666666666666e-05,
      "loss": 0.0026,
      "step": 48710
    },
    {
      "epoch": 2.5984,
      "grad_norm": 0.4764464795589447,
      "learning_rate": 1.752e-05,
      "loss": 0.0029,
      "step": 48720
    },
    {
      "epoch": 2.598933333333333,
      "grad_norm": 0.1988627165555954,
      "learning_rate": 1.7513333333333334e-05,
      "loss": 0.0028,
      "step": 48730
    },
    {
      "epoch": 2.599466666666667,
      "grad_norm": 0.07471591234207153,
      "learning_rate": 1.750666666666667e-05,
      "loss": 0.0027,
      "step": 48740
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.18446482717990875,
      "learning_rate": 1.75e-05,
      "loss": 0.0023,
      "step": 48750
    },
    {
      "epoch": 2.6005333333333334,
      "grad_norm": 0.15632344782352448,
      "learning_rate": 1.7493333333333334e-05,
      "loss": 0.0034,
      "step": 48760
    },
    {
      "epoch": 2.6010666666666666,
      "grad_norm": 0.15523891150951385,
      "learning_rate": 1.7486666666666666e-05,
      "loss": 0.002,
      "step": 48770
    },
    {
      "epoch": 2.6016,
      "grad_norm": 0.7938842177391052,
      "learning_rate": 1.7480000000000002e-05,
      "loss": 0.002,
      "step": 48780
    },
    {
      "epoch": 2.602133333333333,
      "grad_norm": 0.37010815739631653,
      "learning_rate": 1.7473333333333334e-05,
      "loss": 0.0026,
      "step": 48790
    },
    {
      "epoch": 2.602666666666667,
      "grad_norm": 0.1728338748216629,
      "learning_rate": 1.7466666666666667e-05,
      "loss": 0.0028,
      "step": 48800
    },
    {
      "epoch": 2.6032,
      "grad_norm": 0.08263390511274338,
      "learning_rate": 1.7460000000000002e-05,
      "loss": 0.002,
      "step": 48810
    },
    {
      "epoch": 2.6037333333333335,
      "grad_norm": 0.5775482654571533,
      "learning_rate": 1.7453333333333335e-05,
      "loss": 0.0033,
      "step": 48820
    },
    {
      "epoch": 2.6042666666666667,
      "grad_norm": 0.09637971967458725,
      "learning_rate": 1.7446666666666667e-05,
      "loss": 0.002,
      "step": 48830
    },
    {
      "epoch": 2.6048,
      "grad_norm": 0.10169291496276855,
      "learning_rate": 1.7440000000000002e-05,
      "loss": 0.0021,
      "step": 48840
    },
    {
      "epoch": 2.6053333333333333,
      "grad_norm": 0.22251950204372406,
      "learning_rate": 1.7433333333333335e-05,
      "loss": 0.0024,
      "step": 48850
    },
    {
      "epoch": 2.6058666666666666,
      "grad_norm": 0.5744829177856445,
      "learning_rate": 1.7426666666666667e-05,
      "loss": 0.0017,
      "step": 48860
    },
    {
      "epoch": 2.6064,
      "grad_norm": 0.4378952085971832,
      "learning_rate": 1.742e-05,
      "loss": 0.0035,
      "step": 48870
    },
    {
      "epoch": 2.606933333333333,
      "grad_norm": 0.2992519736289978,
      "learning_rate": 1.7413333333333335e-05,
      "loss": 0.0017,
      "step": 48880
    },
    {
      "epoch": 2.607466666666667,
      "grad_norm": 0.7150053977966309,
      "learning_rate": 1.7406666666666667e-05,
      "loss": 0.0034,
      "step": 48890
    },
    {
      "epoch": 2.608,
      "grad_norm": 0.24906885623931885,
      "learning_rate": 1.74e-05,
      "loss": 0.0022,
      "step": 48900
    },
    {
      "epoch": 2.6085333333333334,
      "grad_norm": 0.27825915813446045,
      "learning_rate": 1.7393333333333335e-05,
      "loss": 0.0025,
      "step": 48910
    },
    {
      "epoch": 2.6090666666666666,
      "grad_norm": 0.2942127287387848,
      "learning_rate": 1.7386666666666667e-05,
      "loss": 0.0018,
      "step": 48920
    },
    {
      "epoch": 2.6096,
      "grad_norm": 0.12950043380260468,
      "learning_rate": 1.7380000000000003e-05,
      "loss": 0.0017,
      "step": 48930
    },
    {
      "epoch": 2.610133333333333,
      "grad_norm": 0.3695705235004425,
      "learning_rate": 1.7373333333333332e-05,
      "loss": 0.0031,
      "step": 48940
    },
    {
      "epoch": 2.610666666666667,
      "grad_norm": 0.4833618402481079,
      "learning_rate": 1.7366666666666668e-05,
      "loss": 0.0022,
      "step": 48950
    },
    {
      "epoch": 2.6112,
      "grad_norm": 0.07847683131694794,
      "learning_rate": 1.736e-05,
      "loss": 0.0033,
      "step": 48960
    },
    {
      "epoch": 2.6117333333333335,
      "grad_norm": 0.10972631722688675,
      "learning_rate": 1.7353333333333335e-05,
      "loss": 0.0036,
      "step": 48970
    },
    {
      "epoch": 2.6122666666666667,
      "grad_norm": 0.3335327208042145,
      "learning_rate": 1.7346666666666668e-05,
      "loss": 0.0025,
      "step": 48980
    },
    {
      "epoch": 2.6128,
      "grad_norm": 0.43916741013526917,
      "learning_rate": 1.734e-05,
      "loss": 0.0026,
      "step": 48990
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 0.7176724672317505,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.0021,
      "step": 49000
    },
    {
      "epoch": 2.6138666666666666,
      "grad_norm": 0.17009702324867249,
      "learning_rate": 1.7326666666666668e-05,
      "loss": 0.0028,
      "step": 49010
    },
    {
      "epoch": 2.6144,
      "grad_norm": 0.32760894298553467,
      "learning_rate": 1.732e-05,
      "loss": 0.0023,
      "step": 49020
    },
    {
      "epoch": 2.614933333333333,
      "grad_norm": 0.12150728702545166,
      "learning_rate": 1.7313333333333336e-05,
      "loss": 0.0019,
      "step": 49030
    },
    {
      "epoch": 2.615466666666667,
      "grad_norm": 0.4225437045097351,
      "learning_rate": 1.7306666666666668e-05,
      "loss": 0.0029,
      "step": 49040
    },
    {
      "epoch": 2.616,
      "grad_norm": 0.4265024960041046,
      "learning_rate": 1.73e-05,
      "loss": 0.0017,
      "step": 49050
    },
    {
      "epoch": 2.6165333333333334,
      "grad_norm": 0.2897697687149048,
      "learning_rate": 1.7293333333333333e-05,
      "loss": 0.0019,
      "step": 49060
    },
    {
      "epoch": 2.6170666666666667,
      "grad_norm": 0.44067487120628357,
      "learning_rate": 1.7286666666666668e-05,
      "loss": 0.0033,
      "step": 49070
    },
    {
      "epoch": 2.6176,
      "grad_norm": 0.14716508984565735,
      "learning_rate": 1.728e-05,
      "loss": 0.0019,
      "step": 49080
    },
    {
      "epoch": 2.618133333333333,
      "grad_norm": 0.428752064704895,
      "learning_rate": 1.7273333333333333e-05,
      "loss": 0.004,
      "step": 49090
    },
    {
      "epoch": 2.618666666666667,
      "grad_norm": 0.3489854037761688,
      "learning_rate": 1.726666666666667e-05,
      "loss": 0.0019,
      "step": 49100
    },
    {
      "epoch": 2.6192,
      "grad_norm": 0.20738637447357178,
      "learning_rate": 1.726e-05,
      "loss": 0.0031,
      "step": 49110
    },
    {
      "epoch": 2.6197333333333335,
      "grad_norm": 0.08664118498563766,
      "learning_rate": 1.7253333333333336e-05,
      "loss": 0.0021,
      "step": 49120
    },
    {
      "epoch": 2.6202666666666667,
      "grad_norm": 0.27478981018066406,
      "learning_rate": 1.7246666666666665e-05,
      "loss": 0.0034,
      "step": 49130
    },
    {
      "epoch": 2.6208,
      "grad_norm": 0.20152512192726135,
      "learning_rate": 1.724e-05,
      "loss": 0.0023,
      "step": 49140
    },
    {
      "epoch": 2.6213333333333333,
      "grad_norm": 0.10886912792921066,
      "learning_rate": 1.7233333333333333e-05,
      "loss": 0.0033,
      "step": 49150
    },
    {
      "epoch": 2.6218666666666666,
      "grad_norm": 0.31430602073669434,
      "learning_rate": 1.722666666666667e-05,
      "loss": 0.0027,
      "step": 49160
    },
    {
      "epoch": 2.6224,
      "grad_norm": 0.1129625141620636,
      "learning_rate": 1.722e-05,
      "loss": 0.0023,
      "step": 49170
    },
    {
      "epoch": 2.622933333333333,
      "grad_norm": 0.21882732212543488,
      "learning_rate": 1.7213333333333333e-05,
      "loss": 0.0017,
      "step": 49180
    },
    {
      "epoch": 2.6234666666666664,
      "grad_norm": 0.3377847671508789,
      "learning_rate": 1.720666666666667e-05,
      "loss": 0.0029,
      "step": 49190
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.2274702787399292,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 0.0024,
      "step": 49200
    },
    {
      "epoch": 2.6245333333333334,
      "grad_norm": 0.2267017662525177,
      "learning_rate": 1.7193333333333334e-05,
      "loss": 0.0025,
      "step": 49210
    },
    {
      "epoch": 2.6250666666666667,
      "grad_norm": 0.1888522356748581,
      "learning_rate": 1.718666666666667e-05,
      "loss": 0.0031,
      "step": 49220
    },
    {
      "epoch": 2.6256,
      "grad_norm": 0.36702993512153625,
      "learning_rate": 1.718e-05,
      "loss": 0.0022,
      "step": 49230
    },
    {
      "epoch": 2.626133333333333,
      "grad_norm": 0.18744447827339172,
      "learning_rate": 1.7173333333333334e-05,
      "loss": 0.0034,
      "step": 49240
    },
    {
      "epoch": 2.626666666666667,
      "grad_norm": 0.40430736541748047,
      "learning_rate": 1.7166666666666666e-05,
      "loss": 0.0017,
      "step": 49250
    },
    {
      "epoch": 2.6272,
      "grad_norm": 0.229098379611969,
      "learning_rate": 1.7160000000000002e-05,
      "loss": 0.0019,
      "step": 49260
    },
    {
      "epoch": 2.6277333333333335,
      "grad_norm": 0.16213449835777283,
      "learning_rate": 1.7153333333333334e-05,
      "loss": 0.0027,
      "step": 49270
    },
    {
      "epoch": 2.6282666666666668,
      "grad_norm": 0.158662810921669,
      "learning_rate": 1.7146666666666666e-05,
      "loss": 0.0019,
      "step": 49280
    },
    {
      "epoch": 2.6288,
      "grad_norm": 0.42017507553100586,
      "learning_rate": 1.7140000000000002e-05,
      "loss": 0.0021,
      "step": 49290
    },
    {
      "epoch": 2.6293333333333333,
      "grad_norm": 0.5470754504203796,
      "learning_rate": 1.7133333333333334e-05,
      "loss": 0.0019,
      "step": 49300
    },
    {
      "epoch": 2.6298666666666666,
      "grad_norm": 0.5108591914176941,
      "learning_rate": 1.712666666666667e-05,
      "loss": 0.0025,
      "step": 49310
    },
    {
      "epoch": 2.6304,
      "grad_norm": 0.6576279997825623,
      "learning_rate": 1.712e-05,
      "loss": 0.0034,
      "step": 49320
    },
    {
      "epoch": 2.630933333333333,
      "grad_norm": 0.24507682025432587,
      "learning_rate": 1.7113333333333334e-05,
      "loss": 0.0017,
      "step": 49330
    },
    {
      "epoch": 2.6314666666666664,
      "grad_norm": 0.3425053060054779,
      "learning_rate": 1.7106666666666667e-05,
      "loss": 0.003,
      "step": 49340
    },
    {
      "epoch": 2.632,
      "grad_norm": 0.1141607016324997,
      "learning_rate": 1.7100000000000002e-05,
      "loss": 0.002,
      "step": 49350
    },
    {
      "epoch": 2.6325333333333334,
      "grad_norm": 0.44600552320480347,
      "learning_rate": 1.7093333333333335e-05,
      "loss": 0.0021,
      "step": 49360
    },
    {
      "epoch": 2.6330666666666667,
      "grad_norm": 0.35415276885032654,
      "learning_rate": 1.7086666666666667e-05,
      "loss": 0.0029,
      "step": 49370
    },
    {
      "epoch": 2.6336,
      "grad_norm": 0.18698376417160034,
      "learning_rate": 1.7080000000000002e-05,
      "loss": 0.0026,
      "step": 49380
    },
    {
      "epoch": 2.634133333333333,
      "grad_norm": 0.2621491253376007,
      "learning_rate": 1.707333333333333e-05,
      "loss": 0.0026,
      "step": 49390
    },
    {
      "epoch": 2.634666666666667,
      "grad_norm": 0.38641121983528137,
      "learning_rate": 1.7066666666666667e-05,
      "loss": 0.0034,
      "step": 49400
    },
    {
      "epoch": 2.6352,
      "grad_norm": 0.6162703037261963,
      "learning_rate": 1.706e-05,
      "loss": 0.0027,
      "step": 49410
    },
    {
      "epoch": 2.6357333333333335,
      "grad_norm": 0.3976513147354126,
      "learning_rate": 1.7053333333333335e-05,
      "loss": 0.0021,
      "step": 49420
    },
    {
      "epoch": 2.6362666666666668,
      "grad_norm": 0.4470535218715668,
      "learning_rate": 1.7046666666666667e-05,
      "loss": 0.003,
      "step": 49430
    },
    {
      "epoch": 2.6368,
      "grad_norm": 0.29552480578422546,
      "learning_rate": 1.704e-05,
      "loss": 0.0032,
      "step": 49440
    },
    {
      "epoch": 2.6373333333333333,
      "grad_norm": 0.38101497292518616,
      "learning_rate": 1.7033333333333335e-05,
      "loss": 0.0027,
      "step": 49450
    },
    {
      "epoch": 2.6378666666666666,
      "grad_norm": 0.31158125400543213,
      "learning_rate": 1.7026666666666667e-05,
      "loss": 0.0035,
      "step": 49460
    },
    {
      "epoch": 2.6384,
      "grad_norm": 0.4092366695404053,
      "learning_rate": 1.702e-05,
      "loss": 0.0033,
      "step": 49470
    },
    {
      "epoch": 2.638933333333333,
      "grad_norm": 0.19638536870479584,
      "learning_rate": 1.7013333333333335e-05,
      "loss": 0.0018,
      "step": 49480
    },
    {
      "epoch": 2.6394666666666664,
      "grad_norm": 0.09468189626932144,
      "learning_rate": 1.7006666666666668e-05,
      "loss": 0.002,
      "step": 49490
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.45134609937667847,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.0017,
      "step": 49500
    },
    {
      "epoch": 2.6405333333333334,
      "grad_norm": 0.16825337707996368,
      "learning_rate": 1.6993333333333332e-05,
      "loss": 0.0025,
      "step": 49510
    },
    {
      "epoch": 2.6410666666666667,
      "grad_norm": 0.059418704360723495,
      "learning_rate": 1.6986666666666668e-05,
      "loss": 0.0019,
      "step": 49520
    },
    {
      "epoch": 2.6416,
      "grad_norm": 0.3112041652202606,
      "learning_rate": 1.698e-05,
      "loss": 0.0023,
      "step": 49530
    },
    {
      "epoch": 2.6421333333333332,
      "grad_norm": 0.3945668041706085,
      "learning_rate": 1.6973333333333336e-05,
      "loss": 0.0019,
      "step": 49540
    },
    {
      "epoch": 2.642666666666667,
      "grad_norm": 0.464154988527298,
      "learning_rate": 1.6966666666666668e-05,
      "loss": 0.0021,
      "step": 49550
    },
    {
      "epoch": 2.6432,
      "grad_norm": 0.6100073456764221,
      "learning_rate": 1.696e-05,
      "loss": 0.002,
      "step": 49560
    },
    {
      "epoch": 2.6437333333333335,
      "grad_norm": 0.44586023688316345,
      "learning_rate": 1.6953333333333336e-05,
      "loss": 0.0027,
      "step": 49570
    },
    {
      "epoch": 2.6442666666666668,
      "grad_norm": 0.18767432868480682,
      "learning_rate": 1.6946666666666665e-05,
      "loss": 0.0023,
      "step": 49580
    },
    {
      "epoch": 2.6448,
      "grad_norm": 0.47622793912887573,
      "learning_rate": 1.694e-05,
      "loss": 0.0027,
      "step": 49590
    },
    {
      "epoch": 2.6453333333333333,
      "grad_norm": 0.43888622522354126,
      "learning_rate": 1.6933333333333333e-05,
      "loss": 0.0032,
      "step": 49600
    },
    {
      "epoch": 2.6458666666666666,
      "grad_norm": 0.2538295090198517,
      "learning_rate": 1.692666666666667e-05,
      "loss": 0.0022,
      "step": 49610
    },
    {
      "epoch": 2.6464,
      "grad_norm": 0.15888847410678864,
      "learning_rate": 1.692e-05,
      "loss": 0.0032,
      "step": 49620
    },
    {
      "epoch": 2.646933333333333,
      "grad_norm": 0.44695982336997986,
      "learning_rate": 1.6913333333333333e-05,
      "loss": 0.0026,
      "step": 49630
    },
    {
      "epoch": 2.6474666666666664,
      "grad_norm": 0.24513289332389832,
      "learning_rate": 1.690666666666667e-05,
      "loss": 0.0019,
      "step": 49640
    },
    {
      "epoch": 2.648,
      "grad_norm": 0.17858758568763733,
      "learning_rate": 1.69e-05,
      "loss": 0.0019,
      "step": 49650
    },
    {
      "epoch": 2.6485333333333334,
      "grad_norm": 0.586846649646759,
      "learning_rate": 1.6893333333333333e-05,
      "loss": 0.0019,
      "step": 49660
    },
    {
      "epoch": 2.6490666666666667,
      "grad_norm": 0.3977664113044739,
      "learning_rate": 1.688666666666667e-05,
      "loss": 0.0018,
      "step": 49670
    },
    {
      "epoch": 2.6496,
      "grad_norm": 0.055201344192028046,
      "learning_rate": 1.688e-05,
      "loss": 0.0022,
      "step": 49680
    },
    {
      "epoch": 2.6501333333333332,
      "grad_norm": 0.056791264563798904,
      "learning_rate": 1.6873333333333337e-05,
      "loss": 0.0022,
      "step": 49690
    },
    {
      "epoch": 2.6506666666666665,
      "grad_norm": 0.10719452053308487,
      "learning_rate": 1.6866666666666666e-05,
      "loss": 0.0021,
      "step": 49700
    },
    {
      "epoch": 2.6512000000000002,
      "grad_norm": 0.281162291765213,
      "learning_rate": 1.686e-05,
      "loss": 0.0031,
      "step": 49710
    },
    {
      "epoch": 2.6517333333333335,
      "grad_norm": 0.6155064105987549,
      "learning_rate": 1.6853333333333333e-05,
      "loss": 0.0024,
      "step": 49720
    },
    {
      "epoch": 2.6522666666666668,
      "grad_norm": 0.7061325311660767,
      "learning_rate": 1.6846666666666666e-05,
      "loss": 0.0022,
      "step": 49730
    },
    {
      "epoch": 2.6528,
      "grad_norm": 0.4558578133583069,
      "learning_rate": 1.684e-05,
      "loss": 0.0019,
      "step": 49740
    },
    {
      "epoch": 2.6533333333333333,
      "grad_norm": 0.13815484941005707,
      "learning_rate": 1.6833333333333334e-05,
      "loss": 0.0022,
      "step": 49750
    },
    {
      "epoch": 2.6538666666666666,
      "grad_norm": 0.35729190707206726,
      "learning_rate": 1.682666666666667e-05,
      "loss": 0.0017,
      "step": 49760
    },
    {
      "epoch": 2.6544,
      "grad_norm": 0.3623334765434265,
      "learning_rate": 1.6819999999999998e-05,
      "loss": 0.002,
      "step": 49770
    },
    {
      "epoch": 2.654933333333333,
      "grad_norm": 0.16883230209350586,
      "learning_rate": 1.6813333333333334e-05,
      "loss": 0.0018,
      "step": 49780
    },
    {
      "epoch": 2.6554666666666664,
      "grad_norm": 0.3883025050163269,
      "learning_rate": 1.6806666666666666e-05,
      "loss": 0.0036,
      "step": 49790
    },
    {
      "epoch": 2.656,
      "grad_norm": 0.2573212683200836,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.0023,
      "step": 49800
    },
    {
      "epoch": 2.6565333333333334,
      "grad_norm": 0.3128308355808258,
      "learning_rate": 1.6793333333333334e-05,
      "loss": 0.0021,
      "step": 49810
    },
    {
      "epoch": 2.6570666666666667,
      "grad_norm": 0.27927136421203613,
      "learning_rate": 1.6786666666666666e-05,
      "loss": 0.0028,
      "step": 49820
    },
    {
      "epoch": 2.6576,
      "grad_norm": 0.6469941139221191,
      "learning_rate": 1.6780000000000002e-05,
      "loss": 0.0027,
      "step": 49830
    },
    {
      "epoch": 2.6581333333333332,
      "grad_norm": 0.6753665208816528,
      "learning_rate": 1.6773333333333334e-05,
      "loss": 0.002,
      "step": 49840
    },
    {
      "epoch": 2.6586666666666665,
      "grad_norm": 0.2637639045715332,
      "learning_rate": 1.6766666666666667e-05,
      "loss": 0.0028,
      "step": 49850
    },
    {
      "epoch": 2.6592000000000002,
      "grad_norm": 0.24250659346580505,
      "learning_rate": 1.6760000000000002e-05,
      "loss": 0.0037,
      "step": 49860
    },
    {
      "epoch": 2.6597333333333335,
      "grad_norm": 0.20532670617103577,
      "learning_rate": 1.6753333333333334e-05,
      "loss": 0.0023,
      "step": 49870
    },
    {
      "epoch": 2.660266666666667,
      "grad_norm": 0.3928755819797516,
      "learning_rate": 1.674666666666667e-05,
      "loss": 0.0026,
      "step": 49880
    },
    {
      "epoch": 2.6608,
      "grad_norm": 0.40820083022117615,
      "learning_rate": 1.674e-05,
      "loss": 0.0021,
      "step": 49890
    },
    {
      "epoch": 2.6613333333333333,
      "grad_norm": 0.21829268336296082,
      "learning_rate": 1.6733333333333335e-05,
      "loss": 0.0021,
      "step": 49900
    },
    {
      "epoch": 2.6618666666666666,
      "grad_norm": 0.0910266861319542,
      "learning_rate": 1.6726666666666667e-05,
      "loss": 0.0019,
      "step": 49910
    },
    {
      "epoch": 2.6624,
      "grad_norm": 0.20710933208465576,
      "learning_rate": 1.672e-05,
      "loss": 0.0023,
      "step": 49920
    },
    {
      "epoch": 2.662933333333333,
      "grad_norm": 0.24060069024562836,
      "learning_rate": 1.6713333333333335e-05,
      "loss": 0.0035,
      "step": 49930
    },
    {
      "epoch": 2.6634666666666664,
      "grad_norm": 0.6117770671844482,
      "learning_rate": 1.6706666666666667e-05,
      "loss": 0.0024,
      "step": 49940
    },
    {
      "epoch": 2.664,
      "grad_norm": 0.5237263441085815,
      "learning_rate": 1.6700000000000003e-05,
      "loss": 0.0019,
      "step": 49950
    },
    {
      "epoch": 2.6645333333333334,
      "grad_norm": 0.8788825869560242,
      "learning_rate": 1.669333333333333e-05,
      "loss": 0.0028,
      "step": 49960
    },
    {
      "epoch": 2.6650666666666667,
      "grad_norm": 0.3946225643157959,
      "learning_rate": 1.6686666666666667e-05,
      "loss": 0.0021,
      "step": 49970
    },
    {
      "epoch": 2.6656,
      "grad_norm": 0.4553084671497345,
      "learning_rate": 1.668e-05,
      "loss": 0.002,
      "step": 49980
    },
    {
      "epoch": 2.6661333333333332,
      "grad_norm": 0.5519055724143982,
      "learning_rate": 1.6673333333333335e-05,
      "loss": 0.0026,
      "step": 49990
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.475324422121048,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.0036,
      "step": 50000
    },
    {
      "epoch": 2.6672000000000002,
      "grad_norm": 0.5342229008674622,
      "learning_rate": 1.666e-05,
      "loss": 0.0024,
      "step": 50010
    },
    {
      "epoch": 2.6677333333333335,
      "grad_norm": 0.09375622123479843,
      "learning_rate": 1.6653333333333335e-05,
      "loss": 0.0034,
      "step": 50020
    },
    {
      "epoch": 2.668266666666667,
      "grad_norm": 0.150741845369339,
      "learning_rate": 1.6646666666666668e-05,
      "loss": 0.0027,
      "step": 50030
    },
    {
      "epoch": 2.6688,
      "grad_norm": 0.07573925703763962,
      "learning_rate": 1.664e-05,
      "loss": 0.0027,
      "step": 50040
    },
    {
      "epoch": 2.6693333333333333,
      "grad_norm": 0.44632816314697266,
      "learning_rate": 1.6633333333333336e-05,
      "loss": 0.0016,
      "step": 50050
    },
    {
      "epoch": 2.6698666666666666,
      "grad_norm": 0.2421811819076538,
      "learning_rate": 1.6626666666666668e-05,
      "loss": 0.0017,
      "step": 50060
    },
    {
      "epoch": 2.6704,
      "grad_norm": 0.23934315145015717,
      "learning_rate": 1.662e-05,
      "loss": 0.0024,
      "step": 50070
    },
    {
      "epoch": 2.670933333333333,
      "grad_norm": 0.20792222023010254,
      "learning_rate": 1.6613333333333332e-05,
      "loss": 0.0018,
      "step": 50080
    },
    {
      "epoch": 2.6714666666666664,
      "grad_norm": 0.055937282741069794,
      "learning_rate": 1.6606666666666668e-05,
      "loss": 0.0039,
      "step": 50090
    },
    {
      "epoch": 2.672,
      "grad_norm": 0.3292594254016876,
      "learning_rate": 1.66e-05,
      "loss": 0.0022,
      "step": 50100
    },
    {
      "epoch": 2.6725333333333334,
      "grad_norm": 0.2368728071451187,
      "learning_rate": 1.6593333333333333e-05,
      "loss": 0.0025,
      "step": 50110
    },
    {
      "epoch": 2.6730666666666667,
      "grad_norm": 0.18469803035259247,
      "learning_rate": 1.6586666666666668e-05,
      "loss": 0.0025,
      "step": 50120
    },
    {
      "epoch": 2.6736,
      "grad_norm": 0.4667949378490448,
      "learning_rate": 1.658e-05,
      "loss": 0.0025,
      "step": 50130
    },
    {
      "epoch": 2.6741333333333333,
      "grad_norm": 0.6557618379592896,
      "learning_rate": 1.6573333333333336e-05,
      "loss": 0.0019,
      "step": 50140
    },
    {
      "epoch": 2.6746666666666665,
      "grad_norm": 0.36125648021698,
      "learning_rate": 1.6566666666666665e-05,
      "loss": 0.0031,
      "step": 50150
    },
    {
      "epoch": 2.6752000000000002,
      "grad_norm": 0.4231109619140625,
      "learning_rate": 1.656e-05,
      "loss": 0.0017,
      "step": 50160
    },
    {
      "epoch": 2.6757333333333335,
      "grad_norm": 0.4672473073005676,
      "learning_rate": 1.6553333333333333e-05,
      "loss": 0.0018,
      "step": 50170
    },
    {
      "epoch": 2.676266666666667,
      "grad_norm": 0.49367639422416687,
      "learning_rate": 1.654666666666667e-05,
      "loss": 0.002,
      "step": 50180
    },
    {
      "epoch": 2.6768,
      "grad_norm": 0.33639124035835266,
      "learning_rate": 1.654e-05,
      "loss": 0.002,
      "step": 50190
    },
    {
      "epoch": 2.6773333333333333,
      "grad_norm": 0.15896102786064148,
      "learning_rate": 1.6533333333333333e-05,
      "loss": 0.0021,
      "step": 50200
    },
    {
      "epoch": 2.6778666666666666,
      "grad_norm": 0.39310282468795776,
      "learning_rate": 1.652666666666667e-05,
      "loss": 0.0032,
      "step": 50210
    },
    {
      "epoch": 2.6784,
      "grad_norm": 0.3534400165081024,
      "learning_rate": 1.652e-05,
      "loss": 0.0039,
      "step": 50220
    },
    {
      "epoch": 2.678933333333333,
      "grad_norm": 0.18414926528930664,
      "learning_rate": 1.6513333333333333e-05,
      "loss": 0.0024,
      "step": 50230
    },
    {
      "epoch": 2.6794666666666664,
      "grad_norm": 0.4539363980293274,
      "learning_rate": 1.650666666666667e-05,
      "loss": 0.0029,
      "step": 50240
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.4200435280799866,
      "learning_rate": 1.65e-05,
      "loss": 0.0024,
      "step": 50250
    },
    {
      "epoch": 2.6805333333333334,
      "grad_norm": 0.1422341763973236,
      "learning_rate": 1.6493333333333334e-05,
      "loss": 0.0034,
      "step": 50260
    },
    {
      "epoch": 2.6810666666666667,
      "grad_norm": 0.20501497387886047,
      "learning_rate": 1.6486666666666666e-05,
      "loss": 0.0025,
      "step": 50270
    },
    {
      "epoch": 2.6816,
      "grad_norm": 0.10592678934335709,
      "learning_rate": 1.648e-05,
      "loss": 0.0028,
      "step": 50280
    },
    {
      "epoch": 2.6821333333333333,
      "grad_norm": 0.2127407342195511,
      "learning_rate": 1.6473333333333334e-05,
      "loss": 0.002,
      "step": 50290
    },
    {
      "epoch": 2.6826666666666665,
      "grad_norm": 0.5109238624572754,
      "learning_rate": 1.6466666666666666e-05,
      "loss": 0.0018,
      "step": 50300
    },
    {
      "epoch": 2.6832000000000003,
      "grad_norm": 0.4462279975414276,
      "learning_rate": 1.646e-05,
      "loss": 0.0018,
      "step": 50310
    },
    {
      "epoch": 2.6837333333333335,
      "grad_norm": 0.11661554127931595,
      "learning_rate": 1.6453333333333334e-05,
      "loss": 0.0018,
      "step": 50320
    },
    {
      "epoch": 2.684266666666667,
      "grad_norm": 0.09795992076396942,
      "learning_rate": 1.644666666666667e-05,
      "loss": 0.003,
      "step": 50330
    },
    {
      "epoch": 2.6848,
      "grad_norm": 0.28595224022865295,
      "learning_rate": 1.644e-05,
      "loss": 0.0027,
      "step": 50340
    },
    {
      "epoch": 2.6853333333333333,
      "grad_norm": 0.4181809425354004,
      "learning_rate": 1.6433333333333334e-05,
      "loss": 0.0022,
      "step": 50350
    },
    {
      "epoch": 2.6858666666666666,
      "grad_norm": 0.09424051642417908,
      "learning_rate": 1.6426666666666666e-05,
      "loss": 0.0021,
      "step": 50360
    },
    {
      "epoch": 2.6864,
      "grad_norm": 0.5321242809295654,
      "learning_rate": 1.6420000000000002e-05,
      "loss": 0.0021,
      "step": 50370
    },
    {
      "epoch": 2.686933333333333,
      "grad_norm": 0.38571473956108093,
      "learning_rate": 1.6413333333333334e-05,
      "loss": 0.0033,
      "step": 50380
    },
    {
      "epoch": 2.6874666666666664,
      "grad_norm": 0.13778479397296906,
      "learning_rate": 1.6406666666666667e-05,
      "loss": 0.0036,
      "step": 50390
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 0.23063233494758606,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 0.0022,
      "step": 50400
    },
    {
      "epoch": 2.6885333333333334,
      "grad_norm": 0.21733801066875458,
      "learning_rate": 1.639333333333333e-05,
      "loss": 0.0019,
      "step": 50410
    },
    {
      "epoch": 2.6890666666666667,
      "grad_norm": 0.13568352162837982,
      "learning_rate": 1.6386666666666667e-05,
      "loss": 0.002,
      "step": 50420
    },
    {
      "epoch": 2.6896,
      "grad_norm": 0.24013204872608185,
      "learning_rate": 1.6380000000000002e-05,
      "loss": 0.003,
      "step": 50430
    },
    {
      "epoch": 2.6901333333333333,
      "grad_norm": 0.3014410436153412,
      "learning_rate": 1.6373333333333335e-05,
      "loss": 0.0021,
      "step": 50440
    },
    {
      "epoch": 2.6906666666666665,
      "grad_norm": 0.7210023403167725,
      "learning_rate": 1.6366666666666667e-05,
      "loss": 0.0017,
      "step": 50450
    },
    {
      "epoch": 2.6912000000000003,
      "grad_norm": 0.3182471692562103,
      "learning_rate": 1.636e-05,
      "loss": 0.0027,
      "step": 50460
    },
    {
      "epoch": 2.6917333333333335,
      "grad_norm": 0.40518537163734436,
      "learning_rate": 1.6353333333333335e-05,
      "loss": 0.0034,
      "step": 50470
    },
    {
      "epoch": 2.692266666666667,
      "grad_norm": 0.4997130036354065,
      "learning_rate": 1.6346666666666667e-05,
      "loss": 0.0026,
      "step": 50480
    },
    {
      "epoch": 2.6928,
      "grad_norm": 0.7521635293960571,
      "learning_rate": 1.634e-05,
      "loss": 0.0019,
      "step": 50490
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 0.1700751930475235,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 0.0023,
      "step": 50500
    },
    {
      "epoch": 2.6938666666666666,
      "grad_norm": 0.3263399302959442,
      "learning_rate": 1.6326666666666667e-05,
      "loss": 0.0015,
      "step": 50510
    },
    {
      "epoch": 2.6944,
      "grad_norm": 0.23857641220092773,
      "learning_rate": 1.6320000000000003e-05,
      "loss": 0.0015,
      "step": 50520
    },
    {
      "epoch": 2.694933333333333,
      "grad_norm": 0.10165799409151077,
      "learning_rate": 1.6313333333333332e-05,
      "loss": 0.0026,
      "step": 50530
    },
    {
      "epoch": 2.6954666666666665,
      "grad_norm": 0.3944031000137329,
      "learning_rate": 1.6306666666666668e-05,
      "loss": 0.0021,
      "step": 50540
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 0.7343631386756897,
      "learning_rate": 1.63e-05,
      "loss": 0.0027,
      "step": 50550
    },
    {
      "epoch": 2.6965333333333334,
      "grad_norm": 0.2691689729690552,
      "learning_rate": 1.6293333333333335e-05,
      "loss": 0.0033,
      "step": 50560
    },
    {
      "epoch": 2.6970666666666667,
      "grad_norm": 0.2723941206932068,
      "learning_rate": 1.6286666666666668e-05,
      "loss": 0.0022,
      "step": 50570
    },
    {
      "epoch": 2.6976,
      "grad_norm": 0.2380630224943161,
      "learning_rate": 1.628e-05,
      "loss": 0.0028,
      "step": 50580
    },
    {
      "epoch": 2.6981333333333333,
      "grad_norm": 0.46419304609298706,
      "learning_rate": 1.6273333333333336e-05,
      "loss": 0.0017,
      "step": 50590
    },
    {
      "epoch": 2.6986666666666665,
      "grad_norm": 0.25715547800064087,
      "learning_rate": 1.6266666666666665e-05,
      "loss": 0.0028,
      "step": 50600
    },
    {
      "epoch": 2.6992000000000003,
      "grad_norm": 0.42064738273620605,
      "learning_rate": 1.626e-05,
      "loss": 0.003,
      "step": 50610
    },
    {
      "epoch": 2.6997333333333335,
      "grad_norm": 0.09283044189214706,
      "learning_rate": 1.6253333333333336e-05,
      "loss": 0.0022,
      "step": 50620
    },
    {
      "epoch": 2.700266666666667,
      "grad_norm": 0.38631826639175415,
      "learning_rate": 1.6246666666666668e-05,
      "loss": 0.002,
      "step": 50630
    },
    {
      "epoch": 2.7008,
      "grad_norm": 0.07169134169816971,
      "learning_rate": 1.624e-05,
      "loss": 0.0021,
      "step": 50640
    },
    {
      "epoch": 2.7013333333333334,
      "grad_norm": 0.328557550907135,
      "learning_rate": 1.6233333333333333e-05,
      "loss": 0.0018,
      "step": 50650
    },
    {
      "epoch": 2.7018666666666666,
      "grad_norm": 0.2617937922477722,
      "learning_rate": 1.6226666666666668e-05,
      "loss": 0.0017,
      "step": 50660
    },
    {
      "epoch": 2.7024,
      "grad_norm": 0.2223082035779953,
      "learning_rate": 1.622e-05,
      "loss": 0.0018,
      "step": 50670
    },
    {
      "epoch": 2.702933333333333,
      "grad_norm": 0.3945600390434265,
      "learning_rate": 1.6213333333333333e-05,
      "loss": 0.0043,
      "step": 50680
    },
    {
      "epoch": 2.7034666666666665,
      "grad_norm": 0.32562848925590515,
      "learning_rate": 1.620666666666667e-05,
      "loss": 0.0034,
      "step": 50690
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.07825059443712234,
      "learning_rate": 1.62e-05,
      "loss": 0.0027,
      "step": 50700
    },
    {
      "epoch": 2.7045333333333335,
      "grad_norm": 0.3940238654613495,
      "learning_rate": 1.6193333333333336e-05,
      "loss": 0.0023,
      "step": 50710
    },
    {
      "epoch": 2.7050666666666667,
      "grad_norm": 0.3037545382976532,
      "learning_rate": 1.6186666666666665e-05,
      "loss": 0.0032,
      "step": 50720
    },
    {
      "epoch": 2.7056,
      "grad_norm": 0.20866963267326355,
      "learning_rate": 1.618e-05,
      "loss": 0.0035,
      "step": 50730
    },
    {
      "epoch": 2.7061333333333333,
      "grad_norm": 0.22877250611782074,
      "learning_rate": 1.6173333333333333e-05,
      "loss": 0.0028,
      "step": 50740
    },
    {
      "epoch": 2.7066666666666666,
      "grad_norm": 0.1183730885386467,
      "learning_rate": 1.6166666666666665e-05,
      "loss": 0.0024,
      "step": 50750
    },
    {
      "epoch": 2.7072000000000003,
      "grad_norm": 0.6570828557014465,
      "learning_rate": 1.616e-05,
      "loss": 0.004,
      "step": 50760
    },
    {
      "epoch": 2.7077333333333335,
      "grad_norm": 0.06411166489124298,
      "learning_rate": 1.6153333333333333e-05,
      "loss": 0.0022,
      "step": 50770
    },
    {
      "epoch": 2.708266666666667,
      "grad_norm": 0.15896064043045044,
      "learning_rate": 1.614666666666667e-05,
      "loss": 0.0038,
      "step": 50780
    },
    {
      "epoch": 2.7088,
      "grad_norm": 0.10543493926525116,
      "learning_rate": 1.6139999999999998e-05,
      "loss": 0.0022,
      "step": 50790
    },
    {
      "epoch": 2.7093333333333334,
      "grad_norm": 0.3990609347820282,
      "learning_rate": 1.6133333333333334e-05,
      "loss": 0.0021,
      "step": 50800
    },
    {
      "epoch": 2.7098666666666666,
      "grad_norm": 0.2462453842163086,
      "learning_rate": 1.612666666666667e-05,
      "loss": 0.0027,
      "step": 50810
    },
    {
      "epoch": 2.7104,
      "grad_norm": 0.08418796956539154,
      "learning_rate": 1.612e-05,
      "loss": 0.0018,
      "step": 50820
    },
    {
      "epoch": 2.710933333333333,
      "grad_norm": 0.2243935465812683,
      "learning_rate": 1.6113333333333334e-05,
      "loss": 0.0034,
      "step": 50830
    },
    {
      "epoch": 2.7114666666666665,
      "grad_norm": 0.09043777734041214,
      "learning_rate": 1.6106666666666666e-05,
      "loss": 0.0025,
      "step": 50840
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 0.19382651150226593,
      "learning_rate": 1.6100000000000002e-05,
      "loss": 0.0023,
      "step": 50850
    },
    {
      "epoch": 2.7125333333333335,
      "grad_norm": 0.3637017607688904,
      "learning_rate": 1.6093333333333334e-05,
      "loss": 0.0028,
      "step": 50860
    },
    {
      "epoch": 2.7130666666666667,
      "grad_norm": 0.3373466432094574,
      "learning_rate": 1.6086666666666666e-05,
      "loss": 0.0026,
      "step": 50870
    },
    {
      "epoch": 2.7136,
      "grad_norm": 0.19154779613018036,
      "learning_rate": 1.6080000000000002e-05,
      "loss": 0.0039,
      "step": 50880
    },
    {
      "epoch": 2.7141333333333333,
      "grad_norm": 0.6784380078315735,
      "learning_rate": 1.6073333333333334e-05,
      "loss": 0.0025,
      "step": 50890
    },
    {
      "epoch": 2.7146666666666666,
      "grad_norm": 0.2132171392440796,
      "learning_rate": 1.606666666666667e-05,
      "loss": 0.0028,
      "step": 50900
    },
    {
      "epoch": 2.7152,
      "grad_norm": 0.20810572803020477,
      "learning_rate": 1.606e-05,
      "loss": 0.0022,
      "step": 50910
    },
    {
      "epoch": 2.7157333333333336,
      "grad_norm": 0.29127320647239685,
      "learning_rate": 1.6053333333333334e-05,
      "loss": 0.0023,
      "step": 50920
    },
    {
      "epoch": 2.716266666666667,
      "grad_norm": 0.12479356676340103,
      "learning_rate": 1.6046666666666667e-05,
      "loss": 0.0027,
      "step": 50930
    },
    {
      "epoch": 2.7168,
      "grad_norm": 0.6793844103813171,
      "learning_rate": 1.604e-05,
      "loss": 0.0018,
      "step": 50940
    },
    {
      "epoch": 2.7173333333333334,
      "grad_norm": 0.4246634542942047,
      "learning_rate": 1.6033333333333335e-05,
      "loss": 0.0023,
      "step": 50950
    },
    {
      "epoch": 2.7178666666666667,
      "grad_norm": 0.1417170912027359,
      "learning_rate": 1.6026666666666667e-05,
      "loss": 0.0021,
      "step": 50960
    },
    {
      "epoch": 2.7184,
      "grad_norm": 0.3472289741039276,
      "learning_rate": 1.6020000000000002e-05,
      "loss": 0.0026,
      "step": 50970
    },
    {
      "epoch": 2.718933333333333,
      "grad_norm": 0.26447832584381104,
      "learning_rate": 1.601333333333333e-05,
      "loss": 0.0025,
      "step": 50980
    },
    {
      "epoch": 2.7194666666666665,
      "grad_norm": 0.3433744013309479,
      "learning_rate": 1.6006666666666667e-05,
      "loss": 0.0021,
      "step": 50990
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.1220063716173172,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.0029,
      "step": 51000
    },
    {
      "epoch": 2.7205333333333335,
      "grad_norm": 0.33076968789100647,
      "learning_rate": 1.5993333333333335e-05,
      "loss": 0.0021,
      "step": 51010
    },
    {
      "epoch": 2.7210666666666667,
      "grad_norm": 0.24110251665115356,
      "learning_rate": 1.5986666666666667e-05,
      "loss": 0.0018,
      "step": 51020
    },
    {
      "epoch": 2.7216,
      "grad_norm": 0.25346165895462036,
      "learning_rate": 1.598e-05,
      "loss": 0.0027,
      "step": 51030
    },
    {
      "epoch": 2.7221333333333333,
      "grad_norm": 0.1478395164012909,
      "learning_rate": 1.5973333333333335e-05,
      "loss": 0.002,
      "step": 51040
    },
    {
      "epoch": 2.7226666666666666,
      "grad_norm": 0.63496333360672,
      "learning_rate": 1.5966666666666667e-05,
      "loss": 0.0026,
      "step": 51050
    },
    {
      "epoch": 2.7232,
      "grad_norm": 0.07909972220659256,
      "learning_rate": 1.596e-05,
      "loss": 0.0022,
      "step": 51060
    },
    {
      "epoch": 2.7237333333333336,
      "grad_norm": 0.07541696727275848,
      "learning_rate": 1.5953333333333335e-05,
      "loss": 0.0021,
      "step": 51070
    },
    {
      "epoch": 2.724266666666667,
      "grad_norm": 0.7255991697311401,
      "learning_rate": 1.5946666666666668e-05,
      "loss": 0.0022,
      "step": 51080
    },
    {
      "epoch": 2.7248,
      "grad_norm": 0.41901084780693054,
      "learning_rate": 1.594e-05,
      "loss": 0.0019,
      "step": 51090
    },
    {
      "epoch": 2.7253333333333334,
      "grad_norm": 0.3365546464920044,
      "learning_rate": 1.5933333333333332e-05,
      "loss": 0.0023,
      "step": 51100
    },
    {
      "epoch": 2.7258666666666667,
      "grad_norm": 0.2844615876674652,
      "learning_rate": 1.5926666666666668e-05,
      "loss": 0.0017,
      "step": 51110
    },
    {
      "epoch": 2.7264,
      "grad_norm": 0.4575580954551697,
      "learning_rate": 1.592e-05,
      "loss": 0.0033,
      "step": 51120
    },
    {
      "epoch": 2.726933333333333,
      "grad_norm": 0.16584019362926483,
      "learning_rate": 1.5913333333333332e-05,
      "loss": 0.0035,
      "step": 51130
    },
    {
      "epoch": 2.7274666666666665,
      "grad_norm": 0.1723838746547699,
      "learning_rate": 1.5906666666666668e-05,
      "loss": 0.002,
      "step": 51140
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 0.29967090487480164,
      "learning_rate": 1.59e-05,
      "loss": 0.0027,
      "step": 51150
    },
    {
      "epoch": 2.7285333333333335,
      "grad_norm": 0.11533128470182419,
      "learning_rate": 1.5893333333333336e-05,
      "loss": 0.0023,
      "step": 51160
    },
    {
      "epoch": 2.7290666666666668,
      "grad_norm": 0.18506480753421783,
      "learning_rate": 1.5886666666666665e-05,
      "loss": 0.002,
      "step": 51170
    },
    {
      "epoch": 2.7296,
      "grad_norm": 0.31061992049217224,
      "learning_rate": 1.588e-05,
      "loss": 0.0029,
      "step": 51180
    },
    {
      "epoch": 2.7301333333333333,
      "grad_norm": 0.08017266541719437,
      "learning_rate": 1.5873333333333336e-05,
      "loss": 0.0021,
      "step": 51190
    },
    {
      "epoch": 2.7306666666666666,
      "grad_norm": 0.18309606611728668,
      "learning_rate": 1.586666666666667e-05,
      "loss": 0.0016,
      "step": 51200
    },
    {
      "epoch": 2.7312,
      "grad_norm": 0.17584307491779327,
      "learning_rate": 1.586e-05,
      "loss": 0.0037,
      "step": 51210
    },
    {
      "epoch": 2.7317333333333336,
      "grad_norm": 0.11936203390359879,
      "learning_rate": 1.5853333333333333e-05,
      "loss": 0.0026,
      "step": 51220
    },
    {
      "epoch": 2.732266666666667,
      "grad_norm": 0.08205445855855942,
      "learning_rate": 1.584666666666667e-05,
      "loss": 0.0022,
      "step": 51230
    },
    {
      "epoch": 2.7328,
      "grad_norm": 0.4261542558670044,
      "learning_rate": 1.584e-05,
      "loss": 0.003,
      "step": 51240
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 0.2364306002855301,
      "learning_rate": 1.5833333333333333e-05,
      "loss": 0.004,
      "step": 51250
    },
    {
      "epoch": 2.7338666666666667,
      "grad_norm": 0.08430943638086319,
      "learning_rate": 1.582666666666667e-05,
      "loss": 0.0017,
      "step": 51260
    },
    {
      "epoch": 2.7344,
      "grad_norm": 0.08071953803300858,
      "learning_rate": 1.582e-05,
      "loss": 0.0019,
      "step": 51270
    },
    {
      "epoch": 2.734933333333333,
      "grad_norm": 0.15714630484580994,
      "learning_rate": 1.5813333333333333e-05,
      "loss": 0.0025,
      "step": 51280
    },
    {
      "epoch": 2.7354666666666665,
      "grad_norm": 0.07694558054208755,
      "learning_rate": 1.5806666666666666e-05,
      "loss": 0.0026,
      "step": 51290
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 0.1720396876335144,
      "learning_rate": 1.58e-05,
      "loss": 0.0026,
      "step": 51300
    },
    {
      "epoch": 2.7365333333333335,
      "grad_norm": 0.43002673983573914,
      "learning_rate": 1.5793333333333333e-05,
      "loss": 0.0016,
      "step": 51310
    },
    {
      "epoch": 2.7370666666666668,
      "grad_norm": 0.5973057150840759,
      "learning_rate": 1.5786666666666666e-05,
      "loss": 0.0028,
      "step": 51320
    },
    {
      "epoch": 2.7376,
      "grad_norm": 0.24443450570106506,
      "learning_rate": 1.578e-05,
      "loss": 0.0024,
      "step": 51330
    },
    {
      "epoch": 2.7381333333333333,
      "grad_norm": 0.3455212116241455,
      "learning_rate": 1.5773333333333334e-05,
      "loss": 0.0034,
      "step": 51340
    },
    {
      "epoch": 2.7386666666666666,
      "grad_norm": 0.24094904959201813,
      "learning_rate": 1.576666666666667e-05,
      "loss": 0.0017,
      "step": 51350
    },
    {
      "epoch": 2.7392,
      "grad_norm": 0.14755380153656006,
      "learning_rate": 1.5759999999999998e-05,
      "loss": 0.0019,
      "step": 51360
    },
    {
      "epoch": 2.7397333333333336,
      "grad_norm": 0.052068401128053665,
      "learning_rate": 1.5753333333333334e-05,
      "loss": 0.0028,
      "step": 51370
    },
    {
      "epoch": 2.740266666666667,
      "grad_norm": 0.07782266288995743,
      "learning_rate": 1.574666666666667e-05,
      "loss": 0.0017,
      "step": 51380
    },
    {
      "epoch": 2.7408,
      "grad_norm": 0.30653947591781616,
      "learning_rate": 1.5740000000000002e-05,
      "loss": 0.0034,
      "step": 51390
    },
    {
      "epoch": 2.7413333333333334,
      "grad_norm": 0.42289188504219055,
      "learning_rate": 1.5733333333333334e-05,
      "loss": 0.0018,
      "step": 51400
    },
    {
      "epoch": 2.7418666666666667,
      "grad_norm": 0.05814582481980324,
      "learning_rate": 1.5726666666666666e-05,
      "loss": 0.0027,
      "step": 51410
    },
    {
      "epoch": 2.7424,
      "grad_norm": 0.6513527631759644,
      "learning_rate": 1.5720000000000002e-05,
      "loss": 0.0019,
      "step": 51420
    },
    {
      "epoch": 2.7429333333333332,
      "grad_norm": 0.13807465136051178,
      "learning_rate": 1.5713333333333334e-05,
      "loss": 0.0019,
      "step": 51430
    },
    {
      "epoch": 2.7434666666666665,
      "grad_norm": 0.31197431683540344,
      "learning_rate": 1.5706666666666666e-05,
      "loss": 0.002,
      "step": 51440
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 0.3696141839027405,
      "learning_rate": 1.5700000000000002e-05,
      "loss": 0.0034,
      "step": 51450
    },
    {
      "epoch": 2.7445333333333335,
      "grad_norm": 0.2429760843515396,
      "learning_rate": 1.5693333333333334e-05,
      "loss": 0.0022,
      "step": 51460
    },
    {
      "epoch": 2.7450666666666668,
      "grad_norm": 0.34539949893951416,
      "learning_rate": 1.5686666666666667e-05,
      "loss": 0.0019,
      "step": 51470
    },
    {
      "epoch": 2.7456,
      "grad_norm": 0.3975902199745178,
      "learning_rate": 1.568e-05,
      "loss": 0.0022,
      "step": 51480
    },
    {
      "epoch": 2.7461333333333333,
      "grad_norm": 0.3483186364173889,
      "learning_rate": 1.5673333333333335e-05,
      "loss": 0.0026,
      "step": 51490
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 0.3689095973968506,
      "learning_rate": 1.5666666666666667e-05,
      "loss": 0.0019,
      "step": 51500
    },
    {
      "epoch": 2.7472,
      "grad_norm": 0.30107712745666504,
      "learning_rate": 1.566e-05,
      "loss": 0.0025,
      "step": 51510
    },
    {
      "epoch": 2.7477333333333336,
      "grad_norm": 0.41436606645584106,
      "learning_rate": 1.5653333333333335e-05,
      "loss": 0.0031,
      "step": 51520
    },
    {
      "epoch": 2.748266666666667,
      "grad_norm": 0.16118043661117554,
      "learning_rate": 1.5646666666666667e-05,
      "loss": 0.003,
      "step": 51530
    },
    {
      "epoch": 2.7488,
      "grad_norm": 0.37880516052246094,
      "learning_rate": 1.5640000000000003e-05,
      "loss": 0.003,
      "step": 51540
    },
    {
      "epoch": 2.7493333333333334,
      "grad_norm": 0.47897711396217346,
      "learning_rate": 1.563333333333333e-05,
      "loss": 0.0021,
      "step": 51550
    },
    {
      "epoch": 2.7498666666666667,
      "grad_norm": 0.18375132977962494,
      "learning_rate": 1.5626666666666667e-05,
      "loss": 0.0024,
      "step": 51560
    },
    {
      "epoch": 2.7504,
      "grad_norm": 0.30001795291900635,
      "learning_rate": 1.5620000000000003e-05,
      "loss": 0.0032,
      "step": 51570
    },
    {
      "epoch": 2.7509333333333332,
      "grad_norm": 0.06454774737358093,
      "learning_rate": 1.5613333333333335e-05,
      "loss": 0.0027,
      "step": 51580
    },
    {
      "epoch": 2.7514666666666665,
      "grad_norm": 0.5723360776901245,
      "learning_rate": 1.5606666666666667e-05,
      "loss": 0.0024,
      "step": 51590
    },
    {
      "epoch": 2.752,
      "grad_norm": 0.13286888599395752,
      "learning_rate": 1.56e-05,
      "loss": 0.0018,
      "step": 51600
    },
    {
      "epoch": 2.7525333333333335,
      "grad_norm": 0.4779298007488251,
      "learning_rate": 1.5593333333333335e-05,
      "loss": 0.0028,
      "step": 51610
    },
    {
      "epoch": 2.7530666666666668,
      "grad_norm": 0.21233625710010529,
      "learning_rate": 1.5586666666666668e-05,
      "loss": 0.0037,
      "step": 51620
    },
    {
      "epoch": 2.7536,
      "grad_norm": 0.13355150818824768,
      "learning_rate": 1.558e-05,
      "loss": 0.002,
      "step": 51630
    },
    {
      "epoch": 2.7541333333333333,
      "grad_norm": 0.17779287695884705,
      "learning_rate": 1.5573333333333336e-05,
      "loss": 0.0029,
      "step": 51640
    },
    {
      "epoch": 2.7546666666666666,
      "grad_norm": 0.15330663323402405,
      "learning_rate": 1.5566666666666668e-05,
      "loss": 0.0029,
      "step": 51650
    },
    {
      "epoch": 2.7552,
      "grad_norm": 0.0905940905213356,
      "learning_rate": 1.556e-05,
      "loss": 0.0018,
      "step": 51660
    },
    {
      "epoch": 2.7557333333333336,
      "grad_norm": 0.25330331921577454,
      "learning_rate": 1.5553333333333332e-05,
      "loss": 0.002,
      "step": 51670
    },
    {
      "epoch": 2.756266666666667,
      "grad_norm": 0.873050332069397,
      "learning_rate": 1.5546666666666668e-05,
      "loss": 0.0023,
      "step": 51680
    },
    {
      "epoch": 2.7568,
      "grad_norm": 0.10483288764953613,
      "learning_rate": 1.554e-05,
      "loss": 0.0021,
      "step": 51690
    },
    {
      "epoch": 2.7573333333333334,
      "grad_norm": 0.11424808204174042,
      "learning_rate": 1.5533333333333333e-05,
      "loss": 0.0019,
      "step": 51700
    },
    {
      "epoch": 2.7578666666666667,
      "grad_norm": 0.21871522068977356,
      "learning_rate": 1.5526666666666668e-05,
      "loss": 0.0029,
      "step": 51710
    },
    {
      "epoch": 2.7584,
      "grad_norm": 0.11413134634494781,
      "learning_rate": 1.552e-05,
      "loss": 0.0039,
      "step": 51720
    },
    {
      "epoch": 2.7589333333333332,
      "grad_norm": 0.18306711316108704,
      "learning_rate": 1.5513333333333336e-05,
      "loss": 0.0035,
      "step": 51730
    },
    {
      "epoch": 2.7594666666666665,
      "grad_norm": 0.45318111777305603,
      "learning_rate": 1.5506666666666665e-05,
      "loss": 0.0017,
      "step": 51740
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.2670213580131531,
      "learning_rate": 1.55e-05,
      "loss": 0.002,
      "step": 51750
    },
    {
      "epoch": 2.760533333333333,
      "grad_norm": 0.26309892535209656,
      "learning_rate": 1.5493333333333336e-05,
      "loss": 0.0025,
      "step": 51760
    },
    {
      "epoch": 2.761066666666667,
      "grad_norm": 0.11553993821144104,
      "learning_rate": 1.548666666666667e-05,
      "loss": 0.0028,
      "step": 51770
    },
    {
      "epoch": 2.7616,
      "grad_norm": 0.5793582797050476,
      "learning_rate": 1.548e-05,
      "loss": 0.0018,
      "step": 51780
    },
    {
      "epoch": 2.7621333333333333,
      "grad_norm": 0.3068496286869049,
      "learning_rate": 1.5473333333333333e-05,
      "loss": 0.0028,
      "step": 51790
    },
    {
      "epoch": 2.7626666666666666,
      "grad_norm": 0.3642309904098511,
      "learning_rate": 1.546666666666667e-05,
      "loss": 0.0026,
      "step": 51800
    },
    {
      "epoch": 2.7632,
      "grad_norm": 0.3112300932407379,
      "learning_rate": 1.546e-05,
      "loss": 0.002,
      "step": 51810
    },
    {
      "epoch": 2.7637333333333336,
      "grad_norm": 0.0774930939078331,
      "learning_rate": 1.5453333333333333e-05,
      "loss": 0.0017,
      "step": 51820
    },
    {
      "epoch": 2.764266666666667,
      "grad_norm": 0.14448021352291107,
      "learning_rate": 1.544666666666667e-05,
      "loss": 0.0021,
      "step": 51830
    },
    {
      "epoch": 2.7648,
      "grad_norm": 0.18442043662071228,
      "learning_rate": 1.544e-05,
      "loss": 0.0031,
      "step": 51840
    },
    {
      "epoch": 2.7653333333333334,
      "grad_norm": 0.17313338816165924,
      "learning_rate": 1.5433333333333334e-05,
      "loss": 0.0038,
      "step": 51850
    },
    {
      "epoch": 2.7658666666666667,
      "grad_norm": 0.1533041149377823,
      "learning_rate": 1.5426666666666666e-05,
      "loss": 0.0022,
      "step": 51860
    },
    {
      "epoch": 2.7664,
      "grad_norm": 0.23036251962184906,
      "learning_rate": 1.542e-05,
      "loss": 0.0019,
      "step": 51870
    },
    {
      "epoch": 2.7669333333333332,
      "grad_norm": 0.5037755370140076,
      "learning_rate": 1.5413333333333334e-05,
      "loss": 0.002,
      "step": 51880
    },
    {
      "epoch": 2.7674666666666665,
      "grad_norm": 0.34208759665489197,
      "learning_rate": 1.5406666666666666e-05,
      "loss": 0.0027,
      "step": 51890
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.18451055884361267,
      "learning_rate": 1.54e-05,
      "loss": 0.0031,
      "step": 51900
    },
    {
      "epoch": 2.768533333333333,
      "grad_norm": 0.07028079777956009,
      "learning_rate": 1.5393333333333334e-05,
      "loss": 0.002,
      "step": 51910
    },
    {
      "epoch": 2.769066666666667,
      "grad_norm": 0.7032302618026733,
      "learning_rate": 1.538666666666667e-05,
      "loss": 0.0022,
      "step": 51920
    },
    {
      "epoch": 2.7696,
      "grad_norm": 0.5731802582740784,
      "learning_rate": 1.538e-05,
      "loss": 0.0017,
      "step": 51930
    },
    {
      "epoch": 2.7701333333333333,
      "grad_norm": 0.14503146708011627,
      "learning_rate": 1.5373333333333334e-05,
      "loss": 0.0026,
      "step": 51940
    },
    {
      "epoch": 2.7706666666666666,
      "grad_norm": 0.0830504447221756,
      "learning_rate": 1.536666666666667e-05,
      "loss": 0.0029,
      "step": 51950
    },
    {
      "epoch": 2.7712,
      "grad_norm": 0.13318105041980743,
      "learning_rate": 1.536e-05,
      "loss": 0.0031,
      "step": 51960
    },
    {
      "epoch": 2.7717333333333336,
      "grad_norm": 0.27590882778167725,
      "learning_rate": 1.5353333333333334e-05,
      "loss": 0.0018,
      "step": 51970
    },
    {
      "epoch": 2.772266666666667,
      "grad_norm": 0.0684371069073677,
      "learning_rate": 1.5346666666666667e-05,
      "loss": 0.0025,
      "step": 51980
    },
    {
      "epoch": 2.7728,
      "grad_norm": 0.3754594624042511,
      "learning_rate": 1.5340000000000002e-05,
      "loss": 0.0025,
      "step": 51990
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 0.3254186809062958,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 0.0036,
      "step": 52000
    },
    {
      "epoch": 2.7738666666666667,
      "grad_norm": 0.5282202959060669,
      "learning_rate": 1.5326666666666667e-05,
      "loss": 0.0033,
      "step": 52010
    },
    {
      "epoch": 2.7744,
      "grad_norm": 0.10194192826747894,
      "learning_rate": 1.5320000000000002e-05,
      "loss": 0.0031,
      "step": 52020
    },
    {
      "epoch": 2.7749333333333333,
      "grad_norm": 0.505325198173523,
      "learning_rate": 1.5313333333333335e-05,
      "loss": 0.0018,
      "step": 52030
    },
    {
      "epoch": 2.7754666666666665,
      "grad_norm": 0.08787529915571213,
      "learning_rate": 1.5306666666666667e-05,
      "loss": 0.0017,
      "step": 52040
    },
    {
      "epoch": 2.776,
      "grad_norm": 0.2990364730358124,
      "learning_rate": 1.53e-05,
      "loss": 0.0024,
      "step": 52050
    },
    {
      "epoch": 2.776533333333333,
      "grad_norm": 0.15091314911842346,
      "learning_rate": 1.5293333333333335e-05,
      "loss": 0.0029,
      "step": 52060
    },
    {
      "epoch": 2.777066666666667,
      "grad_norm": 0.24032610654830933,
      "learning_rate": 1.5286666666666667e-05,
      "loss": 0.003,
      "step": 52070
    },
    {
      "epoch": 2.7776,
      "grad_norm": 0.6387102603912354,
      "learning_rate": 1.528e-05,
      "loss": 0.0025,
      "step": 52080
    },
    {
      "epoch": 2.7781333333333333,
      "grad_norm": 0.07665590941905975,
      "learning_rate": 1.5273333333333335e-05,
      "loss": 0.0019,
      "step": 52090
    },
    {
      "epoch": 2.7786666666666666,
      "grad_norm": 0.3968328535556793,
      "learning_rate": 1.5266666666666667e-05,
      "loss": 0.002,
      "step": 52100
    },
    {
      "epoch": 2.7792,
      "grad_norm": 0.42302119731903076,
      "learning_rate": 1.5260000000000003e-05,
      "loss": 0.0029,
      "step": 52110
    },
    {
      "epoch": 2.779733333333333,
      "grad_norm": 0.23821361362934113,
      "learning_rate": 1.5253333333333334e-05,
      "loss": 0.0024,
      "step": 52120
    },
    {
      "epoch": 2.780266666666667,
      "grad_norm": 0.07412172853946686,
      "learning_rate": 1.5246666666666668e-05,
      "loss": 0.0027,
      "step": 52130
    },
    {
      "epoch": 2.7808,
      "grad_norm": 0.3241822123527527,
      "learning_rate": 1.5240000000000001e-05,
      "loss": 0.0034,
      "step": 52140
    },
    {
      "epoch": 2.7813333333333334,
      "grad_norm": 0.11787623912096024,
      "learning_rate": 1.5233333333333332e-05,
      "loss": 0.0021,
      "step": 52150
    },
    {
      "epoch": 2.7818666666666667,
      "grad_norm": 0.12061876803636551,
      "learning_rate": 1.5226666666666668e-05,
      "loss": 0.0025,
      "step": 52160
    },
    {
      "epoch": 2.7824,
      "grad_norm": 0.6717800498008728,
      "learning_rate": 1.5220000000000002e-05,
      "loss": 0.0034,
      "step": 52170
    },
    {
      "epoch": 2.7829333333333333,
      "grad_norm": 0.5702599287033081,
      "learning_rate": 1.5213333333333336e-05,
      "loss": 0.002,
      "step": 52180
    },
    {
      "epoch": 2.7834666666666665,
      "grad_norm": 0.23228926956653595,
      "learning_rate": 1.5206666666666666e-05,
      "loss": 0.0021,
      "step": 52190
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.17530296742916107,
      "learning_rate": 1.52e-05,
      "loss": 0.002,
      "step": 52200
    },
    {
      "epoch": 2.784533333333333,
      "grad_norm": 0.13444483280181885,
      "learning_rate": 1.5193333333333334e-05,
      "loss": 0.0026,
      "step": 52210
    },
    {
      "epoch": 2.785066666666667,
      "grad_norm": 0.4107939302921295,
      "learning_rate": 1.5186666666666668e-05,
      "loss": 0.0038,
      "step": 52220
    },
    {
      "epoch": 2.7856,
      "grad_norm": 0.53566575050354,
      "learning_rate": 1.518e-05,
      "loss": 0.0026,
      "step": 52230
    },
    {
      "epoch": 2.7861333333333334,
      "grad_norm": 0.22870030999183655,
      "learning_rate": 1.5173333333333334e-05,
      "loss": 0.0018,
      "step": 52240
    },
    {
      "epoch": 2.7866666666666666,
      "grad_norm": 0.25126272439956665,
      "learning_rate": 1.5166666666666668e-05,
      "loss": 0.0029,
      "step": 52250
    },
    {
      "epoch": 2.7872,
      "grad_norm": 0.31753554940223694,
      "learning_rate": 1.5160000000000002e-05,
      "loss": 0.002,
      "step": 52260
    },
    {
      "epoch": 2.787733333333333,
      "grad_norm": 0.5116140842437744,
      "learning_rate": 1.5153333333333333e-05,
      "loss": 0.0027,
      "step": 52270
    },
    {
      "epoch": 2.788266666666667,
      "grad_norm": 1.0382068157196045,
      "learning_rate": 1.5146666666666667e-05,
      "loss": 0.0021,
      "step": 52280
    },
    {
      "epoch": 2.7888,
      "grad_norm": 0.360553115606308,
      "learning_rate": 1.514e-05,
      "loss": 0.0027,
      "step": 52290
    },
    {
      "epoch": 2.7893333333333334,
      "grad_norm": 0.3461160361766815,
      "learning_rate": 1.5133333333333333e-05,
      "loss": 0.0026,
      "step": 52300
    },
    {
      "epoch": 2.7898666666666667,
      "grad_norm": 0.10409390181303024,
      "learning_rate": 1.5126666666666667e-05,
      "loss": 0.0024,
      "step": 52310
    },
    {
      "epoch": 2.7904,
      "grad_norm": 0.616663932800293,
      "learning_rate": 1.5120000000000001e-05,
      "loss": 0.0019,
      "step": 52320
    },
    {
      "epoch": 2.7909333333333333,
      "grad_norm": 0.18865415453910828,
      "learning_rate": 1.5113333333333335e-05,
      "loss": 0.002,
      "step": 52330
    },
    {
      "epoch": 2.7914666666666665,
      "grad_norm": 0.45982471108436584,
      "learning_rate": 1.5106666666666665e-05,
      "loss": 0.0051,
      "step": 52340
    },
    {
      "epoch": 2.792,
      "grad_norm": 0.3948432207107544,
      "learning_rate": 1.51e-05,
      "loss": 0.0021,
      "step": 52350
    },
    {
      "epoch": 2.792533333333333,
      "grad_norm": 0.36471644043922424,
      "learning_rate": 1.5093333333333335e-05,
      "loss": 0.002,
      "step": 52360
    },
    {
      "epoch": 2.793066666666667,
      "grad_norm": 0.3537016212940216,
      "learning_rate": 1.5086666666666669e-05,
      "loss": 0.0017,
      "step": 52370
    },
    {
      "epoch": 2.7936,
      "grad_norm": 0.22910845279693604,
      "learning_rate": 1.508e-05,
      "loss": 0.0019,
      "step": 52380
    },
    {
      "epoch": 2.7941333333333334,
      "grad_norm": 0.38292860984802246,
      "learning_rate": 1.5073333333333334e-05,
      "loss": 0.0025,
      "step": 52390
    },
    {
      "epoch": 2.7946666666666666,
      "grad_norm": 0.23635926842689514,
      "learning_rate": 1.5066666666666668e-05,
      "loss": 0.0024,
      "step": 52400
    },
    {
      "epoch": 2.7952,
      "grad_norm": 0.0998186245560646,
      "learning_rate": 1.5060000000000001e-05,
      "loss": 0.0022,
      "step": 52410
    },
    {
      "epoch": 2.795733333333333,
      "grad_norm": 0.7811414003372192,
      "learning_rate": 1.5053333333333334e-05,
      "loss": 0.0044,
      "step": 52420
    },
    {
      "epoch": 2.796266666666667,
      "grad_norm": 0.28199663758277893,
      "learning_rate": 1.5046666666666668e-05,
      "loss": 0.0016,
      "step": 52430
    },
    {
      "epoch": 2.7968,
      "grad_norm": 0.18037430942058563,
      "learning_rate": 1.5040000000000002e-05,
      "loss": 0.0027,
      "step": 52440
    },
    {
      "epoch": 2.7973333333333334,
      "grad_norm": 0.09928056597709656,
      "learning_rate": 1.5033333333333336e-05,
      "loss": 0.0035,
      "step": 52450
    },
    {
      "epoch": 2.7978666666666667,
      "grad_norm": 0.2671414613723755,
      "learning_rate": 1.5026666666666666e-05,
      "loss": 0.0021,
      "step": 52460
    },
    {
      "epoch": 2.7984,
      "grad_norm": 0.10587695240974426,
      "learning_rate": 1.502e-05,
      "loss": 0.0035,
      "step": 52470
    },
    {
      "epoch": 2.7989333333333333,
      "grad_norm": 0.2041223645210266,
      "learning_rate": 1.5013333333333334e-05,
      "loss": 0.0024,
      "step": 52480
    },
    {
      "epoch": 2.7994666666666665,
      "grad_norm": 0.4980185031890869,
      "learning_rate": 1.5006666666666666e-05,
      "loss": 0.0018,
      "step": 52490
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.327786386013031,
      "learning_rate": 1.5e-05,
      "loss": 0.0017,
      "step": 52500
    },
    {
      "epoch": 2.800533333333333,
      "grad_norm": 0.6141586899757385,
      "learning_rate": 1.4993333333333334e-05,
      "loss": 0.0017,
      "step": 52510
    },
    {
      "epoch": 2.801066666666667,
      "grad_norm": 0.07831515371799469,
      "learning_rate": 1.4986666666666668e-05,
      "loss": 0.0022,
      "step": 52520
    },
    {
      "epoch": 2.8016,
      "grad_norm": 0.3422483801841736,
      "learning_rate": 1.4979999999999999e-05,
      "loss": 0.0034,
      "step": 52530
    },
    {
      "epoch": 2.8021333333333334,
      "grad_norm": 0.4134931266307831,
      "learning_rate": 1.4973333333333333e-05,
      "loss": 0.0018,
      "step": 52540
    },
    {
      "epoch": 2.8026666666666666,
      "grad_norm": 0.1131783053278923,
      "learning_rate": 1.4966666666666668e-05,
      "loss": 0.0026,
      "step": 52550
    },
    {
      "epoch": 2.8032,
      "grad_norm": 0.33754679560661316,
      "learning_rate": 1.4960000000000002e-05,
      "loss": 0.002,
      "step": 52560
    },
    {
      "epoch": 2.803733333333333,
      "grad_norm": 0.2032676786184311,
      "learning_rate": 1.4953333333333333e-05,
      "loss": 0.0031,
      "step": 52570
    },
    {
      "epoch": 2.804266666666667,
      "grad_norm": 0.12489886581897736,
      "learning_rate": 1.4946666666666667e-05,
      "loss": 0.0032,
      "step": 52580
    },
    {
      "epoch": 2.8048,
      "grad_norm": 0.22849556803703308,
      "learning_rate": 1.4940000000000001e-05,
      "loss": 0.0022,
      "step": 52590
    },
    {
      "epoch": 2.8053333333333335,
      "grad_norm": 0.2960645258426666,
      "learning_rate": 1.4933333333333335e-05,
      "loss": 0.0026,
      "step": 52600
    },
    {
      "epoch": 2.8058666666666667,
      "grad_norm": 0.2443501502275467,
      "learning_rate": 1.4926666666666667e-05,
      "loss": 0.0026,
      "step": 52610
    },
    {
      "epoch": 2.8064,
      "grad_norm": 0.2978043258190155,
      "learning_rate": 1.4920000000000001e-05,
      "loss": 0.0031,
      "step": 52620
    },
    {
      "epoch": 2.8069333333333333,
      "grad_norm": 0.5829580426216125,
      "learning_rate": 1.4913333333333335e-05,
      "loss": 0.0022,
      "step": 52630
    },
    {
      "epoch": 2.8074666666666666,
      "grad_norm": 0.13271868228912354,
      "learning_rate": 1.4906666666666666e-05,
      "loss": 0.003,
      "step": 52640
    },
    {
      "epoch": 2.808,
      "grad_norm": 0.3634788691997528,
      "learning_rate": 1.49e-05,
      "loss": 0.0018,
      "step": 52650
    },
    {
      "epoch": 2.808533333333333,
      "grad_norm": 0.43714022636413574,
      "learning_rate": 1.4893333333333334e-05,
      "loss": 0.0021,
      "step": 52660
    },
    {
      "epoch": 2.809066666666667,
      "grad_norm": 0.3682388961315155,
      "learning_rate": 1.4886666666666668e-05,
      "loss": 0.0035,
      "step": 52670
    },
    {
      "epoch": 2.8096,
      "grad_norm": 0.1394178718328476,
      "learning_rate": 1.488e-05,
      "loss": 0.002,
      "step": 52680
    },
    {
      "epoch": 2.8101333333333334,
      "grad_norm": 0.600715160369873,
      "learning_rate": 1.4873333333333334e-05,
      "loss": 0.002,
      "step": 52690
    },
    {
      "epoch": 2.8106666666666666,
      "grad_norm": 0.06990635395050049,
      "learning_rate": 1.4866666666666668e-05,
      "loss": 0.0019,
      "step": 52700
    },
    {
      "epoch": 2.8112,
      "grad_norm": 0.12423722445964813,
      "learning_rate": 1.4860000000000002e-05,
      "loss": 0.0018,
      "step": 52710
    },
    {
      "epoch": 2.811733333333333,
      "grad_norm": 0.32340431213378906,
      "learning_rate": 1.4853333333333332e-05,
      "loss": 0.0027,
      "step": 52720
    },
    {
      "epoch": 2.812266666666667,
      "grad_norm": 0.3546088933944702,
      "learning_rate": 1.4846666666666666e-05,
      "loss": 0.0033,
      "step": 52730
    },
    {
      "epoch": 2.8128,
      "grad_norm": 0.23757871985435486,
      "learning_rate": 1.4840000000000002e-05,
      "loss": 0.0021,
      "step": 52740
    },
    {
      "epoch": 2.8133333333333335,
      "grad_norm": 0.2937939763069153,
      "learning_rate": 1.4833333333333336e-05,
      "loss": 0.0019,
      "step": 52750
    },
    {
      "epoch": 2.8138666666666667,
      "grad_norm": 0.1523437649011612,
      "learning_rate": 1.4826666666666666e-05,
      "loss": 0.003,
      "step": 52760
    },
    {
      "epoch": 2.8144,
      "grad_norm": 0.35544323921203613,
      "learning_rate": 1.482e-05,
      "loss": 0.0016,
      "step": 52770
    },
    {
      "epoch": 2.8149333333333333,
      "grad_norm": 0.24187877774238586,
      "learning_rate": 1.4813333333333334e-05,
      "loss": 0.0023,
      "step": 52780
    },
    {
      "epoch": 2.8154666666666666,
      "grad_norm": 0.3636474907398224,
      "learning_rate": 1.4806666666666668e-05,
      "loss": 0.0024,
      "step": 52790
    },
    {
      "epoch": 2.816,
      "grad_norm": 0.2142466902732849,
      "learning_rate": 1.48e-05,
      "loss": 0.002,
      "step": 52800
    },
    {
      "epoch": 2.816533333333333,
      "grad_norm": 0.3616986572742462,
      "learning_rate": 1.4793333333333335e-05,
      "loss": 0.0026,
      "step": 52810
    },
    {
      "epoch": 2.817066666666667,
      "grad_norm": 0.3962932825088501,
      "learning_rate": 1.4786666666666669e-05,
      "loss": 0.0021,
      "step": 52820
    },
    {
      "epoch": 2.8176,
      "grad_norm": 0.0709313303232193,
      "learning_rate": 1.4779999999999999e-05,
      "loss": 0.0036,
      "step": 52830
    },
    {
      "epoch": 2.8181333333333334,
      "grad_norm": 0.31346750259399414,
      "learning_rate": 1.4773333333333333e-05,
      "loss": 0.0021,
      "step": 52840
    },
    {
      "epoch": 2.8186666666666667,
      "grad_norm": 0.31784480810165405,
      "learning_rate": 1.4766666666666667e-05,
      "loss": 0.002,
      "step": 52850
    },
    {
      "epoch": 2.8192,
      "grad_norm": 0.3279985785484314,
      "learning_rate": 1.4760000000000001e-05,
      "loss": 0.0025,
      "step": 52860
    },
    {
      "epoch": 2.819733333333333,
      "grad_norm": 0.16660892963409424,
      "learning_rate": 1.4753333333333333e-05,
      "loss": 0.0031,
      "step": 52870
    },
    {
      "epoch": 2.820266666666667,
      "grad_norm": 0.09328823536634445,
      "learning_rate": 1.4746666666666667e-05,
      "loss": 0.0024,
      "step": 52880
    },
    {
      "epoch": 2.8208,
      "grad_norm": 0.24755536019802094,
      "learning_rate": 1.4740000000000001e-05,
      "loss": 0.0043,
      "step": 52890
    },
    {
      "epoch": 2.8213333333333335,
      "grad_norm": 0.13674505054950714,
      "learning_rate": 1.4733333333333335e-05,
      "loss": 0.0028,
      "step": 52900
    },
    {
      "epoch": 2.8218666666666667,
      "grad_norm": 0.429000586271286,
      "learning_rate": 1.4726666666666666e-05,
      "loss": 0.0024,
      "step": 52910
    },
    {
      "epoch": 2.8224,
      "grad_norm": 0.20460015535354614,
      "learning_rate": 1.472e-05,
      "loss": 0.0019,
      "step": 52920
    },
    {
      "epoch": 2.8229333333333333,
      "grad_norm": 0.35919174551963806,
      "learning_rate": 1.4713333333333335e-05,
      "loss": 0.003,
      "step": 52930
    },
    {
      "epoch": 2.8234666666666666,
      "grad_norm": 0.5624071955680847,
      "learning_rate": 1.470666666666667e-05,
      "loss": 0.0024,
      "step": 52940
    },
    {
      "epoch": 2.824,
      "grad_norm": 0.10280590504407883,
      "learning_rate": 1.47e-05,
      "loss": 0.0024,
      "step": 52950
    },
    {
      "epoch": 2.824533333333333,
      "grad_norm": 0.2354988306760788,
      "learning_rate": 1.4693333333333334e-05,
      "loss": 0.0024,
      "step": 52960
    },
    {
      "epoch": 2.8250666666666664,
      "grad_norm": 0.2845505177974701,
      "learning_rate": 1.4686666666666668e-05,
      "loss": 0.002,
      "step": 52970
    },
    {
      "epoch": 2.8256,
      "grad_norm": 0.13980181515216827,
      "learning_rate": 1.4680000000000002e-05,
      "loss": 0.0022,
      "step": 52980
    },
    {
      "epoch": 2.8261333333333334,
      "grad_norm": 0.5825150012969971,
      "learning_rate": 1.4673333333333334e-05,
      "loss": 0.003,
      "step": 52990
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 0.2431168258190155,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 0.0019,
      "step": 53000
    },
    {
      "epoch": 2.8272,
      "grad_norm": 0.3399164080619812,
      "learning_rate": 1.4660000000000002e-05,
      "loss": 0.0023,
      "step": 53010
    },
    {
      "epoch": 2.827733333333333,
      "grad_norm": 0.3867303729057312,
      "learning_rate": 1.4653333333333333e-05,
      "loss": 0.0029,
      "step": 53020
    },
    {
      "epoch": 2.828266666666667,
      "grad_norm": 0.23079951107501984,
      "learning_rate": 1.4646666666666666e-05,
      "loss": 0.0039,
      "step": 53030
    },
    {
      "epoch": 2.8288,
      "grad_norm": 0.2646559476852417,
      "learning_rate": 1.464e-05,
      "loss": 0.002,
      "step": 53040
    },
    {
      "epoch": 2.8293333333333335,
      "grad_norm": 0.3981356620788574,
      "learning_rate": 1.4633333333333334e-05,
      "loss": 0.0021,
      "step": 53050
    },
    {
      "epoch": 2.8298666666666668,
      "grad_norm": 0.3565289378166199,
      "learning_rate": 1.4626666666666667e-05,
      "loss": 0.0016,
      "step": 53060
    },
    {
      "epoch": 2.8304,
      "grad_norm": 0.14135108888149261,
      "learning_rate": 1.462e-05,
      "loss": 0.0018,
      "step": 53070
    },
    {
      "epoch": 2.8309333333333333,
      "grad_norm": 0.0904400646686554,
      "learning_rate": 1.4613333333333335e-05,
      "loss": 0.0024,
      "step": 53080
    },
    {
      "epoch": 2.8314666666666666,
      "grad_norm": 0.22357980906963348,
      "learning_rate": 1.4606666666666669e-05,
      "loss": 0.0026,
      "step": 53090
    },
    {
      "epoch": 2.832,
      "grad_norm": 0.1427161544561386,
      "learning_rate": 1.4599999999999999e-05,
      "loss": 0.0023,
      "step": 53100
    },
    {
      "epoch": 2.832533333333333,
      "grad_norm": 0.675443708896637,
      "learning_rate": 1.4593333333333333e-05,
      "loss": 0.0027,
      "step": 53110
    },
    {
      "epoch": 2.8330666666666664,
      "grad_norm": 0.35443979501724243,
      "learning_rate": 1.4586666666666669e-05,
      "loss": 0.0029,
      "step": 53120
    },
    {
      "epoch": 2.8336,
      "grad_norm": 0.09004596620798111,
      "learning_rate": 1.4580000000000003e-05,
      "loss": 0.0022,
      "step": 53130
    },
    {
      "epoch": 2.8341333333333334,
      "grad_norm": 0.39597800374031067,
      "learning_rate": 1.4573333333333333e-05,
      "loss": 0.0034,
      "step": 53140
    },
    {
      "epoch": 2.8346666666666667,
      "grad_norm": 0.47644609212875366,
      "learning_rate": 1.4566666666666667e-05,
      "loss": 0.0022,
      "step": 53150
    },
    {
      "epoch": 2.8352,
      "grad_norm": 0.747234046459198,
      "learning_rate": 1.4560000000000001e-05,
      "loss": 0.0021,
      "step": 53160
    },
    {
      "epoch": 2.835733333333333,
      "grad_norm": 0.377865195274353,
      "learning_rate": 1.4553333333333333e-05,
      "loss": 0.002,
      "step": 53170
    },
    {
      "epoch": 2.836266666666667,
      "grad_norm": 0.0732337012887001,
      "learning_rate": 1.4546666666666667e-05,
      "loss": 0.0028,
      "step": 53180
    },
    {
      "epoch": 2.8368,
      "grad_norm": 0.4117894768714905,
      "learning_rate": 1.4540000000000001e-05,
      "loss": 0.0033,
      "step": 53190
    },
    {
      "epoch": 2.8373333333333335,
      "grad_norm": 0.25103381276130676,
      "learning_rate": 1.4533333333333335e-05,
      "loss": 0.0025,
      "step": 53200
    },
    {
      "epoch": 2.8378666666666668,
      "grad_norm": 0.10450856387615204,
      "learning_rate": 1.4526666666666666e-05,
      "loss": 0.0029,
      "step": 53210
    },
    {
      "epoch": 2.8384,
      "grad_norm": 0.28117433190345764,
      "learning_rate": 1.452e-05,
      "loss": 0.0041,
      "step": 53220
    },
    {
      "epoch": 2.8389333333333333,
      "grad_norm": 0.34975793957710266,
      "learning_rate": 1.4513333333333334e-05,
      "loss": 0.0036,
      "step": 53230
    },
    {
      "epoch": 2.8394666666666666,
      "grad_norm": 0.319174587726593,
      "learning_rate": 1.4506666666666668e-05,
      "loss": 0.0019,
      "step": 53240
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.12724293768405914,
      "learning_rate": 1.45e-05,
      "loss": 0.0027,
      "step": 53250
    },
    {
      "epoch": 2.840533333333333,
      "grad_norm": 0.19409844279289246,
      "learning_rate": 1.4493333333333334e-05,
      "loss": 0.0025,
      "step": 53260
    },
    {
      "epoch": 2.8410666666666664,
      "grad_norm": 0.2998517155647278,
      "learning_rate": 1.4486666666666668e-05,
      "loss": 0.004,
      "step": 53270
    },
    {
      "epoch": 2.8416,
      "grad_norm": 0.30323517322540283,
      "learning_rate": 1.4480000000000002e-05,
      "loss": 0.0023,
      "step": 53280
    },
    {
      "epoch": 2.8421333333333334,
      "grad_norm": 0.09669390320777893,
      "learning_rate": 1.4473333333333333e-05,
      "loss": 0.0035,
      "step": 53290
    },
    {
      "epoch": 2.8426666666666667,
      "grad_norm": 0.20924292504787445,
      "learning_rate": 1.4466666666666667e-05,
      "loss": 0.0018,
      "step": 53300
    },
    {
      "epoch": 2.8432,
      "grad_norm": 0.6491920948028564,
      "learning_rate": 1.4460000000000002e-05,
      "loss": 0.0026,
      "step": 53310
    },
    {
      "epoch": 2.8437333333333332,
      "grad_norm": 0.21765448153018951,
      "learning_rate": 1.4453333333333336e-05,
      "loss": 0.0043,
      "step": 53320
    },
    {
      "epoch": 2.844266666666667,
      "grad_norm": 0.11572219431400299,
      "learning_rate": 1.4446666666666667e-05,
      "loss": 0.0036,
      "step": 53330
    },
    {
      "epoch": 2.8448,
      "grad_norm": 0.36629825830459595,
      "learning_rate": 1.444e-05,
      "loss": 0.002,
      "step": 53340
    },
    {
      "epoch": 2.8453333333333335,
      "grad_norm": 0.3647058606147766,
      "learning_rate": 1.4433333333333335e-05,
      "loss": 0.0029,
      "step": 53350
    },
    {
      "epoch": 2.8458666666666668,
      "grad_norm": 0.248961940407753,
      "learning_rate": 1.4426666666666667e-05,
      "loss": 0.0029,
      "step": 53360
    },
    {
      "epoch": 2.8464,
      "grad_norm": 0.3070993721485138,
      "learning_rate": 1.4420000000000001e-05,
      "loss": 0.0024,
      "step": 53370
    },
    {
      "epoch": 2.8469333333333333,
      "grad_norm": 0.23950794339179993,
      "learning_rate": 1.4413333333333335e-05,
      "loss": 0.0019,
      "step": 53380
    },
    {
      "epoch": 2.8474666666666666,
      "grad_norm": 0.2264622449874878,
      "learning_rate": 1.4406666666666669e-05,
      "loss": 0.0024,
      "step": 53390
    },
    {
      "epoch": 2.848,
      "grad_norm": 0.05942728370428085,
      "learning_rate": 1.44e-05,
      "loss": 0.0025,
      "step": 53400
    },
    {
      "epoch": 2.848533333333333,
      "grad_norm": 0.18575718998908997,
      "learning_rate": 1.4393333333333333e-05,
      "loss": 0.0017,
      "step": 53410
    },
    {
      "epoch": 2.8490666666666664,
      "grad_norm": 0.3832989037036896,
      "learning_rate": 1.4386666666666667e-05,
      "loss": 0.0034,
      "step": 53420
    },
    {
      "epoch": 2.8496,
      "grad_norm": 0.36958882212638855,
      "learning_rate": 1.4380000000000001e-05,
      "loss": 0.003,
      "step": 53430
    },
    {
      "epoch": 2.8501333333333334,
      "grad_norm": 0.15287433564662933,
      "learning_rate": 1.4373333333333334e-05,
      "loss": 0.002,
      "step": 53440
    },
    {
      "epoch": 2.8506666666666667,
      "grad_norm": 0.11076781898736954,
      "learning_rate": 1.4366666666666667e-05,
      "loss": 0.0024,
      "step": 53450
    },
    {
      "epoch": 2.8512,
      "grad_norm": 0.21638700366020203,
      "learning_rate": 1.4360000000000001e-05,
      "loss": 0.0019,
      "step": 53460
    },
    {
      "epoch": 2.8517333333333332,
      "grad_norm": 0.35891827940940857,
      "learning_rate": 1.4353333333333335e-05,
      "loss": 0.0023,
      "step": 53470
    },
    {
      "epoch": 2.8522666666666665,
      "grad_norm": 0.3329958915710449,
      "learning_rate": 1.4346666666666666e-05,
      "loss": 0.0023,
      "step": 53480
    },
    {
      "epoch": 2.8528000000000002,
      "grad_norm": 0.06025196239352226,
      "learning_rate": 1.434e-05,
      "loss": 0.0025,
      "step": 53490
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 0.16066497564315796,
      "learning_rate": 1.4333333333333334e-05,
      "loss": 0.0021,
      "step": 53500
    },
    {
      "epoch": 2.8538666666666668,
      "grad_norm": 0.12572041153907776,
      "learning_rate": 1.4326666666666666e-05,
      "loss": 0.0019,
      "step": 53510
    },
    {
      "epoch": 2.8544,
      "grad_norm": 0.23873059451580048,
      "learning_rate": 1.432e-05,
      "loss": 0.0019,
      "step": 53520
    },
    {
      "epoch": 2.8549333333333333,
      "grad_norm": 0.09513667225837708,
      "learning_rate": 1.4313333333333334e-05,
      "loss": 0.003,
      "step": 53530
    },
    {
      "epoch": 2.8554666666666666,
      "grad_norm": 0.10532380640506744,
      "learning_rate": 1.4306666666666668e-05,
      "loss": 0.002,
      "step": 53540
    },
    {
      "epoch": 2.856,
      "grad_norm": 0.09077910333871841,
      "learning_rate": 1.43e-05,
      "loss": 0.0017,
      "step": 53550
    },
    {
      "epoch": 2.856533333333333,
      "grad_norm": 0.5873187184333801,
      "learning_rate": 1.4293333333333334e-05,
      "loss": 0.0026,
      "step": 53560
    },
    {
      "epoch": 2.8570666666666664,
      "grad_norm": 0.2726653814315796,
      "learning_rate": 1.4286666666666668e-05,
      "loss": 0.0034,
      "step": 53570
    },
    {
      "epoch": 2.8576,
      "grad_norm": 0.19866405427455902,
      "learning_rate": 1.4280000000000002e-05,
      "loss": 0.0022,
      "step": 53580
    },
    {
      "epoch": 2.8581333333333334,
      "grad_norm": 0.318445086479187,
      "learning_rate": 1.4273333333333333e-05,
      "loss": 0.0031,
      "step": 53590
    },
    {
      "epoch": 2.8586666666666667,
      "grad_norm": 0.25496187806129456,
      "learning_rate": 1.4266666666666667e-05,
      "loss": 0.0029,
      "step": 53600
    },
    {
      "epoch": 2.8592,
      "grad_norm": 0.15361618995666504,
      "learning_rate": 1.426e-05,
      "loss": 0.0026,
      "step": 53610
    },
    {
      "epoch": 2.8597333333333332,
      "grad_norm": 0.3076733350753784,
      "learning_rate": 1.4253333333333335e-05,
      "loss": 0.0024,
      "step": 53620
    },
    {
      "epoch": 2.8602666666666665,
      "grad_norm": 0.23407229781150818,
      "learning_rate": 1.4246666666666667e-05,
      "loss": 0.0028,
      "step": 53630
    },
    {
      "epoch": 2.8608000000000002,
      "grad_norm": 0.2155689299106598,
      "learning_rate": 1.4240000000000001e-05,
      "loss": 0.003,
      "step": 53640
    },
    {
      "epoch": 2.8613333333333335,
      "grad_norm": 0.42204126715660095,
      "learning_rate": 1.4233333333333335e-05,
      "loss": 0.0019,
      "step": 53650
    },
    {
      "epoch": 2.861866666666667,
      "grad_norm": 0.37471628189086914,
      "learning_rate": 1.4226666666666669e-05,
      "loss": 0.0023,
      "step": 53660
    },
    {
      "epoch": 2.8624,
      "grad_norm": 0.2695854604244232,
      "learning_rate": 1.422e-05,
      "loss": 0.0028,
      "step": 53670
    },
    {
      "epoch": 2.8629333333333333,
      "grad_norm": 0.3136465847492218,
      "learning_rate": 1.4213333333333333e-05,
      "loss": 0.0029,
      "step": 53680
    },
    {
      "epoch": 2.8634666666666666,
      "grad_norm": 0.5002381801605225,
      "learning_rate": 1.4206666666666667e-05,
      "loss": 0.0032,
      "step": 53690
    },
    {
      "epoch": 2.864,
      "grad_norm": 0.5856355428695679,
      "learning_rate": 1.42e-05,
      "loss": 0.0031,
      "step": 53700
    },
    {
      "epoch": 2.864533333333333,
      "grad_norm": 0.29072874784469604,
      "learning_rate": 1.4193333333333334e-05,
      "loss": 0.0021,
      "step": 53710
    },
    {
      "epoch": 2.8650666666666664,
      "grad_norm": 0.32029250264167786,
      "learning_rate": 1.4186666666666667e-05,
      "loss": 0.0035,
      "step": 53720
    },
    {
      "epoch": 2.8656,
      "grad_norm": 0.47636112570762634,
      "learning_rate": 1.4180000000000001e-05,
      "loss": 0.0031,
      "step": 53730
    },
    {
      "epoch": 2.8661333333333334,
      "grad_norm": 0.5830753445625305,
      "learning_rate": 1.4173333333333334e-05,
      "loss": 0.0022,
      "step": 53740
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 0.3126693665981293,
      "learning_rate": 1.4166666666666668e-05,
      "loss": 0.0026,
      "step": 53750
    },
    {
      "epoch": 2.8672,
      "grad_norm": 0.15613548457622528,
      "learning_rate": 1.4160000000000002e-05,
      "loss": 0.0037,
      "step": 53760
    },
    {
      "epoch": 2.8677333333333332,
      "grad_norm": 0.1722463071346283,
      "learning_rate": 1.4153333333333336e-05,
      "loss": 0.0018,
      "step": 53770
    },
    {
      "epoch": 2.8682666666666665,
      "grad_norm": 0.07447077333927155,
      "learning_rate": 1.4146666666666666e-05,
      "loss": 0.0031,
      "step": 53780
    },
    {
      "epoch": 2.8688000000000002,
      "grad_norm": 0.23600973188877106,
      "learning_rate": 1.414e-05,
      "loss": 0.0023,
      "step": 53790
    },
    {
      "epoch": 2.8693333333333335,
      "grad_norm": 0.4431033134460449,
      "learning_rate": 1.4133333333333334e-05,
      "loss": 0.0029,
      "step": 53800
    },
    {
      "epoch": 2.869866666666667,
      "grad_norm": 0.2694185674190521,
      "learning_rate": 1.4126666666666668e-05,
      "loss": 0.0029,
      "step": 53810
    },
    {
      "epoch": 2.8704,
      "grad_norm": 0.596505343914032,
      "learning_rate": 1.412e-05,
      "loss": 0.0025,
      "step": 53820
    },
    {
      "epoch": 2.8709333333333333,
      "grad_norm": 0.11930800974369049,
      "learning_rate": 1.4113333333333334e-05,
      "loss": 0.0019,
      "step": 53830
    },
    {
      "epoch": 2.8714666666666666,
      "grad_norm": 0.20699211955070496,
      "learning_rate": 1.4106666666666668e-05,
      "loss": 0.0028,
      "step": 53840
    },
    {
      "epoch": 2.872,
      "grad_norm": 0.14182908833026886,
      "learning_rate": 1.4099999999999999e-05,
      "loss": 0.0025,
      "step": 53850
    },
    {
      "epoch": 2.872533333333333,
      "grad_norm": 0.29130497574806213,
      "learning_rate": 1.4093333333333333e-05,
      "loss": 0.0026,
      "step": 53860
    },
    {
      "epoch": 2.8730666666666664,
      "grad_norm": 0.27594849467277527,
      "learning_rate": 1.4086666666666667e-05,
      "loss": 0.0039,
      "step": 53870
    },
    {
      "epoch": 2.8736,
      "grad_norm": 0.1692766398191452,
      "learning_rate": 1.408e-05,
      "loss": 0.0021,
      "step": 53880
    },
    {
      "epoch": 2.8741333333333334,
      "grad_norm": 0.4605945944786072,
      "learning_rate": 1.4073333333333333e-05,
      "loss": 0.0026,
      "step": 53890
    },
    {
      "epoch": 2.8746666666666667,
      "grad_norm": 0.23353366553783417,
      "learning_rate": 1.4066666666666667e-05,
      "loss": 0.0024,
      "step": 53900
    },
    {
      "epoch": 2.8752,
      "grad_norm": 0.14895065128803253,
      "learning_rate": 1.4060000000000001e-05,
      "loss": 0.0022,
      "step": 53910
    },
    {
      "epoch": 2.8757333333333333,
      "grad_norm": 0.352263480424881,
      "learning_rate": 1.4053333333333335e-05,
      "loss": 0.003,
      "step": 53920
    },
    {
      "epoch": 2.8762666666666665,
      "grad_norm": 0.36926794052124023,
      "learning_rate": 1.4046666666666667e-05,
      "loss": 0.0025,
      "step": 53930
    },
    {
      "epoch": 2.8768000000000002,
      "grad_norm": 0.5108333826065063,
      "learning_rate": 1.4040000000000001e-05,
      "loss": 0.002,
      "step": 53940
    },
    {
      "epoch": 2.8773333333333335,
      "grad_norm": 0.6448608636856079,
      "learning_rate": 1.4033333333333335e-05,
      "loss": 0.0031,
      "step": 53950
    },
    {
      "epoch": 2.877866666666667,
      "grad_norm": 0.5292078256607056,
      "learning_rate": 1.4026666666666669e-05,
      "loss": 0.0025,
      "step": 53960
    },
    {
      "epoch": 2.8784,
      "grad_norm": 0.31721094250679016,
      "learning_rate": 1.402e-05,
      "loss": 0.0027,
      "step": 53970
    },
    {
      "epoch": 2.8789333333333333,
      "grad_norm": 0.20237146317958832,
      "learning_rate": 1.4013333333333334e-05,
      "loss": 0.002,
      "step": 53980
    },
    {
      "epoch": 2.8794666666666666,
      "grad_norm": 0.14293302595615387,
      "learning_rate": 1.4006666666666668e-05,
      "loss": 0.003,
      "step": 53990
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.41993996500968933,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.002,
      "step": 54000
    },
    {
      "epoch": 2.880533333333333,
      "grad_norm": 0.36713555455207825,
      "learning_rate": 1.3993333333333334e-05,
      "loss": 0.0022,
      "step": 54010
    },
    {
      "epoch": 2.8810666666666664,
      "grad_norm": 0.2921968698501587,
      "learning_rate": 1.3986666666666668e-05,
      "loss": 0.003,
      "step": 54020
    },
    {
      "epoch": 2.8816,
      "grad_norm": 0.4277496635913849,
      "learning_rate": 1.3980000000000002e-05,
      "loss": 0.0029,
      "step": 54030
    },
    {
      "epoch": 2.8821333333333334,
      "grad_norm": 0.3937661647796631,
      "learning_rate": 1.3973333333333332e-05,
      "loss": 0.0034,
      "step": 54040
    },
    {
      "epoch": 2.8826666666666667,
      "grad_norm": 0.3402428925037384,
      "learning_rate": 1.3966666666666666e-05,
      "loss": 0.002,
      "step": 54050
    },
    {
      "epoch": 2.8832,
      "grad_norm": 0.40512368083000183,
      "learning_rate": 1.396e-05,
      "loss": 0.0057,
      "step": 54060
    },
    {
      "epoch": 2.8837333333333333,
      "grad_norm": 0.4956135153770447,
      "learning_rate": 1.3953333333333334e-05,
      "loss": 0.0023,
      "step": 54070
    },
    {
      "epoch": 2.8842666666666665,
      "grad_norm": 0.39276954531669617,
      "learning_rate": 1.3946666666666666e-05,
      "loss": 0.0028,
      "step": 54080
    },
    {
      "epoch": 2.8848000000000003,
      "grad_norm": 0.21060514450073242,
      "learning_rate": 1.394e-05,
      "loss": 0.0029,
      "step": 54090
    },
    {
      "epoch": 2.8853333333333335,
      "grad_norm": 0.1526026874780655,
      "learning_rate": 1.3933333333333334e-05,
      "loss": 0.0021,
      "step": 54100
    },
    {
      "epoch": 2.885866666666667,
      "grad_norm": 0.2459406703710556,
      "learning_rate": 1.3926666666666668e-05,
      "loss": 0.0024,
      "step": 54110
    },
    {
      "epoch": 2.8864,
      "grad_norm": 0.14684291183948517,
      "learning_rate": 1.3919999999999999e-05,
      "loss": 0.0019,
      "step": 54120
    },
    {
      "epoch": 2.8869333333333334,
      "grad_norm": 0.17657330632209778,
      "learning_rate": 1.3913333333333335e-05,
      "loss": 0.002,
      "step": 54130
    },
    {
      "epoch": 2.8874666666666666,
      "grad_norm": 0.17074204981327057,
      "learning_rate": 1.3906666666666668e-05,
      "loss": 0.0036,
      "step": 54140
    },
    {
      "epoch": 2.888,
      "grad_norm": 0.21467207372188568,
      "learning_rate": 1.3900000000000002e-05,
      "loss": 0.0026,
      "step": 54150
    },
    {
      "epoch": 2.888533333333333,
      "grad_norm": 0.2935672998428345,
      "learning_rate": 1.3893333333333333e-05,
      "loss": 0.0023,
      "step": 54160
    },
    {
      "epoch": 2.8890666666666664,
      "grad_norm": 0.32099974155426025,
      "learning_rate": 1.3886666666666667e-05,
      "loss": 0.0024,
      "step": 54170
    },
    {
      "epoch": 2.8895999999999997,
      "grad_norm": 0.26705968379974365,
      "learning_rate": 1.3880000000000001e-05,
      "loss": 0.0017,
      "step": 54180
    },
    {
      "epoch": 2.8901333333333334,
      "grad_norm": 0.5147538781166077,
      "learning_rate": 1.3873333333333333e-05,
      "loss": 0.0022,
      "step": 54190
    },
    {
      "epoch": 2.8906666666666667,
      "grad_norm": 0.31433895230293274,
      "learning_rate": 1.3866666666666667e-05,
      "loss": 0.0042,
      "step": 54200
    },
    {
      "epoch": 2.8912,
      "grad_norm": 0.43092530965805054,
      "learning_rate": 1.3860000000000001e-05,
      "loss": 0.0019,
      "step": 54210
    },
    {
      "epoch": 2.8917333333333333,
      "grad_norm": 0.1428159475326538,
      "learning_rate": 1.3853333333333335e-05,
      "loss": 0.0027,
      "step": 54220
    },
    {
      "epoch": 2.8922666666666665,
      "grad_norm": 0.22241829335689545,
      "learning_rate": 1.3846666666666666e-05,
      "loss": 0.0026,
      "step": 54230
    },
    {
      "epoch": 2.8928000000000003,
      "grad_norm": 0.21184560656547546,
      "learning_rate": 1.384e-05,
      "loss": 0.0044,
      "step": 54240
    },
    {
      "epoch": 2.8933333333333335,
      "grad_norm": 0.19682778418064117,
      "learning_rate": 1.3833333333333334e-05,
      "loss": 0.0018,
      "step": 54250
    },
    {
      "epoch": 2.893866666666667,
      "grad_norm": 0.28579267859458923,
      "learning_rate": 1.3826666666666668e-05,
      "loss": 0.0024,
      "step": 54260
    },
    {
      "epoch": 2.8944,
      "grad_norm": 0.38093090057373047,
      "learning_rate": 1.382e-05,
      "loss": 0.0018,
      "step": 54270
    },
    {
      "epoch": 2.8949333333333334,
      "grad_norm": 0.438748836517334,
      "learning_rate": 1.3813333333333334e-05,
      "loss": 0.0018,
      "step": 54280
    },
    {
      "epoch": 2.8954666666666666,
      "grad_norm": 0.34883153438568115,
      "learning_rate": 1.3806666666666668e-05,
      "loss": 0.0041,
      "step": 54290
    },
    {
      "epoch": 2.896,
      "grad_norm": 0.4344257116317749,
      "learning_rate": 1.3800000000000002e-05,
      "loss": 0.0028,
      "step": 54300
    },
    {
      "epoch": 2.896533333333333,
      "grad_norm": 0.27945590019226074,
      "learning_rate": 1.3793333333333332e-05,
      "loss": 0.0017,
      "step": 54310
    },
    {
      "epoch": 2.8970666666666665,
      "grad_norm": 0.11791171133518219,
      "learning_rate": 1.3786666666666668e-05,
      "loss": 0.0021,
      "step": 54320
    },
    {
      "epoch": 2.8975999999999997,
      "grad_norm": 0.20520879328250885,
      "learning_rate": 1.3780000000000002e-05,
      "loss": 0.0034,
      "step": 54330
    },
    {
      "epoch": 2.8981333333333335,
      "grad_norm": 0.9556797742843628,
      "learning_rate": 1.3773333333333336e-05,
      "loss": 0.0023,
      "step": 54340
    },
    {
      "epoch": 2.8986666666666667,
      "grad_norm": 0.6751295924186707,
      "learning_rate": 1.3766666666666666e-05,
      "loss": 0.0025,
      "step": 54350
    },
    {
      "epoch": 2.8992,
      "grad_norm": 0.22011005878448486,
      "learning_rate": 1.376e-05,
      "loss": 0.0019,
      "step": 54360
    },
    {
      "epoch": 2.8997333333333333,
      "grad_norm": 0.4049343764781952,
      "learning_rate": 1.3753333333333334e-05,
      "loss": 0.0029,
      "step": 54370
    },
    {
      "epoch": 2.9002666666666665,
      "grad_norm": 0.07295241206884384,
      "learning_rate": 1.3746666666666667e-05,
      "loss": 0.0021,
      "step": 54380
    },
    {
      "epoch": 2.9008000000000003,
      "grad_norm": 0.09011873602867126,
      "learning_rate": 1.374e-05,
      "loss": 0.002,
      "step": 54390
    },
    {
      "epoch": 2.9013333333333335,
      "grad_norm": 0.14679677784442902,
      "learning_rate": 1.3733333333333335e-05,
      "loss": 0.0023,
      "step": 54400
    },
    {
      "epoch": 2.901866666666667,
      "grad_norm": 0.23139943182468414,
      "learning_rate": 1.3726666666666669e-05,
      "loss": 0.0024,
      "step": 54410
    },
    {
      "epoch": 2.9024,
      "grad_norm": 0.20724636316299438,
      "learning_rate": 1.3719999999999999e-05,
      "loss": 0.0027,
      "step": 54420
    },
    {
      "epoch": 2.9029333333333334,
      "grad_norm": 0.6100102066993713,
      "learning_rate": 1.3713333333333333e-05,
      "loss": 0.0029,
      "step": 54430
    },
    {
      "epoch": 2.9034666666666666,
      "grad_norm": 0.13077634572982788,
      "learning_rate": 1.3706666666666667e-05,
      "loss": 0.0021,
      "step": 54440
    },
    {
      "epoch": 2.904,
      "grad_norm": 0.23085319995880127,
      "learning_rate": 1.3700000000000001e-05,
      "loss": 0.003,
      "step": 54450
    },
    {
      "epoch": 2.904533333333333,
      "grad_norm": 0.2758113443851471,
      "learning_rate": 1.3693333333333333e-05,
      "loss": 0.0024,
      "step": 54460
    },
    {
      "epoch": 2.9050666666666665,
      "grad_norm": 0.377315878868103,
      "learning_rate": 1.3686666666666667e-05,
      "loss": 0.0022,
      "step": 54470
    },
    {
      "epoch": 2.9055999999999997,
      "grad_norm": 0.4140024185180664,
      "learning_rate": 1.3680000000000001e-05,
      "loss": 0.0035,
      "step": 54480
    },
    {
      "epoch": 2.9061333333333335,
      "grad_norm": 0.24160826206207275,
      "learning_rate": 1.3673333333333335e-05,
      "loss": 0.0028,
      "step": 54490
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 0.08126731961965561,
      "learning_rate": 1.3666666666666666e-05,
      "loss": 0.0026,
      "step": 54500
    },
    {
      "epoch": 2.9072,
      "grad_norm": 0.5775682330131531,
      "learning_rate": 1.3660000000000001e-05,
      "loss": 0.0025,
      "step": 54510
    },
    {
      "epoch": 2.9077333333333333,
      "grad_norm": 0.1792754977941513,
      "learning_rate": 1.3653333333333335e-05,
      "loss": 0.002,
      "step": 54520
    },
    {
      "epoch": 2.9082666666666666,
      "grad_norm": 0.13377463817596436,
      "learning_rate": 1.3646666666666666e-05,
      "loss": 0.0029,
      "step": 54530
    },
    {
      "epoch": 2.9088000000000003,
      "grad_norm": 0.42543238401412964,
      "learning_rate": 1.364e-05,
      "loss": 0.0036,
      "step": 54540
    },
    {
      "epoch": 2.9093333333333335,
      "grad_norm": 0.21547359228134155,
      "learning_rate": 1.3633333333333334e-05,
      "loss": 0.003,
      "step": 54550
    },
    {
      "epoch": 2.909866666666667,
      "grad_norm": 0.19586382806301117,
      "learning_rate": 1.3626666666666668e-05,
      "loss": 0.0017,
      "step": 54560
    },
    {
      "epoch": 2.9104,
      "grad_norm": 0.3415966331958771,
      "learning_rate": 1.362e-05,
      "loss": 0.0019,
      "step": 54570
    },
    {
      "epoch": 2.9109333333333334,
      "grad_norm": 0.5066153407096863,
      "learning_rate": 1.3613333333333334e-05,
      "loss": 0.0018,
      "step": 54580
    },
    {
      "epoch": 2.9114666666666666,
      "grad_norm": 0.11882247030735016,
      "learning_rate": 1.3606666666666668e-05,
      "loss": 0.002,
      "step": 54590
    },
    {
      "epoch": 2.912,
      "grad_norm": 0.8017581105232239,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.0019,
      "step": 54600
    },
    {
      "epoch": 2.912533333333333,
      "grad_norm": 0.19939939677715302,
      "learning_rate": 1.3593333333333332e-05,
      "loss": 0.0028,
      "step": 54610
    },
    {
      "epoch": 2.9130666666666665,
      "grad_norm": 0.5511412620544434,
      "learning_rate": 1.3586666666666666e-05,
      "loss": 0.0018,
      "step": 54620
    },
    {
      "epoch": 2.9135999999999997,
      "grad_norm": 0.17214763164520264,
      "learning_rate": 1.358e-05,
      "loss": 0.0027,
      "step": 54630
    },
    {
      "epoch": 2.9141333333333335,
      "grad_norm": 0.0933888778090477,
      "learning_rate": 1.3573333333333334e-05,
      "loss": 0.0025,
      "step": 54640
    },
    {
      "epoch": 2.9146666666666667,
      "grad_norm": 0.35923025012016296,
      "learning_rate": 1.3566666666666667e-05,
      "loss": 0.0024,
      "step": 54650
    },
    {
      "epoch": 2.9152,
      "grad_norm": 0.4455510973930359,
      "learning_rate": 1.356e-05,
      "loss": 0.0027,
      "step": 54660
    },
    {
      "epoch": 2.9157333333333333,
      "grad_norm": 0.24854670464992523,
      "learning_rate": 1.3553333333333335e-05,
      "loss": 0.0018,
      "step": 54670
    },
    {
      "epoch": 2.9162666666666666,
      "grad_norm": 0.11612962931394577,
      "learning_rate": 1.3546666666666669e-05,
      "loss": 0.0018,
      "step": 54680
    },
    {
      "epoch": 2.9168,
      "grad_norm": 0.06568793952465057,
      "learning_rate": 1.3539999999999999e-05,
      "loss": 0.0019,
      "step": 54690
    },
    {
      "epoch": 2.9173333333333336,
      "grad_norm": 0.4360959231853485,
      "learning_rate": 1.3533333333333335e-05,
      "loss": 0.0031,
      "step": 54700
    },
    {
      "epoch": 2.917866666666667,
      "grad_norm": 0.17308692634105682,
      "learning_rate": 1.3526666666666669e-05,
      "loss": 0.0034,
      "step": 54710
    },
    {
      "epoch": 2.9184,
      "grad_norm": 0.15601032972335815,
      "learning_rate": 1.352e-05,
      "loss": 0.0018,
      "step": 54720
    },
    {
      "epoch": 2.9189333333333334,
      "grad_norm": 0.33329713344573975,
      "learning_rate": 1.3513333333333333e-05,
      "loss": 0.0024,
      "step": 54730
    },
    {
      "epoch": 2.9194666666666667,
      "grad_norm": 0.1722554713487625,
      "learning_rate": 1.3506666666666667e-05,
      "loss": 0.0028,
      "step": 54740
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.35270407795906067,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 0.002,
      "step": 54750
    },
    {
      "epoch": 2.920533333333333,
      "grad_norm": 0.18743175268173218,
      "learning_rate": 1.3493333333333333e-05,
      "loss": 0.0021,
      "step": 54760
    },
    {
      "epoch": 2.9210666666666665,
      "grad_norm": 0.2643774747848511,
      "learning_rate": 1.3486666666666667e-05,
      "loss": 0.0018,
      "step": 54770
    },
    {
      "epoch": 2.9215999999999998,
      "grad_norm": 0.28766506910324097,
      "learning_rate": 1.3480000000000001e-05,
      "loss": 0.0025,
      "step": 54780
    },
    {
      "epoch": 2.9221333333333335,
      "grad_norm": 0.522361695766449,
      "learning_rate": 1.3473333333333335e-05,
      "loss": 0.0029,
      "step": 54790
    },
    {
      "epoch": 2.9226666666666667,
      "grad_norm": 0.5666281580924988,
      "learning_rate": 1.3466666666666666e-05,
      "loss": 0.0027,
      "step": 54800
    },
    {
      "epoch": 2.9232,
      "grad_norm": 0.10345488786697388,
      "learning_rate": 1.346e-05,
      "loss": 0.0019,
      "step": 54810
    },
    {
      "epoch": 2.9237333333333333,
      "grad_norm": 0.4773399531841278,
      "learning_rate": 1.3453333333333334e-05,
      "loss": 0.0021,
      "step": 54820
    },
    {
      "epoch": 2.9242666666666666,
      "grad_norm": 0.15958411991596222,
      "learning_rate": 1.3446666666666668e-05,
      "loss": 0.002,
      "step": 54830
    },
    {
      "epoch": 2.9248,
      "grad_norm": 0.4819905161857605,
      "learning_rate": 1.344e-05,
      "loss": 0.0018,
      "step": 54840
    },
    {
      "epoch": 2.9253333333333336,
      "grad_norm": 0.5691487193107605,
      "learning_rate": 1.3433333333333334e-05,
      "loss": 0.002,
      "step": 54850
    },
    {
      "epoch": 2.925866666666667,
      "grad_norm": 0.21005623042583466,
      "learning_rate": 1.3426666666666668e-05,
      "loss": 0.0027,
      "step": 54860
    },
    {
      "epoch": 2.9264,
      "grad_norm": 0.28567972779273987,
      "learning_rate": 1.3420000000000002e-05,
      "loss": 0.002,
      "step": 54870
    },
    {
      "epoch": 2.9269333333333334,
      "grad_norm": 0.2923671007156372,
      "learning_rate": 1.3413333333333333e-05,
      "loss": 0.0029,
      "step": 54880
    },
    {
      "epoch": 2.9274666666666667,
      "grad_norm": 0.28453880548477173,
      "learning_rate": 1.3406666666666668e-05,
      "loss": 0.0031,
      "step": 54890
    },
    {
      "epoch": 2.928,
      "grad_norm": 0.566595733165741,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 0.0021,
      "step": 54900
    },
    {
      "epoch": 2.928533333333333,
      "grad_norm": 0.23519809544086456,
      "learning_rate": 1.3393333333333333e-05,
      "loss": 0.002,
      "step": 54910
    },
    {
      "epoch": 2.9290666666666665,
      "grad_norm": 0.4222012162208557,
      "learning_rate": 1.3386666666666667e-05,
      "loss": 0.002,
      "step": 54920
    },
    {
      "epoch": 2.9295999999999998,
      "grad_norm": 0.43429288268089294,
      "learning_rate": 1.338e-05,
      "loss": 0.0043,
      "step": 54930
    },
    {
      "epoch": 2.9301333333333335,
      "grad_norm": 0.2257544845342636,
      "learning_rate": 1.3373333333333335e-05,
      "loss": 0.0025,
      "step": 54940
    },
    {
      "epoch": 2.9306666666666668,
      "grad_norm": 0.1990254670381546,
      "learning_rate": 1.3366666666666667e-05,
      "loss": 0.0022,
      "step": 54950
    },
    {
      "epoch": 2.9312,
      "grad_norm": 0.2002156525850296,
      "learning_rate": 1.336e-05,
      "loss": 0.0021,
      "step": 54960
    },
    {
      "epoch": 2.9317333333333333,
      "grad_norm": 0.2026171237230301,
      "learning_rate": 1.3353333333333335e-05,
      "loss": 0.0025,
      "step": 54970
    },
    {
      "epoch": 2.9322666666666666,
      "grad_norm": 0.13201427459716797,
      "learning_rate": 1.3346666666666669e-05,
      "loss": 0.0023,
      "step": 54980
    },
    {
      "epoch": 2.9328,
      "grad_norm": 0.4382084608078003,
      "learning_rate": 1.334e-05,
      "loss": 0.0021,
      "step": 54990
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 0.17594082653522491,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.0025,
      "step": 55000
    },
    {
      "epoch": 2.933866666666667,
      "grad_norm": 0.27059900760650635,
      "learning_rate": 1.3326666666666667e-05,
      "loss": 0.0019,
      "step": 55010
    },
    {
      "epoch": 2.9344,
      "grad_norm": 0.20879188179969788,
      "learning_rate": 1.3320000000000001e-05,
      "loss": 0.0024,
      "step": 55020
    },
    {
      "epoch": 2.9349333333333334,
      "grad_norm": 0.18350403010845184,
      "learning_rate": 1.3313333333333333e-05,
      "loss": 0.002,
      "step": 55030
    },
    {
      "epoch": 2.9354666666666667,
      "grad_norm": 0.9823576211929321,
      "learning_rate": 1.3306666666666667e-05,
      "loss": 0.0029,
      "step": 55040
    },
    {
      "epoch": 2.936,
      "grad_norm": 0.32488036155700684,
      "learning_rate": 1.3300000000000001e-05,
      "loss": 0.003,
      "step": 55050
    },
    {
      "epoch": 2.936533333333333,
      "grad_norm": 0.25690680742263794,
      "learning_rate": 1.3293333333333332e-05,
      "loss": 0.0021,
      "step": 55060
    },
    {
      "epoch": 2.9370666666666665,
      "grad_norm": 0.3662566542625427,
      "learning_rate": 1.3286666666666666e-05,
      "loss": 0.0018,
      "step": 55070
    },
    {
      "epoch": 2.9375999999999998,
      "grad_norm": 0.6289854049682617,
      "learning_rate": 1.3280000000000002e-05,
      "loss": 0.0018,
      "step": 55080
    },
    {
      "epoch": 2.9381333333333335,
      "grad_norm": 0.9050646424293518,
      "learning_rate": 1.3273333333333336e-05,
      "loss": 0.0018,
      "step": 55090
    },
    {
      "epoch": 2.9386666666666668,
      "grad_norm": 0.3635214865207672,
      "learning_rate": 1.3266666666666666e-05,
      "loss": 0.0018,
      "step": 55100
    },
    {
      "epoch": 2.9392,
      "grad_norm": 0.46594947576522827,
      "learning_rate": 1.326e-05,
      "loss": 0.0034,
      "step": 55110
    },
    {
      "epoch": 2.9397333333333333,
      "grad_norm": 0.3157791793346405,
      "learning_rate": 1.3253333333333334e-05,
      "loss": 0.0021,
      "step": 55120
    },
    {
      "epoch": 2.9402666666666666,
      "grad_norm": 0.0707409605383873,
      "learning_rate": 1.3246666666666668e-05,
      "loss": 0.0023,
      "step": 55130
    },
    {
      "epoch": 2.9408,
      "grad_norm": 0.5957966446876526,
      "learning_rate": 1.324e-05,
      "loss": 0.0019,
      "step": 55140
    },
    {
      "epoch": 2.9413333333333336,
      "grad_norm": 0.12289126217365265,
      "learning_rate": 1.3233333333333334e-05,
      "loss": 0.0029,
      "step": 55150
    },
    {
      "epoch": 2.941866666666667,
      "grad_norm": 0.42640817165374756,
      "learning_rate": 1.3226666666666668e-05,
      "loss": 0.0019,
      "step": 55160
    },
    {
      "epoch": 2.9424,
      "grad_norm": 0.35043463110923767,
      "learning_rate": 1.3220000000000002e-05,
      "loss": 0.0034,
      "step": 55170
    },
    {
      "epoch": 2.9429333333333334,
      "grad_norm": 0.4154374301433563,
      "learning_rate": 1.3213333333333333e-05,
      "loss": 0.0028,
      "step": 55180
    },
    {
      "epoch": 2.9434666666666667,
      "grad_norm": 0.2645573616027832,
      "learning_rate": 1.3206666666666667e-05,
      "loss": 0.0019,
      "step": 55190
    },
    {
      "epoch": 2.944,
      "grad_norm": 0.8414800763130188,
      "learning_rate": 1.32e-05,
      "loss": 0.0022,
      "step": 55200
    },
    {
      "epoch": 2.9445333333333332,
      "grad_norm": 0.5966900587081909,
      "learning_rate": 1.3193333333333335e-05,
      "loss": 0.0024,
      "step": 55210
    },
    {
      "epoch": 2.9450666666666665,
      "grad_norm": 0.3029438257217407,
      "learning_rate": 1.3186666666666667e-05,
      "loss": 0.0027,
      "step": 55220
    },
    {
      "epoch": 2.9455999999999998,
      "grad_norm": 0.4335938096046448,
      "learning_rate": 1.3180000000000001e-05,
      "loss": 0.0021,
      "step": 55230
    },
    {
      "epoch": 2.9461333333333335,
      "grad_norm": 0.3773660361766815,
      "learning_rate": 1.3173333333333335e-05,
      "loss": 0.004,
      "step": 55240
    },
    {
      "epoch": 2.9466666666666668,
      "grad_norm": 0.2153797447681427,
      "learning_rate": 1.3166666666666665e-05,
      "loss": 0.0031,
      "step": 55250
    },
    {
      "epoch": 2.9472,
      "grad_norm": 0.6676674485206604,
      "learning_rate": 1.316e-05,
      "loss": 0.0023,
      "step": 55260
    },
    {
      "epoch": 2.9477333333333333,
      "grad_norm": 0.08820951730012894,
      "learning_rate": 1.3153333333333335e-05,
      "loss": 0.0021,
      "step": 55270
    },
    {
      "epoch": 2.9482666666666666,
      "grad_norm": 0.3733995854854584,
      "learning_rate": 1.3146666666666669e-05,
      "loss": 0.002,
      "step": 55280
    },
    {
      "epoch": 2.9488,
      "grad_norm": 0.5367169976234436,
      "learning_rate": 1.314e-05,
      "loss": 0.0022,
      "step": 55290
    },
    {
      "epoch": 2.9493333333333336,
      "grad_norm": 0.7530726194381714,
      "learning_rate": 1.3133333333333334e-05,
      "loss": 0.0024,
      "step": 55300
    },
    {
      "epoch": 2.949866666666667,
      "grad_norm": 0.26694345474243164,
      "learning_rate": 1.3126666666666667e-05,
      "loss": 0.0027,
      "step": 55310
    },
    {
      "epoch": 2.9504,
      "grad_norm": 0.2466738075017929,
      "learning_rate": 1.3120000000000001e-05,
      "loss": 0.0026,
      "step": 55320
    },
    {
      "epoch": 2.9509333333333334,
      "grad_norm": 0.3308679461479187,
      "learning_rate": 1.3113333333333334e-05,
      "loss": 0.0024,
      "step": 55330
    },
    {
      "epoch": 2.9514666666666667,
      "grad_norm": 0.3383665084838867,
      "learning_rate": 1.3106666666666668e-05,
      "loss": 0.0022,
      "step": 55340
    },
    {
      "epoch": 2.952,
      "grad_norm": 0.21859605610370636,
      "learning_rate": 1.3100000000000002e-05,
      "loss": 0.0023,
      "step": 55350
    },
    {
      "epoch": 2.9525333333333332,
      "grad_norm": 0.42566075921058655,
      "learning_rate": 1.3093333333333336e-05,
      "loss": 0.002,
      "step": 55360
    },
    {
      "epoch": 2.9530666666666665,
      "grad_norm": 0.22742140293121338,
      "learning_rate": 1.3086666666666666e-05,
      "loss": 0.002,
      "step": 55370
    },
    {
      "epoch": 2.9536,
      "grad_norm": 0.05939393863081932,
      "learning_rate": 1.308e-05,
      "loss": 0.0025,
      "step": 55380
    },
    {
      "epoch": 2.9541333333333335,
      "grad_norm": 0.1896117776632309,
      "learning_rate": 1.3073333333333334e-05,
      "loss": 0.003,
      "step": 55390
    },
    {
      "epoch": 2.9546666666666668,
      "grad_norm": 0.0886741429567337,
      "learning_rate": 1.3066666666666666e-05,
      "loss": 0.0019,
      "step": 55400
    },
    {
      "epoch": 2.9552,
      "grad_norm": 0.3678174316883087,
      "learning_rate": 1.306e-05,
      "loss": 0.003,
      "step": 55410
    },
    {
      "epoch": 2.9557333333333333,
      "grad_norm": 0.13842220604419708,
      "learning_rate": 1.3053333333333334e-05,
      "loss": 0.0017,
      "step": 55420
    },
    {
      "epoch": 2.9562666666666666,
      "grad_norm": 0.1401924341917038,
      "learning_rate": 1.3046666666666668e-05,
      "loss": 0.0019,
      "step": 55430
    },
    {
      "epoch": 2.9568,
      "grad_norm": 0.14855296909809113,
      "learning_rate": 1.3039999999999999e-05,
      "loss": 0.0018,
      "step": 55440
    },
    {
      "epoch": 2.9573333333333336,
      "grad_norm": 0.22918660938739777,
      "learning_rate": 1.3033333333333333e-05,
      "loss": 0.0047,
      "step": 55450
    },
    {
      "epoch": 2.957866666666667,
      "grad_norm": 0.5168160796165466,
      "learning_rate": 1.3026666666666667e-05,
      "loss": 0.0022,
      "step": 55460
    },
    {
      "epoch": 2.9584,
      "grad_norm": 0.07396060973405838,
      "learning_rate": 1.3020000000000002e-05,
      "loss": 0.0034,
      "step": 55470
    },
    {
      "epoch": 2.9589333333333334,
      "grad_norm": 0.0665498822927475,
      "learning_rate": 1.3013333333333333e-05,
      "loss": 0.0019,
      "step": 55480
    },
    {
      "epoch": 2.9594666666666667,
      "grad_norm": 0.1350984424352646,
      "learning_rate": 1.3006666666666667e-05,
      "loss": 0.0033,
      "step": 55490
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.2881772816181183,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.0022,
      "step": 55500
    },
    {
      "epoch": 2.9605333333333332,
      "grad_norm": 0.3572128415107727,
      "learning_rate": 1.2993333333333335e-05,
      "loss": 0.0019,
      "step": 55510
    },
    {
      "epoch": 2.9610666666666665,
      "grad_norm": 0.2766716480255127,
      "learning_rate": 1.2986666666666667e-05,
      "loss": 0.0028,
      "step": 55520
    },
    {
      "epoch": 2.9616,
      "grad_norm": 0.18197773396968842,
      "learning_rate": 1.2980000000000001e-05,
      "loss": 0.0024,
      "step": 55530
    },
    {
      "epoch": 2.962133333333333,
      "grad_norm": 0.22089943289756775,
      "learning_rate": 1.2973333333333335e-05,
      "loss": 0.0029,
      "step": 55540
    },
    {
      "epoch": 2.962666666666667,
      "grad_norm": 0.08399979025125504,
      "learning_rate": 1.2966666666666669e-05,
      "loss": 0.0021,
      "step": 55550
    },
    {
      "epoch": 2.9632,
      "grad_norm": 0.16468675434589386,
      "learning_rate": 1.296e-05,
      "loss": 0.0019,
      "step": 55560
    },
    {
      "epoch": 2.9637333333333333,
      "grad_norm": 0.4245836138725281,
      "learning_rate": 1.2953333333333334e-05,
      "loss": 0.0018,
      "step": 55570
    },
    {
      "epoch": 2.9642666666666666,
      "grad_norm": 0.37651580572128296,
      "learning_rate": 1.2946666666666668e-05,
      "loss": 0.002,
      "step": 55580
    },
    {
      "epoch": 2.9648,
      "grad_norm": 0.4670095145702362,
      "learning_rate": 1.294e-05,
      "loss": 0.0034,
      "step": 55590
    },
    {
      "epoch": 2.9653333333333336,
      "grad_norm": 0.17419175803661346,
      "learning_rate": 1.2933333333333334e-05,
      "loss": 0.0028,
      "step": 55600
    },
    {
      "epoch": 2.965866666666667,
      "grad_norm": 0.4898969829082489,
      "learning_rate": 1.2926666666666668e-05,
      "loss": 0.002,
      "step": 55610
    },
    {
      "epoch": 2.9664,
      "grad_norm": 0.1554316133260727,
      "learning_rate": 1.2920000000000002e-05,
      "loss": 0.0044,
      "step": 55620
    },
    {
      "epoch": 2.9669333333333334,
      "grad_norm": 0.25635653734207153,
      "learning_rate": 1.2913333333333332e-05,
      "loss": 0.0019,
      "step": 55630
    },
    {
      "epoch": 2.9674666666666667,
      "grad_norm": 0.32207560539245605,
      "learning_rate": 1.2906666666666666e-05,
      "loss": 0.0026,
      "step": 55640
    },
    {
      "epoch": 2.968,
      "grad_norm": 0.46851614117622375,
      "learning_rate": 1.29e-05,
      "loss": 0.0019,
      "step": 55650
    },
    {
      "epoch": 2.9685333333333332,
      "grad_norm": 0.17841728031635284,
      "learning_rate": 1.2893333333333336e-05,
      "loss": 0.0035,
      "step": 55660
    },
    {
      "epoch": 2.9690666666666665,
      "grad_norm": 0.0840168297290802,
      "learning_rate": 1.2886666666666666e-05,
      "loss": 0.0019,
      "step": 55670
    },
    {
      "epoch": 2.9696,
      "grad_norm": 0.3692467212677002,
      "learning_rate": 1.288e-05,
      "loss": 0.0025,
      "step": 55680
    },
    {
      "epoch": 2.970133333333333,
      "grad_norm": 0.45342883467674255,
      "learning_rate": 1.2873333333333334e-05,
      "loss": 0.0026,
      "step": 55690
    },
    {
      "epoch": 2.970666666666667,
      "grad_norm": 0.18720780313014984,
      "learning_rate": 1.2866666666666668e-05,
      "loss": 0.0021,
      "step": 55700
    },
    {
      "epoch": 2.9712,
      "grad_norm": 0.4321836829185486,
      "learning_rate": 1.286e-05,
      "loss": 0.003,
      "step": 55710
    },
    {
      "epoch": 2.9717333333333333,
      "grad_norm": 0.10546141862869263,
      "learning_rate": 1.2853333333333335e-05,
      "loss": 0.0023,
      "step": 55720
    },
    {
      "epoch": 2.9722666666666666,
      "grad_norm": 0.3992640972137451,
      "learning_rate": 1.2846666666666668e-05,
      "loss": 0.0016,
      "step": 55730
    },
    {
      "epoch": 2.9728,
      "grad_norm": 0.3727133274078369,
      "learning_rate": 1.2839999999999999e-05,
      "loss": 0.0022,
      "step": 55740
    },
    {
      "epoch": 2.9733333333333336,
      "grad_norm": 0.19592858850955963,
      "learning_rate": 1.2833333333333333e-05,
      "loss": 0.0025,
      "step": 55750
    },
    {
      "epoch": 2.973866666666667,
      "grad_norm": 0.12503066658973694,
      "learning_rate": 1.2826666666666667e-05,
      "loss": 0.0023,
      "step": 55760
    },
    {
      "epoch": 2.9744,
      "grad_norm": 0.3705979883670807,
      "learning_rate": 1.2820000000000001e-05,
      "loss": 0.0019,
      "step": 55770
    },
    {
      "epoch": 2.9749333333333334,
      "grad_norm": 0.33088013529777527,
      "learning_rate": 1.2813333333333333e-05,
      "loss": 0.0023,
      "step": 55780
    },
    {
      "epoch": 2.9754666666666667,
      "grad_norm": 0.20651660859584808,
      "learning_rate": 1.2806666666666667e-05,
      "loss": 0.0025,
      "step": 55790
    },
    {
      "epoch": 2.976,
      "grad_norm": 0.17985542118549347,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.0025,
      "step": 55800
    },
    {
      "epoch": 2.9765333333333333,
      "grad_norm": 0.29351359605789185,
      "learning_rate": 1.2793333333333335e-05,
      "loss": 0.0018,
      "step": 55810
    },
    {
      "epoch": 2.9770666666666665,
      "grad_norm": 0.6150551438331604,
      "learning_rate": 1.2786666666666666e-05,
      "loss": 0.0022,
      "step": 55820
    },
    {
      "epoch": 2.9776,
      "grad_norm": 0.11348801851272583,
      "learning_rate": 1.278e-05,
      "loss": 0.0018,
      "step": 55830
    },
    {
      "epoch": 2.978133333333333,
      "grad_norm": 0.20527197420597076,
      "learning_rate": 1.2773333333333334e-05,
      "loss": 0.0052,
      "step": 55840
    },
    {
      "epoch": 2.978666666666667,
      "grad_norm": 0.4664691388607025,
      "learning_rate": 1.276666666666667e-05,
      "loss": 0.0019,
      "step": 55850
    },
    {
      "epoch": 2.9792,
      "grad_norm": 0.06757790595293045,
      "learning_rate": 1.276e-05,
      "loss": 0.0017,
      "step": 55860
    },
    {
      "epoch": 2.9797333333333333,
      "grad_norm": 0.17546577751636505,
      "learning_rate": 1.2753333333333334e-05,
      "loss": 0.0021,
      "step": 55870
    },
    {
      "epoch": 2.9802666666666666,
      "grad_norm": 0.0884377658367157,
      "learning_rate": 1.2746666666666668e-05,
      "loss": 0.0034,
      "step": 55880
    },
    {
      "epoch": 2.9808,
      "grad_norm": 0.24465064704418182,
      "learning_rate": 1.2740000000000002e-05,
      "loss": 0.0024,
      "step": 55890
    },
    {
      "epoch": 2.981333333333333,
      "grad_norm": 0.3286358714103699,
      "learning_rate": 1.2733333333333334e-05,
      "loss": 0.0019,
      "step": 55900
    },
    {
      "epoch": 2.981866666666667,
      "grad_norm": 0.7214838862419128,
      "learning_rate": 1.2726666666666668e-05,
      "loss": 0.0019,
      "step": 55910
    },
    {
      "epoch": 2.9824,
      "grad_norm": 0.4757748544216156,
      "learning_rate": 1.2720000000000002e-05,
      "loss": 0.0029,
      "step": 55920
    },
    {
      "epoch": 2.9829333333333334,
      "grad_norm": 0.06088636443018913,
      "learning_rate": 1.2713333333333332e-05,
      "loss": 0.0019,
      "step": 55930
    },
    {
      "epoch": 2.9834666666666667,
      "grad_norm": 0.0631965771317482,
      "learning_rate": 1.2706666666666666e-05,
      "loss": 0.002,
      "step": 55940
    },
    {
      "epoch": 2.984,
      "grad_norm": 0.06770119816064835,
      "learning_rate": 1.27e-05,
      "loss": 0.0031,
      "step": 55950
    },
    {
      "epoch": 2.9845333333333333,
      "grad_norm": 0.12068641185760498,
      "learning_rate": 1.2693333333333334e-05,
      "loss": 0.002,
      "step": 55960
    },
    {
      "epoch": 2.9850666666666665,
      "grad_norm": 0.3674083948135376,
      "learning_rate": 1.2686666666666667e-05,
      "loss": 0.002,
      "step": 55970
    },
    {
      "epoch": 2.9856,
      "grad_norm": 0.6809283494949341,
      "learning_rate": 1.268e-05,
      "loss": 0.0029,
      "step": 55980
    },
    {
      "epoch": 2.986133333333333,
      "grad_norm": 0.3120385706424713,
      "learning_rate": 1.2673333333333335e-05,
      "loss": 0.002,
      "step": 55990
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 0.22691839933395386,
      "learning_rate": 1.2666666666666668e-05,
      "loss": 0.0024,
      "step": 56000
    },
    {
      "epoch": 2.9872,
      "grad_norm": 0.335561066865921,
      "learning_rate": 1.2659999999999999e-05,
      "loss": 0.002,
      "step": 56010
    },
    {
      "epoch": 2.9877333333333334,
      "grad_norm": 0.8652195930480957,
      "learning_rate": 1.2653333333333333e-05,
      "loss": 0.003,
      "step": 56020
    },
    {
      "epoch": 2.9882666666666666,
      "grad_norm": 0.2094123363494873,
      "learning_rate": 1.2646666666666667e-05,
      "loss": 0.0024,
      "step": 56030
    },
    {
      "epoch": 2.9888,
      "grad_norm": 0.09312712401151657,
      "learning_rate": 1.2640000000000003e-05,
      "loss": 0.002,
      "step": 56040
    },
    {
      "epoch": 2.989333333333333,
      "grad_norm": 0.3478047251701355,
      "learning_rate": 1.2633333333333333e-05,
      "loss": 0.002,
      "step": 56050
    },
    {
      "epoch": 2.989866666666667,
      "grad_norm": 0.14026403427124023,
      "learning_rate": 1.2626666666666667e-05,
      "loss": 0.0019,
      "step": 56060
    },
    {
      "epoch": 2.9904,
      "grad_norm": 0.30851542949676514,
      "learning_rate": 1.2620000000000001e-05,
      "loss": 0.0038,
      "step": 56070
    },
    {
      "epoch": 2.9909333333333334,
      "grad_norm": 0.630405604839325,
      "learning_rate": 1.2613333333333332e-05,
      "loss": 0.0025,
      "step": 56080
    },
    {
      "epoch": 2.9914666666666667,
      "grad_norm": 0.25342434644699097,
      "learning_rate": 1.2606666666666667e-05,
      "loss": 0.0032,
      "step": 56090
    },
    {
      "epoch": 2.992,
      "grad_norm": 0.2568802237510681,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 0.0021,
      "step": 56100
    },
    {
      "epoch": 2.9925333333333333,
      "grad_norm": 0.08564216643571854,
      "learning_rate": 1.2593333333333335e-05,
      "loss": 0.002,
      "step": 56110
    },
    {
      "epoch": 2.9930666666666665,
      "grad_norm": 0.3042650818824768,
      "learning_rate": 1.2586666666666666e-05,
      "loss": 0.0024,
      "step": 56120
    },
    {
      "epoch": 2.9936,
      "grad_norm": 0.10330396145582199,
      "learning_rate": 1.258e-05,
      "loss": 0.0033,
      "step": 56130
    },
    {
      "epoch": 2.994133333333333,
      "grad_norm": 0.3334878087043762,
      "learning_rate": 1.2573333333333334e-05,
      "loss": 0.002,
      "step": 56140
    },
    {
      "epoch": 2.994666666666667,
      "grad_norm": 0.11606530845165253,
      "learning_rate": 1.2566666666666668e-05,
      "loss": 0.0035,
      "step": 56150
    },
    {
      "epoch": 2.9952,
      "grad_norm": 0.08308442682027817,
      "learning_rate": 1.256e-05,
      "loss": 0.0017,
      "step": 56160
    },
    {
      "epoch": 2.9957333333333334,
      "grad_norm": 0.15860220789909363,
      "learning_rate": 1.2553333333333334e-05,
      "loss": 0.0034,
      "step": 56170
    },
    {
      "epoch": 2.9962666666666666,
      "grad_norm": 0.5569580793380737,
      "learning_rate": 1.2546666666666668e-05,
      "loss": 0.0019,
      "step": 56180
    },
    {
      "epoch": 2.9968,
      "grad_norm": 0.2602299749851227,
      "learning_rate": 1.2540000000000002e-05,
      "loss": 0.0021,
      "step": 56190
    },
    {
      "epoch": 2.997333333333333,
      "grad_norm": 0.3880760967731476,
      "learning_rate": 1.2533333333333332e-05,
      "loss": 0.002,
      "step": 56200
    },
    {
      "epoch": 2.997866666666667,
      "grad_norm": 0.09163752943277359,
      "learning_rate": 1.2526666666666666e-05,
      "loss": 0.0032,
      "step": 56210
    },
    {
      "epoch": 2.9984,
      "grad_norm": 0.17526112496852875,
      "learning_rate": 1.252e-05,
      "loss": 0.0022,
      "step": 56220
    },
    {
      "epoch": 2.9989333333333335,
      "grad_norm": 0.18108384311199188,
      "learning_rate": 1.2513333333333336e-05,
      "loss": 0.0019,
      "step": 56230
    },
    {
      "epoch": 2.9994666666666667,
      "grad_norm": 0.2640571594238281,
      "learning_rate": 1.2506666666666667e-05,
      "loss": 0.0024,
      "step": 56240
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.2997400164604187,
      "learning_rate": 1.25e-05,
      "loss": 0.0031,
      "step": 56250
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.0024835672229528427,
      "eval_runtime": 171.0971,
      "eval_samples_per_second": 1461.158,
      "eval_steps_per_second": 36.529,
      "step": 56250
    },
    {
      "epoch": 3.0005333333333333,
      "grad_norm": 0.14135098457336426,
      "learning_rate": 1.2493333333333333e-05,
      "loss": 0.0029,
      "step": 56260
    },
    {
      "epoch": 3.0010666666666665,
      "grad_norm": 0.5096641778945923,
      "learning_rate": 1.2486666666666667e-05,
      "loss": 0.0022,
      "step": 56270
    },
    {
      "epoch": 3.0016,
      "grad_norm": 0.3466844856739044,
      "learning_rate": 1.248e-05,
      "loss": 0.0028,
      "step": 56280
    },
    {
      "epoch": 3.0021333333333335,
      "grad_norm": 0.48361408710479736,
      "learning_rate": 1.2473333333333335e-05,
      "loss": 0.0018,
      "step": 56290
    },
    {
      "epoch": 3.002666666666667,
      "grad_norm": 0.591457724571228,
      "learning_rate": 1.2466666666666667e-05,
      "loss": 0.0022,
      "step": 56300
    },
    {
      "epoch": 3.0032,
      "grad_norm": 0.43835994601249695,
      "learning_rate": 1.2460000000000001e-05,
      "loss": 0.0021,
      "step": 56310
    },
    {
      "epoch": 3.0037333333333334,
      "grad_norm": 0.5305781960487366,
      "learning_rate": 1.2453333333333333e-05,
      "loss": 0.0027,
      "step": 56320
    },
    {
      "epoch": 3.0042666666666666,
      "grad_norm": 0.45200034976005554,
      "learning_rate": 1.2446666666666667e-05,
      "loss": 0.002,
      "step": 56330
    },
    {
      "epoch": 3.0048,
      "grad_norm": 0.2291100025177002,
      "learning_rate": 1.244e-05,
      "loss": 0.0032,
      "step": 56340
    },
    {
      "epoch": 3.005333333333333,
      "grad_norm": 0.1207185611128807,
      "learning_rate": 1.2433333333333335e-05,
      "loss": 0.0017,
      "step": 56350
    },
    {
      "epoch": 3.0058666666666665,
      "grad_norm": 0.3272395431995392,
      "learning_rate": 1.2426666666666667e-05,
      "loss": 0.0018,
      "step": 56360
    },
    {
      "epoch": 3.0064,
      "grad_norm": 0.09711459279060364,
      "learning_rate": 1.2420000000000001e-05,
      "loss": 0.0019,
      "step": 56370
    },
    {
      "epoch": 3.0069333333333335,
      "grad_norm": 0.11840660125017166,
      "learning_rate": 1.2413333333333334e-05,
      "loss": 0.002,
      "step": 56380
    },
    {
      "epoch": 3.0074666666666667,
      "grad_norm": 0.45892268419265747,
      "learning_rate": 1.2406666666666668e-05,
      "loss": 0.0033,
      "step": 56390
    },
    {
      "epoch": 3.008,
      "grad_norm": 0.36116957664489746,
      "learning_rate": 1.24e-05,
      "loss": 0.002,
      "step": 56400
    },
    {
      "epoch": 3.0085333333333333,
      "grad_norm": 0.3357006013393402,
      "learning_rate": 1.2393333333333334e-05,
      "loss": 0.0026,
      "step": 56410
    },
    {
      "epoch": 3.0090666666666666,
      "grad_norm": 0.3212277591228485,
      "learning_rate": 1.2386666666666668e-05,
      "loss": 0.0021,
      "step": 56420
    },
    {
      "epoch": 3.0096,
      "grad_norm": 0.23840449750423431,
      "learning_rate": 1.238e-05,
      "loss": 0.0034,
      "step": 56430
    },
    {
      "epoch": 3.0101333333333335,
      "grad_norm": 0.27487194538116455,
      "learning_rate": 1.2373333333333334e-05,
      "loss": 0.0025,
      "step": 56440
    },
    {
      "epoch": 3.010666666666667,
      "grad_norm": 0.2965843975543976,
      "learning_rate": 1.2366666666666666e-05,
      "loss": 0.0028,
      "step": 56450
    },
    {
      "epoch": 3.0112,
      "grad_norm": 0.19942930340766907,
      "learning_rate": 1.236e-05,
      "loss": 0.0024,
      "step": 56460
    },
    {
      "epoch": 3.0117333333333334,
      "grad_norm": 0.3323020040988922,
      "learning_rate": 1.2353333333333334e-05,
      "loss": 0.002,
      "step": 56470
    },
    {
      "epoch": 3.0122666666666666,
      "grad_norm": 0.1768736094236374,
      "learning_rate": 1.2346666666666668e-05,
      "loss": 0.0022,
      "step": 56480
    },
    {
      "epoch": 3.0128,
      "grad_norm": 0.08750346302986145,
      "learning_rate": 1.234e-05,
      "loss": 0.0025,
      "step": 56490
    },
    {
      "epoch": 3.013333333333333,
      "grad_norm": 0.16046036779880524,
      "learning_rate": 1.2333333333333334e-05,
      "loss": 0.0021,
      "step": 56500
    },
    {
      "epoch": 3.0138666666666665,
      "grad_norm": 0.6074503064155579,
      "learning_rate": 1.2326666666666667e-05,
      "loss": 0.0017,
      "step": 56510
    },
    {
      "epoch": 3.0144,
      "grad_norm": 0.10545273125171661,
      "learning_rate": 1.232e-05,
      "loss": 0.0019,
      "step": 56520
    },
    {
      "epoch": 3.0149333333333335,
      "grad_norm": 0.10585661977529526,
      "learning_rate": 1.2313333333333333e-05,
      "loss": 0.0017,
      "step": 56530
    },
    {
      "epoch": 3.0154666666666667,
      "grad_norm": 0.3180826008319855,
      "learning_rate": 1.2306666666666669e-05,
      "loss": 0.0019,
      "step": 56540
    },
    {
      "epoch": 3.016,
      "grad_norm": 0.1459948867559433,
      "learning_rate": 1.23e-05,
      "loss": 0.0029,
      "step": 56550
    },
    {
      "epoch": 3.0165333333333333,
      "grad_norm": 0.3949650526046753,
      "learning_rate": 1.2293333333333335e-05,
      "loss": 0.0025,
      "step": 56560
    },
    {
      "epoch": 3.0170666666666666,
      "grad_norm": 0.09060686081647873,
      "learning_rate": 1.2286666666666667e-05,
      "loss": 0.0023,
      "step": 56570
    },
    {
      "epoch": 3.0176,
      "grad_norm": 0.3289698362350464,
      "learning_rate": 1.2280000000000001e-05,
      "loss": 0.0037,
      "step": 56580
    },
    {
      "epoch": 3.018133333333333,
      "grad_norm": 0.1390419453382492,
      "learning_rate": 1.2273333333333333e-05,
      "loss": 0.0017,
      "step": 56590
    },
    {
      "epoch": 3.018666666666667,
      "grad_norm": 0.08202964812517166,
      "learning_rate": 1.2266666666666667e-05,
      "loss": 0.0016,
      "step": 56600
    },
    {
      "epoch": 3.0192,
      "grad_norm": 0.24924702942371368,
      "learning_rate": 1.2260000000000001e-05,
      "loss": 0.0027,
      "step": 56610
    },
    {
      "epoch": 3.0197333333333334,
      "grad_norm": 0.6212421655654907,
      "learning_rate": 1.2253333333333333e-05,
      "loss": 0.0016,
      "step": 56620
    },
    {
      "epoch": 3.0202666666666667,
      "grad_norm": 0.3308524489402771,
      "learning_rate": 1.2246666666666667e-05,
      "loss": 0.0029,
      "step": 56630
    },
    {
      "epoch": 3.0208,
      "grad_norm": 0.3422001302242279,
      "learning_rate": 1.224e-05,
      "loss": 0.0021,
      "step": 56640
    },
    {
      "epoch": 3.021333333333333,
      "grad_norm": 0.14848735928535461,
      "learning_rate": 1.2233333333333334e-05,
      "loss": 0.0027,
      "step": 56650
    },
    {
      "epoch": 3.0218666666666665,
      "grad_norm": 0.30230605602264404,
      "learning_rate": 1.2226666666666668e-05,
      "loss": 0.0026,
      "step": 56660
    },
    {
      "epoch": 3.0224,
      "grad_norm": 0.26068514585494995,
      "learning_rate": 1.2220000000000002e-05,
      "loss": 0.0037,
      "step": 56670
    },
    {
      "epoch": 3.0229333333333335,
      "grad_norm": 0.08207954466342926,
      "learning_rate": 1.2213333333333334e-05,
      "loss": 0.0028,
      "step": 56680
    },
    {
      "epoch": 3.0234666666666667,
      "grad_norm": 0.1179186999797821,
      "learning_rate": 1.2206666666666668e-05,
      "loss": 0.0028,
      "step": 56690
    },
    {
      "epoch": 3.024,
      "grad_norm": 0.17757274210453033,
      "learning_rate": 1.22e-05,
      "loss": 0.0023,
      "step": 56700
    },
    {
      "epoch": 3.0245333333333333,
      "grad_norm": 0.1862214058637619,
      "learning_rate": 1.2193333333333334e-05,
      "loss": 0.0023,
      "step": 56710
    },
    {
      "epoch": 3.0250666666666666,
      "grad_norm": 0.18499238789081573,
      "learning_rate": 1.2186666666666666e-05,
      "loss": 0.0027,
      "step": 56720
    },
    {
      "epoch": 3.0256,
      "grad_norm": 0.32167091965675354,
      "learning_rate": 1.2180000000000002e-05,
      "loss": 0.0017,
      "step": 56730
    },
    {
      "epoch": 3.026133333333333,
      "grad_norm": 0.1738118827342987,
      "learning_rate": 1.2173333333333334e-05,
      "loss": 0.0016,
      "step": 56740
    },
    {
      "epoch": 3.026666666666667,
      "grad_norm": 0.5620628595352173,
      "learning_rate": 1.2166666666666668e-05,
      "loss": 0.0019,
      "step": 56750
    },
    {
      "epoch": 3.0272,
      "grad_norm": 0.3649837076663971,
      "learning_rate": 1.216e-05,
      "loss": 0.0044,
      "step": 56760
    },
    {
      "epoch": 3.0277333333333334,
      "grad_norm": 0.2097318023443222,
      "learning_rate": 1.2153333333333333e-05,
      "loss": 0.0022,
      "step": 56770
    },
    {
      "epoch": 3.0282666666666667,
      "grad_norm": 0.21144677698612213,
      "learning_rate": 1.2146666666666667e-05,
      "loss": 0.0022,
      "step": 56780
    },
    {
      "epoch": 3.0288,
      "grad_norm": 0.1372690498828888,
      "learning_rate": 1.214e-05,
      "loss": 0.0021,
      "step": 56790
    },
    {
      "epoch": 3.029333333333333,
      "grad_norm": 0.11446207016706467,
      "learning_rate": 1.2133333333333335e-05,
      "loss": 0.0017,
      "step": 56800
    },
    {
      "epoch": 3.0298666666666665,
      "grad_norm": 0.15382349491119385,
      "learning_rate": 1.2126666666666667e-05,
      "loss": 0.0039,
      "step": 56810
    },
    {
      "epoch": 3.0304,
      "grad_norm": 0.20195460319519043,
      "learning_rate": 1.2120000000000001e-05,
      "loss": 0.0023,
      "step": 56820
    },
    {
      "epoch": 3.0309333333333335,
      "grad_norm": 0.13356740772724152,
      "learning_rate": 1.2113333333333333e-05,
      "loss": 0.0018,
      "step": 56830
    },
    {
      "epoch": 3.0314666666666668,
      "grad_norm": 0.14478404819965363,
      "learning_rate": 1.2106666666666667e-05,
      "loss": 0.0023,
      "step": 56840
    },
    {
      "epoch": 3.032,
      "grad_norm": 0.3515782356262207,
      "learning_rate": 1.2100000000000001e-05,
      "loss": 0.002,
      "step": 56850
    },
    {
      "epoch": 3.0325333333333333,
      "grad_norm": 0.19898980855941772,
      "learning_rate": 1.2093333333333335e-05,
      "loss": 0.0021,
      "step": 56860
    },
    {
      "epoch": 3.0330666666666666,
      "grad_norm": 0.10385113954544067,
      "learning_rate": 1.2086666666666667e-05,
      "loss": 0.0022,
      "step": 56870
    },
    {
      "epoch": 3.0336,
      "grad_norm": 0.12206798046827316,
      "learning_rate": 1.2080000000000001e-05,
      "loss": 0.0019,
      "step": 56880
    },
    {
      "epoch": 3.034133333333333,
      "grad_norm": 0.7012053728103638,
      "learning_rate": 1.2073333333333333e-05,
      "loss": 0.0028,
      "step": 56890
    },
    {
      "epoch": 3.034666666666667,
      "grad_norm": 0.468399316072464,
      "learning_rate": 1.2066666666666667e-05,
      "loss": 0.0037,
      "step": 56900
    },
    {
      "epoch": 3.0352,
      "grad_norm": 0.0587390698492527,
      "learning_rate": 1.206e-05,
      "loss": 0.0028,
      "step": 56910
    },
    {
      "epoch": 3.0357333333333334,
      "grad_norm": 0.1058209016919136,
      "learning_rate": 1.2053333333333334e-05,
      "loss": 0.0024,
      "step": 56920
    },
    {
      "epoch": 3.0362666666666667,
      "grad_norm": 0.39462676644325256,
      "learning_rate": 1.2046666666666668e-05,
      "loss": 0.0022,
      "step": 56930
    },
    {
      "epoch": 3.0368,
      "grad_norm": 0.0993908941745758,
      "learning_rate": 1.204e-05,
      "loss": 0.0017,
      "step": 56940
    },
    {
      "epoch": 3.037333333333333,
      "grad_norm": 0.632585346698761,
      "learning_rate": 1.2033333333333334e-05,
      "loss": 0.004,
      "step": 56950
    },
    {
      "epoch": 3.0378666666666665,
      "grad_norm": 0.47014087438583374,
      "learning_rate": 1.2026666666666666e-05,
      "loss": 0.0026,
      "step": 56960
    },
    {
      "epoch": 3.0384,
      "grad_norm": 0.29446303844451904,
      "learning_rate": 1.202e-05,
      "loss": 0.0017,
      "step": 56970
    },
    {
      "epoch": 3.0389333333333335,
      "grad_norm": 0.3543497323989868,
      "learning_rate": 1.2013333333333334e-05,
      "loss": 0.0027,
      "step": 56980
    },
    {
      "epoch": 3.0394666666666668,
      "grad_norm": 0.08554824441671371,
      "learning_rate": 1.2006666666666668e-05,
      "loss": 0.003,
      "step": 56990
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.39069244265556335,
      "learning_rate": 1.2e-05,
      "loss": 0.0024,
      "step": 57000
    },
    {
      "epoch": 3.0405333333333333,
      "grad_norm": 0.3297370374202728,
      "learning_rate": 1.1993333333333334e-05,
      "loss": 0.003,
      "step": 57010
    },
    {
      "epoch": 3.0410666666666666,
      "grad_norm": 0.1696096658706665,
      "learning_rate": 1.1986666666666667e-05,
      "loss": 0.0029,
      "step": 57020
    },
    {
      "epoch": 3.0416,
      "grad_norm": 0.23834379017353058,
      "learning_rate": 1.198e-05,
      "loss": 0.0018,
      "step": 57030
    },
    {
      "epoch": 3.042133333333333,
      "grad_norm": 0.42345887422561646,
      "learning_rate": 1.1973333333333334e-05,
      "loss": 0.0025,
      "step": 57040
    },
    {
      "epoch": 3.042666666666667,
      "grad_norm": 0.4889928996562958,
      "learning_rate": 1.1966666666666668e-05,
      "loss": 0.0019,
      "step": 57050
    },
    {
      "epoch": 3.0432,
      "grad_norm": 0.35676491260528564,
      "learning_rate": 1.196e-05,
      "loss": 0.0026,
      "step": 57060
    },
    {
      "epoch": 3.0437333333333334,
      "grad_norm": 0.4062224328517914,
      "learning_rate": 1.1953333333333335e-05,
      "loss": 0.0019,
      "step": 57070
    },
    {
      "epoch": 3.0442666666666667,
      "grad_norm": 0.12925082445144653,
      "learning_rate": 1.1946666666666667e-05,
      "loss": 0.0034,
      "step": 57080
    },
    {
      "epoch": 3.0448,
      "grad_norm": 0.1785172075033188,
      "learning_rate": 1.1940000000000001e-05,
      "loss": 0.0018,
      "step": 57090
    },
    {
      "epoch": 3.0453333333333332,
      "grad_norm": 0.5008898973464966,
      "learning_rate": 1.1933333333333333e-05,
      "loss": 0.002,
      "step": 57100
    },
    {
      "epoch": 3.0458666666666665,
      "grad_norm": 0.07473038136959076,
      "learning_rate": 1.1926666666666667e-05,
      "loss": 0.0022,
      "step": 57110
    },
    {
      "epoch": 3.0464,
      "grad_norm": 0.22366109490394592,
      "learning_rate": 1.1920000000000001e-05,
      "loss": 0.0021,
      "step": 57120
    },
    {
      "epoch": 3.0469333333333335,
      "grad_norm": 0.1679784059524536,
      "learning_rate": 1.1913333333333333e-05,
      "loss": 0.0034,
      "step": 57130
    },
    {
      "epoch": 3.0474666666666668,
      "grad_norm": 0.2883811891078949,
      "learning_rate": 1.1906666666666667e-05,
      "loss": 0.0027,
      "step": 57140
    },
    {
      "epoch": 3.048,
      "grad_norm": 0.3982324004173279,
      "learning_rate": 1.19e-05,
      "loss": 0.002,
      "step": 57150
    },
    {
      "epoch": 3.0485333333333333,
      "grad_norm": 0.4381425976753235,
      "learning_rate": 1.1893333333333334e-05,
      "loss": 0.0018,
      "step": 57160
    },
    {
      "epoch": 3.0490666666666666,
      "grad_norm": 0.4282165467739105,
      "learning_rate": 1.1886666666666667e-05,
      "loss": 0.0035,
      "step": 57170
    },
    {
      "epoch": 3.0496,
      "grad_norm": 0.13153010606765747,
      "learning_rate": 1.1880000000000001e-05,
      "loss": 0.0031,
      "step": 57180
    },
    {
      "epoch": 3.050133333333333,
      "grad_norm": 0.13017500936985016,
      "learning_rate": 1.1873333333333334e-05,
      "loss": 0.0018,
      "step": 57190
    },
    {
      "epoch": 3.050666666666667,
      "grad_norm": 0.12063726037740707,
      "learning_rate": 1.1866666666666668e-05,
      "loss": 0.0022,
      "step": 57200
    },
    {
      "epoch": 3.0512,
      "grad_norm": 0.29702457785606384,
      "learning_rate": 1.186e-05,
      "loss": 0.0028,
      "step": 57210
    },
    {
      "epoch": 3.0517333333333334,
      "grad_norm": 0.08210495114326477,
      "learning_rate": 1.1853333333333334e-05,
      "loss": 0.0027,
      "step": 57220
    },
    {
      "epoch": 3.0522666666666667,
      "grad_norm": 0.7009307742118835,
      "learning_rate": 1.1846666666666666e-05,
      "loss": 0.003,
      "step": 57230
    },
    {
      "epoch": 3.0528,
      "grad_norm": 0.15818287432193756,
      "learning_rate": 1.1840000000000002e-05,
      "loss": 0.0023,
      "step": 57240
    },
    {
      "epoch": 3.0533333333333332,
      "grad_norm": 0.3299461901187897,
      "learning_rate": 1.1833333333333334e-05,
      "loss": 0.0026,
      "step": 57250
    },
    {
      "epoch": 3.0538666666666665,
      "grad_norm": 0.11150961369276047,
      "learning_rate": 1.1826666666666668e-05,
      "loss": 0.0019,
      "step": 57260
    },
    {
      "epoch": 3.0544,
      "grad_norm": 0.4258705973625183,
      "learning_rate": 1.182e-05,
      "loss": 0.002,
      "step": 57270
    },
    {
      "epoch": 3.0549333333333335,
      "grad_norm": 0.11031542718410492,
      "learning_rate": 1.1813333333333334e-05,
      "loss": 0.002,
      "step": 57280
    },
    {
      "epoch": 3.0554666666666668,
      "grad_norm": 0.13319356739521027,
      "learning_rate": 1.1806666666666667e-05,
      "loss": 0.0025,
      "step": 57290
    },
    {
      "epoch": 3.056,
      "grad_norm": 0.17341136932373047,
      "learning_rate": 1.18e-05,
      "loss": 0.0019,
      "step": 57300
    },
    {
      "epoch": 3.0565333333333333,
      "grad_norm": 0.17216409742832184,
      "learning_rate": 1.1793333333333334e-05,
      "loss": 0.002,
      "step": 57310
    },
    {
      "epoch": 3.0570666666666666,
      "grad_norm": 0.4856318533420563,
      "learning_rate": 1.1786666666666667e-05,
      "loss": 0.002,
      "step": 57320
    },
    {
      "epoch": 3.0576,
      "grad_norm": 0.29372233152389526,
      "learning_rate": 1.178e-05,
      "loss": 0.0024,
      "step": 57330
    },
    {
      "epoch": 3.058133333333333,
      "grad_norm": 0.6135262250900269,
      "learning_rate": 1.1773333333333333e-05,
      "loss": 0.003,
      "step": 57340
    },
    {
      "epoch": 3.058666666666667,
      "grad_norm": 0.25024479627609253,
      "learning_rate": 1.1766666666666667e-05,
      "loss": 0.0027,
      "step": 57350
    },
    {
      "epoch": 3.0592,
      "grad_norm": 0.25816312432289124,
      "learning_rate": 1.1760000000000001e-05,
      "loss": 0.0021,
      "step": 57360
    },
    {
      "epoch": 3.0597333333333334,
      "grad_norm": 0.13823634386062622,
      "learning_rate": 1.1753333333333335e-05,
      "loss": 0.0027,
      "step": 57370
    },
    {
      "epoch": 3.0602666666666667,
      "grad_norm": 0.14621423184871674,
      "learning_rate": 1.1746666666666667e-05,
      "loss": 0.0022,
      "step": 57380
    },
    {
      "epoch": 3.0608,
      "grad_norm": 0.3403748869895935,
      "learning_rate": 1.1740000000000001e-05,
      "loss": 0.0022,
      "step": 57390
    },
    {
      "epoch": 3.0613333333333332,
      "grad_norm": 0.2153352051973343,
      "learning_rate": 1.1733333333333333e-05,
      "loss": 0.0024,
      "step": 57400
    },
    {
      "epoch": 3.0618666666666665,
      "grad_norm": 0.19286511838436127,
      "learning_rate": 1.1726666666666667e-05,
      "loss": 0.0021,
      "step": 57410
    },
    {
      "epoch": 3.0624,
      "grad_norm": 0.11827122420072556,
      "learning_rate": 1.172e-05,
      "loss": 0.002,
      "step": 57420
    },
    {
      "epoch": 3.0629333333333335,
      "grad_norm": 0.11147269606590271,
      "learning_rate": 1.1713333333333335e-05,
      "loss": 0.0021,
      "step": 57430
    },
    {
      "epoch": 3.063466666666667,
      "grad_norm": 0.35805755853652954,
      "learning_rate": 1.1706666666666668e-05,
      "loss": 0.0019,
      "step": 57440
    },
    {
      "epoch": 3.064,
      "grad_norm": 0.5172310471534729,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 0.0026,
      "step": 57450
    },
    {
      "epoch": 3.0645333333333333,
      "grad_norm": 0.3056236803531647,
      "learning_rate": 1.1693333333333334e-05,
      "loss": 0.0034,
      "step": 57460
    },
    {
      "epoch": 3.0650666666666666,
      "grad_norm": 0.1835280954837799,
      "learning_rate": 1.1686666666666666e-05,
      "loss": 0.0023,
      "step": 57470
    },
    {
      "epoch": 3.0656,
      "grad_norm": 0.43922045826911926,
      "learning_rate": 1.168e-05,
      "loss": 0.0019,
      "step": 57480
    },
    {
      "epoch": 3.066133333333333,
      "grad_norm": 0.3007344901561737,
      "learning_rate": 1.1673333333333334e-05,
      "loss": 0.0017,
      "step": 57490
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 0.211911141872406,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 0.0021,
      "step": 57500
    },
    {
      "epoch": 3.0672,
      "grad_norm": 0.18221460282802582,
      "learning_rate": 1.166e-05,
      "loss": 0.0019,
      "step": 57510
    },
    {
      "epoch": 3.0677333333333334,
      "grad_norm": 0.2944723665714264,
      "learning_rate": 1.1653333333333334e-05,
      "loss": 0.0029,
      "step": 57520
    },
    {
      "epoch": 3.0682666666666667,
      "grad_norm": 0.21039699018001556,
      "learning_rate": 1.1646666666666666e-05,
      "loss": 0.0025,
      "step": 57530
    },
    {
      "epoch": 3.0688,
      "grad_norm": 0.3947911560535431,
      "learning_rate": 1.164e-05,
      "loss": 0.0019,
      "step": 57540
    },
    {
      "epoch": 3.0693333333333332,
      "grad_norm": 0.41940048336982727,
      "learning_rate": 1.1633333333333334e-05,
      "loss": 0.0026,
      "step": 57550
    },
    {
      "epoch": 3.0698666666666665,
      "grad_norm": 0.16592855751514435,
      "learning_rate": 1.1626666666666668e-05,
      "loss": 0.0025,
      "step": 57560
    },
    {
      "epoch": 3.0704,
      "grad_norm": 0.21935497224330902,
      "learning_rate": 1.162e-05,
      "loss": 0.0034,
      "step": 57570
    },
    {
      "epoch": 3.0709333333333335,
      "grad_norm": 0.39729368686676025,
      "learning_rate": 1.1613333333333335e-05,
      "loss": 0.0026,
      "step": 57580
    },
    {
      "epoch": 3.071466666666667,
      "grad_norm": 0.3063899278640747,
      "learning_rate": 1.1606666666666667e-05,
      "loss": 0.0024,
      "step": 57590
    },
    {
      "epoch": 3.072,
      "grad_norm": 0.07458619028329849,
      "learning_rate": 1.16e-05,
      "loss": 0.0024,
      "step": 57600
    },
    {
      "epoch": 3.0725333333333333,
      "grad_norm": 0.23364470899105072,
      "learning_rate": 1.1593333333333333e-05,
      "loss": 0.0019,
      "step": 57610
    },
    {
      "epoch": 3.0730666666666666,
      "grad_norm": 0.40605658292770386,
      "learning_rate": 1.1586666666666669e-05,
      "loss": 0.0019,
      "step": 57620
    },
    {
      "epoch": 3.0736,
      "grad_norm": 0.1324172168970108,
      "learning_rate": 1.1580000000000001e-05,
      "loss": 0.0018,
      "step": 57630
    },
    {
      "epoch": 3.074133333333333,
      "grad_norm": 0.42428070306777954,
      "learning_rate": 1.1573333333333333e-05,
      "loss": 0.0021,
      "step": 57640
    },
    {
      "epoch": 3.074666666666667,
      "grad_norm": 0.0870346873998642,
      "learning_rate": 1.1566666666666667e-05,
      "loss": 0.0021,
      "step": 57650
    },
    {
      "epoch": 3.0752,
      "grad_norm": 0.2638472020626068,
      "learning_rate": 1.156e-05,
      "loss": 0.0029,
      "step": 57660
    },
    {
      "epoch": 3.0757333333333334,
      "grad_norm": 0.0644625574350357,
      "learning_rate": 1.1553333333333333e-05,
      "loss": 0.0018,
      "step": 57670
    },
    {
      "epoch": 3.0762666666666667,
      "grad_norm": 0.34073615074157715,
      "learning_rate": 1.1546666666666667e-05,
      "loss": 0.0019,
      "step": 57680
    },
    {
      "epoch": 3.0768,
      "grad_norm": 0.14342208206653595,
      "learning_rate": 1.1540000000000001e-05,
      "loss": 0.0022,
      "step": 57690
    },
    {
      "epoch": 3.0773333333333333,
      "grad_norm": 0.17318864166736603,
      "learning_rate": 1.1533333333333334e-05,
      "loss": 0.0018,
      "step": 57700
    },
    {
      "epoch": 3.0778666666666665,
      "grad_norm": 0.15361405909061432,
      "learning_rate": 1.1526666666666668e-05,
      "loss": 0.0019,
      "step": 57710
    },
    {
      "epoch": 3.0784,
      "grad_norm": 0.2519915997982025,
      "learning_rate": 1.152e-05,
      "loss": 0.0025,
      "step": 57720
    },
    {
      "epoch": 3.0789333333333335,
      "grad_norm": 0.054759375751018524,
      "learning_rate": 1.1513333333333334e-05,
      "loss": 0.0025,
      "step": 57730
    },
    {
      "epoch": 3.079466666666667,
      "grad_norm": 0.4284976124763489,
      "learning_rate": 1.1506666666666668e-05,
      "loss": 0.0028,
      "step": 57740
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.6083275675773621,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 0.0019,
      "step": 57750
    },
    {
      "epoch": 3.0805333333333333,
      "grad_norm": 0.22156956791877747,
      "learning_rate": 1.1493333333333334e-05,
      "loss": 0.0027,
      "step": 57760
    },
    {
      "epoch": 3.0810666666666666,
      "grad_norm": 0.3420529067516327,
      "learning_rate": 1.1486666666666668e-05,
      "loss": 0.0029,
      "step": 57770
    },
    {
      "epoch": 3.0816,
      "grad_norm": 0.09419277310371399,
      "learning_rate": 1.148e-05,
      "loss": 0.0021,
      "step": 57780
    },
    {
      "epoch": 3.082133333333333,
      "grad_norm": 0.518237829208374,
      "learning_rate": 1.1473333333333334e-05,
      "loss": 0.003,
      "step": 57790
    },
    {
      "epoch": 3.0826666666666664,
      "grad_norm": 0.1679040640592575,
      "learning_rate": 1.1466666666666666e-05,
      "loss": 0.0025,
      "step": 57800
    },
    {
      "epoch": 3.0832,
      "grad_norm": 0.18201115727424622,
      "learning_rate": 1.146e-05,
      "loss": 0.002,
      "step": 57810
    },
    {
      "epoch": 3.0837333333333334,
      "grad_norm": 0.45386752486228943,
      "learning_rate": 1.1453333333333334e-05,
      "loss": 0.0037,
      "step": 57820
    },
    {
      "epoch": 3.0842666666666667,
      "grad_norm": 0.23748597502708435,
      "learning_rate": 1.1446666666666667e-05,
      "loss": 0.0027,
      "step": 57830
    },
    {
      "epoch": 3.0848,
      "grad_norm": 0.3655908405780792,
      "learning_rate": 1.144e-05,
      "loss": 0.0039,
      "step": 57840
    },
    {
      "epoch": 3.0853333333333333,
      "grad_norm": 0.14314484596252441,
      "learning_rate": 1.1433333333333333e-05,
      "loss": 0.0032,
      "step": 57850
    },
    {
      "epoch": 3.0858666666666665,
      "grad_norm": 0.32932248711586,
      "learning_rate": 1.1426666666666667e-05,
      "loss": 0.0037,
      "step": 57860
    },
    {
      "epoch": 3.0864,
      "grad_norm": 0.4481487572193146,
      "learning_rate": 1.142e-05,
      "loss": 0.002,
      "step": 57870
    },
    {
      "epoch": 3.0869333333333335,
      "grad_norm": 0.4637686610221863,
      "learning_rate": 1.1413333333333335e-05,
      "loss": 0.0037,
      "step": 57880
    },
    {
      "epoch": 3.087466666666667,
      "grad_norm": 0.4027268588542938,
      "learning_rate": 1.1406666666666667e-05,
      "loss": 0.0026,
      "step": 57890
    },
    {
      "epoch": 3.088,
      "grad_norm": 0.4685365557670593,
      "learning_rate": 1.1400000000000001e-05,
      "loss": 0.002,
      "step": 57900
    },
    {
      "epoch": 3.0885333333333334,
      "grad_norm": 0.13298706710338593,
      "learning_rate": 1.1393333333333333e-05,
      "loss": 0.0026,
      "step": 57910
    },
    {
      "epoch": 3.0890666666666666,
      "grad_norm": 0.08665009588003159,
      "learning_rate": 1.1386666666666667e-05,
      "loss": 0.0032,
      "step": 57920
    },
    {
      "epoch": 3.0896,
      "grad_norm": 0.2646346092224121,
      "learning_rate": 1.1380000000000001e-05,
      "loss": 0.0038,
      "step": 57930
    },
    {
      "epoch": 3.090133333333333,
      "grad_norm": 0.08636245131492615,
      "learning_rate": 1.1373333333333335e-05,
      "loss": 0.0036,
      "step": 57940
    },
    {
      "epoch": 3.0906666666666665,
      "grad_norm": 0.28628671169281006,
      "learning_rate": 1.1366666666666667e-05,
      "loss": 0.0018,
      "step": 57950
    },
    {
      "epoch": 3.0912,
      "grad_norm": 0.22058607637882233,
      "learning_rate": 1.1360000000000001e-05,
      "loss": 0.0016,
      "step": 57960
    },
    {
      "epoch": 3.0917333333333334,
      "grad_norm": 0.3099425435066223,
      "learning_rate": 1.1353333333333334e-05,
      "loss": 0.0018,
      "step": 57970
    },
    {
      "epoch": 3.0922666666666667,
      "grad_norm": 0.11428242921829224,
      "learning_rate": 1.1346666666666666e-05,
      "loss": 0.0031,
      "step": 57980
    },
    {
      "epoch": 3.0928,
      "grad_norm": 0.10916823893785477,
      "learning_rate": 1.134e-05,
      "loss": 0.0018,
      "step": 57990
    },
    {
      "epoch": 3.0933333333333333,
      "grad_norm": 0.4106776714324951,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 0.0034,
      "step": 58000
    },
    {
      "epoch": 3.0938666666666665,
      "grad_norm": 0.22157862782478333,
      "learning_rate": 1.1326666666666668e-05,
      "loss": 0.003,
      "step": 58010
    },
    {
      "epoch": 3.0944,
      "grad_norm": 0.2875315248966217,
      "learning_rate": 1.132e-05,
      "loss": 0.0019,
      "step": 58020
    },
    {
      "epoch": 3.0949333333333335,
      "grad_norm": 0.16122190654277802,
      "learning_rate": 1.1313333333333334e-05,
      "loss": 0.0028,
      "step": 58030
    },
    {
      "epoch": 3.095466666666667,
      "grad_norm": 0.23659656941890717,
      "learning_rate": 1.1306666666666666e-05,
      "loss": 0.0027,
      "step": 58040
    },
    {
      "epoch": 3.096,
      "grad_norm": 0.18964873254299164,
      "learning_rate": 1.13e-05,
      "loss": 0.0023,
      "step": 58050
    },
    {
      "epoch": 3.0965333333333334,
      "grad_norm": 0.42339691519737244,
      "learning_rate": 1.1293333333333334e-05,
      "loss": 0.0044,
      "step": 58060
    },
    {
      "epoch": 3.0970666666666666,
      "grad_norm": 0.2769961953163147,
      "learning_rate": 1.1286666666666668e-05,
      "loss": 0.0025,
      "step": 58070
    },
    {
      "epoch": 3.0976,
      "grad_norm": 0.0787886381149292,
      "learning_rate": 1.128e-05,
      "loss": 0.0018,
      "step": 58080
    },
    {
      "epoch": 3.098133333333333,
      "grad_norm": 0.15351204574108124,
      "learning_rate": 1.1273333333333334e-05,
      "loss": 0.0025,
      "step": 58090
    },
    {
      "epoch": 3.0986666666666665,
      "grad_norm": 0.11445444077253342,
      "learning_rate": 1.1266666666666667e-05,
      "loss": 0.0015,
      "step": 58100
    },
    {
      "epoch": 3.0992,
      "grad_norm": 0.14374953508377075,
      "learning_rate": 1.126e-05,
      "loss": 0.0025,
      "step": 58110
    },
    {
      "epoch": 3.0997333333333335,
      "grad_norm": 0.3369385302066803,
      "learning_rate": 1.1253333333333335e-05,
      "loss": 0.0021,
      "step": 58120
    },
    {
      "epoch": 3.1002666666666667,
      "grad_norm": 0.3304426372051239,
      "learning_rate": 1.1246666666666669e-05,
      "loss": 0.0019,
      "step": 58130
    },
    {
      "epoch": 3.1008,
      "grad_norm": 0.3635045886039734,
      "learning_rate": 1.124e-05,
      "loss": 0.0028,
      "step": 58140
    },
    {
      "epoch": 3.1013333333333333,
      "grad_norm": 0.18122997879981995,
      "learning_rate": 1.1233333333333333e-05,
      "loss": 0.0032,
      "step": 58150
    },
    {
      "epoch": 3.1018666666666665,
      "grad_norm": 0.27859747409820557,
      "learning_rate": 1.1226666666666667e-05,
      "loss": 0.0041,
      "step": 58160
    },
    {
      "epoch": 3.1024,
      "grad_norm": 0.16860496997833252,
      "learning_rate": 1.122e-05,
      "loss": 0.0024,
      "step": 58170
    },
    {
      "epoch": 3.1029333333333335,
      "grad_norm": 0.25404033064842224,
      "learning_rate": 1.1213333333333333e-05,
      "loss": 0.0022,
      "step": 58180
    },
    {
      "epoch": 3.103466666666667,
      "grad_norm": 0.17450352013111115,
      "learning_rate": 1.1206666666666667e-05,
      "loss": 0.0018,
      "step": 58190
    },
    {
      "epoch": 3.104,
      "grad_norm": 0.10382638871669769,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.0017,
      "step": 58200
    },
    {
      "epoch": 3.1045333333333334,
      "grad_norm": 0.11403405666351318,
      "learning_rate": 1.1193333333333333e-05,
      "loss": 0.0037,
      "step": 58210
    },
    {
      "epoch": 3.1050666666666666,
      "grad_norm": 0.29121771454811096,
      "learning_rate": 1.1186666666666667e-05,
      "loss": 0.0024,
      "step": 58220
    },
    {
      "epoch": 3.1056,
      "grad_norm": 0.23370540142059326,
      "learning_rate": 1.118e-05,
      "loss": 0.0029,
      "step": 58230
    },
    {
      "epoch": 3.106133333333333,
      "grad_norm": 0.3828863203525543,
      "learning_rate": 1.1173333333333334e-05,
      "loss": 0.0023,
      "step": 58240
    },
    {
      "epoch": 3.1066666666666665,
      "grad_norm": 0.07707836478948593,
      "learning_rate": 1.1166666666666668e-05,
      "loss": 0.002,
      "step": 58250
    },
    {
      "epoch": 3.1072,
      "grad_norm": 0.4529930353164673,
      "learning_rate": 1.1160000000000002e-05,
      "loss": 0.002,
      "step": 58260
    },
    {
      "epoch": 3.1077333333333335,
      "grad_norm": 0.13981980085372925,
      "learning_rate": 1.1153333333333334e-05,
      "loss": 0.002,
      "step": 58270
    },
    {
      "epoch": 3.1082666666666667,
      "grad_norm": 0.19875875115394592,
      "learning_rate": 1.1146666666666668e-05,
      "loss": 0.0022,
      "step": 58280
    },
    {
      "epoch": 3.1088,
      "grad_norm": 0.11007773131132126,
      "learning_rate": 1.114e-05,
      "loss": 0.003,
      "step": 58290
    },
    {
      "epoch": 3.1093333333333333,
      "grad_norm": 0.6968614459037781,
      "learning_rate": 1.1133333333333334e-05,
      "loss": 0.0037,
      "step": 58300
    },
    {
      "epoch": 3.1098666666666666,
      "grad_norm": 0.07488438487052917,
      "learning_rate": 1.1126666666666668e-05,
      "loss": 0.0021,
      "step": 58310
    },
    {
      "epoch": 3.1104,
      "grad_norm": 0.2108459323644638,
      "learning_rate": 1.112e-05,
      "loss": 0.0019,
      "step": 58320
    },
    {
      "epoch": 3.1109333333333336,
      "grad_norm": 0.5684046745300293,
      "learning_rate": 1.1113333333333334e-05,
      "loss": 0.0023,
      "step": 58330
    },
    {
      "epoch": 3.111466666666667,
      "grad_norm": 0.1663304567337036,
      "learning_rate": 1.1106666666666666e-05,
      "loss": 0.0023,
      "step": 58340
    },
    {
      "epoch": 3.112,
      "grad_norm": 0.31603914499282837,
      "learning_rate": 1.11e-05,
      "loss": 0.0032,
      "step": 58350
    },
    {
      "epoch": 3.1125333333333334,
      "grad_norm": 0.6465437412261963,
      "learning_rate": 1.1093333333333333e-05,
      "loss": 0.0018,
      "step": 58360
    },
    {
      "epoch": 3.1130666666666666,
      "grad_norm": 0.30980560183525085,
      "learning_rate": 1.1086666666666667e-05,
      "loss": 0.0021,
      "step": 58370
    },
    {
      "epoch": 3.1136,
      "grad_norm": 0.11073259264230728,
      "learning_rate": 1.108e-05,
      "loss": 0.0026,
      "step": 58380
    },
    {
      "epoch": 3.114133333333333,
      "grad_norm": 0.21188636124134064,
      "learning_rate": 1.1073333333333335e-05,
      "loss": 0.002,
      "step": 58390
    },
    {
      "epoch": 3.1146666666666665,
      "grad_norm": 0.06891857087612152,
      "learning_rate": 1.1066666666666667e-05,
      "loss": 0.0031,
      "step": 58400
    },
    {
      "epoch": 3.1152,
      "grad_norm": 0.2246566116809845,
      "learning_rate": 1.106e-05,
      "loss": 0.0017,
      "step": 58410
    },
    {
      "epoch": 3.1157333333333335,
      "grad_norm": 0.06625183671712875,
      "learning_rate": 1.1053333333333333e-05,
      "loss": 0.0024,
      "step": 58420
    },
    {
      "epoch": 3.1162666666666667,
      "grad_norm": 0.4002162516117096,
      "learning_rate": 1.1046666666666667e-05,
      "loss": 0.0037,
      "step": 58430
    },
    {
      "epoch": 3.1168,
      "grad_norm": 0.24129816889762878,
      "learning_rate": 1.1040000000000001e-05,
      "loss": 0.0037,
      "step": 58440
    },
    {
      "epoch": 3.1173333333333333,
      "grad_norm": 0.2370232343673706,
      "learning_rate": 1.1033333333333335e-05,
      "loss": 0.0024,
      "step": 58450
    },
    {
      "epoch": 3.1178666666666666,
      "grad_norm": 0.14629581570625305,
      "learning_rate": 1.1026666666666667e-05,
      "loss": 0.0037,
      "step": 58460
    },
    {
      "epoch": 3.1184,
      "grad_norm": 0.19510221481323242,
      "learning_rate": 1.1020000000000001e-05,
      "loss": 0.0022,
      "step": 58470
    },
    {
      "epoch": 3.1189333333333336,
      "grad_norm": 0.3809880316257477,
      "learning_rate": 1.1013333333333333e-05,
      "loss": 0.0018,
      "step": 58480
    },
    {
      "epoch": 3.119466666666667,
      "grad_norm": 0.5091502070426941,
      "learning_rate": 1.1006666666666666e-05,
      "loss": 0.0021,
      "step": 58490
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.3434222638607025,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.0018,
      "step": 58500
    },
    {
      "epoch": 3.1205333333333334,
      "grad_norm": 0.2781060039997101,
      "learning_rate": 1.0993333333333334e-05,
      "loss": 0.0027,
      "step": 58510
    },
    {
      "epoch": 3.1210666666666667,
      "grad_norm": 0.07473810017108917,
      "learning_rate": 1.0986666666666668e-05,
      "loss": 0.0015,
      "step": 58520
    },
    {
      "epoch": 3.1216,
      "grad_norm": 0.24115028977394104,
      "learning_rate": 1.098e-05,
      "loss": 0.002,
      "step": 58530
    },
    {
      "epoch": 3.122133333333333,
      "grad_norm": 0.10491781681776047,
      "learning_rate": 1.0973333333333334e-05,
      "loss": 0.0027,
      "step": 58540
    },
    {
      "epoch": 3.1226666666666665,
      "grad_norm": 0.13552486896514893,
      "learning_rate": 1.0966666666666666e-05,
      "loss": 0.0029,
      "step": 58550
    },
    {
      "epoch": 3.1232,
      "grad_norm": 0.47064408659935,
      "learning_rate": 1.096e-05,
      "loss": 0.0017,
      "step": 58560
    },
    {
      "epoch": 3.1237333333333335,
      "grad_norm": 0.4091635048389435,
      "learning_rate": 1.0953333333333334e-05,
      "loss": 0.0025,
      "step": 58570
    },
    {
      "epoch": 3.1242666666666667,
      "grad_norm": 0.4135218560695648,
      "learning_rate": 1.0946666666666668e-05,
      "loss": 0.002,
      "step": 58580
    },
    {
      "epoch": 3.1248,
      "grad_norm": 0.13528575003147125,
      "learning_rate": 1.094e-05,
      "loss": 0.0017,
      "step": 58590
    },
    {
      "epoch": 3.1253333333333333,
      "grad_norm": 0.09723890572786331,
      "learning_rate": 1.0933333333333334e-05,
      "loss": 0.0025,
      "step": 58600
    },
    {
      "epoch": 3.1258666666666666,
      "grad_norm": 0.13103625178337097,
      "learning_rate": 1.0926666666666667e-05,
      "loss": 0.0021,
      "step": 58610
    },
    {
      "epoch": 3.1264,
      "grad_norm": 0.07986941188573837,
      "learning_rate": 1.092e-05,
      "loss": 0.0022,
      "step": 58620
    },
    {
      "epoch": 3.1269333333333336,
      "grad_norm": 0.3369017541408539,
      "learning_rate": 1.0913333333333334e-05,
      "loss": 0.0029,
      "step": 58630
    },
    {
      "epoch": 3.127466666666667,
      "grad_norm": 0.148080974817276,
      "learning_rate": 1.0906666666666668e-05,
      "loss": 0.0021,
      "step": 58640
    },
    {
      "epoch": 3.128,
      "grad_norm": 0.2253028154373169,
      "learning_rate": 1.09e-05,
      "loss": 0.0021,
      "step": 58650
    },
    {
      "epoch": 3.1285333333333334,
      "grad_norm": 0.18940530717372894,
      "learning_rate": 1.0893333333333333e-05,
      "loss": 0.0019,
      "step": 58660
    },
    {
      "epoch": 3.1290666666666667,
      "grad_norm": 0.11837171763181686,
      "learning_rate": 1.0886666666666667e-05,
      "loss": 0.0019,
      "step": 58670
    },
    {
      "epoch": 3.1296,
      "grad_norm": 0.21414007246494293,
      "learning_rate": 1.088e-05,
      "loss": 0.0021,
      "step": 58680
    },
    {
      "epoch": 3.130133333333333,
      "grad_norm": 0.12753956019878387,
      "learning_rate": 1.0873333333333335e-05,
      "loss": 0.0032,
      "step": 58690
    },
    {
      "epoch": 3.1306666666666665,
      "grad_norm": 0.43194901943206787,
      "learning_rate": 1.0866666666666667e-05,
      "loss": 0.002,
      "step": 58700
    },
    {
      "epoch": 3.1312,
      "grad_norm": 0.24806587398052216,
      "learning_rate": 1.0860000000000001e-05,
      "loss": 0.0019,
      "step": 58710
    },
    {
      "epoch": 3.1317333333333335,
      "grad_norm": 0.3175411820411682,
      "learning_rate": 1.0853333333333333e-05,
      "loss": 0.0027,
      "step": 58720
    },
    {
      "epoch": 3.1322666666666668,
      "grad_norm": 0.5336480140686035,
      "learning_rate": 1.0846666666666667e-05,
      "loss": 0.0019,
      "step": 58730
    },
    {
      "epoch": 3.1328,
      "grad_norm": 0.09804212301969528,
      "learning_rate": 1.084e-05,
      "loss": 0.0016,
      "step": 58740
    },
    {
      "epoch": 3.1333333333333333,
      "grad_norm": 0.39757752418518066,
      "learning_rate": 1.0833333333333334e-05,
      "loss": 0.002,
      "step": 58750
    },
    {
      "epoch": 3.1338666666666666,
      "grad_norm": 0.5095799565315247,
      "learning_rate": 1.0826666666666667e-05,
      "loss": 0.0027,
      "step": 58760
    },
    {
      "epoch": 3.1344,
      "grad_norm": 0.14098934829235077,
      "learning_rate": 1.0820000000000001e-05,
      "loss": 0.0021,
      "step": 58770
    },
    {
      "epoch": 3.134933333333333,
      "grad_norm": 0.2806638479232788,
      "learning_rate": 1.0813333333333334e-05,
      "loss": 0.0023,
      "step": 58780
    },
    {
      "epoch": 3.135466666666667,
      "grad_norm": 0.0996319130063057,
      "learning_rate": 1.0806666666666668e-05,
      "loss": 0.0016,
      "step": 58790
    },
    {
      "epoch": 3.136,
      "grad_norm": 0.18337413668632507,
      "learning_rate": 1.08e-05,
      "loss": 0.0024,
      "step": 58800
    },
    {
      "epoch": 3.1365333333333334,
      "grad_norm": 0.4432457983493805,
      "learning_rate": 1.0793333333333334e-05,
      "loss": 0.0016,
      "step": 58810
    },
    {
      "epoch": 3.1370666666666667,
      "grad_norm": 0.3793710768222809,
      "learning_rate": 1.0786666666666668e-05,
      "loss": 0.0028,
      "step": 58820
    },
    {
      "epoch": 3.1376,
      "grad_norm": 0.36851221323013306,
      "learning_rate": 1.0780000000000002e-05,
      "loss": 0.0017,
      "step": 58830
    },
    {
      "epoch": 3.138133333333333,
      "grad_norm": 0.35367318987846375,
      "learning_rate": 1.0773333333333334e-05,
      "loss": 0.0033,
      "step": 58840
    },
    {
      "epoch": 3.1386666666666665,
      "grad_norm": 0.39353659749031067,
      "learning_rate": 1.0766666666666666e-05,
      "loss": 0.0019,
      "step": 58850
    },
    {
      "epoch": 3.1391999999999998,
      "grad_norm": 0.33524397015571594,
      "learning_rate": 1.076e-05,
      "loss": 0.002,
      "step": 58860
    },
    {
      "epoch": 3.1397333333333335,
      "grad_norm": 0.2461717277765274,
      "learning_rate": 1.0753333333333333e-05,
      "loss": 0.0029,
      "step": 58870
    },
    {
      "epoch": 3.1402666666666668,
      "grad_norm": 0.33518239855766296,
      "learning_rate": 1.0746666666666667e-05,
      "loss": 0.0032,
      "step": 58880
    },
    {
      "epoch": 3.1408,
      "grad_norm": 0.24845977127552032,
      "learning_rate": 1.074e-05,
      "loss": 0.0023,
      "step": 58890
    },
    {
      "epoch": 3.1413333333333333,
      "grad_norm": 0.21126319468021393,
      "learning_rate": 1.0733333333333334e-05,
      "loss": 0.0043,
      "step": 58900
    },
    {
      "epoch": 3.1418666666666666,
      "grad_norm": 0.2117116004228592,
      "learning_rate": 1.0726666666666667e-05,
      "loss": 0.003,
      "step": 58910
    },
    {
      "epoch": 3.1424,
      "grad_norm": 0.29081887006759644,
      "learning_rate": 1.072e-05,
      "loss": 0.0021,
      "step": 58920
    },
    {
      "epoch": 3.142933333333333,
      "grad_norm": 0.4520217478275299,
      "learning_rate": 1.0713333333333333e-05,
      "loss": 0.0019,
      "step": 58930
    },
    {
      "epoch": 3.143466666666667,
      "grad_norm": 0.458299845457077,
      "learning_rate": 1.0706666666666667e-05,
      "loss": 0.0029,
      "step": 58940
    },
    {
      "epoch": 3.144,
      "grad_norm": 0.5098749995231628,
      "learning_rate": 1.0700000000000001e-05,
      "loss": 0.002,
      "step": 58950
    },
    {
      "epoch": 3.1445333333333334,
      "grad_norm": 0.22718282043933868,
      "learning_rate": 1.0693333333333335e-05,
      "loss": 0.0023,
      "step": 58960
    },
    {
      "epoch": 3.1450666666666667,
      "grad_norm": 0.238935187458992,
      "learning_rate": 1.0686666666666667e-05,
      "loss": 0.0027,
      "step": 58970
    },
    {
      "epoch": 3.1456,
      "grad_norm": 0.06287269294261932,
      "learning_rate": 1.0680000000000001e-05,
      "loss": 0.0028,
      "step": 58980
    },
    {
      "epoch": 3.1461333333333332,
      "grad_norm": 0.5437855124473572,
      "learning_rate": 1.0673333333333333e-05,
      "loss": 0.002,
      "step": 58990
    },
    {
      "epoch": 3.1466666666666665,
      "grad_norm": 0.338223934173584,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 0.0026,
      "step": 59000
    },
    {
      "epoch": 3.1471999999999998,
      "grad_norm": 0.18626224994659424,
      "learning_rate": 1.0660000000000001e-05,
      "loss": 0.0027,
      "step": 59010
    },
    {
      "epoch": 3.1477333333333335,
      "grad_norm": 0.07869593799114227,
      "learning_rate": 1.0653333333333334e-05,
      "loss": 0.0028,
      "step": 59020
    },
    {
      "epoch": 3.1482666666666668,
      "grad_norm": 0.29699429869651794,
      "learning_rate": 1.0646666666666668e-05,
      "loss": 0.002,
      "step": 59030
    },
    {
      "epoch": 3.1488,
      "grad_norm": 0.4782552123069763,
      "learning_rate": 1.064e-05,
      "loss": 0.0025,
      "step": 59040
    },
    {
      "epoch": 3.1493333333333333,
      "grad_norm": 0.20659582316875458,
      "learning_rate": 1.0633333333333334e-05,
      "loss": 0.0029,
      "step": 59050
    },
    {
      "epoch": 3.1498666666666666,
      "grad_norm": 0.35464975237846375,
      "learning_rate": 1.0626666666666666e-05,
      "loss": 0.0021,
      "step": 59060
    },
    {
      "epoch": 3.1504,
      "grad_norm": 0.274634450674057,
      "learning_rate": 1.062e-05,
      "loss": 0.0019,
      "step": 59070
    },
    {
      "epoch": 3.150933333333333,
      "grad_norm": 0.09960003197193146,
      "learning_rate": 1.0613333333333334e-05,
      "loss": 0.0029,
      "step": 59080
    },
    {
      "epoch": 3.151466666666667,
      "grad_norm": 0.21221376955509186,
      "learning_rate": 1.0606666666666668e-05,
      "loss": 0.0024,
      "step": 59090
    },
    {
      "epoch": 3.152,
      "grad_norm": 0.23715171217918396,
      "learning_rate": 1.06e-05,
      "loss": 0.0018,
      "step": 59100
    },
    {
      "epoch": 3.1525333333333334,
      "grad_norm": 0.2946825623512268,
      "learning_rate": 1.0593333333333334e-05,
      "loss": 0.0021,
      "step": 59110
    },
    {
      "epoch": 3.1530666666666667,
      "grad_norm": 0.16729816794395447,
      "learning_rate": 1.0586666666666666e-05,
      "loss": 0.0033,
      "step": 59120
    },
    {
      "epoch": 3.1536,
      "grad_norm": 0.22077676653862,
      "learning_rate": 1.058e-05,
      "loss": 0.0029,
      "step": 59130
    },
    {
      "epoch": 3.1541333333333332,
      "grad_norm": 0.3505411744117737,
      "learning_rate": 1.0573333333333334e-05,
      "loss": 0.0021,
      "step": 59140
    },
    {
      "epoch": 3.1546666666666665,
      "grad_norm": 0.23337547481060028,
      "learning_rate": 1.0566666666666668e-05,
      "loss": 0.0017,
      "step": 59150
    },
    {
      "epoch": 3.1552,
      "grad_norm": 0.15047509968280792,
      "learning_rate": 1.056e-05,
      "loss": 0.0019,
      "step": 59160
    },
    {
      "epoch": 3.1557333333333335,
      "grad_norm": 0.14863091707229614,
      "learning_rate": 1.0553333333333335e-05,
      "loss": 0.0022,
      "step": 59170
    },
    {
      "epoch": 3.1562666666666668,
      "grad_norm": 0.31470564007759094,
      "learning_rate": 1.0546666666666667e-05,
      "loss": 0.0023,
      "step": 59180
    },
    {
      "epoch": 3.1568,
      "grad_norm": 0.29986411333084106,
      "learning_rate": 1.0539999999999999e-05,
      "loss": 0.002,
      "step": 59190
    },
    {
      "epoch": 3.1573333333333333,
      "grad_norm": 0.4597889482975006,
      "learning_rate": 1.0533333333333335e-05,
      "loss": 0.0022,
      "step": 59200
    },
    {
      "epoch": 3.1578666666666666,
      "grad_norm": 0.11721491068601608,
      "learning_rate": 1.0526666666666667e-05,
      "loss": 0.0022,
      "step": 59210
    },
    {
      "epoch": 3.1584,
      "grad_norm": 0.343465656042099,
      "learning_rate": 1.0520000000000001e-05,
      "loss": 0.0019,
      "step": 59220
    },
    {
      "epoch": 3.158933333333333,
      "grad_norm": 0.1592240035533905,
      "learning_rate": 1.0513333333333333e-05,
      "loss": 0.0019,
      "step": 59230
    },
    {
      "epoch": 3.159466666666667,
      "grad_norm": 0.23731756210327148,
      "learning_rate": 1.0506666666666667e-05,
      "loss": 0.0027,
      "step": 59240
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.19975224137306213,
      "learning_rate": 1.05e-05,
      "loss": 0.002,
      "step": 59250
    },
    {
      "epoch": 3.1605333333333334,
      "grad_norm": 0.2689341604709625,
      "learning_rate": 1.0493333333333333e-05,
      "loss": 0.0033,
      "step": 59260
    },
    {
      "epoch": 3.1610666666666667,
      "grad_norm": 0.39407894015312195,
      "learning_rate": 1.0486666666666667e-05,
      "loss": 0.0021,
      "step": 59270
    },
    {
      "epoch": 3.1616,
      "grad_norm": 0.47479838132858276,
      "learning_rate": 1.0480000000000001e-05,
      "loss": 0.0021,
      "step": 59280
    },
    {
      "epoch": 3.1621333333333332,
      "grad_norm": 0.12724457681179047,
      "learning_rate": 1.0473333333333334e-05,
      "loss": 0.0021,
      "step": 59290
    },
    {
      "epoch": 3.1626666666666665,
      "grad_norm": 0.1375027596950531,
      "learning_rate": 1.0466666666666668e-05,
      "loss": 0.0029,
      "step": 59300
    },
    {
      "epoch": 3.1632,
      "grad_norm": 0.3619665801525116,
      "learning_rate": 1.046e-05,
      "loss": 0.002,
      "step": 59310
    },
    {
      "epoch": 3.1637333333333335,
      "grad_norm": 0.35484808683395386,
      "learning_rate": 1.0453333333333334e-05,
      "loss": 0.0027,
      "step": 59320
    },
    {
      "epoch": 3.164266666666667,
      "grad_norm": 0.1549232453107834,
      "learning_rate": 1.0446666666666668e-05,
      "loss": 0.002,
      "step": 59330
    },
    {
      "epoch": 3.1648,
      "grad_norm": 0.1837744116783142,
      "learning_rate": 1.0440000000000002e-05,
      "loss": 0.0031,
      "step": 59340
    },
    {
      "epoch": 3.1653333333333333,
      "grad_norm": 0.1576862931251526,
      "learning_rate": 1.0433333333333334e-05,
      "loss": 0.0027,
      "step": 59350
    },
    {
      "epoch": 3.1658666666666666,
      "grad_norm": 0.3251185119152069,
      "learning_rate": 1.0426666666666666e-05,
      "loss": 0.002,
      "step": 59360
    },
    {
      "epoch": 3.1664,
      "grad_norm": 0.08617151528596878,
      "learning_rate": 1.042e-05,
      "loss": 0.0028,
      "step": 59370
    },
    {
      "epoch": 3.166933333333333,
      "grad_norm": 0.26617231965065,
      "learning_rate": 1.0413333333333332e-05,
      "loss": 0.0021,
      "step": 59380
    },
    {
      "epoch": 3.167466666666667,
      "grad_norm": 0.26972562074661255,
      "learning_rate": 1.0406666666666668e-05,
      "loss": 0.0019,
      "step": 59390
    },
    {
      "epoch": 3.168,
      "grad_norm": 0.0833759531378746,
      "learning_rate": 1.04e-05,
      "loss": 0.0022,
      "step": 59400
    },
    {
      "epoch": 3.1685333333333334,
      "grad_norm": 0.2764008045196533,
      "learning_rate": 1.0393333333333334e-05,
      "loss": 0.0025,
      "step": 59410
    },
    {
      "epoch": 3.1690666666666667,
      "grad_norm": 0.23705533146858215,
      "learning_rate": 1.0386666666666667e-05,
      "loss": 0.0033,
      "step": 59420
    },
    {
      "epoch": 3.1696,
      "grad_norm": 0.1506936252117157,
      "learning_rate": 1.038e-05,
      "loss": 0.0028,
      "step": 59430
    },
    {
      "epoch": 3.1701333333333332,
      "grad_norm": 0.14379742741584778,
      "learning_rate": 1.0373333333333333e-05,
      "loss": 0.0019,
      "step": 59440
    },
    {
      "epoch": 3.1706666666666665,
      "grad_norm": 0.08466377854347229,
      "learning_rate": 1.0366666666666667e-05,
      "loss": 0.0018,
      "step": 59450
    },
    {
      "epoch": 3.1712,
      "grad_norm": 0.09131267666816711,
      "learning_rate": 1.036e-05,
      "loss": 0.0028,
      "step": 59460
    },
    {
      "epoch": 3.1717333333333335,
      "grad_norm": 0.09780466556549072,
      "learning_rate": 1.0353333333333335e-05,
      "loss": 0.002,
      "step": 59470
    },
    {
      "epoch": 3.172266666666667,
      "grad_norm": 0.34743550419807434,
      "learning_rate": 1.0346666666666667e-05,
      "loss": 0.0027,
      "step": 59480
    },
    {
      "epoch": 3.1728,
      "grad_norm": 0.38298553228378296,
      "learning_rate": 1.0340000000000001e-05,
      "loss": 0.0018,
      "step": 59490
    },
    {
      "epoch": 3.1733333333333333,
      "grad_norm": 0.10656888782978058,
      "learning_rate": 1.0333333333333333e-05,
      "loss": 0.0024,
      "step": 59500
    },
    {
      "epoch": 3.1738666666666666,
      "grad_norm": 0.286547988653183,
      "learning_rate": 1.0326666666666667e-05,
      "loss": 0.0019,
      "step": 59510
    },
    {
      "epoch": 3.1744,
      "grad_norm": 0.3851794898509979,
      "learning_rate": 1.0320000000000001e-05,
      "loss": 0.0025,
      "step": 59520
    },
    {
      "epoch": 3.174933333333333,
      "grad_norm": 0.16714152693748474,
      "learning_rate": 1.0313333333333333e-05,
      "loss": 0.0024,
      "step": 59530
    },
    {
      "epoch": 3.175466666666667,
      "grad_norm": 0.2832683026790619,
      "learning_rate": 1.0306666666666667e-05,
      "loss": 0.0029,
      "step": 59540
    },
    {
      "epoch": 3.176,
      "grad_norm": 0.18868032097816467,
      "learning_rate": 1.03e-05,
      "loss": 0.0023,
      "step": 59550
    },
    {
      "epoch": 3.1765333333333334,
      "grad_norm": 0.4217071831226349,
      "learning_rate": 1.0293333333333334e-05,
      "loss": 0.0018,
      "step": 59560
    },
    {
      "epoch": 3.1770666666666667,
      "grad_norm": 0.15596169233322144,
      "learning_rate": 1.0286666666666666e-05,
      "loss": 0.0017,
      "step": 59570
    },
    {
      "epoch": 3.1776,
      "grad_norm": 0.2335219532251358,
      "learning_rate": 1.0280000000000002e-05,
      "loss": 0.0021,
      "step": 59580
    },
    {
      "epoch": 3.1781333333333333,
      "grad_norm": 0.3855139911174774,
      "learning_rate": 1.0273333333333334e-05,
      "loss": 0.0023,
      "step": 59590
    },
    {
      "epoch": 3.1786666666666665,
      "grad_norm": 0.36866599321365356,
      "learning_rate": 1.0266666666666668e-05,
      "loss": 0.0033,
      "step": 59600
    },
    {
      "epoch": 3.1792,
      "grad_norm": 0.36032989621162415,
      "learning_rate": 1.026e-05,
      "loss": 0.0016,
      "step": 59610
    },
    {
      "epoch": 3.1797333333333335,
      "grad_norm": 0.11490558832883835,
      "learning_rate": 1.0253333333333334e-05,
      "loss": 0.0017,
      "step": 59620
    },
    {
      "epoch": 3.180266666666667,
      "grad_norm": 0.14096318185329437,
      "learning_rate": 1.0246666666666666e-05,
      "loss": 0.0029,
      "step": 59630
    },
    {
      "epoch": 3.1808,
      "grad_norm": 0.082692451775074,
      "learning_rate": 1.024e-05,
      "loss": 0.0025,
      "step": 59640
    },
    {
      "epoch": 3.1813333333333333,
      "grad_norm": 0.638758659362793,
      "learning_rate": 1.0233333333333334e-05,
      "loss": 0.0018,
      "step": 59650
    },
    {
      "epoch": 3.1818666666666666,
      "grad_norm": 0.5561994314193726,
      "learning_rate": 1.0226666666666668e-05,
      "loss": 0.0023,
      "step": 59660
    },
    {
      "epoch": 3.1824,
      "grad_norm": 0.4110630452632904,
      "learning_rate": 1.022e-05,
      "loss": 0.0018,
      "step": 59670
    },
    {
      "epoch": 3.182933333333333,
      "grad_norm": 0.11835119873285294,
      "learning_rate": 1.0213333333333334e-05,
      "loss": 0.0022,
      "step": 59680
    },
    {
      "epoch": 3.183466666666667,
      "grad_norm": 0.11023180186748505,
      "learning_rate": 1.0206666666666667e-05,
      "loss": 0.0033,
      "step": 59690
    },
    {
      "epoch": 3.184,
      "grad_norm": 0.1636156439781189,
      "learning_rate": 1.02e-05,
      "loss": 0.0022,
      "step": 59700
    },
    {
      "epoch": 3.1845333333333334,
      "grad_norm": 0.29594576358795166,
      "learning_rate": 1.0193333333333335e-05,
      "loss": 0.0027,
      "step": 59710
    },
    {
      "epoch": 3.1850666666666667,
      "grad_norm": 0.10589192062616348,
      "learning_rate": 1.0186666666666667e-05,
      "loss": 0.0028,
      "step": 59720
    },
    {
      "epoch": 3.1856,
      "grad_norm": 0.4077503979206085,
      "learning_rate": 1.018e-05,
      "loss": 0.002,
      "step": 59730
    },
    {
      "epoch": 3.1861333333333333,
      "grad_norm": 0.11823979020118713,
      "learning_rate": 1.0173333333333333e-05,
      "loss": 0.003,
      "step": 59740
    },
    {
      "epoch": 3.1866666666666665,
      "grad_norm": 0.21254749596118927,
      "learning_rate": 1.0166666666666667e-05,
      "loss": 0.0033,
      "step": 59750
    },
    {
      "epoch": 3.1872,
      "grad_norm": 0.4312285780906677,
      "learning_rate": 1.016e-05,
      "loss": 0.0019,
      "step": 59760
    },
    {
      "epoch": 3.1877333333333335,
      "grad_norm": 0.15695470571517944,
      "learning_rate": 1.0153333333333335e-05,
      "loss": 0.002,
      "step": 59770
    },
    {
      "epoch": 3.188266666666667,
      "grad_norm": 0.24485458433628082,
      "learning_rate": 1.0146666666666667e-05,
      "loss": 0.0026,
      "step": 59780
    },
    {
      "epoch": 3.1888,
      "grad_norm": 0.4591352045536041,
      "learning_rate": 1.0140000000000001e-05,
      "loss": 0.0034,
      "step": 59790
    },
    {
      "epoch": 3.1893333333333334,
      "grad_norm": 0.2421783208847046,
      "learning_rate": 1.0133333333333333e-05,
      "loss": 0.0017,
      "step": 59800
    },
    {
      "epoch": 3.1898666666666666,
      "grad_norm": 0.4283263683319092,
      "learning_rate": 1.0126666666666667e-05,
      "loss": 0.0028,
      "step": 59810
    },
    {
      "epoch": 3.1904,
      "grad_norm": 0.09482260048389435,
      "learning_rate": 1.012e-05,
      "loss": 0.0021,
      "step": 59820
    },
    {
      "epoch": 3.190933333333333,
      "grad_norm": 0.5346293449401855,
      "learning_rate": 1.0113333333333334e-05,
      "loss": 0.0019,
      "step": 59830
    },
    {
      "epoch": 3.191466666666667,
      "grad_norm": 0.33040177822113037,
      "learning_rate": 1.0106666666666668e-05,
      "loss": 0.0026,
      "step": 59840
    },
    {
      "epoch": 3.192,
      "grad_norm": 0.18565824627876282,
      "learning_rate": 1.0100000000000002e-05,
      "loss": 0.002,
      "step": 59850
    },
    {
      "epoch": 3.1925333333333334,
      "grad_norm": 0.08180615305900574,
      "learning_rate": 1.0093333333333334e-05,
      "loss": 0.0026,
      "step": 59860
    },
    {
      "epoch": 3.1930666666666667,
      "grad_norm": 0.12668591737747192,
      "learning_rate": 1.0086666666666666e-05,
      "loss": 0.002,
      "step": 59870
    },
    {
      "epoch": 3.1936,
      "grad_norm": 0.33571332693099976,
      "learning_rate": 1.008e-05,
      "loss": 0.0022,
      "step": 59880
    },
    {
      "epoch": 3.1941333333333333,
      "grad_norm": 0.13279350101947784,
      "learning_rate": 1.0073333333333334e-05,
      "loss": 0.0036,
      "step": 59890
    },
    {
      "epoch": 3.1946666666666665,
      "grad_norm": 0.25213363766670227,
      "learning_rate": 1.0066666666666668e-05,
      "loss": 0.0035,
      "step": 59900
    },
    {
      "epoch": 3.1952,
      "grad_norm": 0.2721111476421356,
      "learning_rate": 1.006e-05,
      "loss": 0.0024,
      "step": 59910
    },
    {
      "epoch": 3.1957333333333335,
      "grad_norm": 0.1636350154876709,
      "learning_rate": 1.0053333333333334e-05,
      "loss": 0.0022,
      "step": 59920
    },
    {
      "epoch": 3.196266666666667,
      "grad_norm": 0.161085844039917,
      "learning_rate": 1.0046666666666666e-05,
      "loss": 0.0031,
      "step": 59930
    },
    {
      "epoch": 3.1968,
      "grad_norm": 0.25664931535720825,
      "learning_rate": 1.004e-05,
      "loss": 0.0035,
      "step": 59940
    },
    {
      "epoch": 3.1973333333333334,
      "grad_norm": 0.1711750030517578,
      "learning_rate": 1.0033333333333333e-05,
      "loss": 0.0027,
      "step": 59950
    },
    {
      "epoch": 3.1978666666666666,
      "grad_norm": 0.1945224553346634,
      "learning_rate": 1.0026666666666668e-05,
      "loss": 0.0023,
      "step": 59960
    },
    {
      "epoch": 3.1984,
      "grad_norm": 0.3814516067504883,
      "learning_rate": 1.002e-05,
      "loss": 0.0025,
      "step": 59970
    },
    {
      "epoch": 3.198933333333333,
      "grad_norm": 0.260773241519928,
      "learning_rate": 1.0013333333333335e-05,
      "loss": 0.0034,
      "step": 59980
    },
    {
      "epoch": 3.1994666666666665,
      "grad_norm": 0.28565430641174316,
      "learning_rate": 1.0006666666666667e-05,
      "loss": 0.0018,
      "step": 59990
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.3577200472354889,
      "learning_rate": 1e-05,
      "loss": 0.0025,
      "step": 60000
    },
    {
      "epoch": 3.2005333333333335,
      "grad_norm": 0.3863205313682556,
      "learning_rate": 9.993333333333333e-06,
      "loss": 0.0021,
      "step": 60010
    },
    {
      "epoch": 3.2010666666666667,
      "grad_norm": 0.20991547405719757,
      "learning_rate": 9.986666666666667e-06,
      "loss": 0.0024,
      "step": 60020
    },
    {
      "epoch": 3.2016,
      "grad_norm": 0.6703778505325317,
      "learning_rate": 9.980000000000001e-06,
      "loss": 0.0027,
      "step": 60030
    },
    {
      "epoch": 3.2021333333333333,
      "grad_norm": 0.3023572862148285,
      "learning_rate": 9.973333333333333e-06,
      "loss": 0.0028,
      "step": 60040
    },
    {
      "epoch": 3.2026666666666666,
      "grad_norm": 0.28496482968330383,
      "learning_rate": 9.966666666666667e-06,
      "loss": 0.0034,
      "step": 60050
    },
    {
      "epoch": 3.2032,
      "grad_norm": 0.43494176864624023,
      "learning_rate": 9.96e-06,
      "loss": 0.002,
      "step": 60060
    },
    {
      "epoch": 3.203733333333333,
      "grad_norm": 0.1373152732849121,
      "learning_rate": 9.953333333333333e-06,
      "loss": 0.0027,
      "step": 60070
    },
    {
      "epoch": 3.204266666666667,
      "grad_norm": 0.22330495715141296,
      "learning_rate": 9.946666666666667e-06,
      "loss": 0.0036,
      "step": 60080
    },
    {
      "epoch": 3.2048,
      "grad_norm": 0.06266241520643234,
      "learning_rate": 9.940000000000001e-06,
      "loss": 0.003,
      "step": 60090
    },
    {
      "epoch": 3.2053333333333334,
      "grad_norm": 0.09448094666004181,
      "learning_rate": 9.933333333333334e-06,
      "loss": 0.0028,
      "step": 60100
    },
    {
      "epoch": 3.2058666666666666,
      "grad_norm": 0.23127759993076324,
      "learning_rate": 9.926666666666668e-06,
      "loss": 0.0028,
      "step": 60110
    },
    {
      "epoch": 3.2064,
      "grad_norm": 0.3846607208251953,
      "learning_rate": 9.92e-06,
      "loss": 0.002,
      "step": 60120
    },
    {
      "epoch": 3.206933333333333,
      "grad_norm": 0.27894464135169983,
      "learning_rate": 9.913333333333334e-06,
      "loss": 0.0025,
      "step": 60130
    },
    {
      "epoch": 3.2074666666666665,
      "grad_norm": 0.3272826075553894,
      "learning_rate": 9.906666666666666e-06,
      "loss": 0.0027,
      "step": 60140
    },
    {
      "epoch": 3.208,
      "grad_norm": 0.31914204359054565,
      "learning_rate": 9.900000000000002e-06,
      "loss": 0.0023,
      "step": 60150
    },
    {
      "epoch": 3.2085333333333335,
      "grad_norm": 0.35881489515304565,
      "learning_rate": 9.893333333333334e-06,
      "loss": 0.0017,
      "step": 60160
    },
    {
      "epoch": 3.2090666666666667,
      "grad_norm": 0.48411011695861816,
      "learning_rate": 9.886666666666668e-06,
      "loss": 0.0021,
      "step": 60170
    },
    {
      "epoch": 3.2096,
      "grad_norm": 0.3048887252807617,
      "learning_rate": 9.88e-06,
      "loss": 0.0029,
      "step": 60180
    },
    {
      "epoch": 3.2101333333333333,
      "grad_norm": 0.36946988105773926,
      "learning_rate": 9.873333333333334e-06,
      "loss": 0.0028,
      "step": 60190
    },
    {
      "epoch": 3.2106666666666666,
      "grad_norm": 0.3768930733203888,
      "learning_rate": 9.866666666666667e-06,
      "loss": 0.0036,
      "step": 60200
    },
    {
      "epoch": 3.2112,
      "grad_norm": 0.6113607287406921,
      "learning_rate": 9.86e-06,
      "loss": 0.0022,
      "step": 60210
    },
    {
      "epoch": 3.211733333333333,
      "grad_norm": 0.1445275843143463,
      "learning_rate": 9.853333333333334e-06,
      "loss": 0.0023,
      "step": 60220
    },
    {
      "epoch": 3.212266666666667,
      "grad_norm": 0.5749504566192627,
      "learning_rate": 9.846666666666667e-06,
      "loss": 0.0023,
      "step": 60230
    },
    {
      "epoch": 3.2128,
      "grad_norm": 0.32797273993492126,
      "learning_rate": 9.84e-06,
      "loss": 0.0018,
      "step": 60240
    },
    {
      "epoch": 3.2133333333333334,
      "grad_norm": 0.06266727298498154,
      "learning_rate": 9.833333333333333e-06,
      "loss": 0.0031,
      "step": 60250
    },
    {
      "epoch": 3.2138666666666666,
      "grad_norm": 0.610168993473053,
      "learning_rate": 9.826666666666667e-06,
      "loss": 0.0034,
      "step": 60260
    },
    {
      "epoch": 3.2144,
      "grad_norm": 0.4843328893184662,
      "learning_rate": 9.820000000000001e-06,
      "loss": 0.0018,
      "step": 60270
    },
    {
      "epoch": 3.214933333333333,
      "grad_norm": 0.2427399456501007,
      "learning_rate": 9.813333333333335e-06,
      "loss": 0.0016,
      "step": 60280
    },
    {
      "epoch": 3.2154666666666665,
      "grad_norm": 0.1588696986436844,
      "learning_rate": 9.806666666666667e-06,
      "loss": 0.0018,
      "step": 60290
    },
    {
      "epoch": 3.216,
      "grad_norm": 0.09153539687395096,
      "learning_rate": 9.800000000000001e-06,
      "loss": 0.0023,
      "step": 60300
    },
    {
      "epoch": 3.2165333333333335,
      "grad_norm": 0.1549738198518753,
      "learning_rate": 9.793333333333333e-06,
      "loss": 0.0017,
      "step": 60310
    },
    {
      "epoch": 3.2170666666666667,
      "grad_norm": 0.07783833891153336,
      "learning_rate": 9.786666666666667e-06,
      "loss": 0.0027,
      "step": 60320
    },
    {
      "epoch": 3.2176,
      "grad_norm": 0.1354164034128189,
      "learning_rate": 9.78e-06,
      "loss": 0.0019,
      "step": 60330
    },
    {
      "epoch": 3.2181333333333333,
      "grad_norm": 0.2460816353559494,
      "learning_rate": 9.773333333333333e-06,
      "loss": 0.0026,
      "step": 60340
    },
    {
      "epoch": 3.2186666666666666,
      "grad_norm": 0.5342069864273071,
      "learning_rate": 9.766666666666667e-06,
      "loss": 0.0038,
      "step": 60350
    },
    {
      "epoch": 3.2192,
      "grad_norm": 0.2630729079246521,
      "learning_rate": 9.760000000000001e-06,
      "loss": 0.0025,
      "step": 60360
    },
    {
      "epoch": 3.219733333333333,
      "grad_norm": 0.2222638726234436,
      "learning_rate": 9.753333333333334e-06,
      "loss": 0.0022,
      "step": 60370
    },
    {
      "epoch": 3.220266666666667,
      "grad_norm": 0.5014618039131165,
      "learning_rate": 9.746666666666666e-06,
      "loss": 0.0019,
      "step": 60380
    },
    {
      "epoch": 3.2208,
      "grad_norm": 0.6443408727645874,
      "learning_rate": 9.74e-06,
      "loss": 0.0029,
      "step": 60390
    },
    {
      "epoch": 3.2213333333333334,
      "grad_norm": 0.21800580620765686,
      "learning_rate": 9.733333333333334e-06,
      "loss": 0.002,
      "step": 60400
    },
    {
      "epoch": 3.2218666666666667,
      "grad_norm": 0.36949628591537476,
      "learning_rate": 9.726666666666668e-06,
      "loss": 0.0034,
      "step": 60410
    },
    {
      "epoch": 3.2224,
      "grad_norm": 0.4092705249786377,
      "learning_rate": 9.72e-06,
      "loss": 0.0019,
      "step": 60420
    },
    {
      "epoch": 3.222933333333333,
      "grad_norm": 0.2879371643066406,
      "learning_rate": 9.713333333333334e-06,
      "loss": 0.0034,
      "step": 60430
    },
    {
      "epoch": 3.2234666666666665,
      "grad_norm": 0.09834540635347366,
      "learning_rate": 9.706666666666666e-06,
      "loss": 0.0018,
      "step": 60440
    },
    {
      "epoch": 3.224,
      "grad_norm": 0.21852320432662964,
      "learning_rate": 9.7e-06,
      "loss": 0.0023,
      "step": 60450
    },
    {
      "epoch": 3.2245333333333335,
      "grad_norm": 0.33259034156799316,
      "learning_rate": 9.693333333333334e-06,
      "loss": 0.0024,
      "step": 60460
    },
    {
      "epoch": 3.2250666666666667,
      "grad_norm": 0.5114115476608276,
      "learning_rate": 9.686666666666668e-06,
      "loss": 0.0022,
      "step": 60470
    },
    {
      "epoch": 3.2256,
      "grad_norm": 0.3581010699272156,
      "learning_rate": 9.68e-06,
      "loss": 0.0017,
      "step": 60480
    },
    {
      "epoch": 3.2261333333333333,
      "grad_norm": 0.36082231998443604,
      "learning_rate": 9.673333333333334e-06,
      "loss": 0.003,
      "step": 60490
    },
    {
      "epoch": 3.2266666666666666,
      "grad_norm": 0.2753596603870392,
      "learning_rate": 9.666666666666667e-06,
      "loss": 0.0024,
      "step": 60500
    },
    {
      "epoch": 3.2272,
      "grad_norm": 0.40511244535446167,
      "learning_rate": 9.66e-06,
      "loss": 0.004,
      "step": 60510
    },
    {
      "epoch": 3.227733333333333,
      "grad_norm": 0.17741869390010834,
      "learning_rate": 9.653333333333333e-06,
      "loss": 0.0026,
      "step": 60520
    },
    {
      "epoch": 3.228266666666667,
      "grad_norm": 0.30275365710258484,
      "learning_rate": 9.646666666666667e-06,
      "loss": 0.0018,
      "step": 60530
    },
    {
      "epoch": 3.2288,
      "grad_norm": 0.3261701762676239,
      "learning_rate": 9.640000000000001e-06,
      "loss": 0.0017,
      "step": 60540
    },
    {
      "epoch": 3.2293333333333334,
      "grad_norm": 0.8406731486320496,
      "learning_rate": 9.633333333333335e-06,
      "loss": 0.0025,
      "step": 60550
    },
    {
      "epoch": 3.2298666666666667,
      "grad_norm": 0.1728486269712448,
      "learning_rate": 9.626666666666667e-06,
      "loss": 0.0017,
      "step": 60560
    },
    {
      "epoch": 3.2304,
      "grad_norm": 0.13652759790420532,
      "learning_rate": 9.62e-06,
      "loss": 0.0024,
      "step": 60570
    },
    {
      "epoch": 3.230933333333333,
      "grad_norm": 0.44583529233932495,
      "learning_rate": 9.613333333333333e-06,
      "loss": 0.0018,
      "step": 60580
    },
    {
      "epoch": 3.2314666666666665,
      "grad_norm": 0.0638115257024765,
      "learning_rate": 9.606666666666667e-06,
      "loss": 0.0019,
      "step": 60590
    },
    {
      "epoch": 3.232,
      "grad_norm": 0.31926482915878296,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.002,
      "step": 60600
    },
    {
      "epoch": 3.2325333333333335,
      "grad_norm": 0.21508830785751343,
      "learning_rate": 9.593333333333334e-06,
      "loss": 0.0026,
      "step": 60610
    },
    {
      "epoch": 3.2330666666666668,
      "grad_norm": 0.2523701786994934,
      "learning_rate": 9.586666666666667e-06,
      "loss": 0.0019,
      "step": 60620
    },
    {
      "epoch": 3.2336,
      "grad_norm": 0.08349587768316269,
      "learning_rate": 9.58e-06,
      "loss": 0.0023,
      "step": 60630
    },
    {
      "epoch": 3.2341333333333333,
      "grad_norm": 0.1553884744644165,
      "learning_rate": 9.573333333333334e-06,
      "loss": 0.0019,
      "step": 60640
    },
    {
      "epoch": 3.2346666666666666,
      "grad_norm": 0.23280739784240723,
      "learning_rate": 9.566666666666666e-06,
      "loss": 0.0019,
      "step": 60650
    },
    {
      "epoch": 3.2352,
      "grad_norm": 0.2658531069755554,
      "learning_rate": 9.560000000000002e-06,
      "loss": 0.0017,
      "step": 60660
    },
    {
      "epoch": 3.235733333333333,
      "grad_norm": 0.2672150731086731,
      "learning_rate": 9.553333333333334e-06,
      "loss": 0.0021,
      "step": 60670
    },
    {
      "epoch": 3.236266666666667,
      "grad_norm": 0.1854558289051056,
      "learning_rate": 9.546666666666668e-06,
      "loss": 0.0024,
      "step": 60680
    },
    {
      "epoch": 3.2368,
      "grad_norm": 0.6419172286987305,
      "learning_rate": 9.54e-06,
      "loss": 0.0021,
      "step": 60690
    },
    {
      "epoch": 3.2373333333333334,
      "grad_norm": 0.3464081287384033,
      "learning_rate": 9.533333333333334e-06,
      "loss": 0.0026,
      "step": 60700
    },
    {
      "epoch": 3.2378666666666667,
      "grad_norm": 0.3166743516921997,
      "learning_rate": 9.526666666666666e-06,
      "loss": 0.0031,
      "step": 60710
    },
    {
      "epoch": 3.2384,
      "grad_norm": 0.48060545325279236,
      "learning_rate": 9.52e-06,
      "loss": 0.0019,
      "step": 60720
    },
    {
      "epoch": 3.238933333333333,
      "grad_norm": 0.37371349334716797,
      "learning_rate": 9.513333333333334e-06,
      "loss": 0.0027,
      "step": 60730
    },
    {
      "epoch": 3.2394666666666665,
      "grad_norm": 0.28080955147743225,
      "learning_rate": 9.506666666666667e-06,
      "loss": 0.0022,
      "step": 60740
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.24636606872081757,
      "learning_rate": 9.5e-06,
      "loss": 0.0032,
      "step": 60750
    },
    {
      "epoch": 3.2405333333333335,
      "grad_norm": 0.1835547834634781,
      "learning_rate": 9.493333333333333e-06,
      "loss": 0.0016,
      "step": 60760
    },
    {
      "epoch": 3.2410666666666668,
      "grad_norm": 0.17082247138023376,
      "learning_rate": 9.486666666666667e-06,
      "loss": 0.0016,
      "step": 60770
    },
    {
      "epoch": 3.2416,
      "grad_norm": 0.11585579812526703,
      "learning_rate": 9.48e-06,
      "loss": 0.0029,
      "step": 60780
    },
    {
      "epoch": 3.2421333333333333,
      "grad_norm": 0.31736302375793457,
      "learning_rate": 9.473333333333335e-06,
      "loss": 0.0016,
      "step": 60790
    },
    {
      "epoch": 3.2426666666666666,
      "grad_norm": 0.2386501431465149,
      "learning_rate": 9.466666666666667e-06,
      "loss": 0.0022,
      "step": 60800
    },
    {
      "epoch": 3.2432,
      "grad_norm": 0.44962164759635925,
      "learning_rate": 9.460000000000001e-06,
      "loss": 0.0029,
      "step": 60810
    },
    {
      "epoch": 3.243733333333333,
      "grad_norm": 0.3558715879917145,
      "learning_rate": 9.453333333333333e-06,
      "loss": 0.0028,
      "step": 60820
    },
    {
      "epoch": 3.244266666666667,
      "grad_norm": 0.0707189068198204,
      "learning_rate": 9.446666666666667e-06,
      "loss": 0.0026,
      "step": 60830
    },
    {
      "epoch": 3.2448,
      "grad_norm": 0.13193702697753906,
      "learning_rate": 9.44e-06,
      "loss": 0.0027,
      "step": 60840
    },
    {
      "epoch": 3.2453333333333334,
      "grad_norm": 0.08610966056585312,
      "learning_rate": 9.433333333333335e-06,
      "loss": 0.0018,
      "step": 60850
    },
    {
      "epoch": 3.2458666666666667,
      "grad_norm": 0.11386651545763016,
      "learning_rate": 9.426666666666667e-06,
      "loss": 0.0017,
      "step": 60860
    },
    {
      "epoch": 3.2464,
      "grad_norm": 0.126277357339859,
      "learning_rate": 9.420000000000001e-06,
      "loss": 0.0026,
      "step": 60870
    },
    {
      "epoch": 3.2469333333333332,
      "grad_norm": 0.43211063742637634,
      "learning_rate": 9.413333333333334e-06,
      "loss": 0.0017,
      "step": 60880
    },
    {
      "epoch": 3.2474666666666665,
      "grad_norm": 0.33405712246894836,
      "learning_rate": 9.406666666666668e-06,
      "loss": 0.0028,
      "step": 60890
    },
    {
      "epoch": 3.248,
      "grad_norm": 0.13371038436889648,
      "learning_rate": 9.4e-06,
      "loss": 0.0019,
      "step": 60900
    },
    {
      "epoch": 3.2485333333333335,
      "grad_norm": 0.11604845523834229,
      "learning_rate": 9.393333333333334e-06,
      "loss": 0.0028,
      "step": 60910
    },
    {
      "epoch": 3.2490666666666668,
      "grad_norm": 0.17987598478794098,
      "learning_rate": 9.386666666666668e-06,
      "loss": 0.0023,
      "step": 60920
    },
    {
      "epoch": 3.2496,
      "grad_norm": 0.17052145302295685,
      "learning_rate": 9.38e-06,
      "loss": 0.0018,
      "step": 60930
    },
    {
      "epoch": 3.2501333333333333,
      "grad_norm": 0.15050116181373596,
      "learning_rate": 9.373333333333334e-06,
      "loss": 0.0025,
      "step": 60940
    },
    {
      "epoch": 3.2506666666666666,
      "grad_norm": 0.6487967371940613,
      "learning_rate": 9.366666666666666e-06,
      "loss": 0.0031,
      "step": 60950
    },
    {
      "epoch": 3.2512,
      "grad_norm": 0.1429900825023651,
      "learning_rate": 9.36e-06,
      "loss": 0.0022,
      "step": 60960
    },
    {
      "epoch": 3.251733333333333,
      "grad_norm": 0.24561581015586853,
      "learning_rate": 9.353333333333334e-06,
      "loss": 0.0023,
      "step": 60970
    },
    {
      "epoch": 3.2522666666666664,
      "grad_norm": 0.2672543227672577,
      "learning_rate": 9.346666666666668e-06,
      "loss": 0.0026,
      "step": 60980
    },
    {
      "epoch": 3.2528,
      "grad_norm": 0.3373750150203705,
      "learning_rate": 9.34e-06,
      "loss": 0.0024,
      "step": 60990
    },
    {
      "epoch": 3.2533333333333334,
      "grad_norm": 0.4205702841281891,
      "learning_rate": 9.333333333333334e-06,
      "loss": 0.0041,
      "step": 61000
    },
    {
      "epoch": 3.2538666666666667,
      "grad_norm": 0.1298413723707199,
      "learning_rate": 9.326666666666667e-06,
      "loss": 0.0021,
      "step": 61010
    },
    {
      "epoch": 3.2544,
      "grad_norm": 0.5075132250785828,
      "learning_rate": 9.32e-06,
      "loss": 0.0023,
      "step": 61020
    },
    {
      "epoch": 3.2549333333333332,
      "grad_norm": 0.41578209400177,
      "learning_rate": 9.313333333333333e-06,
      "loss": 0.0028,
      "step": 61030
    },
    {
      "epoch": 3.2554666666666665,
      "grad_norm": 0.36253076791763306,
      "learning_rate": 9.306666666666668e-06,
      "loss": 0.0018,
      "step": 61040
    },
    {
      "epoch": 3.2560000000000002,
      "grad_norm": 0.19017736613750458,
      "learning_rate": 9.3e-06,
      "loss": 0.0045,
      "step": 61050
    },
    {
      "epoch": 3.2565333333333335,
      "grad_norm": 0.1215834692120552,
      "learning_rate": 9.293333333333335e-06,
      "loss": 0.0018,
      "step": 61060
    },
    {
      "epoch": 3.2570666666666668,
      "grad_norm": 0.27286675572395325,
      "learning_rate": 9.286666666666667e-06,
      "loss": 0.0027,
      "step": 61070
    },
    {
      "epoch": 3.2576,
      "grad_norm": 0.0751786008477211,
      "learning_rate": 9.28e-06,
      "loss": 0.0022,
      "step": 61080
    },
    {
      "epoch": 3.2581333333333333,
      "grad_norm": 0.11206979304552078,
      "learning_rate": 9.273333333333333e-06,
      "loss": 0.0028,
      "step": 61090
    },
    {
      "epoch": 3.2586666666666666,
      "grad_norm": 0.10116800665855408,
      "learning_rate": 9.266666666666667e-06,
      "loss": 0.0018,
      "step": 61100
    },
    {
      "epoch": 3.2592,
      "grad_norm": 0.06855251640081406,
      "learning_rate": 9.260000000000001e-06,
      "loss": 0.0017,
      "step": 61110
    },
    {
      "epoch": 3.259733333333333,
      "grad_norm": 0.08220560103654861,
      "learning_rate": 9.253333333333333e-06,
      "loss": 0.0028,
      "step": 61120
    },
    {
      "epoch": 3.2602666666666664,
      "grad_norm": 0.22006726264953613,
      "learning_rate": 9.246666666666667e-06,
      "loss": 0.0017,
      "step": 61130
    },
    {
      "epoch": 3.2608,
      "grad_norm": 0.20694659650325775,
      "learning_rate": 9.24e-06,
      "loss": 0.0019,
      "step": 61140
    },
    {
      "epoch": 3.2613333333333334,
      "grad_norm": 0.2855011522769928,
      "learning_rate": 9.233333333333334e-06,
      "loss": 0.0018,
      "step": 61150
    },
    {
      "epoch": 3.2618666666666667,
      "grad_norm": 0.25936561822891235,
      "learning_rate": 9.226666666666668e-06,
      "loss": 0.0033,
      "step": 61160
    },
    {
      "epoch": 3.2624,
      "grad_norm": 0.28556889295578003,
      "learning_rate": 9.220000000000002e-06,
      "loss": 0.0033,
      "step": 61170
    },
    {
      "epoch": 3.2629333333333332,
      "grad_norm": 0.11468901485204697,
      "learning_rate": 9.213333333333334e-06,
      "loss": 0.0028,
      "step": 61180
    },
    {
      "epoch": 3.2634666666666665,
      "grad_norm": 0.1862465888261795,
      "learning_rate": 9.206666666666668e-06,
      "loss": 0.0017,
      "step": 61190
    },
    {
      "epoch": 3.2640000000000002,
      "grad_norm": 0.3222288191318512,
      "learning_rate": 9.2e-06,
      "loss": 0.0021,
      "step": 61200
    },
    {
      "epoch": 3.2645333333333335,
      "grad_norm": 0.3294341564178467,
      "learning_rate": 9.193333333333334e-06,
      "loss": 0.0032,
      "step": 61210
    },
    {
      "epoch": 3.265066666666667,
      "grad_norm": 0.20180197060108185,
      "learning_rate": 9.186666666666666e-06,
      "loss": 0.0034,
      "step": 61220
    },
    {
      "epoch": 3.2656,
      "grad_norm": 0.10574784874916077,
      "learning_rate": 9.180000000000002e-06,
      "loss": 0.0021,
      "step": 61230
    },
    {
      "epoch": 3.2661333333333333,
      "grad_norm": 0.3233566880226135,
      "learning_rate": 9.173333333333334e-06,
      "loss": 0.0022,
      "step": 61240
    },
    {
      "epoch": 3.2666666666666666,
      "grad_norm": 0.6034696102142334,
      "learning_rate": 9.166666666666666e-06,
      "loss": 0.0029,
      "step": 61250
    },
    {
      "epoch": 3.2672,
      "grad_norm": 0.19234119355678558,
      "learning_rate": 9.16e-06,
      "loss": 0.0017,
      "step": 61260
    },
    {
      "epoch": 3.267733333333333,
      "grad_norm": 0.3284150958061218,
      "learning_rate": 9.153333333333333e-06,
      "loss": 0.0025,
      "step": 61270
    },
    {
      "epoch": 3.2682666666666664,
      "grad_norm": 0.08160986751317978,
      "learning_rate": 9.146666666666667e-06,
      "loss": 0.0016,
      "step": 61280
    },
    {
      "epoch": 3.2688,
      "grad_norm": 0.19356758892536163,
      "learning_rate": 9.14e-06,
      "loss": 0.0028,
      "step": 61290
    },
    {
      "epoch": 3.2693333333333334,
      "grad_norm": 0.2508484423160553,
      "learning_rate": 9.133333333333335e-06,
      "loss": 0.0018,
      "step": 61300
    },
    {
      "epoch": 3.2698666666666667,
      "grad_norm": 0.22507604956626892,
      "learning_rate": 9.126666666666667e-06,
      "loss": 0.0023,
      "step": 61310
    },
    {
      "epoch": 3.2704,
      "grad_norm": 0.12466579675674438,
      "learning_rate": 9.12e-06,
      "loss": 0.0019,
      "step": 61320
    },
    {
      "epoch": 3.2709333333333332,
      "grad_norm": 0.32821041345596313,
      "learning_rate": 9.113333333333333e-06,
      "loss": 0.0023,
      "step": 61330
    },
    {
      "epoch": 3.2714666666666665,
      "grad_norm": 0.6753541827201843,
      "learning_rate": 9.106666666666667e-06,
      "loss": 0.0019,
      "step": 61340
    },
    {
      "epoch": 3.2720000000000002,
      "grad_norm": 0.3045826852321625,
      "learning_rate": 9.100000000000001e-06,
      "loss": 0.002,
      "step": 61350
    },
    {
      "epoch": 3.2725333333333335,
      "grad_norm": 0.13409681618213654,
      "learning_rate": 9.093333333333335e-06,
      "loss": 0.0027,
      "step": 61360
    },
    {
      "epoch": 3.273066666666667,
      "grad_norm": 0.33468469977378845,
      "learning_rate": 9.086666666666667e-06,
      "loss": 0.0017,
      "step": 61370
    },
    {
      "epoch": 3.2736,
      "grad_norm": 0.20806396007537842,
      "learning_rate": 9.080000000000001e-06,
      "loss": 0.0026,
      "step": 61380
    },
    {
      "epoch": 3.2741333333333333,
      "grad_norm": 0.2952941954135895,
      "learning_rate": 9.073333333333333e-06,
      "loss": 0.0027,
      "step": 61390
    },
    {
      "epoch": 3.2746666666666666,
      "grad_norm": 0.20937803387641907,
      "learning_rate": 9.066666666666667e-06,
      "loss": 0.0019,
      "step": 61400
    },
    {
      "epoch": 3.2752,
      "grad_norm": 0.13725119829177856,
      "learning_rate": 9.06e-06,
      "loss": 0.0019,
      "step": 61410
    },
    {
      "epoch": 3.275733333333333,
      "grad_norm": 0.30719804763793945,
      "learning_rate": 9.053333333333334e-06,
      "loss": 0.0022,
      "step": 61420
    },
    {
      "epoch": 3.2762666666666664,
      "grad_norm": 0.4405282735824585,
      "learning_rate": 9.046666666666668e-06,
      "loss": 0.0018,
      "step": 61430
    },
    {
      "epoch": 3.2768,
      "grad_norm": 0.15183663368225098,
      "learning_rate": 9.04e-06,
      "loss": 0.0039,
      "step": 61440
    },
    {
      "epoch": 3.2773333333333334,
      "grad_norm": 0.15979497134685516,
      "learning_rate": 9.033333333333334e-06,
      "loss": 0.0023,
      "step": 61450
    },
    {
      "epoch": 3.2778666666666667,
      "grad_norm": 0.5657995343208313,
      "learning_rate": 9.026666666666666e-06,
      "loss": 0.0019,
      "step": 61460
    },
    {
      "epoch": 3.2784,
      "grad_norm": 0.24858738481998444,
      "learning_rate": 9.02e-06,
      "loss": 0.0032,
      "step": 61470
    },
    {
      "epoch": 3.2789333333333333,
      "grad_norm": 0.2026323527097702,
      "learning_rate": 9.013333333333334e-06,
      "loss": 0.0021,
      "step": 61480
    },
    {
      "epoch": 3.2794666666666665,
      "grad_norm": 0.6329522132873535,
      "learning_rate": 9.006666666666668e-06,
      "loss": 0.0017,
      "step": 61490
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 0.09166834503412247,
      "learning_rate": 9e-06,
      "loss": 0.0037,
      "step": 61500
    },
    {
      "epoch": 3.2805333333333335,
      "grad_norm": 0.36217382550239563,
      "learning_rate": 8.993333333333334e-06,
      "loss": 0.0018,
      "step": 61510
    },
    {
      "epoch": 3.281066666666667,
      "grad_norm": 0.16871890425682068,
      "learning_rate": 8.986666666666666e-06,
      "loss": 0.0021,
      "step": 61520
    },
    {
      "epoch": 3.2816,
      "grad_norm": 0.11234189569950104,
      "learning_rate": 8.98e-06,
      "loss": 0.0035,
      "step": 61530
    },
    {
      "epoch": 3.2821333333333333,
      "grad_norm": 0.33501604199409485,
      "learning_rate": 8.973333333333334e-06,
      "loss": 0.0033,
      "step": 61540
    },
    {
      "epoch": 3.2826666666666666,
      "grad_norm": 0.4062435030937195,
      "learning_rate": 8.966666666666668e-06,
      "loss": 0.0026,
      "step": 61550
    },
    {
      "epoch": 3.2832,
      "grad_norm": 0.23872345685958862,
      "learning_rate": 8.96e-06,
      "loss": 0.0018,
      "step": 61560
    },
    {
      "epoch": 3.283733333333333,
      "grad_norm": 0.20966783165931702,
      "learning_rate": 8.953333333333335e-06,
      "loss": 0.002,
      "step": 61570
    },
    {
      "epoch": 3.2842666666666664,
      "grad_norm": 0.2133023589849472,
      "learning_rate": 8.946666666666667e-06,
      "loss": 0.0028,
      "step": 61580
    },
    {
      "epoch": 3.2848,
      "grad_norm": 0.4083511233329773,
      "learning_rate": 8.939999999999999e-06,
      "loss": 0.002,
      "step": 61590
    },
    {
      "epoch": 3.2853333333333334,
      "grad_norm": 0.24221934378147125,
      "learning_rate": 8.933333333333333e-06,
      "loss": 0.003,
      "step": 61600
    },
    {
      "epoch": 3.2858666666666667,
      "grad_norm": 0.4532771706581116,
      "learning_rate": 8.926666666666667e-06,
      "loss": 0.0018,
      "step": 61610
    },
    {
      "epoch": 3.2864,
      "grad_norm": 0.4045647382736206,
      "learning_rate": 8.920000000000001e-06,
      "loss": 0.0021,
      "step": 61620
    },
    {
      "epoch": 3.2869333333333333,
      "grad_norm": 0.09787122160196304,
      "learning_rate": 8.913333333333333e-06,
      "loss": 0.002,
      "step": 61630
    },
    {
      "epoch": 3.2874666666666665,
      "grad_norm": 0.09002383798360825,
      "learning_rate": 8.906666666666667e-06,
      "loss": 0.0025,
      "step": 61640
    },
    {
      "epoch": 3.288,
      "grad_norm": 0.5804142355918884,
      "learning_rate": 8.9e-06,
      "loss": 0.003,
      "step": 61650
    },
    {
      "epoch": 3.2885333333333335,
      "grad_norm": 0.28481414914131165,
      "learning_rate": 8.893333333333333e-06,
      "loss": 0.002,
      "step": 61660
    },
    {
      "epoch": 3.289066666666667,
      "grad_norm": 0.18352507054805756,
      "learning_rate": 8.886666666666667e-06,
      "loss": 0.003,
      "step": 61670
    },
    {
      "epoch": 3.2896,
      "grad_norm": 0.5449223518371582,
      "learning_rate": 8.880000000000001e-06,
      "loss": 0.0022,
      "step": 61680
    },
    {
      "epoch": 3.2901333333333334,
      "grad_norm": 0.37294238805770874,
      "learning_rate": 8.873333333333334e-06,
      "loss": 0.0025,
      "step": 61690
    },
    {
      "epoch": 3.2906666666666666,
      "grad_norm": 0.11920139193534851,
      "learning_rate": 8.866666666666668e-06,
      "loss": 0.0028,
      "step": 61700
    },
    {
      "epoch": 3.2912,
      "grad_norm": 0.09107568114995956,
      "learning_rate": 8.86e-06,
      "loss": 0.0018,
      "step": 61710
    },
    {
      "epoch": 3.291733333333333,
      "grad_norm": 0.11990360915660858,
      "learning_rate": 8.853333333333334e-06,
      "loss": 0.0031,
      "step": 61720
    },
    {
      "epoch": 3.2922666666666665,
      "grad_norm": 0.3000457286834717,
      "learning_rate": 8.846666666666668e-06,
      "loss": 0.0021,
      "step": 61730
    },
    {
      "epoch": 3.2928,
      "grad_norm": 0.10628262907266617,
      "learning_rate": 8.840000000000002e-06,
      "loss": 0.0037,
      "step": 61740
    },
    {
      "epoch": 3.2933333333333334,
      "grad_norm": 0.083803690969944,
      "learning_rate": 8.833333333333334e-06,
      "loss": 0.0022,
      "step": 61750
    },
    {
      "epoch": 3.2938666666666667,
      "grad_norm": 0.1792418509721756,
      "learning_rate": 8.826666666666666e-06,
      "loss": 0.0019,
      "step": 61760
    },
    {
      "epoch": 3.2944,
      "grad_norm": 0.3134908676147461,
      "learning_rate": 8.82e-06,
      "loss": 0.0021,
      "step": 61770
    },
    {
      "epoch": 3.2949333333333333,
      "grad_norm": 0.3042450547218323,
      "learning_rate": 8.813333333333333e-06,
      "loss": 0.002,
      "step": 61780
    },
    {
      "epoch": 3.2954666666666665,
      "grad_norm": 0.22681447863578796,
      "learning_rate": 8.806666666666666e-06,
      "loss": 0.0045,
      "step": 61790
    },
    {
      "epoch": 3.296,
      "grad_norm": 0.5190064311027527,
      "learning_rate": 8.8e-06,
      "loss": 0.0021,
      "step": 61800
    },
    {
      "epoch": 3.2965333333333335,
      "grad_norm": 0.5085735321044922,
      "learning_rate": 8.793333333333334e-06,
      "loss": 0.0034,
      "step": 61810
    },
    {
      "epoch": 3.297066666666667,
      "grad_norm": 0.4985843300819397,
      "learning_rate": 8.786666666666667e-06,
      "loss": 0.0034,
      "step": 61820
    },
    {
      "epoch": 3.2976,
      "grad_norm": 0.4161955416202545,
      "learning_rate": 8.78e-06,
      "loss": 0.003,
      "step": 61830
    },
    {
      "epoch": 3.2981333333333334,
      "grad_norm": 0.15139570832252502,
      "learning_rate": 8.773333333333333e-06,
      "loss": 0.002,
      "step": 61840
    },
    {
      "epoch": 3.2986666666666666,
      "grad_norm": 0.1634579747915268,
      "learning_rate": 8.766666666666667e-06,
      "loss": 0.0022,
      "step": 61850
    },
    {
      "epoch": 3.2992,
      "grad_norm": 0.3032678961753845,
      "learning_rate": 8.76e-06,
      "loss": 0.0029,
      "step": 61860
    },
    {
      "epoch": 3.299733333333333,
      "grad_norm": 0.29223400354385376,
      "learning_rate": 8.753333333333335e-06,
      "loss": 0.0025,
      "step": 61870
    },
    {
      "epoch": 3.3002666666666665,
      "grad_norm": 0.236839160323143,
      "learning_rate": 8.746666666666667e-06,
      "loss": 0.0028,
      "step": 61880
    },
    {
      "epoch": 3.3008,
      "grad_norm": 0.46671053767204285,
      "learning_rate": 8.740000000000001e-06,
      "loss": 0.0029,
      "step": 61890
    },
    {
      "epoch": 3.3013333333333335,
      "grad_norm": 0.09276293963193893,
      "learning_rate": 8.733333333333333e-06,
      "loss": 0.0022,
      "step": 61900
    },
    {
      "epoch": 3.3018666666666667,
      "grad_norm": 0.4910135865211487,
      "learning_rate": 8.726666666666667e-06,
      "loss": 0.0032,
      "step": 61910
    },
    {
      "epoch": 3.3024,
      "grad_norm": 0.10230556130409241,
      "learning_rate": 8.720000000000001e-06,
      "loss": 0.0029,
      "step": 61920
    },
    {
      "epoch": 3.3029333333333333,
      "grad_norm": 0.1585763841867447,
      "learning_rate": 8.713333333333333e-06,
      "loss": 0.0034,
      "step": 61930
    },
    {
      "epoch": 3.3034666666666666,
      "grad_norm": 0.45854660868644714,
      "learning_rate": 8.706666666666667e-06,
      "loss": 0.0027,
      "step": 61940
    },
    {
      "epoch": 3.304,
      "grad_norm": 0.11879237741231918,
      "learning_rate": 8.7e-06,
      "loss": 0.002,
      "step": 61950
    },
    {
      "epoch": 3.3045333333333335,
      "grad_norm": 0.5846155881881714,
      "learning_rate": 8.693333333333334e-06,
      "loss": 0.0049,
      "step": 61960
    },
    {
      "epoch": 3.305066666666667,
      "grad_norm": 0.08976109325885773,
      "learning_rate": 8.686666666666666e-06,
      "loss": 0.0025,
      "step": 61970
    },
    {
      "epoch": 3.3056,
      "grad_norm": 0.1927994340658188,
      "learning_rate": 8.68e-06,
      "loss": 0.0022,
      "step": 61980
    },
    {
      "epoch": 3.3061333333333334,
      "grad_norm": 0.32932570576667786,
      "learning_rate": 8.673333333333334e-06,
      "loss": 0.0021,
      "step": 61990
    },
    {
      "epoch": 3.3066666666666666,
      "grad_norm": 0.7190654277801514,
      "learning_rate": 8.666666666666668e-06,
      "loss": 0.0019,
      "step": 62000
    },
    {
      "epoch": 3.3072,
      "grad_norm": 0.5839700102806091,
      "learning_rate": 8.66e-06,
      "loss": 0.0036,
      "step": 62010
    },
    {
      "epoch": 3.307733333333333,
      "grad_norm": 0.0647205114364624,
      "learning_rate": 8.653333333333334e-06,
      "loss": 0.0032,
      "step": 62020
    },
    {
      "epoch": 3.3082666666666665,
      "grad_norm": 0.7239461541175842,
      "learning_rate": 8.646666666666666e-06,
      "loss": 0.0017,
      "step": 62030
    },
    {
      "epoch": 3.3088,
      "grad_norm": 0.21145202219486237,
      "learning_rate": 8.64e-06,
      "loss": 0.0027,
      "step": 62040
    },
    {
      "epoch": 3.3093333333333335,
      "grad_norm": 0.45719778537750244,
      "learning_rate": 8.633333333333334e-06,
      "loss": 0.002,
      "step": 62050
    },
    {
      "epoch": 3.3098666666666667,
      "grad_norm": 0.509783148765564,
      "learning_rate": 8.626666666666668e-06,
      "loss": 0.0025,
      "step": 62060
    },
    {
      "epoch": 3.3104,
      "grad_norm": 0.2065507173538208,
      "learning_rate": 8.62e-06,
      "loss": 0.0025,
      "step": 62070
    },
    {
      "epoch": 3.3109333333333333,
      "grad_norm": 0.3361061215400696,
      "learning_rate": 8.613333333333334e-06,
      "loss": 0.0019,
      "step": 62080
    },
    {
      "epoch": 3.3114666666666666,
      "grad_norm": 0.3573189377784729,
      "learning_rate": 8.606666666666667e-06,
      "loss": 0.0027,
      "step": 62090
    },
    {
      "epoch": 3.312,
      "grad_norm": 0.26721152663230896,
      "learning_rate": 8.599999999999999e-06,
      "loss": 0.0033,
      "step": 62100
    },
    {
      "epoch": 3.3125333333333336,
      "grad_norm": 0.22663311660289764,
      "learning_rate": 8.593333333333335e-06,
      "loss": 0.0025,
      "step": 62110
    },
    {
      "epoch": 3.313066666666667,
      "grad_norm": 0.12852762639522552,
      "learning_rate": 8.586666666666667e-06,
      "loss": 0.0017,
      "step": 62120
    },
    {
      "epoch": 3.3136,
      "grad_norm": 0.4883646070957184,
      "learning_rate": 8.580000000000001e-06,
      "loss": 0.0017,
      "step": 62130
    },
    {
      "epoch": 3.3141333333333334,
      "grad_norm": 0.9094903469085693,
      "learning_rate": 8.573333333333333e-06,
      "loss": 0.0034,
      "step": 62140
    },
    {
      "epoch": 3.3146666666666667,
      "grad_norm": 0.23630887269973755,
      "learning_rate": 8.566666666666667e-06,
      "loss": 0.0022,
      "step": 62150
    },
    {
      "epoch": 3.3152,
      "grad_norm": 0.0802496150135994,
      "learning_rate": 8.56e-06,
      "loss": 0.0019,
      "step": 62160
    },
    {
      "epoch": 3.315733333333333,
      "grad_norm": 0.3168109953403473,
      "learning_rate": 8.553333333333333e-06,
      "loss": 0.0016,
      "step": 62170
    },
    {
      "epoch": 3.3162666666666665,
      "grad_norm": 0.13343819975852966,
      "learning_rate": 8.546666666666667e-06,
      "loss": 0.0018,
      "step": 62180
    },
    {
      "epoch": 3.3168,
      "grad_norm": 0.09502091258764267,
      "learning_rate": 8.540000000000001e-06,
      "loss": 0.0029,
      "step": 62190
    },
    {
      "epoch": 3.3173333333333335,
      "grad_norm": 0.35211607813835144,
      "learning_rate": 8.533333333333334e-06,
      "loss": 0.0021,
      "step": 62200
    },
    {
      "epoch": 3.3178666666666667,
      "grad_norm": 0.5534294843673706,
      "learning_rate": 8.526666666666667e-06,
      "loss": 0.0043,
      "step": 62210
    },
    {
      "epoch": 3.3184,
      "grad_norm": 0.11787988245487213,
      "learning_rate": 8.52e-06,
      "loss": 0.0025,
      "step": 62220
    },
    {
      "epoch": 3.3189333333333333,
      "grad_norm": 0.3421002924442291,
      "learning_rate": 8.513333333333334e-06,
      "loss": 0.0034,
      "step": 62230
    },
    {
      "epoch": 3.3194666666666666,
      "grad_norm": 0.10979694128036499,
      "learning_rate": 8.506666666666668e-06,
      "loss": 0.0018,
      "step": 62240
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.10624928027391434,
      "learning_rate": 8.500000000000002e-06,
      "loss": 0.003,
      "step": 62250
    },
    {
      "epoch": 3.3205333333333336,
      "grad_norm": 0.19682346284389496,
      "learning_rate": 8.493333333333334e-06,
      "loss": 0.0018,
      "step": 62260
    },
    {
      "epoch": 3.321066666666667,
      "grad_norm": 0.2353491336107254,
      "learning_rate": 8.486666666666668e-06,
      "loss": 0.0022,
      "step": 62270
    },
    {
      "epoch": 3.3216,
      "grad_norm": 0.1704128086566925,
      "learning_rate": 8.48e-06,
      "loss": 0.003,
      "step": 62280
    },
    {
      "epoch": 3.3221333333333334,
      "grad_norm": 0.08303364366292953,
      "learning_rate": 8.473333333333332e-06,
      "loss": 0.0019,
      "step": 62290
    },
    {
      "epoch": 3.3226666666666667,
      "grad_norm": 0.5389869809150696,
      "learning_rate": 8.466666666666666e-06,
      "loss": 0.0028,
      "step": 62300
    },
    {
      "epoch": 3.3232,
      "grad_norm": 0.2796992063522339,
      "learning_rate": 8.46e-06,
      "loss": 0.0028,
      "step": 62310
    },
    {
      "epoch": 3.323733333333333,
      "grad_norm": 0.3042052686214447,
      "learning_rate": 8.453333333333334e-06,
      "loss": 0.0022,
      "step": 62320
    },
    {
      "epoch": 3.3242666666666665,
      "grad_norm": 0.17067940533161163,
      "learning_rate": 8.446666666666667e-06,
      "loss": 0.0033,
      "step": 62330
    },
    {
      "epoch": 3.3247999999999998,
      "grad_norm": 0.3143223822116852,
      "learning_rate": 8.44e-06,
      "loss": 0.002,
      "step": 62340
    },
    {
      "epoch": 3.3253333333333335,
      "grad_norm": 0.474188894033432,
      "learning_rate": 8.433333333333333e-06,
      "loss": 0.0032,
      "step": 62350
    },
    {
      "epoch": 3.3258666666666667,
      "grad_norm": 0.1830122172832489,
      "learning_rate": 8.426666666666667e-06,
      "loss": 0.0017,
      "step": 62360
    },
    {
      "epoch": 3.3264,
      "grad_norm": 0.22791194915771484,
      "learning_rate": 8.42e-06,
      "loss": 0.0021,
      "step": 62370
    },
    {
      "epoch": 3.3269333333333333,
      "grad_norm": 0.2644746005535126,
      "learning_rate": 8.413333333333335e-06,
      "loss": 0.0041,
      "step": 62380
    },
    {
      "epoch": 3.3274666666666666,
      "grad_norm": 0.1611006110906601,
      "learning_rate": 8.406666666666667e-06,
      "loss": 0.0021,
      "step": 62390
    },
    {
      "epoch": 3.328,
      "grad_norm": 0.06871254742145538,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.0032,
      "step": 62400
    },
    {
      "epoch": 3.3285333333333336,
      "grad_norm": 0.2175452709197998,
      "learning_rate": 8.393333333333333e-06,
      "loss": 0.0025,
      "step": 62410
    },
    {
      "epoch": 3.329066666666667,
      "grad_norm": 0.23405009508132935,
      "learning_rate": 8.386666666666667e-06,
      "loss": 0.0019,
      "step": 62420
    },
    {
      "epoch": 3.3296,
      "grad_norm": 0.5372743010520935,
      "learning_rate": 8.380000000000001e-06,
      "loss": 0.0023,
      "step": 62430
    },
    {
      "epoch": 3.3301333333333334,
      "grad_norm": 0.1785828322172165,
      "learning_rate": 8.373333333333335e-06,
      "loss": 0.002,
      "step": 62440
    },
    {
      "epoch": 3.3306666666666667,
      "grad_norm": 0.4229514002799988,
      "learning_rate": 8.366666666666667e-06,
      "loss": 0.0028,
      "step": 62450
    },
    {
      "epoch": 3.3312,
      "grad_norm": 0.11595374345779419,
      "learning_rate": 8.36e-06,
      "loss": 0.0038,
      "step": 62460
    },
    {
      "epoch": 3.331733333333333,
      "grad_norm": 0.06898471713066101,
      "learning_rate": 8.353333333333334e-06,
      "loss": 0.0028,
      "step": 62470
    },
    {
      "epoch": 3.3322666666666665,
      "grad_norm": 0.11057572811841965,
      "learning_rate": 8.346666666666666e-06,
      "loss": 0.0018,
      "step": 62480
    },
    {
      "epoch": 3.3327999999999998,
      "grad_norm": 0.132564976811409,
      "learning_rate": 8.34e-06,
      "loss": 0.002,
      "step": 62490
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.0998777225613594,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.002,
      "step": 62500
    },
    {
      "epoch": 3.3338666666666668,
      "grad_norm": 0.17595398426055908,
      "learning_rate": 8.326666666666668e-06,
      "loss": 0.0019,
      "step": 62510
    },
    {
      "epoch": 3.3344,
      "grad_norm": 0.24297092854976654,
      "learning_rate": 8.32e-06,
      "loss": 0.0021,
      "step": 62520
    },
    {
      "epoch": 3.3349333333333333,
      "grad_norm": 0.27611878514289856,
      "learning_rate": 8.313333333333334e-06,
      "loss": 0.0022,
      "step": 62530
    },
    {
      "epoch": 3.3354666666666666,
      "grad_norm": 0.28595781326293945,
      "learning_rate": 8.306666666666666e-06,
      "loss": 0.0019,
      "step": 62540
    },
    {
      "epoch": 3.336,
      "grad_norm": 0.24154958128929138,
      "learning_rate": 8.3e-06,
      "loss": 0.0024,
      "step": 62550
    },
    {
      "epoch": 3.3365333333333336,
      "grad_norm": 0.25849947333335876,
      "learning_rate": 8.293333333333334e-06,
      "loss": 0.0016,
      "step": 62560
    },
    {
      "epoch": 3.337066666666667,
      "grad_norm": 0.6220776438713074,
      "learning_rate": 8.286666666666668e-06,
      "loss": 0.0028,
      "step": 62570
    },
    {
      "epoch": 3.3376,
      "grad_norm": 0.3036406934261322,
      "learning_rate": 8.28e-06,
      "loss": 0.0016,
      "step": 62580
    },
    {
      "epoch": 3.3381333333333334,
      "grad_norm": 0.4459547698497772,
      "learning_rate": 8.273333333333334e-06,
      "loss": 0.0027,
      "step": 62590
    },
    {
      "epoch": 3.3386666666666667,
      "grad_norm": 0.20162726938724518,
      "learning_rate": 8.266666666666667e-06,
      "loss": 0.0021,
      "step": 62600
    },
    {
      "epoch": 3.3392,
      "grad_norm": 0.40053606033325195,
      "learning_rate": 8.26e-06,
      "loss": 0.0017,
      "step": 62610
    },
    {
      "epoch": 3.339733333333333,
      "grad_norm": 0.1652398258447647,
      "learning_rate": 8.253333333333334e-06,
      "loss": 0.0016,
      "step": 62620
    },
    {
      "epoch": 3.3402666666666665,
      "grad_norm": 0.20801745355129242,
      "learning_rate": 8.246666666666667e-06,
      "loss": 0.0017,
      "step": 62630
    },
    {
      "epoch": 3.3407999999999998,
      "grad_norm": 0.3461892604827881,
      "learning_rate": 8.24e-06,
      "loss": 0.0046,
      "step": 62640
    },
    {
      "epoch": 3.3413333333333335,
      "grad_norm": 0.07670925557613373,
      "learning_rate": 8.233333333333333e-06,
      "loss": 0.0033,
      "step": 62650
    },
    {
      "epoch": 3.3418666666666668,
      "grad_norm": 0.10487247258424759,
      "learning_rate": 8.226666666666667e-06,
      "loss": 0.0037,
      "step": 62660
    },
    {
      "epoch": 3.3424,
      "grad_norm": 0.2089119553565979,
      "learning_rate": 8.22e-06,
      "loss": 0.002,
      "step": 62670
    },
    {
      "epoch": 3.3429333333333333,
      "grad_norm": 0.09688907861709595,
      "learning_rate": 8.213333333333333e-06,
      "loss": 0.0029,
      "step": 62680
    },
    {
      "epoch": 3.3434666666666666,
      "grad_norm": 0.5120305418968201,
      "learning_rate": 8.206666666666667e-06,
      "loss": 0.0026,
      "step": 62690
    },
    {
      "epoch": 3.344,
      "grad_norm": 0.26174473762512207,
      "learning_rate": 8.200000000000001e-06,
      "loss": 0.0034,
      "step": 62700
    },
    {
      "epoch": 3.3445333333333336,
      "grad_norm": 0.31929218769073486,
      "learning_rate": 8.193333333333333e-06,
      "loss": 0.0033,
      "step": 62710
    },
    {
      "epoch": 3.345066666666667,
      "grad_norm": 0.4766210913658142,
      "learning_rate": 8.186666666666667e-06,
      "loss": 0.0028,
      "step": 62720
    },
    {
      "epoch": 3.3456,
      "grad_norm": 0.296722948551178,
      "learning_rate": 8.18e-06,
      "loss": 0.0027,
      "step": 62730
    },
    {
      "epoch": 3.3461333333333334,
      "grad_norm": 0.18561971187591553,
      "learning_rate": 8.173333333333334e-06,
      "loss": 0.0019,
      "step": 62740
    },
    {
      "epoch": 3.3466666666666667,
      "grad_norm": 0.09796657413244247,
      "learning_rate": 8.166666666666668e-06,
      "loss": 0.0027,
      "step": 62750
    },
    {
      "epoch": 3.3472,
      "grad_norm": 0.43481680750846863,
      "learning_rate": 8.160000000000001e-06,
      "loss": 0.003,
      "step": 62760
    },
    {
      "epoch": 3.3477333333333332,
      "grad_norm": 0.417438805103302,
      "learning_rate": 8.153333333333334e-06,
      "loss": 0.0018,
      "step": 62770
    },
    {
      "epoch": 3.3482666666666665,
      "grad_norm": 0.06811594218015671,
      "learning_rate": 8.146666666666668e-06,
      "loss": 0.003,
      "step": 62780
    },
    {
      "epoch": 3.3487999999999998,
      "grad_norm": 0.2999182939529419,
      "learning_rate": 8.14e-06,
      "loss": 0.0018,
      "step": 62790
    },
    {
      "epoch": 3.3493333333333335,
      "grad_norm": 0.33874982595443726,
      "learning_rate": 8.133333333333332e-06,
      "loss": 0.0035,
      "step": 62800
    },
    {
      "epoch": 3.3498666666666668,
      "grad_norm": 0.22109264135360718,
      "learning_rate": 8.126666666666668e-06,
      "loss": 0.0021,
      "step": 62810
    },
    {
      "epoch": 3.3504,
      "grad_norm": 0.21149848401546478,
      "learning_rate": 8.12e-06,
      "loss": 0.002,
      "step": 62820
    },
    {
      "epoch": 3.3509333333333333,
      "grad_norm": 0.16136343777179718,
      "learning_rate": 8.113333333333334e-06,
      "loss": 0.0022,
      "step": 62830
    },
    {
      "epoch": 3.3514666666666666,
      "grad_norm": 0.15714164078235626,
      "learning_rate": 8.106666666666666e-06,
      "loss": 0.0023,
      "step": 62840
    },
    {
      "epoch": 3.352,
      "grad_norm": 0.5012612342834473,
      "learning_rate": 8.1e-06,
      "loss": 0.0024,
      "step": 62850
    },
    {
      "epoch": 3.352533333333333,
      "grad_norm": 0.282752126455307,
      "learning_rate": 8.093333333333333e-06,
      "loss": 0.0019,
      "step": 62860
    },
    {
      "epoch": 3.353066666666667,
      "grad_norm": 0.0994701012969017,
      "learning_rate": 8.086666666666667e-06,
      "loss": 0.0023,
      "step": 62870
    },
    {
      "epoch": 3.3536,
      "grad_norm": 0.5315019488334656,
      "learning_rate": 8.08e-06,
      "loss": 0.0019,
      "step": 62880
    },
    {
      "epoch": 3.3541333333333334,
      "grad_norm": 0.28405317664146423,
      "learning_rate": 8.073333333333335e-06,
      "loss": 0.0019,
      "step": 62890
    },
    {
      "epoch": 3.3546666666666667,
      "grad_norm": 0.337584525346756,
      "learning_rate": 8.066666666666667e-06,
      "loss": 0.0022,
      "step": 62900
    },
    {
      "epoch": 3.3552,
      "grad_norm": 0.17014512419700623,
      "learning_rate": 8.06e-06,
      "loss": 0.002,
      "step": 62910
    },
    {
      "epoch": 3.3557333333333332,
      "grad_norm": 0.08992034196853638,
      "learning_rate": 8.053333333333333e-06,
      "loss": 0.0019,
      "step": 62920
    },
    {
      "epoch": 3.3562666666666665,
      "grad_norm": 0.18229246139526367,
      "learning_rate": 8.046666666666667e-06,
      "loss": 0.0036,
      "step": 62930
    },
    {
      "epoch": 3.3568,
      "grad_norm": 0.3317693769931793,
      "learning_rate": 8.040000000000001e-06,
      "loss": 0.0027,
      "step": 62940
    },
    {
      "epoch": 3.3573333333333335,
      "grad_norm": 0.11844626814126968,
      "learning_rate": 8.033333333333335e-06,
      "loss": 0.0026,
      "step": 62950
    },
    {
      "epoch": 3.3578666666666668,
      "grad_norm": 0.1903499960899353,
      "learning_rate": 8.026666666666667e-06,
      "loss": 0.0024,
      "step": 62960
    },
    {
      "epoch": 3.3584,
      "grad_norm": 0.15489117801189423,
      "learning_rate": 8.02e-06,
      "loss": 0.0021,
      "step": 62970
    },
    {
      "epoch": 3.3589333333333333,
      "grad_norm": 0.4262617826461792,
      "learning_rate": 8.013333333333333e-06,
      "loss": 0.0027,
      "step": 62980
    },
    {
      "epoch": 3.3594666666666666,
      "grad_norm": 0.10736056417226791,
      "learning_rate": 8.006666666666666e-06,
      "loss": 0.0021,
      "step": 62990
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.2540597915649414,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.0019,
      "step": 63000
    },
    {
      "epoch": 3.360533333333333,
      "grad_norm": 0.41163507103919983,
      "learning_rate": 7.993333333333334e-06,
      "loss": 0.0029,
      "step": 63010
    },
    {
      "epoch": 3.361066666666667,
      "grad_norm": 0.3996376693248749,
      "learning_rate": 7.986666666666668e-06,
      "loss": 0.002,
      "step": 63020
    },
    {
      "epoch": 3.3616,
      "grad_norm": 0.09460309147834778,
      "learning_rate": 7.98e-06,
      "loss": 0.0026,
      "step": 63030
    },
    {
      "epoch": 3.3621333333333334,
      "grad_norm": 0.1892167627811432,
      "learning_rate": 7.973333333333334e-06,
      "loss": 0.0027,
      "step": 63040
    },
    {
      "epoch": 3.3626666666666667,
      "grad_norm": 0.08699588477611542,
      "learning_rate": 7.966666666666666e-06,
      "loss": 0.0026,
      "step": 63050
    },
    {
      "epoch": 3.3632,
      "grad_norm": 0.2314593493938446,
      "learning_rate": 7.96e-06,
      "loss": 0.0041,
      "step": 63060
    },
    {
      "epoch": 3.3637333333333332,
      "grad_norm": 0.4329116940498352,
      "learning_rate": 7.953333333333334e-06,
      "loss": 0.0026,
      "step": 63070
    },
    {
      "epoch": 3.3642666666666665,
      "grad_norm": 0.19723449647426605,
      "learning_rate": 7.946666666666668e-06,
      "loss": 0.0028,
      "step": 63080
    },
    {
      "epoch": 3.3648,
      "grad_norm": 0.41555550694465637,
      "learning_rate": 7.94e-06,
      "loss": 0.002,
      "step": 63090
    },
    {
      "epoch": 3.3653333333333335,
      "grad_norm": 0.10589393973350525,
      "learning_rate": 7.933333333333334e-06,
      "loss": 0.0018,
      "step": 63100
    },
    {
      "epoch": 3.365866666666667,
      "grad_norm": 0.10828299820423126,
      "learning_rate": 7.926666666666666e-06,
      "loss": 0.0022,
      "step": 63110
    },
    {
      "epoch": 3.3664,
      "grad_norm": 0.7528154253959656,
      "learning_rate": 7.92e-06,
      "loss": 0.002,
      "step": 63120
    },
    {
      "epoch": 3.3669333333333333,
      "grad_norm": 0.23331759870052338,
      "learning_rate": 7.913333333333334e-06,
      "loss": 0.0019,
      "step": 63130
    },
    {
      "epoch": 3.3674666666666666,
      "grad_norm": 0.05201641470193863,
      "learning_rate": 7.906666666666667e-06,
      "loss": 0.0017,
      "step": 63140
    },
    {
      "epoch": 3.368,
      "grad_norm": 0.17387396097183228,
      "learning_rate": 7.9e-06,
      "loss": 0.0029,
      "step": 63150
    },
    {
      "epoch": 3.368533333333333,
      "grad_norm": 0.3088676929473877,
      "learning_rate": 7.893333333333333e-06,
      "loss": 0.0028,
      "step": 63160
    },
    {
      "epoch": 3.369066666666667,
      "grad_norm": 0.19765613973140717,
      "learning_rate": 7.886666666666667e-06,
      "loss": 0.0025,
      "step": 63170
    },
    {
      "epoch": 3.3696,
      "grad_norm": 0.1391170471906662,
      "learning_rate": 7.879999999999999e-06,
      "loss": 0.0033,
      "step": 63180
    },
    {
      "epoch": 3.3701333333333334,
      "grad_norm": 0.183065265417099,
      "learning_rate": 7.873333333333335e-06,
      "loss": 0.0018,
      "step": 63190
    },
    {
      "epoch": 3.3706666666666667,
      "grad_norm": 0.22374524176120758,
      "learning_rate": 7.866666666666667e-06,
      "loss": 0.0023,
      "step": 63200
    },
    {
      "epoch": 3.3712,
      "grad_norm": 0.22828525304794312,
      "learning_rate": 7.860000000000001e-06,
      "loss": 0.002,
      "step": 63210
    },
    {
      "epoch": 3.3717333333333332,
      "grad_norm": 0.12364807724952698,
      "learning_rate": 7.853333333333333e-06,
      "loss": 0.0024,
      "step": 63220
    },
    {
      "epoch": 3.3722666666666665,
      "grad_norm": 0.11788784712553024,
      "learning_rate": 7.846666666666667e-06,
      "loss": 0.0023,
      "step": 63230
    },
    {
      "epoch": 3.3728,
      "grad_norm": 0.19132059812545776,
      "learning_rate": 7.84e-06,
      "loss": 0.0026,
      "step": 63240
    },
    {
      "epoch": 3.3733333333333335,
      "grad_norm": 0.3011000156402588,
      "learning_rate": 7.833333333333333e-06,
      "loss": 0.0017,
      "step": 63250
    },
    {
      "epoch": 3.373866666666667,
      "grad_norm": 0.47086501121520996,
      "learning_rate": 7.826666666666667e-06,
      "loss": 0.0038,
      "step": 63260
    },
    {
      "epoch": 3.3744,
      "grad_norm": 0.49853429198265076,
      "learning_rate": 7.820000000000001e-06,
      "loss": 0.0019,
      "step": 63270
    },
    {
      "epoch": 3.3749333333333333,
      "grad_norm": 0.3405860364437103,
      "learning_rate": 7.813333333333334e-06,
      "loss": 0.0018,
      "step": 63280
    },
    {
      "epoch": 3.3754666666666666,
      "grad_norm": 0.19734534621238708,
      "learning_rate": 7.806666666666668e-06,
      "loss": 0.0026,
      "step": 63290
    },
    {
      "epoch": 3.376,
      "grad_norm": 0.31633666157722473,
      "learning_rate": 7.8e-06,
      "loss": 0.0017,
      "step": 63300
    },
    {
      "epoch": 3.376533333333333,
      "grad_norm": 0.09015835076570511,
      "learning_rate": 7.793333333333334e-06,
      "loss": 0.0019,
      "step": 63310
    },
    {
      "epoch": 3.377066666666667,
      "grad_norm": 0.4319303035736084,
      "learning_rate": 7.786666666666668e-06,
      "loss": 0.0019,
      "step": 63320
    },
    {
      "epoch": 3.3776,
      "grad_norm": 0.11296863108873367,
      "learning_rate": 7.78e-06,
      "loss": 0.0028,
      "step": 63330
    },
    {
      "epoch": 3.3781333333333334,
      "grad_norm": 0.2839067876338959,
      "learning_rate": 7.773333333333334e-06,
      "loss": 0.0025,
      "step": 63340
    },
    {
      "epoch": 3.3786666666666667,
      "grad_norm": 0.12081043422222137,
      "learning_rate": 7.766666666666666e-06,
      "loss": 0.0032,
      "step": 63350
    },
    {
      "epoch": 3.3792,
      "grad_norm": 0.4267733097076416,
      "learning_rate": 7.76e-06,
      "loss": 0.0024,
      "step": 63360
    },
    {
      "epoch": 3.3797333333333333,
      "grad_norm": 0.4235476553440094,
      "learning_rate": 7.753333333333333e-06,
      "loss": 0.0024,
      "step": 63370
    },
    {
      "epoch": 3.3802666666666665,
      "grad_norm": 0.07495137304067612,
      "learning_rate": 7.746666666666668e-06,
      "loss": 0.0017,
      "step": 63380
    },
    {
      "epoch": 3.3808,
      "grad_norm": 0.28013110160827637,
      "learning_rate": 7.74e-06,
      "loss": 0.0036,
      "step": 63390
    },
    {
      "epoch": 3.3813333333333335,
      "grad_norm": 0.27094751596450806,
      "learning_rate": 7.733333333333334e-06,
      "loss": 0.0018,
      "step": 63400
    },
    {
      "epoch": 3.381866666666667,
      "grad_norm": 0.3469214141368866,
      "learning_rate": 7.726666666666667e-06,
      "loss": 0.0019,
      "step": 63410
    },
    {
      "epoch": 3.3824,
      "grad_norm": 0.4982624053955078,
      "learning_rate": 7.72e-06,
      "loss": 0.0028,
      "step": 63420
    },
    {
      "epoch": 3.3829333333333333,
      "grad_norm": 0.3009185194969177,
      "learning_rate": 7.713333333333333e-06,
      "loss": 0.0017,
      "step": 63430
    },
    {
      "epoch": 3.3834666666666666,
      "grad_norm": 0.10744205862283707,
      "learning_rate": 7.706666666666667e-06,
      "loss": 0.0019,
      "step": 63440
    },
    {
      "epoch": 3.384,
      "grad_norm": 0.15690863132476807,
      "learning_rate": 7.7e-06,
      "loss": 0.0023,
      "step": 63450
    },
    {
      "epoch": 3.384533333333333,
      "grad_norm": 0.1312033236026764,
      "learning_rate": 7.693333333333335e-06,
      "loss": 0.0021,
      "step": 63460
    },
    {
      "epoch": 3.385066666666667,
      "grad_norm": 0.3374505043029785,
      "learning_rate": 7.686666666666667e-06,
      "loss": 0.0021,
      "step": 63470
    },
    {
      "epoch": 3.3856,
      "grad_norm": 0.5043790936470032,
      "learning_rate": 7.68e-06,
      "loss": 0.0027,
      "step": 63480
    },
    {
      "epoch": 3.3861333333333334,
      "grad_norm": 0.26632028818130493,
      "learning_rate": 7.673333333333333e-06,
      "loss": 0.0026,
      "step": 63490
    },
    {
      "epoch": 3.3866666666666667,
      "grad_norm": 0.2264862060546875,
      "learning_rate": 7.666666666666667e-06,
      "loss": 0.0027,
      "step": 63500
    },
    {
      "epoch": 3.3872,
      "grad_norm": 0.08373281359672546,
      "learning_rate": 7.660000000000001e-06,
      "loss": 0.0029,
      "step": 63510
    },
    {
      "epoch": 3.3877333333333333,
      "grad_norm": 0.6376577615737915,
      "learning_rate": 7.653333333333333e-06,
      "loss": 0.0028,
      "step": 63520
    },
    {
      "epoch": 3.3882666666666665,
      "grad_norm": 0.33640050888061523,
      "learning_rate": 7.646666666666667e-06,
      "loss": 0.0032,
      "step": 63530
    },
    {
      "epoch": 3.3888,
      "grad_norm": 0.05971148982644081,
      "learning_rate": 7.64e-06,
      "loss": 0.0026,
      "step": 63540
    },
    {
      "epoch": 3.389333333333333,
      "grad_norm": 0.43577614426612854,
      "learning_rate": 7.633333333333334e-06,
      "loss": 0.0019,
      "step": 63550
    },
    {
      "epoch": 3.389866666666667,
      "grad_norm": 0.32850781083106995,
      "learning_rate": 7.626666666666667e-06,
      "loss": 0.0028,
      "step": 63560
    },
    {
      "epoch": 3.3904,
      "grad_norm": 0.09209711849689484,
      "learning_rate": 7.620000000000001e-06,
      "loss": 0.0026,
      "step": 63570
    },
    {
      "epoch": 3.3909333333333334,
      "grad_norm": 0.4353711009025574,
      "learning_rate": 7.613333333333334e-06,
      "loss": 0.0024,
      "step": 63580
    },
    {
      "epoch": 3.3914666666666666,
      "grad_norm": 0.12390641868114471,
      "learning_rate": 7.606666666666668e-06,
      "loss": 0.0017,
      "step": 63590
    },
    {
      "epoch": 3.392,
      "grad_norm": 0.20346538722515106,
      "learning_rate": 7.6e-06,
      "loss": 0.0024,
      "step": 63600
    },
    {
      "epoch": 3.392533333333333,
      "grad_norm": 0.37883612513542175,
      "learning_rate": 7.593333333333334e-06,
      "loss": 0.0024,
      "step": 63610
    },
    {
      "epoch": 3.393066666666667,
      "grad_norm": 0.1457914113998413,
      "learning_rate": 7.586666666666667e-06,
      "loss": 0.0027,
      "step": 63620
    },
    {
      "epoch": 3.3936,
      "grad_norm": 0.10260847210884094,
      "learning_rate": 7.580000000000001e-06,
      "loss": 0.0015,
      "step": 63630
    },
    {
      "epoch": 3.3941333333333334,
      "grad_norm": 0.48345568776130676,
      "learning_rate": 7.573333333333333e-06,
      "loss": 0.0024,
      "step": 63640
    },
    {
      "epoch": 3.3946666666666667,
      "grad_norm": 0.4416946768760681,
      "learning_rate": 7.5666666666666665e-06,
      "loss": 0.0019,
      "step": 63650
    },
    {
      "epoch": 3.3952,
      "grad_norm": 0.29036304354667664,
      "learning_rate": 7.5600000000000005e-06,
      "loss": 0.0016,
      "step": 63660
    },
    {
      "epoch": 3.3957333333333333,
      "grad_norm": 0.12144183367490768,
      "learning_rate": 7.553333333333333e-06,
      "loss": 0.0018,
      "step": 63670
    },
    {
      "epoch": 3.3962666666666665,
      "grad_norm": 0.10298295319080353,
      "learning_rate": 7.5466666666666675e-06,
      "loss": 0.0018,
      "step": 63680
    },
    {
      "epoch": 3.3968,
      "grad_norm": 0.20621810853481293,
      "learning_rate": 7.54e-06,
      "loss": 0.002,
      "step": 63690
    },
    {
      "epoch": 3.397333333333333,
      "grad_norm": 0.0760459154844284,
      "learning_rate": 7.533333333333334e-06,
      "loss": 0.0017,
      "step": 63700
    },
    {
      "epoch": 3.397866666666667,
      "grad_norm": 0.20354363322257996,
      "learning_rate": 7.526666666666667e-06,
      "loss": 0.0016,
      "step": 63710
    },
    {
      "epoch": 3.3984,
      "grad_norm": 0.4466417729854584,
      "learning_rate": 7.520000000000001e-06,
      "loss": 0.0029,
      "step": 63720
    },
    {
      "epoch": 3.3989333333333334,
      "grad_norm": 0.13035783171653748,
      "learning_rate": 7.513333333333333e-06,
      "loss": 0.0027,
      "step": 63730
    },
    {
      "epoch": 3.3994666666666666,
      "grad_norm": 0.21071363985538483,
      "learning_rate": 7.506666666666667e-06,
      "loss": 0.002,
      "step": 63740
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.10749120265245438,
      "learning_rate": 7.5e-06,
      "loss": 0.0025,
      "step": 63750
    },
    {
      "epoch": 3.400533333333333,
      "grad_norm": 0.14579856395721436,
      "learning_rate": 7.493333333333334e-06,
      "loss": 0.0029,
      "step": 63760
    },
    {
      "epoch": 3.401066666666667,
      "grad_norm": 0.29873037338256836,
      "learning_rate": 7.486666666666666e-06,
      "loss": 0.0024,
      "step": 63770
    },
    {
      "epoch": 3.4016,
      "grad_norm": 0.23083099722862244,
      "learning_rate": 7.480000000000001e-06,
      "loss": 0.0023,
      "step": 63780
    },
    {
      "epoch": 3.4021333333333335,
      "grad_norm": 0.6523259878158569,
      "learning_rate": 7.4733333333333335e-06,
      "loss": 0.0034,
      "step": 63790
    },
    {
      "epoch": 3.4026666666666667,
      "grad_norm": 0.39446723461151123,
      "learning_rate": 7.4666666666666675e-06,
      "loss": 0.0021,
      "step": 63800
    },
    {
      "epoch": 3.4032,
      "grad_norm": 0.09755747020244598,
      "learning_rate": 7.4600000000000006e-06,
      "loss": 0.0031,
      "step": 63810
    },
    {
      "epoch": 3.4037333333333333,
      "grad_norm": 0.36720800399780273,
      "learning_rate": 7.453333333333333e-06,
      "loss": 0.0022,
      "step": 63820
    },
    {
      "epoch": 3.4042666666666666,
      "grad_norm": 0.12299689650535583,
      "learning_rate": 7.446666666666667e-06,
      "loss": 0.0018,
      "step": 63830
    },
    {
      "epoch": 3.4048,
      "grad_norm": 0.22861184179782867,
      "learning_rate": 7.44e-06,
      "loss": 0.0026,
      "step": 63840
    },
    {
      "epoch": 3.405333333333333,
      "grad_norm": 0.31411629915237427,
      "learning_rate": 7.433333333333334e-06,
      "loss": 0.0029,
      "step": 63850
    },
    {
      "epoch": 3.405866666666667,
      "grad_norm": 0.38042718172073364,
      "learning_rate": 7.426666666666666e-06,
      "loss": 0.0032,
      "step": 63860
    },
    {
      "epoch": 3.4064,
      "grad_norm": 0.12204073369503021,
      "learning_rate": 7.420000000000001e-06,
      "loss": 0.0018,
      "step": 63870
    },
    {
      "epoch": 3.4069333333333334,
      "grad_norm": 0.2199356108903885,
      "learning_rate": 7.413333333333333e-06,
      "loss": 0.0019,
      "step": 63880
    },
    {
      "epoch": 3.4074666666666666,
      "grad_norm": 0.12345153838396072,
      "learning_rate": 7.406666666666667e-06,
      "loss": 0.0041,
      "step": 63890
    },
    {
      "epoch": 3.408,
      "grad_norm": 0.24832500517368317,
      "learning_rate": 7.4e-06,
      "loss": 0.0025,
      "step": 63900
    },
    {
      "epoch": 3.408533333333333,
      "grad_norm": 0.23264673352241516,
      "learning_rate": 7.393333333333334e-06,
      "loss": 0.0019,
      "step": 63910
    },
    {
      "epoch": 3.409066666666667,
      "grad_norm": 0.08695287257432938,
      "learning_rate": 7.3866666666666665e-06,
      "loss": 0.002,
      "step": 63920
    },
    {
      "epoch": 3.4096,
      "grad_norm": 0.4901307225227356,
      "learning_rate": 7.3800000000000005e-06,
      "loss": 0.0019,
      "step": 63930
    },
    {
      "epoch": 3.4101333333333335,
      "grad_norm": 0.13363195955753326,
      "learning_rate": 7.373333333333334e-06,
      "loss": 0.0027,
      "step": 63940
    },
    {
      "epoch": 3.4106666666666667,
      "grad_norm": 0.2829465866088867,
      "learning_rate": 7.3666666666666676e-06,
      "loss": 0.002,
      "step": 63950
    },
    {
      "epoch": 3.4112,
      "grad_norm": 0.06707083433866501,
      "learning_rate": 7.36e-06,
      "loss": 0.0038,
      "step": 63960
    },
    {
      "epoch": 3.4117333333333333,
      "grad_norm": 0.3059801757335663,
      "learning_rate": 7.353333333333335e-06,
      "loss": 0.0025,
      "step": 63970
    },
    {
      "epoch": 3.4122666666666666,
      "grad_norm": 0.08853355795145035,
      "learning_rate": 7.346666666666667e-06,
      "loss": 0.0031,
      "step": 63980
    },
    {
      "epoch": 3.4128,
      "grad_norm": 0.11097574979066849,
      "learning_rate": 7.340000000000001e-06,
      "loss": 0.0027,
      "step": 63990
    },
    {
      "epoch": 3.413333333333333,
      "grad_norm": 0.22297614812850952,
      "learning_rate": 7.333333333333334e-06,
      "loss": 0.0018,
      "step": 64000
    },
    {
      "epoch": 3.413866666666667,
      "grad_norm": 0.08626318722963333,
      "learning_rate": 7.326666666666666e-06,
      "loss": 0.0027,
      "step": 64010
    },
    {
      "epoch": 3.4144,
      "grad_norm": 0.28243085741996765,
      "learning_rate": 7.32e-06,
      "loss": 0.0024,
      "step": 64020
    },
    {
      "epoch": 3.4149333333333334,
      "grad_norm": 0.37660977244377136,
      "learning_rate": 7.313333333333333e-06,
      "loss": 0.0022,
      "step": 64030
    },
    {
      "epoch": 3.4154666666666667,
      "grad_norm": 0.18082329630851746,
      "learning_rate": 7.306666666666667e-06,
      "loss": 0.0049,
      "step": 64040
    },
    {
      "epoch": 3.416,
      "grad_norm": 0.13146483898162842,
      "learning_rate": 7.2999999999999996e-06,
      "loss": 0.0021,
      "step": 64050
    },
    {
      "epoch": 3.416533333333333,
      "grad_norm": 0.3172404170036316,
      "learning_rate": 7.293333333333334e-06,
      "loss": 0.002,
      "step": 64060
    },
    {
      "epoch": 3.4170666666666665,
      "grad_norm": 0.22732196748256683,
      "learning_rate": 7.286666666666667e-06,
      "loss": 0.0019,
      "step": 64070
    },
    {
      "epoch": 3.4176,
      "grad_norm": 0.19273881614208221,
      "learning_rate": 7.280000000000001e-06,
      "loss": 0.0017,
      "step": 64080
    },
    {
      "epoch": 3.4181333333333335,
      "grad_norm": 0.28211212158203125,
      "learning_rate": 7.273333333333334e-06,
      "loss": 0.0025,
      "step": 64090
    },
    {
      "epoch": 3.4186666666666667,
      "grad_norm": 0.4560992419719696,
      "learning_rate": 7.266666666666668e-06,
      "loss": 0.0026,
      "step": 64100
    },
    {
      "epoch": 3.4192,
      "grad_norm": 0.2768341898918152,
      "learning_rate": 7.26e-06,
      "loss": 0.0021,
      "step": 64110
    },
    {
      "epoch": 3.4197333333333333,
      "grad_norm": 0.12178920954465866,
      "learning_rate": 7.253333333333334e-06,
      "loss": 0.0017,
      "step": 64120
    },
    {
      "epoch": 3.4202666666666666,
      "grad_norm": 0.3266599178314209,
      "learning_rate": 7.246666666666667e-06,
      "loss": 0.002,
      "step": 64130
    },
    {
      "epoch": 3.4208,
      "grad_norm": 0.30115434527397156,
      "learning_rate": 7.240000000000001e-06,
      "loss": 0.0024,
      "step": 64140
    },
    {
      "epoch": 3.421333333333333,
      "grad_norm": 0.3535509407520294,
      "learning_rate": 7.233333333333333e-06,
      "loss": 0.0031,
      "step": 64150
    },
    {
      "epoch": 3.421866666666667,
      "grad_norm": 0.17860066890716553,
      "learning_rate": 7.226666666666668e-06,
      "loss": 0.003,
      "step": 64160
    },
    {
      "epoch": 3.4224,
      "grad_norm": 0.24177388846874237,
      "learning_rate": 7.22e-06,
      "loss": 0.0019,
      "step": 64170
    },
    {
      "epoch": 3.4229333333333334,
      "grad_norm": 0.25537610054016113,
      "learning_rate": 7.2133333333333334e-06,
      "loss": 0.0031,
      "step": 64180
    },
    {
      "epoch": 3.4234666666666667,
      "grad_norm": 0.2793835401535034,
      "learning_rate": 7.206666666666667e-06,
      "loss": 0.0019,
      "step": 64190
    },
    {
      "epoch": 3.424,
      "grad_norm": 0.3084561228752136,
      "learning_rate": 7.2e-06,
      "loss": 0.0021,
      "step": 64200
    },
    {
      "epoch": 3.424533333333333,
      "grad_norm": 0.19082055985927582,
      "learning_rate": 7.193333333333334e-06,
      "loss": 0.0024,
      "step": 64210
    },
    {
      "epoch": 3.4250666666666665,
      "grad_norm": 0.3372424244880676,
      "learning_rate": 7.186666666666667e-06,
      "loss": 0.0027,
      "step": 64220
    },
    {
      "epoch": 3.4256,
      "grad_norm": 0.3298755884170532,
      "learning_rate": 7.180000000000001e-06,
      "loss": 0.0019,
      "step": 64230
    },
    {
      "epoch": 3.4261333333333335,
      "grad_norm": 0.18991373479366302,
      "learning_rate": 7.173333333333333e-06,
      "loss": 0.002,
      "step": 64240
    },
    {
      "epoch": 3.4266666666666667,
      "grad_norm": 0.1759284883737564,
      "learning_rate": 7.166666666666667e-06,
      "loss": 0.0023,
      "step": 64250
    },
    {
      "epoch": 3.4272,
      "grad_norm": 0.7334352731704712,
      "learning_rate": 7.16e-06,
      "loss": 0.0034,
      "step": 64260
    },
    {
      "epoch": 3.4277333333333333,
      "grad_norm": 0.22364476323127747,
      "learning_rate": 7.153333333333334e-06,
      "loss": 0.0018,
      "step": 64270
    },
    {
      "epoch": 3.4282666666666666,
      "grad_norm": 0.6793224811553955,
      "learning_rate": 7.146666666666667e-06,
      "loss": 0.0015,
      "step": 64280
    },
    {
      "epoch": 3.4288,
      "grad_norm": 0.7895553708076477,
      "learning_rate": 7.140000000000001e-06,
      "loss": 0.0019,
      "step": 64290
    },
    {
      "epoch": 3.429333333333333,
      "grad_norm": 0.21500670909881592,
      "learning_rate": 7.133333333333333e-06,
      "loss": 0.0034,
      "step": 64300
    },
    {
      "epoch": 3.429866666666667,
      "grad_norm": 0.25549474358558655,
      "learning_rate": 7.126666666666667e-06,
      "loss": 0.0032,
      "step": 64310
    },
    {
      "epoch": 3.4304,
      "grad_norm": 0.40890932083129883,
      "learning_rate": 7.1200000000000004e-06,
      "loss": 0.0018,
      "step": 64320
    },
    {
      "epoch": 3.4309333333333334,
      "grad_norm": 0.10083542764186859,
      "learning_rate": 7.113333333333334e-06,
      "loss": 0.0028,
      "step": 64330
    },
    {
      "epoch": 3.4314666666666667,
      "grad_norm": 0.3353874683380127,
      "learning_rate": 7.106666666666667e-06,
      "loss": 0.003,
      "step": 64340
    },
    {
      "epoch": 3.432,
      "grad_norm": 0.5737147927284241,
      "learning_rate": 7.1e-06,
      "loss": 0.0033,
      "step": 64350
    },
    {
      "epoch": 3.432533333333333,
      "grad_norm": 0.08775510638952255,
      "learning_rate": 7.093333333333334e-06,
      "loss": 0.0021,
      "step": 64360
    },
    {
      "epoch": 3.4330666666666665,
      "grad_norm": 0.4518793523311615,
      "learning_rate": 7.086666666666667e-06,
      "loss": 0.0021,
      "step": 64370
    },
    {
      "epoch": 3.4336,
      "grad_norm": 0.21804019808769226,
      "learning_rate": 7.080000000000001e-06,
      "loss": 0.0019,
      "step": 64380
    },
    {
      "epoch": 3.4341333333333335,
      "grad_norm": 0.2824177145957947,
      "learning_rate": 7.073333333333333e-06,
      "loss": 0.0021,
      "step": 64390
    },
    {
      "epoch": 3.4346666666666668,
      "grad_norm": 0.24343843758106232,
      "learning_rate": 7.066666666666667e-06,
      "loss": 0.0027,
      "step": 64400
    },
    {
      "epoch": 3.4352,
      "grad_norm": 0.2676485478878021,
      "learning_rate": 7.06e-06,
      "loss": 0.0021,
      "step": 64410
    },
    {
      "epoch": 3.4357333333333333,
      "grad_norm": 0.23582018911838531,
      "learning_rate": 7.053333333333334e-06,
      "loss": 0.0032,
      "step": 64420
    },
    {
      "epoch": 3.4362666666666666,
      "grad_norm": 0.43285486102104187,
      "learning_rate": 7.046666666666666e-06,
      "loss": 0.0017,
      "step": 64430
    },
    {
      "epoch": 3.4368,
      "grad_norm": 0.6543681621551514,
      "learning_rate": 7.04e-06,
      "loss": 0.0023,
      "step": 64440
    },
    {
      "epoch": 3.437333333333333,
      "grad_norm": 0.4483688473701477,
      "learning_rate": 7.0333333333333335e-06,
      "loss": 0.0028,
      "step": 64450
    },
    {
      "epoch": 3.437866666666667,
      "grad_norm": 0.4964311420917511,
      "learning_rate": 7.0266666666666674e-06,
      "loss": 0.0027,
      "step": 64460
    },
    {
      "epoch": 3.4384,
      "grad_norm": 0.15638229250907898,
      "learning_rate": 7.0200000000000006e-06,
      "loss": 0.0026,
      "step": 64470
    },
    {
      "epoch": 3.4389333333333334,
      "grad_norm": 0.2903199791908264,
      "learning_rate": 7.0133333333333345e-06,
      "loss": 0.003,
      "step": 64480
    },
    {
      "epoch": 3.4394666666666667,
      "grad_norm": 0.08300823718309402,
      "learning_rate": 7.006666666666667e-06,
      "loss": 0.0019,
      "step": 64490
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.16840770840644836,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.0027,
      "step": 64500
    },
    {
      "epoch": 3.440533333333333,
      "grad_norm": 0.0802190750837326,
      "learning_rate": 6.993333333333334e-06,
      "loss": 0.0024,
      "step": 64510
    },
    {
      "epoch": 3.4410666666666665,
      "grad_norm": 0.11313371360301971,
      "learning_rate": 6.986666666666666e-06,
      "loss": 0.0035,
      "step": 64520
    },
    {
      "epoch": 3.4416,
      "grad_norm": 0.29335924983024597,
      "learning_rate": 6.98e-06,
      "loss": 0.002,
      "step": 64530
    },
    {
      "epoch": 3.4421333333333335,
      "grad_norm": 0.3475915789604187,
      "learning_rate": 6.973333333333333e-06,
      "loss": 0.0022,
      "step": 64540
    },
    {
      "epoch": 3.4426666666666668,
      "grad_norm": 0.2800656855106354,
      "learning_rate": 6.966666666666667e-06,
      "loss": 0.0016,
      "step": 64550
    },
    {
      "epoch": 3.4432,
      "grad_norm": 0.2581719756126404,
      "learning_rate": 6.9599999999999994e-06,
      "loss": 0.0027,
      "step": 64560
    },
    {
      "epoch": 3.4437333333333333,
      "grad_norm": 0.3678911626338959,
      "learning_rate": 6.953333333333334e-06,
      "loss": 0.0019,
      "step": 64570
    },
    {
      "epoch": 3.4442666666666666,
      "grad_norm": 0.21024532616138458,
      "learning_rate": 6.9466666666666665e-06,
      "loss": 0.002,
      "step": 64580
    },
    {
      "epoch": 3.4448,
      "grad_norm": 0.5607898235321045,
      "learning_rate": 6.9400000000000005e-06,
      "loss": 0.0026,
      "step": 64590
    },
    {
      "epoch": 3.445333333333333,
      "grad_norm": 0.06633049994707108,
      "learning_rate": 6.933333333333334e-06,
      "loss": 0.0035,
      "step": 64600
    },
    {
      "epoch": 3.445866666666667,
      "grad_norm": 0.17748795449733734,
      "learning_rate": 6.9266666666666675e-06,
      "loss": 0.0018,
      "step": 64610
    },
    {
      "epoch": 3.4464,
      "grad_norm": 0.5087146759033203,
      "learning_rate": 6.92e-06,
      "loss": 0.0018,
      "step": 64620
    },
    {
      "epoch": 3.4469333333333334,
      "grad_norm": 0.33020254969596863,
      "learning_rate": 6.913333333333334e-06,
      "loss": 0.002,
      "step": 64630
    },
    {
      "epoch": 3.4474666666666667,
      "grad_norm": 0.15053218603134155,
      "learning_rate": 6.906666666666667e-06,
      "loss": 0.0029,
      "step": 64640
    },
    {
      "epoch": 3.448,
      "grad_norm": 0.3664233982563019,
      "learning_rate": 6.900000000000001e-06,
      "loss": 0.0018,
      "step": 64650
    },
    {
      "epoch": 3.4485333333333332,
      "grad_norm": 0.16348595917224884,
      "learning_rate": 6.893333333333334e-06,
      "loss": 0.0024,
      "step": 64660
    },
    {
      "epoch": 3.4490666666666665,
      "grad_norm": 0.8232479691505432,
      "learning_rate": 6.886666666666668e-06,
      "loss": 0.0026,
      "step": 64670
    },
    {
      "epoch": 3.4496,
      "grad_norm": 0.08751668781042099,
      "learning_rate": 6.88e-06,
      "loss": 0.0036,
      "step": 64680
    },
    {
      "epoch": 3.4501333333333335,
      "grad_norm": 0.211172416806221,
      "learning_rate": 6.873333333333333e-06,
      "loss": 0.0027,
      "step": 64690
    },
    {
      "epoch": 3.4506666666666668,
      "grad_norm": 0.12300747632980347,
      "learning_rate": 6.866666666666667e-06,
      "loss": 0.0023,
      "step": 64700
    },
    {
      "epoch": 3.4512,
      "grad_norm": 0.7190704345703125,
      "learning_rate": 6.8599999999999995e-06,
      "loss": 0.0018,
      "step": 64710
    },
    {
      "epoch": 3.4517333333333333,
      "grad_norm": 0.15928111970424652,
      "learning_rate": 6.8533333333333335e-06,
      "loss": 0.0019,
      "step": 64720
    },
    {
      "epoch": 3.4522666666666666,
      "grad_norm": 0.14930979907512665,
      "learning_rate": 6.846666666666667e-06,
      "loss": 0.0017,
      "step": 64730
    },
    {
      "epoch": 3.4528,
      "grad_norm": 0.2214793860912323,
      "learning_rate": 6.840000000000001e-06,
      "loss": 0.0018,
      "step": 64740
    },
    {
      "epoch": 3.453333333333333,
      "grad_norm": 0.32184624671936035,
      "learning_rate": 6.833333333333333e-06,
      "loss": 0.0019,
      "step": 64750
    },
    {
      "epoch": 3.4538666666666664,
      "grad_norm": 0.18748009204864502,
      "learning_rate": 6.826666666666668e-06,
      "loss": 0.0019,
      "step": 64760
    },
    {
      "epoch": 3.4544,
      "grad_norm": 0.1599786877632141,
      "learning_rate": 6.82e-06,
      "loss": 0.0036,
      "step": 64770
    },
    {
      "epoch": 3.4549333333333334,
      "grad_norm": 0.23305152356624603,
      "learning_rate": 6.813333333333334e-06,
      "loss": 0.0027,
      "step": 64780
    },
    {
      "epoch": 3.4554666666666667,
      "grad_norm": 0.11900319159030914,
      "learning_rate": 6.806666666666667e-06,
      "loss": 0.0018,
      "step": 64790
    },
    {
      "epoch": 3.456,
      "grad_norm": 0.3327172100543976,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.0018,
      "step": 64800
    },
    {
      "epoch": 3.4565333333333332,
      "grad_norm": 0.07585147023200989,
      "learning_rate": 6.793333333333333e-06,
      "loss": 0.002,
      "step": 64810
    },
    {
      "epoch": 3.4570666666666665,
      "grad_norm": 0.5610307455062866,
      "learning_rate": 6.786666666666667e-06,
      "loss": 0.0019,
      "step": 64820
    },
    {
      "epoch": 3.4576000000000002,
      "grad_norm": 0.3440196216106415,
      "learning_rate": 6.78e-06,
      "loss": 0.0027,
      "step": 64830
    },
    {
      "epoch": 3.4581333333333335,
      "grad_norm": 0.10935753583908081,
      "learning_rate": 6.773333333333334e-06,
      "loss": 0.0017,
      "step": 64840
    },
    {
      "epoch": 3.458666666666667,
      "grad_norm": 0.310549795627594,
      "learning_rate": 6.766666666666667e-06,
      "loss": 0.0021,
      "step": 64850
    },
    {
      "epoch": 3.4592,
      "grad_norm": 0.5090860724449158,
      "learning_rate": 6.76e-06,
      "loss": 0.0019,
      "step": 64860
    },
    {
      "epoch": 3.4597333333333333,
      "grad_norm": 0.3976374566555023,
      "learning_rate": 6.753333333333334e-06,
      "loss": 0.0026,
      "step": 64870
    },
    {
      "epoch": 3.4602666666666666,
      "grad_norm": 0.24261574447155,
      "learning_rate": 6.746666666666667e-06,
      "loss": 0.0026,
      "step": 64880
    },
    {
      "epoch": 3.4608,
      "grad_norm": 0.4026886224746704,
      "learning_rate": 6.740000000000001e-06,
      "loss": 0.0016,
      "step": 64890
    },
    {
      "epoch": 3.461333333333333,
      "grad_norm": 0.3112921714782715,
      "learning_rate": 6.733333333333333e-06,
      "loss": 0.0023,
      "step": 64900
    },
    {
      "epoch": 3.4618666666666664,
      "grad_norm": 0.14869163930416107,
      "learning_rate": 6.726666666666667e-06,
      "loss": 0.0028,
      "step": 64910
    },
    {
      "epoch": 3.4624,
      "grad_norm": 0.23805053532123566,
      "learning_rate": 6.72e-06,
      "loss": 0.0023,
      "step": 64920
    },
    {
      "epoch": 3.4629333333333334,
      "grad_norm": 0.732929527759552,
      "learning_rate": 6.713333333333334e-06,
      "loss": 0.0025,
      "step": 64930
    },
    {
      "epoch": 3.4634666666666667,
      "grad_norm": 0.4504064619541168,
      "learning_rate": 6.706666666666666e-06,
      "loss": 0.0024,
      "step": 64940
    },
    {
      "epoch": 3.464,
      "grad_norm": 0.4101214110851288,
      "learning_rate": 6.700000000000001e-06,
      "loss": 0.0018,
      "step": 64950
    },
    {
      "epoch": 3.4645333333333332,
      "grad_norm": 0.5869396924972534,
      "learning_rate": 6.693333333333333e-06,
      "loss": 0.0028,
      "step": 64960
    },
    {
      "epoch": 3.4650666666666665,
      "grad_norm": 0.2891426086425781,
      "learning_rate": 6.686666666666667e-06,
      "loss": 0.0026,
      "step": 64970
    },
    {
      "epoch": 3.4656000000000002,
      "grad_norm": 0.4318474233150482,
      "learning_rate": 6.68e-06,
      "loss": 0.0034,
      "step": 64980
    },
    {
      "epoch": 3.4661333333333335,
      "grad_norm": 0.25343090295791626,
      "learning_rate": 6.673333333333334e-06,
      "loss": 0.0017,
      "step": 64990
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 0.0724487155675888,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.0016,
      "step": 65000
    },
    {
      "epoch": 3.4672,
      "grad_norm": 0.2312631607055664,
      "learning_rate": 6.660000000000001e-06,
      "loss": 0.0022,
      "step": 65010
    },
    {
      "epoch": 3.4677333333333333,
      "grad_norm": 0.533557653427124,
      "learning_rate": 6.653333333333334e-06,
      "loss": 0.002,
      "step": 65020
    },
    {
      "epoch": 3.4682666666666666,
      "grad_norm": 0.09787487238645554,
      "learning_rate": 6.646666666666666e-06,
      "loss": 0.0018,
      "step": 65030
    },
    {
      "epoch": 3.4688,
      "grad_norm": 0.0893639624118805,
      "learning_rate": 6.640000000000001e-06,
      "loss": 0.002,
      "step": 65040
    },
    {
      "epoch": 3.469333333333333,
      "grad_norm": 0.08223540335893631,
      "learning_rate": 6.633333333333333e-06,
      "loss": 0.0027,
      "step": 65050
    },
    {
      "epoch": 3.4698666666666664,
      "grad_norm": 0.18967346847057343,
      "learning_rate": 6.626666666666667e-06,
      "loss": 0.0023,
      "step": 65060
    },
    {
      "epoch": 3.4704,
      "grad_norm": 0.38914501667022705,
      "learning_rate": 6.62e-06,
      "loss": 0.0027,
      "step": 65070
    },
    {
      "epoch": 3.4709333333333334,
      "grad_norm": 0.10167068243026733,
      "learning_rate": 6.613333333333334e-06,
      "loss": 0.003,
      "step": 65080
    },
    {
      "epoch": 3.4714666666666667,
      "grad_norm": 0.15666943788528442,
      "learning_rate": 6.606666666666666e-06,
      "loss": 0.0017,
      "step": 65090
    },
    {
      "epoch": 3.472,
      "grad_norm": 0.3233848512172699,
      "learning_rate": 6.6e-06,
      "loss": 0.0032,
      "step": 65100
    },
    {
      "epoch": 3.4725333333333332,
      "grad_norm": 0.25541624426841736,
      "learning_rate": 6.5933333333333335e-06,
      "loss": 0.0019,
      "step": 65110
    },
    {
      "epoch": 3.4730666666666665,
      "grad_norm": 0.22245542705059052,
      "learning_rate": 6.586666666666667e-06,
      "loss": 0.0024,
      "step": 65120
    },
    {
      "epoch": 3.4736000000000002,
      "grad_norm": 0.45929619669914246,
      "learning_rate": 6.58e-06,
      "loss": 0.0022,
      "step": 65130
    },
    {
      "epoch": 3.4741333333333335,
      "grad_norm": 0.38625478744506836,
      "learning_rate": 6.5733333333333345e-06,
      "loss": 0.0017,
      "step": 65140
    },
    {
      "epoch": 3.474666666666667,
      "grad_norm": 0.20517463982105255,
      "learning_rate": 6.566666666666667e-06,
      "loss": 0.0038,
      "step": 65150
    },
    {
      "epoch": 3.4752,
      "grad_norm": 0.13398313522338867,
      "learning_rate": 6.560000000000001e-06,
      "loss": 0.0027,
      "step": 65160
    },
    {
      "epoch": 3.4757333333333333,
      "grad_norm": 0.17587298154830933,
      "learning_rate": 6.553333333333334e-06,
      "loss": 0.0019,
      "step": 65170
    },
    {
      "epoch": 3.4762666666666666,
      "grad_norm": 0.42570072412490845,
      "learning_rate": 6.546666666666668e-06,
      "loss": 0.0022,
      "step": 65180
    },
    {
      "epoch": 3.4768,
      "grad_norm": 0.23885130882263184,
      "learning_rate": 6.54e-06,
      "loss": 0.0031,
      "step": 65190
    },
    {
      "epoch": 3.477333333333333,
      "grad_norm": 0.45172807574272156,
      "learning_rate": 6.533333333333333e-06,
      "loss": 0.0026,
      "step": 65200
    },
    {
      "epoch": 3.4778666666666664,
      "grad_norm": 0.11040958762168884,
      "learning_rate": 6.526666666666667e-06,
      "loss": 0.002,
      "step": 65210
    },
    {
      "epoch": 3.4784,
      "grad_norm": 0.1191905215382576,
      "learning_rate": 6.519999999999999e-06,
      "loss": 0.0018,
      "step": 65220
    },
    {
      "epoch": 3.4789333333333334,
      "grad_norm": 0.11017879843711853,
      "learning_rate": 6.513333333333333e-06,
      "loss": 0.0026,
      "step": 65230
    },
    {
      "epoch": 3.4794666666666667,
      "grad_norm": 0.21021521091461182,
      "learning_rate": 6.5066666666666665e-06,
      "loss": 0.0024,
      "step": 65240
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.46840763092041016,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 0.0033,
      "step": 65250
    },
    {
      "epoch": 3.4805333333333333,
      "grad_norm": 0.10653773695230484,
      "learning_rate": 6.4933333333333336e-06,
      "loss": 0.0024,
      "step": 65260
    },
    {
      "epoch": 3.4810666666666665,
      "grad_norm": 0.4304622709751129,
      "learning_rate": 6.4866666666666675e-06,
      "loss": 0.0025,
      "step": 65270
    },
    {
      "epoch": 3.4816,
      "grad_norm": 0.21139760315418243,
      "learning_rate": 6.48e-06,
      "loss": 0.0021,
      "step": 65280
    },
    {
      "epoch": 3.4821333333333335,
      "grad_norm": 0.17016632854938507,
      "learning_rate": 6.473333333333334e-06,
      "loss": 0.002,
      "step": 65290
    },
    {
      "epoch": 3.482666666666667,
      "grad_norm": 0.41180381178855896,
      "learning_rate": 6.466666666666667e-06,
      "loss": 0.0031,
      "step": 65300
    },
    {
      "epoch": 3.4832,
      "grad_norm": 0.36983373761177063,
      "learning_rate": 6.460000000000001e-06,
      "loss": 0.0035,
      "step": 65310
    },
    {
      "epoch": 3.4837333333333333,
      "grad_norm": 0.1992855966091156,
      "learning_rate": 6.453333333333333e-06,
      "loss": 0.0018,
      "step": 65320
    },
    {
      "epoch": 3.4842666666666666,
      "grad_norm": 0.08232240378856659,
      "learning_rate": 6.446666666666668e-06,
      "loss": 0.0022,
      "step": 65330
    },
    {
      "epoch": 3.4848,
      "grad_norm": 0.1545913815498352,
      "learning_rate": 6.44e-06,
      "loss": 0.002,
      "step": 65340
    },
    {
      "epoch": 3.485333333333333,
      "grad_norm": 0.41892221570014954,
      "learning_rate": 6.433333333333334e-06,
      "loss": 0.0026,
      "step": 65350
    },
    {
      "epoch": 3.4858666666666664,
      "grad_norm": 0.3618175685405731,
      "learning_rate": 6.426666666666667e-06,
      "loss": 0.0018,
      "step": 65360
    },
    {
      "epoch": 3.4864,
      "grad_norm": 0.3081452250480652,
      "learning_rate": 6.4199999999999995e-06,
      "loss": 0.0037,
      "step": 65370
    },
    {
      "epoch": 3.4869333333333334,
      "grad_norm": 0.3328853249549866,
      "learning_rate": 6.4133333333333335e-06,
      "loss": 0.0019,
      "step": 65380
    },
    {
      "epoch": 3.4874666666666667,
      "grad_norm": 0.2976994514465332,
      "learning_rate": 6.406666666666667e-06,
      "loss": 0.0026,
      "step": 65390
    },
    {
      "epoch": 3.488,
      "grad_norm": 0.24763165414333344,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.0024,
      "step": 65400
    },
    {
      "epoch": 3.4885333333333333,
      "grad_norm": 0.6074098348617554,
      "learning_rate": 6.393333333333333e-06,
      "loss": 0.0023,
      "step": 65410
    },
    {
      "epoch": 3.4890666666666665,
      "grad_norm": 0.4730391502380371,
      "learning_rate": 6.386666666666667e-06,
      "loss": 0.004,
      "step": 65420
    },
    {
      "epoch": 3.4896,
      "grad_norm": 0.3563368022441864,
      "learning_rate": 6.38e-06,
      "loss": 0.002,
      "step": 65430
    },
    {
      "epoch": 3.4901333333333335,
      "grad_norm": 0.20447604358196259,
      "learning_rate": 6.373333333333334e-06,
      "loss": 0.0024,
      "step": 65440
    },
    {
      "epoch": 3.490666666666667,
      "grad_norm": 0.2979745864868164,
      "learning_rate": 6.366666666666667e-06,
      "loss": 0.0026,
      "step": 65450
    },
    {
      "epoch": 3.4912,
      "grad_norm": 0.2809487581253052,
      "learning_rate": 6.360000000000001e-06,
      "loss": 0.0027,
      "step": 65460
    },
    {
      "epoch": 3.4917333333333334,
      "grad_norm": 0.2251027673482895,
      "learning_rate": 6.353333333333333e-06,
      "loss": 0.0019,
      "step": 65470
    },
    {
      "epoch": 3.4922666666666666,
      "grad_norm": 0.08310529589653015,
      "learning_rate": 6.346666666666667e-06,
      "loss": 0.0025,
      "step": 65480
    },
    {
      "epoch": 3.4928,
      "grad_norm": 0.2385128289461136,
      "learning_rate": 6.34e-06,
      "loss": 0.0017,
      "step": 65490
    },
    {
      "epoch": 3.493333333333333,
      "grad_norm": 0.24641048908233643,
      "learning_rate": 6.333333333333334e-06,
      "loss": 0.0034,
      "step": 65500
    },
    {
      "epoch": 3.4938666666666665,
      "grad_norm": 0.1505209505558014,
      "learning_rate": 6.3266666666666665e-06,
      "loss": 0.0018,
      "step": 65510
    },
    {
      "epoch": 3.4944,
      "grad_norm": 0.11512818187475204,
      "learning_rate": 6.320000000000001e-06,
      "loss": 0.0029,
      "step": 65520
    },
    {
      "epoch": 3.4949333333333334,
      "grad_norm": 0.27110370993614197,
      "learning_rate": 6.313333333333334e-06,
      "loss": 0.0019,
      "step": 65530
    },
    {
      "epoch": 3.4954666666666667,
      "grad_norm": 0.42434924840927124,
      "learning_rate": 6.306666666666666e-06,
      "loss": 0.0019,
      "step": 65540
    },
    {
      "epoch": 3.496,
      "grad_norm": 0.1504310816526413,
      "learning_rate": 6.300000000000001e-06,
      "loss": 0.0018,
      "step": 65550
    },
    {
      "epoch": 3.4965333333333333,
      "grad_norm": 0.1034771203994751,
      "learning_rate": 6.293333333333333e-06,
      "loss": 0.0023,
      "step": 65560
    },
    {
      "epoch": 3.4970666666666665,
      "grad_norm": 0.161522775888443,
      "learning_rate": 6.286666666666667e-06,
      "loss": 0.0021,
      "step": 65570
    },
    {
      "epoch": 3.4976,
      "grad_norm": 0.28899767994880676,
      "learning_rate": 6.28e-06,
      "loss": 0.002,
      "step": 65580
    },
    {
      "epoch": 3.4981333333333335,
      "grad_norm": 0.2360289841890335,
      "learning_rate": 6.273333333333334e-06,
      "loss": 0.002,
      "step": 65590
    },
    {
      "epoch": 3.498666666666667,
      "grad_norm": 0.3969678580760956,
      "learning_rate": 6.266666666666666e-06,
      "loss": 0.0019,
      "step": 65600
    },
    {
      "epoch": 3.4992,
      "grad_norm": 0.08933275192975998,
      "learning_rate": 6.26e-06,
      "loss": 0.0025,
      "step": 65610
    },
    {
      "epoch": 3.4997333333333334,
      "grad_norm": 0.10101556032896042,
      "learning_rate": 6.253333333333333e-06,
      "loss": 0.0016,
      "step": 65620
    },
    {
      "epoch": 3.5002666666666666,
      "grad_norm": 0.36470839381217957,
      "learning_rate": 6.2466666666666664e-06,
      "loss": 0.0027,
      "step": 65630
    },
    {
      "epoch": 3.5008,
      "grad_norm": 0.47941601276397705,
      "learning_rate": 6.24e-06,
      "loss": 0.002,
      "step": 65640
    },
    {
      "epoch": 3.501333333333333,
      "grad_norm": 0.12192012369632721,
      "learning_rate": 6.2333333333333335e-06,
      "loss": 0.0018,
      "step": 65650
    },
    {
      "epoch": 3.5018666666666665,
      "grad_norm": 0.26477837562561035,
      "learning_rate": 6.226666666666667e-06,
      "loss": 0.0017,
      "step": 65660
    },
    {
      "epoch": 3.5023999999999997,
      "grad_norm": 0.12101604044437408,
      "learning_rate": 6.22e-06,
      "loss": 0.002,
      "step": 65670
    },
    {
      "epoch": 3.5029333333333335,
      "grad_norm": 0.35040605068206787,
      "learning_rate": 6.213333333333334e-06,
      "loss": 0.0017,
      "step": 65680
    },
    {
      "epoch": 3.5034666666666667,
      "grad_norm": 0.21768516302108765,
      "learning_rate": 6.206666666666667e-06,
      "loss": 0.0019,
      "step": 65690
    },
    {
      "epoch": 3.504,
      "grad_norm": 0.10976985841989517,
      "learning_rate": 6.2e-06,
      "loss": 0.0018,
      "step": 65700
    },
    {
      "epoch": 3.5045333333333333,
      "grad_norm": 0.08776390552520752,
      "learning_rate": 6.193333333333334e-06,
      "loss": 0.0022,
      "step": 65710
    },
    {
      "epoch": 3.5050666666666666,
      "grad_norm": 0.32255467772483826,
      "learning_rate": 6.186666666666667e-06,
      "loss": 0.0039,
      "step": 65720
    },
    {
      "epoch": 3.5056000000000003,
      "grad_norm": 0.21074643731117249,
      "learning_rate": 6.18e-06,
      "loss": 0.0022,
      "step": 65730
    },
    {
      "epoch": 3.5061333333333335,
      "grad_norm": 0.4682224988937378,
      "learning_rate": 6.173333333333334e-06,
      "loss": 0.0023,
      "step": 65740
    },
    {
      "epoch": 3.506666666666667,
      "grad_norm": 0.16844359040260315,
      "learning_rate": 6.166666666666667e-06,
      "loss": 0.0024,
      "step": 65750
    },
    {
      "epoch": 3.5072,
      "grad_norm": 0.37039700150489807,
      "learning_rate": 6.16e-06,
      "loss": 0.0026,
      "step": 65760
    },
    {
      "epoch": 3.5077333333333334,
      "grad_norm": 0.16790345311164856,
      "learning_rate": 6.153333333333334e-06,
      "loss": 0.0015,
      "step": 65770
    },
    {
      "epoch": 3.5082666666666666,
      "grad_norm": 0.26604387164115906,
      "learning_rate": 6.146666666666667e-06,
      "loss": 0.0023,
      "step": 65780
    },
    {
      "epoch": 3.5088,
      "grad_norm": 0.1417938619852066,
      "learning_rate": 6.1400000000000005e-06,
      "loss": 0.0022,
      "step": 65790
    },
    {
      "epoch": 3.509333333333333,
      "grad_norm": 0.17589375376701355,
      "learning_rate": 6.133333333333334e-06,
      "loss": 0.0019,
      "step": 65800
    },
    {
      "epoch": 3.5098666666666665,
      "grad_norm": 0.11559566110372543,
      "learning_rate": 6.126666666666667e-06,
      "loss": 0.0027,
      "step": 65810
    },
    {
      "epoch": 3.5103999999999997,
      "grad_norm": 0.11021142452955246,
      "learning_rate": 6.12e-06,
      "loss": 0.0031,
      "step": 65820
    },
    {
      "epoch": 3.5109333333333335,
      "grad_norm": 0.26434949040412903,
      "learning_rate": 6.113333333333334e-06,
      "loss": 0.0026,
      "step": 65830
    },
    {
      "epoch": 3.5114666666666667,
      "grad_norm": 0.32354211807250977,
      "learning_rate": 6.106666666666667e-06,
      "loss": 0.0027,
      "step": 65840
    },
    {
      "epoch": 3.512,
      "grad_norm": 0.2209690660238266,
      "learning_rate": 6.1e-06,
      "loss": 0.0019,
      "step": 65850
    },
    {
      "epoch": 3.5125333333333333,
      "grad_norm": 0.25232750177383423,
      "learning_rate": 6.093333333333333e-06,
      "loss": 0.0017,
      "step": 65860
    },
    {
      "epoch": 3.5130666666666666,
      "grad_norm": 0.34525033831596375,
      "learning_rate": 6.086666666666667e-06,
      "loss": 0.0024,
      "step": 65870
    },
    {
      "epoch": 3.5136,
      "grad_norm": 0.26211097836494446,
      "learning_rate": 6.08e-06,
      "loss": 0.0024,
      "step": 65880
    },
    {
      "epoch": 3.5141333333333336,
      "grad_norm": 0.3634966313838959,
      "learning_rate": 6.073333333333333e-06,
      "loss": 0.0024,
      "step": 65890
    },
    {
      "epoch": 3.514666666666667,
      "grad_norm": 0.16229991614818573,
      "learning_rate": 6.066666666666667e-06,
      "loss": 0.0018,
      "step": 65900
    },
    {
      "epoch": 3.5152,
      "grad_norm": 0.15711840987205505,
      "learning_rate": 6.0600000000000004e-06,
      "loss": 0.002,
      "step": 65910
    },
    {
      "epoch": 3.5157333333333334,
      "grad_norm": 0.21995729207992554,
      "learning_rate": 6.0533333333333335e-06,
      "loss": 0.0025,
      "step": 65920
    },
    {
      "epoch": 3.5162666666666667,
      "grad_norm": 0.21253041923046112,
      "learning_rate": 6.0466666666666675e-06,
      "loss": 0.0027,
      "step": 65930
    },
    {
      "epoch": 3.5168,
      "grad_norm": 0.28347834944725037,
      "learning_rate": 6.040000000000001e-06,
      "loss": 0.0037,
      "step": 65940
    },
    {
      "epoch": 3.517333333333333,
      "grad_norm": 0.1905134618282318,
      "learning_rate": 6.033333333333334e-06,
      "loss": 0.0022,
      "step": 65950
    },
    {
      "epoch": 3.5178666666666665,
      "grad_norm": 0.6633945107460022,
      "learning_rate": 6.026666666666667e-06,
      "loss": 0.0042,
      "step": 65960
    },
    {
      "epoch": 3.5183999999999997,
      "grad_norm": 0.19397155940532684,
      "learning_rate": 6.02e-06,
      "loss": 0.0017,
      "step": 65970
    },
    {
      "epoch": 3.5189333333333335,
      "grad_norm": 0.2293858826160431,
      "learning_rate": 6.013333333333333e-06,
      "loss": 0.002,
      "step": 65980
    },
    {
      "epoch": 3.5194666666666667,
      "grad_norm": 0.14094996452331543,
      "learning_rate": 6.006666666666667e-06,
      "loss": 0.0036,
      "step": 65990
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.2001369148492813,
      "learning_rate": 6e-06,
      "loss": 0.0025,
      "step": 66000
    },
    {
      "epoch": 3.5205333333333333,
      "grad_norm": 0.3227444291114807,
      "learning_rate": 5.993333333333333e-06,
      "loss": 0.003,
      "step": 66010
    },
    {
      "epoch": 3.5210666666666666,
      "grad_norm": 0.10826542228460312,
      "learning_rate": 5.986666666666667e-06,
      "loss": 0.002,
      "step": 66020
    },
    {
      "epoch": 3.5216,
      "grad_norm": 0.29060468077659607,
      "learning_rate": 5.98e-06,
      "loss": 0.0021,
      "step": 66030
    },
    {
      "epoch": 3.5221333333333336,
      "grad_norm": 0.24774198234081268,
      "learning_rate": 5.9733333333333335e-06,
      "loss": 0.0021,
      "step": 66040
    },
    {
      "epoch": 3.522666666666667,
      "grad_norm": 0.3088345527648926,
      "learning_rate": 5.9666666666666666e-06,
      "loss": 0.0019,
      "step": 66050
    },
    {
      "epoch": 3.5232,
      "grad_norm": 0.1294887810945511,
      "learning_rate": 5.9600000000000005e-06,
      "loss": 0.002,
      "step": 66060
    },
    {
      "epoch": 3.5237333333333334,
      "grad_norm": 0.13206595182418823,
      "learning_rate": 5.953333333333334e-06,
      "loss": 0.0035,
      "step": 66070
    },
    {
      "epoch": 3.5242666666666667,
      "grad_norm": 0.20169711112976074,
      "learning_rate": 5.946666666666667e-06,
      "loss": 0.0027,
      "step": 66080
    },
    {
      "epoch": 3.5248,
      "grad_norm": 0.4472859799861908,
      "learning_rate": 5.940000000000001e-06,
      "loss": 0.003,
      "step": 66090
    },
    {
      "epoch": 3.525333333333333,
      "grad_norm": 0.07332999259233475,
      "learning_rate": 5.933333333333334e-06,
      "loss": 0.0022,
      "step": 66100
    },
    {
      "epoch": 3.5258666666666665,
      "grad_norm": 0.5235381722450256,
      "learning_rate": 5.926666666666667e-06,
      "loss": 0.0024,
      "step": 66110
    },
    {
      "epoch": 3.5263999999999998,
      "grad_norm": 0.16540855169296265,
      "learning_rate": 5.920000000000001e-06,
      "loss": 0.0026,
      "step": 66120
    },
    {
      "epoch": 3.5269333333333335,
      "grad_norm": 0.25874757766723633,
      "learning_rate": 5.913333333333334e-06,
      "loss": 0.0016,
      "step": 66130
    },
    {
      "epoch": 3.5274666666666668,
      "grad_norm": 0.11600877344608307,
      "learning_rate": 5.906666666666667e-06,
      "loss": 0.0032,
      "step": 66140
    },
    {
      "epoch": 3.528,
      "grad_norm": 0.3643805682659149,
      "learning_rate": 5.9e-06,
      "loss": 0.0021,
      "step": 66150
    },
    {
      "epoch": 3.5285333333333333,
      "grad_norm": 0.09874199330806732,
      "learning_rate": 5.893333333333333e-06,
      "loss": 0.0025,
      "step": 66160
    },
    {
      "epoch": 3.5290666666666666,
      "grad_norm": 0.3361271619796753,
      "learning_rate": 5.8866666666666665e-06,
      "loss": 0.003,
      "step": 66170
    },
    {
      "epoch": 3.5296,
      "grad_norm": 0.17149174213409424,
      "learning_rate": 5.8800000000000005e-06,
      "loss": 0.0019,
      "step": 66180
    },
    {
      "epoch": 3.5301333333333336,
      "grad_norm": 0.15255914628505707,
      "learning_rate": 5.8733333333333336e-06,
      "loss": 0.002,
      "step": 66190
    },
    {
      "epoch": 3.530666666666667,
      "grad_norm": 0.42831265926361084,
      "learning_rate": 5.866666666666667e-06,
      "loss": 0.0031,
      "step": 66200
    },
    {
      "epoch": 3.5312,
      "grad_norm": 0.13412271440029144,
      "learning_rate": 5.86e-06,
      "loss": 0.0024,
      "step": 66210
    },
    {
      "epoch": 3.5317333333333334,
      "grad_norm": 0.26206958293914795,
      "learning_rate": 5.853333333333334e-06,
      "loss": 0.0018,
      "step": 66220
    },
    {
      "epoch": 3.5322666666666667,
      "grad_norm": 0.22468139231204987,
      "learning_rate": 5.846666666666667e-06,
      "loss": 0.0018,
      "step": 66230
    },
    {
      "epoch": 3.5328,
      "grad_norm": 0.11900586634874344,
      "learning_rate": 5.84e-06,
      "loss": 0.0017,
      "step": 66240
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 0.4005647301673889,
      "learning_rate": 5.833333333333334e-06,
      "loss": 0.0024,
      "step": 66250
    },
    {
      "epoch": 3.5338666666666665,
      "grad_norm": 0.164677232503891,
      "learning_rate": 5.826666666666667e-06,
      "loss": 0.0028,
      "step": 66260
    },
    {
      "epoch": 3.5343999999999998,
      "grad_norm": 0.30625805258750916,
      "learning_rate": 5.82e-06,
      "loss": 0.0019,
      "step": 66270
    },
    {
      "epoch": 3.5349333333333335,
      "grad_norm": 0.35061490535736084,
      "learning_rate": 5.813333333333334e-06,
      "loss": 0.0022,
      "step": 66280
    },
    {
      "epoch": 3.5354666666666668,
      "grad_norm": 0.4601433575153351,
      "learning_rate": 5.806666666666667e-06,
      "loss": 0.0026,
      "step": 66290
    },
    {
      "epoch": 3.536,
      "grad_norm": 0.4555782079696655,
      "learning_rate": 5.8e-06,
      "loss": 0.0018,
      "step": 66300
    },
    {
      "epoch": 3.5365333333333333,
      "grad_norm": 0.24509623646736145,
      "learning_rate": 5.793333333333334e-06,
      "loss": 0.0031,
      "step": 66310
    },
    {
      "epoch": 3.5370666666666666,
      "grad_norm": 0.1448918581008911,
      "learning_rate": 5.786666666666667e-06,
      "loss": 0.0018,
      "step": 66320
    },
    {
      "epoch": 3.5376,
      "grad_norm": 0.34294989705085754,
      "learning_rate": 5.78e-06,
      "loss": 0.0032,
      "step": 66330
    },
    {
      "epoch": 3.5381333333333336,
      "grad_norm": 0.3548142611980438,
      "learning_rate": 5.773333333333334e-06,
      "loss": 0.002,
      "step": 66340
    },
    {
      "epoch": 3.538666666666667,
      "grad_norm": 0.16365136206150055,
      "learning_rate": 5.766666666666667e-06,
      "loss": 0.0019,
      "step": 66350
    },
    {
      "epoch": 3.5392,
      "grad_norm": 0.3631819486618042,
      "learning_rate": 5.76e-06,
      "loss": 0.0017,
      "step": 66360
    },
    {
      "epoch": 3.5397333333333334,
      "grad_norm": 0.07636336237192154,
      "learning_rate": 5.753333333333334e-06,
      "loss": 0.0022,
      "step": 66370
    },
    {
      "epoch": 3.5402666666666667,
      "grad_norm": 0.30238524079322815,
      "learning_rate": 5.746666666666667e-06,
      "loss": 0.0019,
      "step": 66380
    },
    {
      "epoch": 3.5408,
      "grad_norm": 0.08746182173490524,
      "learning_rate": 5.74e-06,
      "loss": 0.0019,
      "step": 66390
    },
    {
      "epoch": 3.541333333333333,
      "grad_norm": 0.20646308362483978,
      "learning_rate": 5.733333333333333e-06,
      "loss": 0.0017,
      "step": 66400
    },
    {
      "epoch": 3.5418666666666665,
      "grad_norm": 0.24850764870643616,
      "learning_rate": 5.726666666666667e-06,
      "loss": 0.0026,
      "step": 66410
    },
    {
      "epoch": 3.5423999999999998,
      "grad_norm": 0.19890132546424866,
      "learning_rate": 5.72e-06,
      "loss": 0.0029,
      "step": 66420
    },
    {
      "epoch": 3.5429333333333335,
      "grad_norm": 0.1270277500152588,
      "learning_rate": 5.713333333333333e-06,
      "loss": 0.0016,
      "step": 66430
    },
    {
      "epoch": 3.5434666666666668,
      "grad_norm": 0.2031937837600708,
      "learning_rate": 5.706666666666667e-06,
      "loss": 0.0027,
      "step": 66440
    },
    {
      "epoch": 3.544,
      "grad_norm": 0.23055630922317505,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 0.0032,
      "step": 66450
    },
    {
      "epoch": 3.5445333333333333,
      "grad_norm": 0.17683956027030945,
      "learning_rate": 5.693333333333334e-06,
      "loss": 0.0025,
      "step": 66460
    },
    {
      "epoch": 3.5450666666666666,
      "grad_norm": 0.23537105321884155,
      "learning_rate": 5.6866666666666676e-06,
      "loss": 0.002,
      "step": 66470
    },
    {
      "epoch": 3.5456,
      "grad_norm": 0.5014141798019409,
      "learning_rate": 5.680000000000001e-06,
      "loss": 0.0018,
      "step": 66480
    },
    {
      "epoch": 3.5461333333333336,
      "grad_norm": 0.359833687543869,
      "learning_rate": 5.673333333333333e-06,
      "loss": 0.0018,
      "step": 66490
    },
    {
      "epoch": 3.546666666666667,
      "grad_norm": 0.14031292498111725,
      "learning_rate": 5.666666666666667e-06,
      "loss": 0.0017,
      "step": 66500
    },
    {
      "epoch": 3.5472,
      "grad_norm": 0.11997532099485397,
      "learning_rate": 5.66e-06,
      "loss": 0.003,
      "step": 66510
    },
    {
      "epoch": 3.5477333333333334,
      "grad_norm": 0.5176597833633423,
      "learning_rate": 5.653333333333333e-06,
      "loss": 0.0019,
      "step": 66520
    },
    {
      "epoch": 3.5482666666666667,
      "grad_norm": 0.19398480653762817,
      "learning_rate": 5.646666666666667e-06,
      "loss": 0.0031,
      "step": 66530
    },
    {
      "epoch": 3.5488,
      "grad_norm": 0.1607607901096344,
      "learning_rate": 5.64e-06,
      "loss": 0.002,
      "step": 66540
    },
    {
      "epoch": 3.5493333333333332,
      "grad_norm": 0.11014828830957413,
      "learning_rate": 5.633333333333333e-06,
      "loss": 0.0019,
      "step": 66550
    },
    {
      "epoch": 3.5498666666666665,
      "grad_norm": 0.23070120811462402,
      "learning_rate": 5.626666666666667e-06,
      "loss": 0.0018,
      "step": 66560
    },
    {
      "epoch": 3.5504,
      "grad_norm": 0.6045531630516052,
      "learning_rate": 5.62e-06,
      "loss": 0.0021,
      "step": 66570
    },
    {
      "epoch": 3.5509333333333335,
      "grad_norm": 0.09871549904346466,
      "learning_rate": 5.6133333333333335e-06,
      "loss": 0.0036,
      "step": 66580
    },
    {
      "epoch": 3.5514666666666668,
      "grad_norm": 0.38860782980918884,
      "learning_rate": 5.606666666666667e-06,
      "loss": 0.004,
      "step": 66590
    },
    {
      "epoch": 3.552,
      "grad_norm": 0.16880646347999573,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.0018,
      "step": 66600
    },
    {
      "epoch": 3.5525333333333333,
      "grad_norm": 0.492735892534256,
      "learning_rate": 5.593333333333334e-06,
      "loss": 0.0035,
      "step": 66610
    },
    {
      "epoch": 3.5530666666666666,
      "grad_norm": 0.3391277492046356,
      "learning_rate": 5.586666666666667e-06,
      "loss": 0.0026,
      "step": 66620
    },
    {
      "epoch": 3.5536,
      "grad_norm": 0.1282232105731964,
      "learning_rate": 5.580000000000001e-06,
      "loss": 0.0021,
      "step": 66630
    },
    {
      "epoch": 3.5541333333333336,
      "grad_norm": 0.24166759848594666,
      "learning_rate": 5.573333333333334e-06,
      "loss": 0.0019,
      "step": 66640
    },
    {
      "epoch": 3.554666666666667,
      "grad_norm": 0.38394254446029663,
      "learning_rate": 5.566666666666667e-06,
      "loss": 0.0036,
      "step": 66650
    },
    {
      "epoch": 3.5552,
      "grad_norm": 0.17163997888565063,
      "learning_rate": 5.56e-06,
      "loss": 0.002,
      "step": 66660
    },
    {
      "epoch": 3.5557333333333334,
      "grad_norm": 0.1679197996854782,
      "learning_rate": 5.553333333333333e-06,
      "loss": 0.0016,
      "step": 66670
    },
    {
      "epoch": 3.5562666666666667,
      "grad_norm": 0.5144314765930176,
      "learning_rate": 5.546666666666666e-06,
      "loss": 0.0023,
      "step": 66680
    },
    {
      "epoch": 3.5568,
      "grad_norm": 0.057176459580659866,
      "learning_rate": 5.54e-06,
      "loss": 0.002,
      "step": 66690
    },
    {
      "epoch": 3.5573333333333332,
      "grad_norm": 0.1596696674823761,
      "learning_rate": 5.5333333333333334e-06,
      "loss": 0.0024,
      "step": 66700
    },
    {
      "epoch": 3.5578666666666665,
      "grad_norm": 0.2632695436477661,
      "learning_rate": 5.5266666666666666e-06,
      "loss": 0.0022,
      "step": 66710
    },
    {
      "epoch": 3.5584,
      "grad_norm": 0.27047252655029297,
      "learning_rate": 5.5200000000000005e-06,
      "loss": 0.0019,
      "step": 66720
    },
    {
      "epoch": 3.558933333333333,
      "grad_norm": 0.3175152540206909,
      "learning_rate": 5.513333333333334e-06,
      "loss": 0.0031,
      "step": 66730
    },
    {
      "epoch": 3.559466666666667,
      "grad_norm": 0.5521640181541443,
      "learning_rate": 5.506666666666667e-06,
      "loss": 0.0025,
      "step": 66740
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.12771056592464447,
      "learning_rate": 5.500000000000001e-06,
      "loss": 0.0021,
      "step": 66750
    },
    {
      "epoch": 3.5605333333333333,
      "grad_norm": 0.17432627081871033,
      "learning_rate": 5.493333333333334e-06,
      "loss": 0.0026,
      "step": 66760
    },
    {
      "epoch": 3.5610666666666666,
      "grad_norm": 0.49110400676727295,
      "learning_rate": 5.486666666666667e-06,
      "loss": 0.0031,
      "step": 66770
    },
    {
      "epoch": 3.5616,
      "grad_norm": 0.4654538035392761,
      "learning_rate": 5.48e-06,
      "loss": 0.002,
      "step": 66780
    },
    {
      "epoch": 3.5621333333333336,
      "grad_norm": 0.135788694024086,
      "learning_rate": 5.473333333333334e-06,
      "loss": 0.0027,
      "step": 66790
    },
    {
      "epoch": 3.562666666666667,
      "grad_norm": 0.11722688376903534,
      "learning_rate": 5.466666666666667e-06,
      "loss": 0.0025,
      "step": 66800
    },
    {
      "epoch": 3.5632,
      "grad_norm": 0.6996089816093445,
      "learning_rate": 5.46e-06,
      "loss": 0.0033,
      "step": 66810
    },
    {
      "epoch": 3.5637333333333334,
      "grad_norm": 0.20554694533348083,
      "learning_rate": 5.453333333333334e-06,
      "loss": 0.0034,
      "step": 66820
    },
    {
      "epoch": 3.5642666666666667,
      "grad_norm": 0.08157254010438919,
      "learning_rate": 5.4466666666666665e-06,
      "loss": 0.0024,
      "step": 66830
    },
    {
      "epoch": 3.5648,
      "grad_norm": 0.24456588923931122,
      "learning_rate": 5.44e-06,
      "loss": 0.0026,
      "step": 66840
    },
    {
      "epoch": 3.5653333333333332,
      "grad_norm": 0.36081406474113464,
      "learning_rate": 5.4333333333333335e-06,
      "loss": 0.002,
      "step": 66850
    },
    {
      "epoch": 3.5658666666666665,
      "grad_norm": 0.09960612654685974,
      "learning_rate": 5.426666666666667e-06,
      "loss": 0.0018,
      "step": 66860
    },
    {
      "epoch": 3.5664,
      "grad_norm": 0.14664240181446075,
      "learning_rate": 5.42e-06,
      "loss": 0.002,
      "step": 66870
    },
    {
      "epoch": 3.566933333333333,
      "grad_norm": 0.3054119944572449,
      "learning_rate": 5.413333333333334e-06,
      "loss": 0.0025,
      "step": 66880
    },
    {
      "epoch": 3.567466666666667,
      "grad_norm": 0.14279119670391083,
      "learning_rate": 5.406666666666667e-06,
      "loss": 0.0025,
      "step": 66890
    },
    {
      "epoch": 3.568,
      "grad_norm": 0.21085911989212036,
      "learning_rate": 5.4e-06,
      "loss": 0.0017,
      "step": 66900
    },
    {
      "epoch": 3.5685333333333333,
      "grad_norm": 0.5092277526855469,
      "learning_rate": 5.393333333333334e-06,
      "loss": 0.0019,
      "step": 66910
    },
    {
      "epoch": 3.5690666666666666,
      "grad_norm": 0.08946698158979416,
      "learning_rate": 5.386666666666667e-06,
      "loss": 0.0018,
      "step": 66920
    },
    {
      "epoch": 3.5696,
      "grad_norm": 0.27793779969215393,
      "learning_rate": 5.38e-06,
      "loss": 0.0033,
      "step": 66930
    },
    {
      "epoch": 3.5701333333333336,
      "grad_norm": 0.07899141311645508,
      "learning_rate": 5.373333333333333e-06,
      "loss": 0.0023,
      "step": 66940
    },
    {
      "epoch": 3.570666666666667,
      "grad_norm": 0.14873364567756653,
      "learning_rate": 5.366666666666667e-06,
      "loss": 0.0019,
      "step": 66950
    },
    {
      "epoch": 3.5712,
      "grad_norm": 0.5637486577033997,
      "learning_rate": 5.36e-06,
      "loss": 0.0026,
      "step": 66960
    },
    {
      "epoch": 3.5717333333333334,
      "grad_norm": 0.0851924791932106,
      "learning_rate": 5.3533333333333335e-06,
      "loss": 0.0017,
      "step": 66970
    },
    {
      "epoch": 3.5722666666666667,
      "grad_norm": 0.28847694396972656,
      "learning_rate": 5.3466666666666674e-06,
      "loss": 0.0016,
      "step": 66980
    },
    {
      "epoch": 3.5728,
      "grad_norm": 0.13454686105251312,
      "learning_rate": 5.3400000000000005e-06,
      "loss": 0.0027,
      "step": 66990
    },
    {
      "epoch": 3.5733333333333333,
      "grad_norm": 0.29452237486839294,
      "learning_rate": 5.333333333333334e-06,
      "loss": 0.0026,
      "step": 67000
    },
    {
      "epoch": 3.5738666666666665,
      "grad_norm": 0.19648753106594086,
      "learning_rate": 5.326666666666667e-06,
      "loss": 0.0028,
      "step": 67010
    },
    {
      "epoch": 3.5744,
      "grad_norm": 0.06814375519752502,
      "learning_rate": 5.32e-06,
      "loss": 0.0024,
      "step": 67020
    },
    {
      "epoch": 3.574933333333333,
      "grad_norm": 0.09029600024223328,
      "learning_rate": 5.313333333333333e-06,
      "loss": 0.0016,
      "step": 67030
    },
    {
      "epoch": 3.575466666666667,
      "grad_norm": 0.17065273225307465,
      "learning_rate": 5.306666666666667e-06,
      "loss": 0.0021,
      "step": 67040
    },
    {
      "epoch": 3.576,
      "grad_norm": 0.12054044008255005,
      "learning_rate": 5.3e-06,
      "loss": 0.0019,
      "step": 67050
    },
    {
      "epoch": 3.5765333333333333,
      "grad_norm": 0.4039180874824524,
      "learning_rate": 5.293333333333333e-06,
      "loss": 0.0018,
      "step": 67060
    },
    {
      "epoch": 3.5770666666666666,
      "grad_norm": 0.5880126357078552,
      "learning_rate": 5.286666666666667e-06,
      "loss": 0.002,
      "step": 67070
    },
    {
      "epoch": 3.5776,
      "grad_norm": 0.10632527619600296,
      "learning_rate": 5.28e-06,
      "loss": 0.0024,
      "step": 67080
    },
    {
      "epoch": 3.5781333333333336,
      "grad_norm": 0.16970646381378174,
      "learning_rate": 5.273333333333333e-06,
      "loss": 0.0018,
      "step": 67090
    },
    {
      "epoch": 3.578666666666667,
      "grad_norm": 0.3507266342639923,
      "learning_rate": 5.266666666666667e-06,
      "loss": 0.0019,
      "step": 67100
    },
    {
      "epoch": 3.5792,
      "grad_norm": 0.09607335925102234,
      "learning_rate": 5.2600000000000005e-06,
      "loss": 0.003,
      "step": 67110
    },
    {
      "epoch": 3.5797333333333334,
      "grad_norm": 0.15958531200885773,
      "learning_rate": 5.2533333333333336e-06,
      "loss": 0.0024,
      "step": 67120
    },
    {
      "epoch": 3.5802666666666667,
      "grad_norm": 0.23905524611473083,
      "learning_rate": 5.246666666666667e-06,
      "loss": 0.0016,
      "step": 67130
    },
    {
      "epoch": 3.5808,
      "grad_norm": 0.09653044492006302,
      "learning_rate": 5.240000000000001e-06,
      "loss": 0.0018,
      "step": 67140
    },
    {
      "epoch": 3.5813333333333333,
      "grad_norm": 0.23673352599143982,
      "learning_rate": 5.233333333333334e-06,
      "loss": 0.0017,
      "step": 67150
    },
    {
      "epoch": 3.5818666666666665,
      "grad_norm": 0.4876445233821869,
      "learning_rate": 5.226666666666667e-06,
      "loss": 0.002,
      "step": 67160
    },
    {
      "epoch": 3.5824,
      "grad_norm": 0.3769426941871643,
      "learning_rate": 5.220000000000001e-06,
      "loss": 0.0019,
      "step": 67170
    },
    {
      "epoch": 3.582933333333333,
      "grad_norm": 0.2303629070520401,
      "learning_rate": 5.213333333333333e-06,
      "loss": 0.0018,
      "step": 67180
    },
    {
      "epoch": 3.583466666666667,
      "grad_norm": 0.2257685363292694,
      "learning_rate": 5.206666666666666e-06,
      "loss": 0.0029,
      "step": 67190
    },
    {
      "epoch": 3.584,
      "grad_norm": 0.2621367573738098,
      "learning_rate": 5.2e-06,
      "loss": 0.0021,
      "step": 67200
    },
    {
      "epoch": 3.5845333333333333,
      "grad_norm": 0.11595553904771805,
      "learning_rate": 5.193333333333333e-06,
      "loss": 0.0016,
      "step": 67210
    },
    {
      "epoch": 3.5850666666666666,
      "grad_norm": 0.3849950432777405,
      "learning_rate": 5.186666666666666e-06,
      "loss": 0.0032,
      "step": 67220
    },
    {
      "epoch": 3.5856,
      "grad_norm": 0.16870081424713135,
      "learning_rate": 5.18e-06,
      "loss": 0.0021,
      "step": 67230
    },
    {
      "epoch": 3.586133333333333,
      "grad_norm": 0.11039553582668304,
      "learning_rate": 5.1733333333333335e-06,
      "loss": 0.002,
      "step": 67240
    },
    {
      "epoch": 3.586666666666667,
      "grad_norm": 0.2503603994846344,
      "learning_rate": 5.166666666666667e-06,
      "loss": 0.0019,
      "step": 67250
    },
    {
      "epoch": 3.5872,
      "grad_norm": 0.4566757082939148,
      "learning_rate": 5.1600000000000006e-06,
      "loss": 0.0028,
      "step": 67260
    },
    {
      "epoch": 3.5877333333333334,
      "grad_norm": 0.22301335632801056,
      "learning_rate": 5.153333333333334e-06,
      "loss": 0.0018,
      "step": 67270
    },
    {
      "epoch": 3.5882666666666667,
      "grad_norm": 0.2532079219818115,
      "learning_rate": 5.146666666666667e-06,
      "loss": 0.0019,
      "step": 67280
    },
    {
      "epoch": 3.5888,
      "grad_norm": 0.7698344588279724,
      "learning_rate": 5.140000000000001e-06,
      "loss": 0.0017,
      "step": 67290
    },
    {
      "epoch": 3.5893333333333333,
      "grad_norm": 0.3103189468383789,
      "learning_rate": 5.133333333333334e-06,
      "loss": 0.0023,
      "step": 67300
    },
    {
      "epoch": 3.5898666666666665,
      "grad_norm": 0.40761786699295044,
      "learning_rate": 5.126666666666667e-06,
      "loss": 0.0019,
      "step": 67310
    },
    {
      "epoch": 3.5904,
      "grad_norm": 0.29861441254615784,
      "learning_rate": 5.12e-06,
      "loss": 0.0017,
      "step": 67320
    },
    {
      "epoch": 3.590933333333333,
      "grad_norm": 0.24021269381046295,
      "learning_rate": 5.113333333333334e-06,
      "loss": 0.0024,
      "step": 67330
    },
    {
      "epoch": 3.591466666666667,
      "grad_norm": 0.4257866442203522,
      "learning_rate": 5.106666666666667e-06,
      "loss": 0.0018,
      "step": 67340
    },
    {
      "epoch": 3.592,
      "grad_norm": 0.2051209956407547,
      "learning_rate": 5.1e-06,
      "loss": 0.0017,
      "step": 67350
    },
    {
      "epoch": 3.5925333333333334,
      "grad_norm": 0.29722732305526733,
      "learning_rate": 5.093333333333333e-06,
      "loss": 0.0018,
      "step": 67360
    },
    {
      "epoch": 3.5930666666666666,
      "grad_norm": 0.5521097779273987,
      "learning_rate": 5.0866666666666665e-06,
      "loss": 0.0021,
      "step": 67370
    },
    {
      "epoch": 3.5936,
      "grad_norm": 0.08231957256793976,
      "learning_rate": 5.08e-06,
      "loss": 0.0029,
      "step": 67380
    },
    {
      "epoch": 3.594133333333333,
      "grad_norm": 0.19854706525802612,
      "learning_rate": 5.073333333333334e-06,
      "loss": 0.0032,
      "step": 67390
    },
    {
      "epoch": 3.594666666666667,
      "grad_norm": 0.3668239414691925,
      "learning_rate": 5.066666666666667e-06,
      "loss": 0.0023,
      "step": 67400
    },
    {
      "epoch": 3.5952,
      "grad_norm": 0.23938478529453278,
      "learning_rate": 5.06e-06,
      "loss": 0.0024,
      "step": 67410
    },
    {
      "epoch": 3.5957333333333334,
      "grad_norm": 0.41390424966812134,
      "learning_rate": 5.053333333333334e-06,
      "loss": 0.0025,
      "step": 67420
    },
    {
      "epoch": 3.5962666666666667,
      "grad_norm": 0.3019956946372986,
      "learning_rate": 5.046666666666667e-06,
      "loss": 0.0025,
      "step": 67430
    },
    {
      "epoch": 3.5968,
      "grad_norm": 0.15828976035118103,
      "learning_rate": 5.04e-06,
      "loss": 0.0025,
      "step": 67440
    },
    {
      "epoch": 3.5973333333333333,
      "grad_norm": 0.3581680953502655,
      "learning_rate": 5.033333333333334e-06,
      "loss": 0.0018,
      "step": 67450
    },
    {
      "epoch": 3.5978666666666665,
      "grad_norm": 0.10789772868156433,
      "learning_rate": 5.026666666666667e-06,
      "loss": 0.0018,
      "step": 67460
    },
    {
      "epoch": 3.5984,
      "grad_norm": 0.14278851449489594,
      "learning_rate": 5.02e-06,
      "loss": 0.0018,
      "step": 67470
    },
    {
      "epoch": 3.598933333333333,
      "grad_norm": 0.07007242739200592,
      "learning_rate": 5.013333333333334e-06,
      "loss": 0.0021,
      "step": 67480
    },
    {
      "epoch": 3.599466666666667,
      "grad_norm": 0.2482718676328659,
      "learning_rate": 5.006666666666667e-06,
      "loss": 0.0026,
      "step": 67490
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.171203151345253,
      "learning_rate": 5e-06,
      "loss": 0.002,
      "step": 67500
    },
    {
      "epoch": 3.6005333333333334,
      "grad_norm": 0.1769273430109024,
      "learning_rate": 4.9933333333333335e-06,
      "loss": 0.0025,
      "step": 67510
    },
    {
      "epoch": 3.6010666666666666,
      "grad_norm": 0.14975255727767944,
      "learning_rate": 4.986666666666667e-06,
      "loss": 0.0048,
      "step": 67520
    },
    {
      "epoch": 3.6016,
      "grad_norm": 0.41845327615737915,
      "learning_rate": 4.98e-06,
      "loss": 0.0021,
      "step": 67530
    },
    {
      "epoch": 3.602133333333333,
      "grad_norm": 0.38673165440559387,
      "learning_rate": 4.973333333333334e-06,
      "loss": 0.0018,
      "step": 67540
    },
    {
      "epoch": 3.602666666666667,
      "grad_norm": 0.3186061680316925,
      "learning_rate": 4.966666666666667e-06,
      "loss": 0.0017,
      "step": 67550
    },
    {
      "epoch": 3.6032,
      "grad_norm": 0.14398567378520966,
      "learning_rate": 4.96e-06,
      "loss": 0.0024,
      "step": 67560
    },
    {
      "epoch": 3.6037333333333335,
      "grad_norm": 0.273735910654068,
      "learning_rate": 4.953333333333333e-06,
      "loss": 0.0016,
      "step": 67570
    },
    {
      "epoch": 3.6042666666666667,
      "grad_norm": 0.15526410937309265,
      "learning_rate": 4.946666666666667e-06,
      "loss": 0.002,
      "step": 67580
    },
    {
      "epoch": 3.6048,
      "grad_norm": 0.2386426329612732,
      "learning_rate": 4.94e-06,
      "loss": 0.0036,
      "step": 67590
    },
    {
      "epoch": 3.6053333333333333,
      "grad_norm": 0.06769444793462753,
      "learning_rate": 4.933333333333333e-06,
      "loss": 0.0019,
      "step": 67600
    },
    {
      "epoch": 3.6058666666666666,
      "grad_norm": 0.17312633991241455,
      "learning_rate": 4.926666666666667e-06,
      "loss": 0.0026,
      "step": 67610
    },
    {
      "epoch": 3.6064,
      "grad_norm": 0.4808450937271118,
      "learning_rate": 4.92e-06,
      "loss": 0.0018,
      "step": 67620
    },
    {
      "epoch": 3.606933333333333,
      "grad_norm": 0.3295436501502991,
      "learning_rate": 4.9133333333333334e-06,
      "loss": 0.0018,
      "step": 67630
    },
    {
      "epoch": 3.607466666666667,
      "grad_norm": 0.45982909202575684,
      "learning_rate": 4.906666666666667e-06,
      "loss": 0.0018,
      "step": 67640
    },
    {
      "epoch": 3.608,
      "grad_norm": 0.44704151153564453,
      "learning_rate": 4.9000000000000005e-06,
      "loss": 0.0027,
      "step": 67650
    },
    {
      "epoch": 3.6085333333333334,
      "grad_norm": 0.2354162186384201,
      "learning_rate": 4.893333333333334e-06,
      "loss": 0.0029,
      "step": 67660
    },
    {
      "epoch": 3.6090666666666666,
      "grad_norm": 0.22542637586593628,
      "learning_rate": 4.886666666666667e-06,
      "loss": 0.0042,
      "step": 67670
    },
    {
      "epoch": 3.6096,
      "grad_norm": 0.10785537958145142,
      "learning_rate": 4.880000000000001e-06,
      "loss": 0.002,
      "step": 67680
    },
    {
      "epoch": 3.610133333333333,
      "grad_norm": 0.1228087842464447,
      "learning_rate": 4.873333333333333e-06,
      "loss": 0.0019,
      "step": 67690
    },
    {
      "epoch": 3.610666666666667,
      "grad_norm": 0.2006666213274002,
      "learning_rate": 4.866666666666667e-06,
      "loss": 0.0026,
      "step": 67700
    },
    {
      "epoch": 3.6112,
      "grad_norm": 0.12809087336063385,
      "learning_rate": 4.86e-06,
      "loss": 0.0021,
      "step": 67710
    },
    {
      "epoch": 3.6117333333333335,
      "grad_norm": 0.7665076851844788,
      "learning_rate": 4.853333333333333e-06,
      "loss": 0.0019,
      "step": 67720
    },
    {
      "epoch": 3.6122666666666667,
      "grad_norm": 0.12582330405712128,
      "learning_rate": 4.846666666666667e-06,
      "loss": 0.0034,
      "step": 67730
    },
    {
      "epoch": 3.6128,
      "grad_norm": 0.1864299774169922,
      "learning_rate": 4.84e-06,
      "loss": 0.0031,
      "step": 67740
    },
    {
      "epoch": 3.6133333333333333,
      "grad_norm": 0.14628617465496063,
      "learning_rate": 4.833333333333333e-06,
      "loss": 0.0021,
      "step": 67750
    },
    {
      "epoch": 3.6138666666666666,
      "grad_norm": 0.18843398988246918,
      "learning_rate": 4.8266666666666665e-06,
      "loss": 0.0022,
      "step": 67760
    },
    {
      "epoch": 3.6144,
      "grad_norm": 0.13818039000034332,
      "learning_rate": 4.8200000000000004e-06,
      "loss": 0.0018,
      "step": 67770
    },
    {
      "epoch": 3.614933333333333,
      "grad_norm": 0.27740606665611267,
      "learning_rate": 4.8133333333333336e-06,
      "loss": 0.0023,
      "step": 67780
    },
    {
      "epoch": 3.615466666666667,
      "grad_norm": 0.10384707897901535,
      "learning_rate": 4.806666666666667e-06,
      "loss": 0.0019,
      "step": 67790
    },
    {
      "epoch": 3.616,
      "grad_norm": 0.3787892162799835,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.0033,
      "step": 67800
    },
    {
      "epoch": 3.6165333333333334,
      "grad_norm": 0.20445644855499268,
      "learning_rate": 4.793333333333334e-06,
      "loss": 0.0019,
      "step": 67810
    },
    {
      "epoch": 3.6170666666666667,
      "grad_norm": 0.2888701856136322,
      "learning_rate": 4.786666666666667e-06,
      "loss": 0.002,
      "step": 67820
    },
    {
      "epoch": 3.6176,
      "grad_norm": 0.31807640194892883,
      "learning_rate": 4.780000000000001e-06,
      "loss": 0.0038,
      "step": 67830
    },
    {
      "epoch": 3.618133333333333,
      "grad_norm": 0.21199803054332733,
      "learning_rate": 4.773333333333334e-06,
      "loss": 0.0018,
      "step": 67840
    },
    {
      "epoch": 3.618666666666667,
      "grad_norm": 0.22447851300239563,
      "learning_rate": 4.766666666666667e-06,
      "loss": 0.0018,
      "step": 67850
    },
    {
      "epoch": 3.6192,
      "grad_norm": 0.13990437984466553,
      "learning_rate": 4.76e-06,
      "loss": 0.0019,
      "step": 67860
    },
    {
      "epoch": 3.6197333333333335,
      "grad_norm": 0.2000788301229477,
      "learning_rate": 4.753333333333333e-06,
      "loss": 0.0017,
      "step": 67870
    },
    {
      "epoch": 3.6202666666666667,
      "grad_norm": 0.10535144060850143,
      "learning_rate": 4.746666666666666e-06,
      "loss": 0.0029,
      "step": 67880
    },
    {
      "epoch": 3.6208,
      "grad_norm": 0.4917571246623993,
      "learning_rate": 4.74e-06,
      "loss": 0.0018,
      "step": 67890
    },
    {
      "epoch": 3.6213333333333333,
      "grad_norm": 0.13267309963703156,
      "learning_rate": 4.7333333333333335e-06,
      "loss": 0.0019,
      "step": 67900
    },
    {
      "epoch": 3.6218666666666666,
      "grad_norm": 0.2621312439441681,
      "learning_rate": 4.726666666666667e-06,
      "loss": 0.0018,
      "step": 67910
    },
    {
      "epoch": 3.6224,
      "grad_norm": 0.13990703225135803,
      "learning_rate": 4.72e-06,
      "loss": 0.002,
      "step": 67920
    },
    {
      "epoch": 3.622933333333333,
      "grad_norm": 0.27597278356552124,
      "learning_rate": 4.713333333333334e-06,
      "loss": 0.0026,
      "step": 67930
    },
    {
      "epoch": 3.6234666666666664,
      "grad_norm": 0.35818469524383545,
      "learning_rate": 4.706666666666667e-06,
      "loss": 0.002,
      "step": 67940
    },
    {
      "epoch": 3.624,
      "grad_norm": 0.3929307162761688,
      "learning_rate": 4.7e-06,
      "loss": 0.002,
      "step": 67950
    },
    {
      "epoch": 3.6245333333333334,
      "grad_norm": 0.4855521321296692,
      "learning_rate": 4.693333333333334e-06,
      "loss": 0.0026,
      "step": 67960
    },
    {
      "epoch": 3.6250666666666667,
      "grad_norm": 0.3630894124507904,
      "learning_rate": 4.686666666666667e-06,
      "loss": 0.002,
      "step": 67970
    },
    {
      "epoch": 3.6256,
      "grad_norm": 0.4246516525745392,
      "learning_rate": 4.68e-06,
      "loss": 0.0018,
      "step": 67980
    },
    {
      "epoch": 3.626133333333333,
      "grad_norm": 0.6647111773490906,
      "learning_rate": 4.673333333333334e-06,
      "loss": 0.0028,
      "step": 67990
    },
    {
      "epoch": 3.626666666666667,
      "grad_norm": 0.22544129192829132,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.0021,
      "step": 68000
    },
    {
      "epoch": 3.6272,
      "grad_norm": 0.264952689409256,
      "learning_rate": 4.66e-06,
      "loss": 0.0025,
      "step": 68010
    },
    {
      "epoch": 3.6277333333333335,
      "grad_norm": 0.3353665769100189,
      "learning_rate": 4.653333333333334e-06,
      "loss": 0.0033,
      "step": 68020
    },
    {
      "epoch": 3.6282666666666668,
      "grad_norm": 0.10074163228273392,
      "learning_rate": 4.646666666666667e-06,
      "loss": 0.002,
      "step": 68030
    },
    {
      "epoch": 3.6288,
      "grad_norm": 0.23151715099811554,
      "learning_rate": 4.64e-06,
      "loss": 0.0018,
      "step": 68040
    },
    {
      "epoch": 3.6293333333333333,
      "grad_norm": 0.4200131893157959,
      "learning_rate": 4.633333333333334e-06,
      "loss": 0.0028,
      "step": 68050
    },
    {
      "epoch": 3.6298666666666666,
      "grad_norm": 0.5869055986404419,
      "learning_rate": 4.626666666666667e-06,
      "loss": 0.0019,
      "step": 68060
    },
    {
      "epoch": 3.6304,
      "grad_norm": 0.3694101870059967,
      "learning_rate": 4.62e-06,
      "loss": 0.004,
      "step": 68070
    },
    {
      "epoch": 3.630933333333333,
      "grad_norm": 0.10580616444349289,
      "learning_rate": 4.613333333333334e-06,
      "loss": 0.0026,
      "step": 68080
    },
    {
      "epoch": 3.6314666666666664,
      "grad_norm": 0.4439353942871094,
      "learning_rate": 4.606666666666667e-06,
      "loss": 0.0021,
      "step": 68090
    },
    {
      "epoch": 3.632,
      "grad_norm": 0.4493564963340759,
      "learning_rate": 4.6e-06,
      "loss": 0.0017,
      "step": 68100
    },
    {
      "epoch": 3.6325333333333334,
      "grad_norm": 0.25123071670532227,
      "learning_rate": 4.593333333333333e-06,
      "loss": 0.0026,
      "step": 68110
    },
    {
      "epoch": 3.6330666666666667,
      "grad_norm": 0.25471076369285583,
      "learning_rate": 4.586666666666667e-06,
      "loss": 0.0023,
      "step": 68120
    },
    {
      "epoch": 3.6336,
      "grad_norm": 0.14340358972549438,
      "learning_rate": 4.58e-06,
      "loss": 0.0016,
      "step": 68130
    },
    {
      "epoch": 3.634133333333333,
      "grad_norm": 0.4394332468509674,
      "learning_rate": 4.573333333333333e-06,
      "loss": 0.0033,
      "step": 68140
    },
    {
      "epoch": 3.634666666666667,
      "grad_norm": 0.590559184551239,
      "learning_rate": 4.566666666666667e-06,
      "loss": 0.0019,
      "step": 68150
    },
    {
      "epoch": 3.6352,
      "grad_norm": 0.2057003527879715,
      "learning_rate": 4.56e-06,
      "loss": 0.0021,
      "step": 68160
    },
    {
      "epoch": 3.6357333333333335,
      "grad_norm": 0.06930682808160782,
      "learning_rate": 4.5533333333333335e-06,
      "loss": 0.0017,
      "step": 68170
    },
    {
      "epoch": 3.6362666666666668,
      "grad_norm": 0.41299885511398315,
      "learning_rate": 4.5466666666666675e-06,
      "loss": 0.0026,
      "step": 68180
    },
    {
      "epoch": 3.6368,
      "grad_norm": 0.43769371509552,
      "learning_rate": 4.540000000000001e-06,
      "loss": 0.0025,
      "step": 68190
    },
    {
      "epoch": 3.6373333333333333,
      "grad_norm": 0.08664301782846451,
      "learning_rate": 4.533333333333334e-06,
      "loss": 0.0018,
      "step": 68200
    },
    {
      "epoch": 3.6378666666666666,
      "grad_norm": 0.11678281426429749,
      "learning_rate": 4.526666666666667e-06,
      "loss": 0.002,
      "step": 68210
    },
    {
      "epoch": 3.6384,
      "grad_norm": 0.2512892782688141,
      "learning_rate": 4.52e-06,
      "loss": 0.0026,
      "step": 68220
    },
    {
      "epoch": 3.638933333333333,
      "grad_norm": 0.25734803080558777,
      "learning_rate": 4.513333333333333e-06,
      "loss": 0.0018,
      "step": 68230
    },
    {
      "epoch": 3.6394666666666664,
      "grad_norm": 0.118287093937397,
      "learning_rate": 4.506666666666667e-06,
      "loss": 0.0017,
      "step": 68240
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.08359064161777496,
      "learning_rate": 4.5e-06,
      "loss": 0.0019,
      "step": 68250
    },
    {
      "epoch": 3.6405333333333334,
      "grad_norm": 0.14155985414981842,
      "learning_rate": 4.493333333333333e-06,
      "loss": 0.0017,
      "step": 68260
    },
    {
      "epoch": 3.6410666666666667,
      "grad_norm": 0.10643426328897476,
      "learning_rate": 4.486666666666667e-06,
      "loss": 0.0021,
      "step": 68270
    },
    {
      "epoch": 3.6416,
      "grad_norm": 0.12285149097442627,
      "learning_rate": 4.48e-06,
      "loss": 0.0019,
      "step": 68280
    },
    {
      "epoch": 3.6421333333333332,
      "grad_norm": 0.3614780306816101,
      "learning_rate": 4.473333333333333e-06,
      "loss": 0.0023,
      "step": 68290
    },
    {
      "epoch": 3.642666666666667,
      "grad_norm": 0.1926911175251007,
      "learning_rate": 4.4666666666666665e-06,
      "loss": 0.0017,
      "step": 68300
    },
    {
      "epoch": 3.6432,
      "grad_norm": 0.3677765429019928,
      "learning_rate": 4.4600000000000005e-06,
      "loss": 0.0037,
      "step": 68310
    },
    {
      "epoch": 3.6437333333333335,
      "grad_norm": 0.1900463104248047,
      "learning_rate": 4.453333333333334e-06,
      "loss": 0.0018,
      "step": 68320
    },
    {
      "epoch": 3.6442666666666668,
      "grad_norm": 0.22041185200214386,
      "learning_rate": 4.446666666666667e-06,
      "loss": 0.0023,
      "step": 68330
    },
    {
      "epoch": 3.6448,
      "grad_norm": 0.2695012092590332,
      "learning_rate": 4.440000000000001e-06,
      "loss": 0.002,
      "step": 68340
    },
    {
      "epoch": 3.6453333333333333,
      "grad_norm": 0.1748563051223755,
      "learning_rate": 4.433333333333334e-06,
      "loss": 0.0017,
      "step": 68350
    },
    {
      "epoch": 3.6458666666666666,
      "grad_norm": 0.2722185552120209,
      "learning_rate": 4.426666666666667e-06,
      "loss": 0.002,
      "step": 68360
    },
    {
      "epoch": 3.6464,
      "grad_norm": 0.2481037974357605,
      "learning_rate": 4.420000000000001e-06,
      "loss": 0.0019,
      "step": 68370
    },
    {
      "epoch": 3.646933333333333,
      "grad_norm": 0.24836577475070953,
      "learning_rate": 4.413333333333333e-06,
      "loss": 0.0017,
      "step": 68380
    },
    {
      "epoch": 3.6474666666666664,
      "grad_norm": 0.32561546564102173,
      "learning_rate": 4.406666666666666e-06,
      "loss": 0.0034,
      "step": 68390
    },
    {
      "epoch": 3.648,
      "grad_norm": 0.1973509043455124,
      "learning_rate": 4.4e-06,
      "loss": 0.0018,
      "step": 68400
    },
    {
      "epoch": 3.6485333333333334,
      "grad_norm": 0.2619110941886902,
      "learning_rate": 4.393333333333333e-06,
      "loss": 0.0025,
      "step": 68410
    },
    {
      "epoch": 3.6490666666666667,
      "grad_norm": 0.23554649949073792,
      "learning_rate": 4.3866666666666665e-06,
      "loss": 0.0034,
      "step": 68420
    },
    {
      "epoch": 3.6496,
      "grad_norm": 0.10844293236732483,
      "learning_rate": 4.38e-06,
      "loss": 0.0026,
      "step": 68430
    },
    {
      "epoch": 3.6501333333333332,
      "grad_norm": 0.2230994701385498,
      "learning_rate": 4.3733333333333335e-06,
      "loss": 0.0034,
      "step": 68440
    },
    {
      "epoch": 3.6506666666666665,
      "grad_norm": 0.3418181836605072,
      "learning_rate": 4.366666666666667e-06,
      "loss": 0.0027,
      "step": 68450
    },
    {
      "epoch": 3.6512000000000002,
      "grad_norm": 0.32723963260650635,
      "learning_rate": 4.360000000000001e-06,
      "loss": 0.0025,
      "step": 68460
    },
    {
      "epoch": 3.6517333333333335,
      "grad_norm": 0.15137869119644165,
      "learning_rate": 4.353333333333334e-06,
      "loss": 0.0018,
      "step": 68470
    },
    {
      "epoch": 3.6522666666666668,
      "grad_norm": 0.12997470796108246,
      "learning_rate": 4.346666666666667e-06,
      "loss": 0.0018,
      "step": 68480
    },
    {
      "epoch": 3.6528,
      "grad_norm": 0.6894087791442871,
      "learning_rate": 4.34e-06,
      "loss": 0.002,
      "step": 68490
    },
    {
      "epoch": 3.6533333333333333,
      "grad_norm": 0.07495259493589401,
      "learning_rate": 4.333333333333334e-06,
      "loss": 0.0034,
      "step": 68500
    },
    {
      "epoch": 3.6538666666666666,
      "grad_norm": 0.11547272652387619,
      "learning_rate": 4.326666666666667e-06,
      "loss": 0.0022,
      "step": 68510
    },
    {
      "epoch": 3.6544,
      "grad_norm": 0.13604292273521423,
      "learning_rate": 4.32e-06,
      "loss": 0.0019,
      "step": 68520
    },
    {
      "epoch": 3.654933333333333,
      "grad_norm": 0.18871396780014038,
      "learning_rate": 4.313333333333334e-06,
      "loss": 0.0034,
      "step": 68530
    },
    {
      "epoch": 3.6554666666666664,
      "grad_norm": 0.1302482783794403,
      "learning_rate": 4.306666666666667e-06,
      "loss": 0.0028,
      "step": 68540
    },
    {
      "epoch": 3.656,
      "grad_norm": 0.1689489483833313,
      "learning_rate": 4.2999999999999995e-06,
      "loss": 0.0034,
      "step": 68550
    },
    {
      "epoch": 3.6565333333333334,
      "grad_norm": 0.15390798449516296,
      "learning_rate": 4.2933333333333334e-06,
      "loss": 0.0025,
      "step": 68560
    },
    {
      "epoch": 3.6570666666666667,
      "grad_norm": 0.09022995829582214,
      "learning_rate": 4.2866666666666666e-06,
      "loss": 0.0039,
      "step": 68570
    },
    {
      "epoch": 3.6576,
      "grad_norm": 0.3678956925868988,
      "learning_rate": 4.28e-06,
      "loss": 0.0018,
      "step": 68580
    },
    {
      "epoch": 3.6581333333333332,
      "grad_norm": 0.4160515069961548,
      "learning_rate": 4.273333333333334e-06,
      "loss": 0.0027,
      "step": 68590
    },
    {
      "epoch": 3.6586666666666665,
      "grad_norm": 0.2676776647567749,
      "learning_rate": 4.266666666666667e-06,
      "loss": 0.0027,
      "step": 68600
    },
    {
      "epoch": 3.6592000000000002,
      "grad_norm": 0.4135199785232544,
      "learning_rate": 4.26e-06,
      "loss": 0.0017,
      "step": 68610
    },
    {
      "epoch": 3.6597333333333335,
      "grad_norm": 0.3171752989292145,
      "learning_rate": 4.253333333333334e-06,
      "loss": 0.0027,
      "step": 68620
    },
    {
      "epoch": 3.660266666666667,
      "grad_norm": 0.10209417343139648,
      "learning_rate": 4.246666666666667e-06,
      "loss": 0.0027,
      "step": 68630
    },
    {
      "epoch": 3.6608,
      "grad_norm": 0.15018202364444733,
      "learning_rate": 4.24e-06,
      "loss": 0.0025,
      "step": 68640
    },
    {
      "epoch": 3.6613333333333333,
      "grad_norm": 0.25674617290496826,
      "learning_rate": 4.233333333333333e-06,
      "loss": 0.0042,
      "step": 68650
    },
    {
      "epoch": 3.6618666666666666,
      "grad_norm": 0.3935696482658386,
      "learning_rate": 4.226666666666667e-06,
      "loss": 0.0025,
      "step": 68660
    },
    {
      "epoch": 3.6624,
      "grad_norm": 0.1912807673215866,
      "learning_rate": 4.22e-06,
      "loss": 0.0026,
      "step": 68670
    },
    {
      "epoch": 3.662933333333333,
      "grad_norm": 0.15892574191093445,
      "learning_rate": 4.213333333333333e-06,
      "loss": 0.0017,
      "step": 68680
    },
    {
      "epoch": 3.6634666666666664,
      "grad_norm": 0.23213405907154083,
      "learning_rate": 4.206666666666667e-06,
      "loss": 0.0018,
      "step": 68690
    },
    {
      "epoch": 3.664,
      "grad_norm": 0.1292496770620346,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 0.0019,
      "step": 68700
    },
    {
      "epoch": 3.6645333333333334,
      "grad_norm": 0.09319120645523071,
      "learning_rate": 4.1933333333333336e-06,
      "loss": 0.0031,
      "step": 68710
    },
    {
      "epoch": 3.6650666666666667,
      "grad_norm": 0.5623605847358704,
      "learning_rate": 4.1866666666666675e-06,
      "loss": 0.0019,
      "step": 68720
    },
    {
      "epoch": 3.6656,
      "grad_norm": 0.25371962785720825,
      "learning_rate": 4.18e-06,
      "loss": 0.0016,
      "step": 68730
    },
    {
      "epoch": 3.6661333333333332,
      "grad_norm": 0.11256468296051025,
      "learning_rate": 4.173333333333333e-06,
      "loss": 0.0024,
      "step": 68740
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.45134711265563965,
      "learning_rate": 4.166666666666667e-06,
      "loss": 0.0018,
      "step": 68750
    },
    {
      "epoch": 3.6672000000000002,
      "grad_norm": 0.2534133791923523,
      "learning_rate": 4.16e-06,
      "loss": 0.0023,
      "step": 68760
    },
    {
      "epoch": 3.6677333333333335,
      "grad_norm": 0.19745200872421265,
      "learning_rate": 4.153333333333333e-06,
      "loss": 0.002,
      "step": 68770
    },
    {
      "epoch": 3.668266666666667,
      "grad_norm": 0.07870908826589584,
      "learning_rate": 4.146666666666667e-06,
      "loss": 0.0019,
      "step": 68780
    },
    {
      "epoch": 3.6688,
      "grad_norm": 0.14294840395450592,
      "learning_rate": 4.14e-06,
      "loss": 0.003,
      "step": 68790
    },
    {
      "epoch": 3.6693333333333333,
      "grad_norm": 0.5710743069648743,
      "learning_rate": 4.133333333333333e-06,
      "loss": 0.0026,
      "step": 68800
    },
    {
      "epoch": 3.6698666666666666,
      "grad_norm": 0.11718118190765381,
      "learning_rate": 4.126666666666667e-06,
      "loss": 0.0019,
      "step": 68810
    },
    {
      "epoch": 3.6704,
      "grad_norm": 0.2148159146308899,
      "learning_rate": 4.12e-06,
      "loss": 0.0018,
      "step": 68820
    },
    {
      "epoch": 3.670933333333333,
      "grad_norm": 0.36607542634010315,
      "learning_rate": 4.1133333333333335e-06,
      "loss": 0.002,
      "step": 68830
    },
    {
      "epoch": 3.6714666666666664,
      "grad_norm": 0.205561101436615,
      "learning_rate": 4.106666666666667e-06,
      "loss": 0.0026,
      "step": 68840
    },
    {
      "epoch": 3.672,
      "grad_norm": 0.15865127742290497,
      "learning_rate": 4.1000000000000006e-06,
      "loss": 0.0018,
      "step": 68850
    },
    {
      "epoch": 3.6725333333333334,
      "grad_norm": 0.664321780204773,
      "learning_rate": 4.093333333333334e-06,
      "loss": 0.0027,
      "step": 68860
    },
    {
      "epoch": 3.6730666666666667,
      "grad_norm": 0.23297226428985596,
      "learning_rate": 4.086666666666667e-06,
      "loss": 0.0024,
      "step": 68870
    },
    {
      "epoch": 3.6736,
      "grad_norm": 0.17637114226818085,
      "learning_rate": 4.080000000000001e-06,
      "loss": 0.0019,
      "step": 68880
    },
    {
      "epoch": 3.6741333333333333,
      "grad_norm": 0.7222252488136292,
      "learning_rate": 4.073333333333334e-06,
      "loss": 0.0018,
      "step": 68890
    },
    {
      "epoch": 3.6746666666666665,
      "grad_norm": 0.10574792325496674,
      "learning_rate": 4.066666666666666e-06,
      "loss": 0.0018,
      "step": 68900
    },
    {
      "epoch": 3.6752000000000002,
      "grad_norm": 0.0939294770359993,
      "learning_rate": 4.06e-06,
      "loss": 0.003,
      "step": 68910
    },
    {
      "epoch": 3.6757333333333335,
      "grad_norm": 0.13404007256031036,
      "learning_rate": 4.053333333333333e-06,
      "loss": 0.0018,
      "step": 68920
    },
    {
      "epoch": 3.676266666666667,
      "grad_norm": 0.16370834410190582,
      "learning_rate": 4.046666666666666e-06,
      "loss": 0.0016,
      "step": 68930
    },
    {
      "epoch": 3.6768,
      "grad_norm": 0.26215702295303345,
      "learning_rate": 4.04e-06,
      "loss": 0.0038,
      "step": 68940
    },
    {
      "epoch": 3.6773333333333333,
      "grad_norm": 0.286410391330719,
      "learning_rate": 4.033333333333333e-06,
      "loss": 0.0023,
      "step": 68950
    },
    {
      "epoch": 3.6778666666666666,
      "grad_norm": 0.10260777175426483,
      "learning_rate": 4.0266666666666665e-06,
      "loss": 0.0033,
      "step": 68960
    },
    {
      "epoch": 3.6784,
      "grad_norm": 0.2575007975101471,
      "learning_rate": 4.0200000000000005e-06,
      "loss": 0.0019,
      "step": 68970
    },
    {
      "epoch": 3.678933333333333,
      "grad_norm": 0.31497758626937866,
      "learning_rate": 4.013333333333334e-06,
      "loss": 0.0021,
      "step": 68980
    },
    {
      "epoch": 3.6794666666666664,
      "grad_norm": 0.22426249086856842,
      "learning_rate": 4.006666666666667e-06,
      "loss": 0.0019,
      "step": 68990
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.33918339014053345,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.0025,
      "step": 69000
    },
    {
      "epoch": 3.6805333333333334,
      "grad_norm": 0.22497043013572693,
      "learning_rate": 3.993333333333334e-06,
      "loss": 0.0017,
      "step": 69010
    },
    {
      "epoch": 3.6810666666666667,
      "grad_norm": 0.3933468163013458,
      "learning_rate": 3.986666666666667e-06,
      "loss": 0.0025,
      "step": 69020
    },
    {
      "epoch": 3.6816,
      "grad_norm": 0.17795124650001526,
      "learning_rate": 3.98e-06,
      "loss": 0.0017,
      "step": 69030
    },
    {
      "epoch": 3.6821333333333333,
      "grad_norm": 0.260558545589447,
      "learning_rate": 3.973333333333334e-06,
      "loss": 0.0019,
      "step": 69040
    },
    {
      "epoch": 3.6826666666666665,
      "grad_norm": 0.49859854578971863,
      "learning_rate": 3.966666666666667e-06,
      "loss": 0.0022,
      "step": 69050
    },
    {
      "epoch": 3.6832000000000003,
      "grad_norm": 0.09415829181671143,
      "learning_rate": 3.96e-06,
      "loss": 0.0027,
      "step": 69060
    },
    {
      "epoch": 3.6837333333333335,
      "grad_norm": 0.26653409004211426,
      "learning_rate": 3.953333333333333e-06,
      "loss": 0.0025,
      "step": 69070
    },
    {
      "epoch": 3.684266666666667,
      "grad_norm": 0.3159819543361664,
      "learning_rate": 3.9466666666666664e-06,
      "loss": 0.0027,
      "step": 69080
    },
    {
      "epoch": 3.6848,
      "grad_norm": 0.45812609791755676,
      "learning_rate": 3.9399999999999995e-06,
      "loss": 0.0017,
      "step": 69090
    },
    {
      "epoch": 3.6853333333333333,
      "grad_norm": 0.28289029002189636,
      "learning_rate": 3.9333333333333335e-06,
      "loss": 0.0031,
      "step": 69100
    },
    {
      "epoch": 3.6858666666666666,
      "grad_norm": 0.21762563288211823,
      "learning_rate": 3.926666666666667e-06,
      "loss": 0.0025,
      "step": 69110
    },
    {
      "epoch": 3.6864,
      "grad_norm": 0.284973680973053,
      "learning_rate": 3.92e-06,
      "loss": 0.0032,
      "step": 69120
    },
    {
      "epoch": 3.686933333333333,
      "grad_norm": 0.11361560225486755,
      "learning_rate": 3.913333333333334e-06,
      "loss": 0.0043,
      "step": 69130
    },
    {
      "epoch": 3.6874666666666664,
      "grad_norm": 0.1518392413854599,
      "learning_rate": 3.906666666666667e-06,
      "loss": 0.0026,
      "step": 69140
    },
    {
      "epoch": 3.6879999999999997,
      "grad_norm": 0.388850599527359,
      "learning_rate": 3.9e-06,
      "loss": 0.0017,
      "step": 69150
    },
    {
      "epoch": 3.6885333333333334,
      "grad_norm": 0.24051767587661743,
      "learning_rate": 3.893333333333334e-06,
      "loss": 0.0024,
      "step": 69160
    },
    {
      "epoch": 3.6890666666666667,
      "grad_norm": 0.4954676628112793,
      "learning_rate": 3.886666666666667e-06,
      "loss": 0.0027,
      "step": 69170
    },
    {
      "epoch": 3.6896,
      "grad_norm": 0.12829189002513885,
      "learning_rate": 3.88e-06,
      "loss": 0.0017,
      "step": 69180
    },
    {
      "epoch": 3.6901333333333333,
      "grad_norm": 0.08775655180215836,
      "learning_rate": 3.873333333333334e-06,
      "loss": 0.0027,
      "step": 69190
    },
    {
      "epoch": 3.6906666666666665,
      "grad_norm": 0.19750481843948364,
      "learning_rate": 3.866666666666667e-06,
      "loss": 0.0027,
      "step": 69200
    },
    {
      "epoch": 3.6912000000000003,
      "grad_norm": 0.5008994340896606,
      "learning_rate": 3.86e-06,
      "loss": 0.0024,
      "step": 69210
    },
    {
      "epoch": 3.6917333333333335,
      "grad_norm": 0.33789488673210144,
      "learning_rate": 3.8533333333333334e-06,
      "loss": 0.0025,
      "step": 69220
    },
    {
      "epoch": 3.692266666666667,
      "grad_norm": 0.11384914815425873,
      "learning_rate": 3.846666666666667e-06,
      "loss": 0.0021,
      "step": 69230
    },
    {
      "epoch": 3.6928,
      "grad_norm": 0.09662915021181107,
      "learning_rate": 3.84e-06,
      "loss": 0.0022,
      "step": 69240
    },
    {
      "epoch": 3.6933333333333334,
      "grad_norm": 0.18376567959785461,
      "learning_rate": 3.833333333333334e-06,
      "loss": 0.0014,
      "step": 69250
    },
    {
      "epoch": 3.6938666666666666,
      "grad_norm": 0.12683604657649994,
      "learning_rate": 3.826666666666667e-06,
      "loss": 0.0019,
      "step": 69260
    },
    {
      "epoch": 3.6944,
      "grad_norm": 0.5212700963020325,
      "learning_rate": 3.82e-06,
      "loss": 0.0025,
      "step": 69270
    },
    {
      "epoch": 3.694933333333333,
      "grad_norm": 0.3232014775276184,
      "learning_rate": 3.8133333333333334e-06,
      "loss": 0.0033,
      "step": 69280
    },
    {
      "epoch": 3.6954666666666665,
      "grad_norm": 0.33419620990753174,
      "learning_rate": 3.806666666666667e-06,
      "loss": 0.002,
      "step": 69290
    },
    {
      "epoch": 3.6959999999999997,
      "grad_norm": 0.4355071783065796,
      "learning_rate": 3.8e-06,
      "loss": 0.0023,
      "step": 69300
    },
    {
      "epoch": 3.6965333333333334,
      "grad_norm": 0.2611217498779297,
      "learning_rate": 3.7933333333333336e-06,
      "loss": 0.0024,
      "step": 69310
    },
    {
      "epoch": 3.6970666666666667,
      "grad_norm": 0.35932210087776184,
      "learning_rate": 3.7866666666666667e-06,
      "loss": 0.0034,
      "step": 69320
    },
    {
      "epoch": 3.6976,
      "grad_norm": 0.2999041676521301,
      "learning_rate": 3.7800000000000002e-06,
      "loss": 0.0023,
      "step": 69330
    },
    {
      "epoch": 3.6981333333333333,
      "grad_norm": 0.15263892710208893,
      "learning_rate": 3.7733333333333338e-06,
      "loss": 0.0019,
      "step": 69340
    },
    {
      "epoch": 3.6986666666666665,
      "grad_norm": 0.37569475173950195,
      "learning_rate": 3.766666666666667e-06,
      "loss": 0.0036,
      "step": 69350
    },
    {
      "epoch": 3.6992000000000003,
      "grad_norm": 0.10003295540809631,
      "learning_rate": 3.7600000000000004e-06,
      "loss": 0.0024,
      "step": 69360
    },
    {
      "epoch": 3.6997333333333335,
      "grad_norm": 0.4943001866340637,
      "learning_rate": 3.7533333333333335e-06,
      "loss": 0.0018,
      "step": 69370
    },
    {
      "epoch": 3.700266666666667,
      "grad_norm": 0.34218859672546387,
      "learning_rate": 3.746666666666667e-06,
      "loss": 0.0019,
      "step": 69380
    },
    {
      "epoch": 3.7008,
      "grad_norm": 0.3823016881942749,
      "learning_rate": 3.7400000000000006e-06,
      "loss": 0.0047,
      "step": 69390
    },
    {
      "epoch": 3.7013333333333334,
      "grad_norm": 0.31573307514190674,
      "learning_rate": 3.7333333333333337e-06,
      "loss": 0.0047,
      "step": 69400
    },
    {
      "epoch": 3.7018666666666666,
      "grad_norm": 0.10942549258470535,
      "learning_rate": 3.7266666666666664e-06,
      "loss": 0.0034,
      "step": 69410
    },
    {
      "epoch": 3.7024,
      "grad_norm": 0.19952869415283203,
      "learning_rate": 3.72e-06,
      "loss": 0.0015,
      "step": 69420
    },
    {
      "epoch": 3.702933333333333,
      "grad_norm": 0.3511792719364166,
      "learning_rate": 3.713333333333333e-06,
      "loss": 0.0027,
      "step": 69430
    },
    {
      "epoch": 3.7034666666666665,
      "grad_norm": 0.15343426167964935,
      "learning_rate": 3.7066666666666666e-06,
      "loss": 0.0018,
      "step": 69440
    },
    {
      "epoch": 3.7039999999999997,
      "grad_norm": 0.3111582100391388,
      "learning_rate": 3.7e-06,
      "loss": 0.0034,
      "step": 69450
    },
    {
      "epoch": 3.7045333333333335,
      "grad_norm": 0.1344618946313858,
      "learning_rate": 3.6933333333333333e-06,
      "loss": 0.0017,
      "step": 69460
    },
    {
      "epoch": 3.7050666666666667,
      "grad_norm": 0.07486338168382645,
      "learning_rate": 3.686666666666667e-06,
      "loss": 0.0027,
      "step": 69470
    },
    {
      "epoch": 3.7056,
      "grad_norm": 0.13448864221572876,
      "learning_rate": 3.68e-06,
      "loss": 0.0023,
      "step": 69480
    },
    {
      "epoch": 3.7061333333333333,
      "grad_norm": 0.14542321860790253,
      "learning_rate": 3.6733333333333335e-06,
      "loss": 0.0025,
      "step": 69490
    },
    {
      "epoch": 3.7066666666666666,
      "grad_norm": 0.357584685087204,
      "learning_rate": 3.666666666666667e-06,
      "loss": 0.0018,
      "step": 69500
    },
    {
      "epoch": 3.7072000000000003,
      "grad_norm": 0.17001540958881378,
      "learning_rate": 3.66e-06,
      "loss": 0.0017,
      "step": 69510
    },
    {
      "epoch": 3.7077333333333335,
      "grad_norm": 0.38452813029289246,
      "learning_rate": 3.6533333333333336e-06,
      "loss": 0.0035,
      "step": 69520
    },
    {
      "epoch": 3.708266666666667,
      "grad_norm": 0.19837115705013275,
      "learning_rate": 3.646666666666667e-06,
      "loss": 0.0025,
      "step": 69530
    },
    {
      "epoch": 3.7088,
      "grad_norm": 0.14512793719768524,
      "learning_rate": 3.6400000000000003e-06,
      "loss": 0.002,
      "step": 69540
    },
    {
      "epoch": 3.7093333333333334,
      "grad_norm": 0.2523547410964966,
      "learning_rate": 3.633333333333334e-06,
      "loss": 0.0019,
      "step": 69550
    },
    {
      "epoch": 3.7098666666666666,
      "grad_norm": 0.7576634883880615,
      "learning_rate": 3.626666666666667e-06,
      "loss": 0.0025,
      "step": 69560
    },
    {
      "epoch": 3.7104,
      "grad_norm": 0.538371741771698,
      "learning_rate": 3.6200000000000005e-06,
      "loss": 0.0024,
      "step": 69570
    },
    {
      "epoch": 3.710933333333333,
      "grad_norm": 0.09900172054767609,
      "learning_rate": 3.613333333333334e-06,
      "loss": 0.0028,
      "step": 69580
    },
    {
      "epoch": 3.7114666666666665,
      "grad_norm": 0.09075070917606354,
      "learning_rate": 3.6066666666666667e-06,
      "loss": 0.0026,
      "step": 69590
    },
    {
      "epoch": 3.7119999999999997,
      "grad_norm": 0.11218514293432236,
      "learning_rate": 3.6e-06,
      "loss": 0.0024,
      "step": 69600
    },
    {
      "epoch": 3.7125333333333335,
      "grad_norm": 0.40039685368537903,
      "learning_rate": 3.5933333333333334e-06,
      "loss": 0.002,
      "step": 69610
    },
    {
      "epoch": 3.7130666666666667,
      "grad_norm": 0.28550487756729126,
      "learning_rate": 3.5866666666666665e-06,
      "loss": 0.0024,
      "step": 69620
    },
    {
      "epoch": 3.7136,
      "grad_norm": 0.5931511521339417,
      "learning_rate": 3.58e-06,
      "loss": 0.0019,
      "step": 69630
    },
    {
      "epoch": 3.7141333333333333,
      "grad_norm": 0.12759874761104584,
      "learning_rate": 3.5733333333333336e-06,
      "loss": 0.0026,
      "step": 69640
    },
    {
      "epoch": 3.7146666666666666,
      "grad_norm": 0.2943223714828491,
      "learning_rate": 3.5666666666666667e-06,
      "loss": 0.0027,
      "step": 69650
    },
    {
      "epoch": 3.7152,
      "grad_norm": 0.6115248799324036,
      "learning_rate": 3.5600000000000002e-06,
      "loss": 0.0029,
      "step": 69660
    },
    {
      "epoch": 3.7157333333333336,
      "grad_norm": 0.09402377903461456,
      "learning_rate": 3.5533333333333333e-06,
      "loss": 0.0024,
      "step": 69670
    },
    {
      "epoch": 3.716266666666667,
      "grad_norm": 0.507562518119812,
      "learning_rate": 3.546666666666667e-06,
      "loss": 0.0025,
      "step": 69680
    },
    {
      "epoch": 3.7168,
      "grad_norm": 0.17378708720207214,
      "learning_rate": 3.5400000000000004e-06,
      "loss": 0.0017,
      "step": 69690
    },
    {
      "epoch": 3.7173333333333334,
      "grad_norm": 0.19034314155578613,
      "learning_rate": 3.5333333333333335e-06,
      "loss": 0.0026,
      "step": 69700
    },
    {
      "epoch": 3.7178666666666667,
      "grad_norm": 0.42900341749191284,
      "learning_rate": 3.526666666666667e-06,
      "loss": 0.0016,
      "step": 69710
    },
    {
      "epoch": 3.7184,
      "grad_norm": 0.22867955267429352,
      "learning_rate": 3.52e-06,
      "loss": 0.0022,
      "step": 69720
    },
    {
      "epoch": 3.718933333333333,
      "grad_norm": 0.3073138892650604,
      "learning_rate": 3.5133333333333337e-06,
      "loss": 0.0027,
      "step": 69730
    },
    {
      "epoch": 3.7194666666666665,
      "grad_norm": 0.11689314246177673,
      "learning_rate": 3.5066666666666673e-06,
      "loss": 0.0019,
      "step": 69740
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 0.5884069204330444,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 0.0032,
      "step": 69750
    },
    {
      "epoch": 3.7205333333333335,
      "grad_norm": 0.2804632782936096,
      "learning_rate": 3.493333333333333e-06,
      "loss": 0.0036,
      "step": 69760
    },
    {
      "epoch": 3.7210666666666667,
      "grad_norm": 0.14584553241729736,
      "learning_rate": 3.4866666666666666e-06,
      "loss": 0.0022,
      "step": 69770
    },
    {
      "epoch": 3.7216,
      "grad_norm": 0.3185023069381714,
      "learning_rate": 3.4799999999999997e-06,
      "loss": 0.0024,
      "step": 69780
    },
    {
      "epoch": 3.7221333333333333,
      "grad_norm": 0.17893601953983307,
      "learning_rate": 3.4733333333333333e-06,
      "loss": 0.0018,
      "step": 69790
    },
    {
      "epoch": 3.7226666666666666,
      "grad_norm": 0.2674039602279663,
      "learning_rate": 3.466666666666667e-06,
      "loss": 0.0019,
      "step": 69800
    },
    {
      "epoch": 3.7232,
      "grad_norm": 0.06356516480445862,
      "learning_rate": 3.46e-06,
      "loss": 0.002,
      "step": 69810
    },
    {
      "epoch": 3.7237333333333336,
      "grad_norm": 0.13965344429016113,
      "learning_rate": 3.4533333333333334e-06,
      "loss": 0.0036,
      "step": 69820
    },
    {
      "epoch": 3.724266666666667,
      "grad_norm": 0.10440263897180557,
      "learning_rate": 3.446666666666667e-06,
      "loss": 0.0043,
      "step": 69830
    },
    {
      "epoch": 3.7248,
      "grad_norm": 0.06010914221405983,
      "learning_rate": 3.44e-06,
      "loss": 0.0038,
      "step": 69840
    },
    {
      "epoch": 3.7253333333333334,
      "grad_norm": 0.07584740221500397,
      "learning_rate": 3.4333333333333336e-06,
      "loss": 0.0023,
      "step": 69850
    },
    {
      "epoch": 3.7258666666666667,
      "grad_norm": 0.1364269107580185,
      "learning_rate": 3.4266666666666668e-06,
      "loss": 0.0026,
      "step": 69860
    },
    {
      "epoch": 3.7264,
      "grad_norm": 0.10346169769763947,
      "learning_rate": 3.4200000000000003e-06,
      "loss": 0.0032,
      "step": 69870
    },
    {
      "epoch": 3.726933333333333,
      "grad_norm": 0.13023002445697784,
      "learning_rate": 3.413333333333334e-06,
      "loss": 0.0022,
      "step": 69880
    },
    {
      "epoch": 3.7274666666666665,
      "grad_norm": 0.13812917470932007,
      "learning_rate": 3.406666666666667e-06,
      "loss": 0.0026,
      "step": 69890
    },
    {
      "epoch": 3.7279999999999998,
      "grad_norm": 0.19227911531925201,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 0.0016,
      "step": 69900
    },
    {
      "epoch": 3.7285333333333335,
      "grad_norm": 0.14791223406791687,
      "learning_rate": 3.3933333333333336e-06,
      "loss": 0.0027,
      "step": 69910
    },
    {
      "epoch": 3.7290666666666668,
      "grad_norm": 0.18184541165828705,
      "learning_rate": 3.386666666666667e-06,
      "loss": 0.0025,
      "step": 69920
    },
    {
      "epoch": 3.7296,
      "grad_norm": 0.1376194953918457,
      "learning_rate": 3.38e-06,
      "loss": 0.0027,
      "step": 69930
    },
    {
      "epoch": 3.7301333333333333,
      "grad_norm": 0.30586692690849304,
      "learning_rate": 3.3733333333333334e-06,
      "loss": 0.0017,
      "step": 69940
    },
    {
      "epoch": 3.7306666666666666,
      "grad_norm": 0.4014451205730438,
      "learning_rate": 3.3666666666666665e-06,
      "loss": 0.0017,
      "step": 69950
    },
    {
      "epoch": 3.7312,
      "grad_norm": 0.37905099987983704,
      "learning_rate": 3.36e-06,
      "loss": 0.0018,
      "step": 69960
    },
    {
      "epoch": 3.7317333333333336,
      "grad_norm": 0.11727286875247955,
      "learning_rate": 3.353333333333333e-06,
      "loss": 0.0019,
      "step": 69970
    },
    {
      "epoch": 3.732266666666667,
      "grad_norm": 0.23789024353027344,
      "learning_rate": 3.3466666666666667e-06,
      "loss": 0.002,
      "step": 69980
    },
    {
      "epoch": 3.7328,
      "grad_norm": 0.12602220475673676,
      "learning_rate": 3.34e-06,
      "loss": 0.0017,
      "step": 69990
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 0.2464423030614853,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.0021,
      "step": 70000
    },
    {
      "epoch": 3.7338666666666667,
      "grad_norm": 0.2749984562397003,
      "learning_rate": 3.326666666666667e-06,
      "loss": 0.002,
      "step": 70010
    },
    {
      "epoch": 3.7344,
      "grad_norm": 0.07287716120481491,
      "learning_rate": 3.3200000000000004e-06,
      "loss": 0.0031,
      "step": 70020
    },
    {
      "epoch": 3.734933333333333,
      "grad_norm": 0.0802682563662529,
      "learning_rate": 3.3133333333333335e-06,
      "loss": 0.0023,
      "step": 70030
    },
    {
      "epoch": 3.7354666666666665,
      "grad_norm": 0.14518997073173523,
      "learning_rate": 3.306666666666667e-06,
      "loss": 0.0018,
      "step": 70040
    },
    {
      "epoch": 3.7359999999999998,
      "grad_norm": 0.3264356255531311,
      "learning_rate": 3.3e-06,
      "loss": 0.0023,
      "step": 70050
    },
    {
      "epoch": 3.7365333333333335,
      "grad_norm": 0.5650314092636108,
      "learning_rate": 3.2933333333333337e-06,
      "loss": 0.003,
      "step": 70060
    },
    {
      "epoch": 3.7370666666666668,
      "grad_norm": 0.12944748997688293,
      "learning_rate": 3.2866666666666672e-06,
      "loss": 0.0026,
      "step": 70070
    },
    {
      "epoch": 3.7376,
      "grad_norm": 0.13320080935955048,
      "learning_rate": 3.2800000000000004e-06,
      "loss": 0.0029,
      "step": 70080
    },
    {
      "epoch": 3.7381333333333333,
      "grad_norm": 0.11649951338768005,
      "learning_rate": 3.273333333333334e-06,
      "loss": 0.0024,
      "step": 70090
    },
    {
      "epoch": 3.7386666666666666,
      "grad_norm": 0.08877840638160706,
      "learning_rate": 3.2666666666666666e-06,
      "loss": 0.0017,
      "step": 70100
    },
    {
      "epoch": 3.7392,
      "grad_norm": 0.38601693511009216,
      "learning_rate": 3.2599999999999997e-06,
      "loss": 0.0031,
      "step": 70110
    },
    {
      "epoch": 3.7397333333333336,
      "grad_norm": 0.31693747639656067,
      "learning_rate": 3.2533333333333332e-06,
      "loss": 0.0018,
      "step": 70120
    },
    {
      "epoch": 3.740266666666667,
      "grad_norm": 0.15248723328113556,
      "learning_rate": 3.2466666666666668e-06,
      "loss": 0.002,
      "step": 70130
    },
    {
      "epoch": 3.7408,
      "grad_norm": 0.23000717163085938,
      "learning_rate": 3.24e-06,
      "loss": 0.0017,
      "step": 70140
    },
    {
      "epoch": 3.7413333333333334,
      "grad_norm": 0.3064846694469452,
      "learning_rate": 3.2333333333333334e-06,
      "loss": 0.0018,
      "step": 70150
    },
    {
      "epoch": 3.7418666666666667,
      "grad_norm": 0.20318110287189484,
      "learning_rate": 3.2266666666666665e-06,
      "loss": 0.0019,
      "step": 70160
    },
    {
      "epoch": 3.7424,
      "grad_norm": 0.23618941009044647,
      "learning_rate": 3.22e-06,
      "loss": 0.002,
      "step": 70170
    },
    {
      "epoch": 3.7429333333333332,
      "grad_norm": 0.3389948308467865,
      "learning_rate": 3.2133333333333336e-06,
      "loss": 0.002,
      "step": 70180
    },
    {
      "epoch": 3.7434666666666665,
      "grad_norm": 0.11415902525186539,
      "learning_rate": 3.2066666666666667e-06,
      "loss": 0.0021,
      "step": 70190
    },
    {
      "epoch": 3.7439999999999998,
      "grad_norm": 0.07092137634754181,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.0018,
      "step": 70200
    },
    {
      "epoch": 3.7445333333333335,
      "grad_norm": 0.34768980741500854,
      "learning_rate": 3.1933333333333334e-06,
      "loss": 0.002,
      "step": 70210
    },
    {
      "epoch": 3.7450666666666668,
      "grad_norm": 0.2935452163219452,
      "learning_rate": 3.186666666666667e-06,
      "loss": 0.0019,
      "step": 70220
    },
    {
      "epoch": 3.7456,
      "grad_norm": 0.18135744333267212,
      "learning_rate": 3.1800000000000005e-06,
      "loss": 0.0045,
      "step": 70230
    },
    {
      "epoch": 3.7461333333333333,
      "grad_norm": 0.18065397441387177,
      "learning_rate": 3.1733333333333336e-06,
      "loss": 0.0026,
      "step": 70240
    },
    {
      "epoch": 3.7466666666666666,
      "grad_norm": 0.142349973320961,
      "learning_rate": 3.166666666666667e-06,
      "loss": 0.0017,
      "step": 70250
    },
    {
      "epoch": 3.7472,
      "grad_norm": 0.41862812638282776,
      "learning_rate": 3.1600000000000007e-06,
      "loss": 0.0019,
      "step": 70260
    },
    {
      "epoch": 3.7477333333333336,
      "grad_norm": 0.3113158941268921,
      "learning_rate": 3.153333333333333e-06,
      "loss": 0.0038,
      "step": 70270
    },
    {
      "epoch": 3.748266666666667,
      "grad_norm": 0.39164987206459045,
      "learning_rate": 3.1466666666666665e-06,
      "loss": 0.0018,
      "step": 70280
    },
    {
      "epoch": 3.7488,
      "grad_norm": 0.2871929109096527,
      "learning_rate": 3.14e-06,
      "loss": 0.0019,
      "step": 70290
    },
    {
      "epoch": 3.7493333333333334,
      "grad_norm": 0.29771414399147034,
      "learning_rate": 3.133333333333333e-06,
      "loss": 0.0025,
      "step": 70300
    },
    {
      "epoch": 3.7498666666666667,
      "grad_norm": 0.17374150454998016,
      "learning_rate": 3.1266666666666667e-06,
      "loss": 0.0018,
      "step": 70310
    },
    {
      "epoch": 3.7504,
      "grad_norm": 0.13280825316905975,
      "learning_rate": 3.12e-06,
      "loss": 0.0017,
      "step": 70320
    },
    {
      "epoch": 3.7509333333333332,
      "grad_norm": 0.23259687423706055,
      "learning_rate": 3.1133333333333333e-06,
      "loss": 0.002,
      "step": 70330
    },
    {
      "epoch": 3.7514666666666665,
      "grad_norm": 0.12023825198411942,
      "learning_rate": 3.106666666666667e-06,
      "loss": 0.0018,
      "step": 70340
    },
    {
      "epoch": 3.752,
      "grad_norm": 0.14541180431842804,
      "learning_rate": 3.1e-06,
      "loss": 0.0017,
      "step": 70350
    },
    {
      "epoch": 3.7525333333333335,
      "grad_norm": 0.16586743295192719,
      "learning_rate": 3.0933333333333335e-06,
      "loss": 0.0019,
      "step": 70360
    },
    {
      "epoch": 3.7530666666666668,
      "grad_norm": 0.14061269164085388,
      "learning_rate": 3.086666666666667e-06,
      "loss": 0.0018,
      "step": 70370
    },
    {
      "epoch": 3.7536,
      "grad_norm": 0.2460726946592331,
      "learning_rate": 3.08e-06,
      "loss": 0.0025,
      "step": 70380
    },
    {
      "epoch": 3.7541333333333333,
      "grad_norm": 0.2340960055589676,
      "learning_rate": 3.0733333333333337e-06,
      "loss": 0.0033,
      "step": 70390
    },
    {
      "epoch": 3.7546666666666666,
      "grad_norm": 0.554261326789856,
      "learning_rate": 3.066666666666667e-06,
      "loss": 0.0027,
      "step": 70400
    },
    {
      "epoch": 3.7552,
      "grad_norm": 0.46660035848617554,
      "learning_rate": 3.06e-06,
      "loss": 0.0026,
      "step": 70410
    },
    {
      "epoch": 3.7557333333333336,
      "grad_norm": 0.3268372714519501,
      "learning_rate": 3.0533333333333335e-06,
      "loss": 0.0024,
      "step": 70420
    },
    {
      "epoch": 3.756266666666667,
      "grad_norm": 0.29416412115097046,
      "learning_rate": 3.0466666666666666e-06,
      "loss": 0.0021,
      "step": 70430
    },
    {
      "epoch": 3.7568,
      "grad_norm": 0.2957708239555359,
      "learning_rate": 3.04e-06,
      "loss": 0.0018,
      "step": 70440
    },
    {
      "epoch": 3.7573333333333334,
      "grad_norm": 0.0886329635977745,
      "learning_rate": 3.0333333333333337e-06,
      "loss": 0.002,
      "step": 70450
    },
    {
      "epoch": 3.7578666666666667,
      "grad_norm": 0.43951988220214844,
      "learning_rate": 3.0266666666666668e-06,
      "loss": 0.0024,
      "step": 70460
    },
    {
      "epoch": 3.7584,
      "grad_norm": 0.23197633028030396,
      "learning_rate": 3.0200000000000003e-06,
      "loss": 0.0025,
      "step": 70470
    },
    {
      "epoch": 3.7589333333333332,
      "grad_norm": 0.07463233172893524,
      "learning_rate": 3.0133333333333334e-06,
      "loss": 0.0027,
      "step": 70480
    },
    {
      "epoch": 3.7594666666666665,
      "grad_norm": 0.18570704758167267,
      "learning_rate": 3.0066666666666665e-06,
      "loss": 0.0019,
      "step": 70490
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.323645681142807,
      "learning_rate": 3e-06,
      "loss": 0.0025,
      "step": 70500
    },
    {
      "epoch": 3.760533333333333,
      "grad_norm": 0.15309256315231323,
      "learning_rate": 2.9933333333333336e-06,
      "loss": 0.0017,
      "step": 70510
    },
    {
      "epoch": 3.761066666666667,
      "grad_norm": 0.27348875999450684,
      "learning_rate": 2.9866666666666667e-06,
      "loss": 0.0028,
      "step": 70520
    },
    {
      "epoch": 3.7616,
      "grad_norm": 0.21236929297447205,
      "learning_rate": 2.9800000000000003e-06,
      "loss": 0.0018,
      "step": 70530
    },
    {
      "epoch": 3.7621333333333333,
      "grad_norm": 0.11785117536783218,
      "learning_rate": 2.9733333333333334e-06,
      "loss": 0.0023,
      "step": 70540
    },
    {
      "epoch": 3.7626666666666666,
      "grad_norm": 0.2037304937839508,
      "learning_rate": 2.966666666666667e-06,
      "loss": 0.0015,
      "step": 70550
    },
    {
      "epoch": 3.7632,
      "grad_norm": 0.15086938440799713,
      "learning_rate": 2.9600000000000005e-06,
      "loss": 0.003,
      "step": 70560
    },
    {
      "epoch": 3.7637333333333336,
      "grad_norm": 0.3217185139656067,
      "learning_rate": 2.9533333333333336e-06,
      "loss": 0.0022,
      "step": 70570
    },
    {
      "epoch": 3.764266666666667,
      "grad_norm": 0.29043135046958923,
      "learning_rate": 2.9466666666666667e-06,
      "loss": 0.002,
      "step": 70580
    },
    {
      "epoch": 3.7648,
      "grad_norm": 0.23086519539356232,
      "learning_rate": 2.9400000000000002e-06,
      "loss": 0.0025,
      "step": 70590
    },
    {
      "epoch": 3.7653333333333334,
      "grad_norm": 0.16120900213718414,
      "learning_rate": 2.9333333333333333e-06,
      "loss": 0.0018,
      "step": 70600
    },
    {
      "epoch": 3.7658666666666667,
      "grad_norm": 0.659515380859375,
      "learning_rate": 2.926666666666667e-06,
      "loss": 0.0021,
      "step": 70610
    },
    {
      "epoch": 3.7664,
      "grad_norm": 0.13684645295143127,
      "learning_rate": 2.92e-06,
      "loss": 0.0021,
      "step": 70620
    },
    {
      "epoch": 3.7669333333333332,
      "grad_norm": 0.2684221863746643,
      "learning_rate": 2.9133333333333335e-06,
      "loss": 0.0025,
      "step": 70630
    },
    {
      "epoch": 3.7674666666666665,
      "grad_norm": 0.5074824094772339,
      "learning_rate": 2.906666666666667e-06,
      "loss": 0.0025,
      "step": 70640
    },
    {
      "epoch": 3.768,
      "grad_norm": 0.11738797277212143,
      "learning_rate": 2.9e-06,
      "loss": 0.002,
      "step": 70650
    },
    {
      "epoch": 3.768533333333333,
      "grad_norm": 0.6474831700325012,
      "learning_rate": 2.8933333333333333e-06,
      "loss": 0.0022,
      "step": 70660
    },
    {
      "epoch": 3.769066666666667,
      "grad_norm": 0.20014412701129913,
      "learning_rate": 2.886666666666667e-06,
      "loss": 0.0025,
      "step": 70670
    },
    {
      "epoch": 3.7696,
      "grad_norm": 0.1782708466053009,
      "learning_rate": 2.88e-06,
      "loss": 0.0033,
      "step": 70680
    },
    {
      "epoch": 3.7701333333333333,
      "grad_norm": 0.13767209649085999,
      "learning_rate": 2.8733333333333335e-06,
      "loss": 0.0017,
      "step": 70690
    },
    {
      "epoch": 3.7706666666666666,
      "grad_norm": 0.1336493045091629,
      "learning_rate": 2.8666666666666666e-06,
      "loss": 0.0045,
      "step": 70700
    },
    {
      "epoch": 3.7712,
      "grad_norm": 0.2772148549556732,
      "learning_rate": 2.86e-06,
      "loss": 0.0024,
      "step": 70710
    },
    {
      "epoch": 3.7717333333333336,
      "grad_norm": 0.23409225046634674,
      "learning_rate": 2.8533333333333337e-06,
      "loss": 0.0028,
      "step": 70720
    },
    {
      "epoch": 3.772266666666667,
      "grad_norm": 0.4345968961715698,
      "learning_rate": 2.846666666666667e-06,
      "loss": 0.0024,
      "step": 70730
    },
    {
      "epoch": 3.7728,
      "grad_norm": 0.18868686258792877,
      "learning_rate": 2.8400000000000003e-06,
      "loss": 0.0019,
      "step": 70740
    },
    {
      "epoch": 3.7733333333333334,
      "grad_norm": 0.08643728494644165,
      "learning_rate": 2.8333333333333335e-06,
      "loss": 0.002,
      "step": 70750
    },
    {
      "epoch": 3.7738666666666667,
      "grad_norm": 0.4040044844150543,
      "learning_rate": 2.8266666666666666e-06,
      "loss": 0.0034,
      "step": 70760
    },
    {
      "epoch": 3.7744,
      "grad_norm": 0.19624312222003937,
      "learning_rate": 2.82e-06,
      "loss": 0.0026,
      "step": 70770
    },
    {
      "epoch": 3.7749333333333333,
      "grad_norm": 0.4438134431838989,
      "learning_rate": 2.8133333333333336e-06,
      "loss": 0.0018,
      "step": 70780
    },
    {
      "epoch": 3.7754666666666665,
      "grad_norm": 0.33634915947914124,
      "learning_rate": 2.8066666666666668e-06,
      "loss": 0.0031,
      "step": 70790
    },
    {
      "epoch": 3.776,
      "grad_norm": 0.3546002507209778,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.004,
      "step": 70800
    },
    {
      "epoch": 3.776533333333333,
      "grad_norm": 0.22007271647453308,
      "learning_rate": 2.7933333333333334e-06,
      "loss": 0.0019,
      "step": 70810
    },
    {
      "epoch": 3.777066666666667,
      "grad_norm": 0.4992624521255493,
      "learning_rate": 2.786666666666667e-06,
      "loss": 0.002,
      "step": 70820
    },
    {
      "epoch": 3.7776,
      "grad_norm": 0.6404247879981995,
      "learning_rate": 2.78e-06,
      "loss": 0.0021,
      "step": 70830
    },
    {
      "epoch": 3.7781333333333333,
      "grad_norm": 0.38859671354293823,
      "learning_rate": 2.773333333333333e-06,
      "loss": 0.0025,
      "step": 70840
    },
    {
      "epoch": 3.7786666666666666,
      "grad_norm": 0.23510748147964478,
      "learning_rate": 2.7666666666666667e-06,
      "loss": 0.0018,
      "step": 70850
    },
    {
      "epoch": 3.7792,
      "grad_norm": 0.1217896118760109,
      "learning_rate": 2.7600000000000003e-06,
      "loss": 0.0025,
      "step": 70860
    },
    {
      "epoch": 3.779733333333333,
      "grad_norm": 0.29788434505462646,
      "learning_rate": 2.7533333333333334e-06,
      "loss": 0.0032,
      "step": 70870
    },
    {
      "epoch": 3.780266666666667,
      "grad_norm": 0.4130871295928955,
      "learning_rate": 2.746666666666667e-06,
      "loss": 0.0017,
      "step": 70880
    },
    {
      "epoch": 3.7808,
      "grad_norm": 0.2023787498474121,
      "learning_rate": 2.74e-06,
      "loss": 0.0017,
      "step": 70890
    },
    {
      "epoch": 3.7813333333333334,
      "grad_norm": 0.10880917310714722,
      "learning_rate": 2.7333333333333336e-06,
      "loss": 0.0035,
      "step": 70900
    },
    {
      "epoch": 3.7818666666666667,
      "grad_norm": 0.19364413619041443,
      "learning_rate": 2.726666666666667e-06,
      "loss": 0.0028,
      "step": 70910
    },
    {
      "epoch": 3.7824,
      "grad_norm": 0.11402255296707153,
      "learning_rate": 2.72e-06,
      "loss": 0.0029,
      "step": 70920
    },
    {
      "epoch": 3.7829333333333333,
      "grad_norm": 0.17654691636562347,
      "learning_rate": 2.7133333333333333e-06,
      "loss": 0.0026,
      "step": 70930
    },
    {
      "epoch": 3.7834666666666665,
      "grad_norm": 0.14758612215518951,
      "learning_rate": 2.706666666666667e-06,
      "loss": 0.0019,
      "step": 70940
    },
    {
      "epoch": 3.784,
      "grad_norm": 0.2849957346916199,
      "learning_rate": 2.7e-06,
      "loss": 0.0026,
      "step": 70950
    },
    {
      "epoch": 3.784533333333333,
      "grad_norm": 0.26922568678855896,
      "learning_rate": 2.6933333333333335e-06,
      "loss": 0.0017,
      "step": 70960
    },
    {
      "epoch": 3.785066666666667,
      "grad_norm": 0.14218653738498688,
      "learning_rate": 2.6866666666666666e-06,
      "loss": 0.0026,
      "step": 70970
    },
    {
      "epoch": 3.7856,
      "grad_norm": 0.22516639530658722,
      "learning_rate": 2.68e-06,
      "loss": 0.0016,
      "step": 70980
    },
    {
      "epoch": 3.7861333333333334,
      "grad_norm": 0.220199853181839,
      "learning_rate": 2.6733333333333337e-06,
      "loss": 0.0016,
      "step": 70990
    },
    {
      "epoch": 3.7866666666666666,
      "grad_norm": 0.11561235785484314,
      "learning_rate": 2.666666666666667e-06,
      "loss": 0.0019,
      "step": 71000
    },
    {
      "epoch": 3.7872,
      "grad_norm": 0.17434754967689514,
      "learning_rate": 2.66e-06,
      "loss": 0.0035,
      "step": 71010
    },
    {
      "epoch": 3.787733333333333,
      "grad_norm": 0.7492344975471497,
      "learning_rate": 2.6533333333333335e-06,
      "loss": 0.0021,
      "step": 71020
    },
    {
      "epoch": 3.788266666666667,
      "grad_norm": 0.3463689386844635,
      "learning_rate": 2.6466666666666666e-06,
      "loss": 0.0021,
      "step": 71030
    },
    {
      "epoch": 3.7888,
      "grad_norm": 0.1781536340713501,
      "learning_rate": 2.64e-06,
      "loss": 0.0017,
      "step": 71040
    },
    {
      "epoch": 3.7893333333333334,
      "grad_norm": 0.4120965003967285,
      "learning_rate": 2.6333333333333337e-06,
      "loss": 0.0041,
      "step": 71050
    },
    {
      "epoch": 3.7898666666666667,
      "grad_norm": 0.5677671432495117,
      "learning_rate": 2.6266666666666668e-06,
      "loss": 0.0026,
      "step": 71060
    },
    {
      "epoch": 3.7904,
      "grad_norm": 0.1779809147119522,
      "learning_rate": 2.6200000000000003e-06,
      "loss": 0.0032,
      "step": 71070
    },
    {
      "epoch": 3.7909333333333333,
      "grad_norm": 0.07777506113052368,
      "learning_rate": 2.6133333333333334e-06,
      "loss": 0.0026,
      "step": 71080
    },
    {
      "epoch": 3.7914666666666665,
      "grad_norm": 0.43983983993530273,
      "learning_rate": 2.6066666666666666e-06,
      "loss": 0.0024,
      "step": 71090
    },
    {
      "epoch": 3.792,
      "grad_norm": 0.47818854451179504,
      "learning_rate": 2.6e-06,
      "loss": 0.0018,
      "step": 71100
    },
    {
      "epoch": 3.792533333333333,
      "grad_norm": 0.07062012702226639,
      "learning_rate": 2.593333333333333e-06,
      "loss": 0.0018,
      "step": 71110
    },
    {
      "epoch": 3.793066666666667,
      "grad_norm": 0.2033884972333908,
      "learning_rate": 2.5866666666666667e-06,
      "loss": 0.002,
      "step": 71120
    },
    {
      "epoch": 3.7936,
      "grad_norm": 0.25520992279052734,
      "learning_rate": 2.5800000000000003e-06,
      "loss": 0.0033,
      "step": 71130
    },
    {
      "epoch": 3.7941333333333334,
      "grad_norm": 0.3618979752063751,
      "learning_rate": 2.5733333333333334e-06,
      "loss": 0.0025,
      "step": 71140
    },
    {
      "epoch": 3.7946666666666666,
      "grad_norm": 0.10771434009075165,
      "learning_rate": 2.566666666666667e-06,
      "loss": 0.0017,
      "step": 71150
    },
    {
      "epoch": 3.7952,
      "grad_norm": 0.5059956908226013,
      "learning_rate": 2.56e-06,
      "loss": 0.0023,
      "step": 71160
    },
    {
      "epoch": 3.795733333333333,
      "grad_norm": 0.15010209381580353,
      "learning_rate": 2.5533333333333336e-06,
      "loss": 0.0035,
      "step": 71170
    },
    {
      "epoch": 3.796266666666667,
      "grad_norm": 0.08504336327314377,
      "learning_rate": 2.5466666666666667e-06,
      "loss": 0.0017,
      "step": 71180
    },
    {
      "epoch": 3.7968,
      "grad_norm": 0.2520683705806732,
      "learning_rate": 2.54e-06,
      "loss": 0.0029,
      "step": 71190
    },
    {
      "epoch": 3.7973333333333334,
      "grad_norm": 0.09845950454473495,
      "learning_rate": 2.5333333333333334e-06,
      "loss": 0.0035,
      "step": 71200
    },
    {
      "epoch": 3.7978666666666667,
      "grad_norm": 0.17327655851840973,
      "learning_rate": 2.526666666666667e-06,
      "loss": 0.003,
      "step": 71210
    },
    {
      "epoch": 3.7984,
      "grad_norm": 0.28793609142303467,
      "learning_rate": 2.52e-06,
      "loss": 0.002,
      "step": 71220
    },
    {
      "epoch": 3.7989333333333333,
      "grad_norm": 0.09136476367712021,
      "learning_rate": 2.5133333333333336e-06,
      "loss": 0.0027,
      "step": 71230
    },
    {
      "epoch": 3.7994666666666665,
      "grad_norm": 0.5933492183685303,
      "learning_rate": 2.506666666666667e-06,
      "loss": 0.003,
      "step": 71240
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.27449503540992737,
      "learning_rate": 2.5e-06,
      "loss": 0.0018,
      "step": 71250
    },
    {
      "epoch": 3.800533333333333,
      "grad_norm": 0.23766690492630005,
      "learning_rate": 2.4933333333333333e-06,
      "loss": 0.0018,
      "step": 71260
    },
    {
      "epoch": 3.801066666666667,
      "grad_norm": 0.07393714040517807,
      "learning_rate": 2.486666666666667e-06,
      "loss": 0.0026,
      "step": 71270
    },
    {
      "epoch": 3.8016,
      "grad_norm": 0.1538887321949005,
      "learning_rate": 2.48e-06,
      "loss": 0.0016,
      "step": 71280
    },
    {
      "epoch": 3.8021333333333334,
      "grad_norm": 0.22221818566322327,
      "learning_rate": 2.4733333333333335e-06,
      "loss": 0.0021,
      "step": 71290
    },
    {
      "epoch": 3.8026666666666666,
      "grad_norm": 0.5421671867370605,
      "learning_rate": 2.4666666666666666e-06,
      "loss": 0.0022,
      "step": 71300
    },
    {
      "epoch": 3.8032,
      "grad_norm": 0.282891184091568,
      "learning_rate": 2.46e-06,
      "loss": 0.0026,
      "step": 71310
    },
    {
      "epoch": 3.803733333333333,
      "grad_norm": 0.1485789716243744,
      "learning_rate": 2.4533333333333337e-06,
      "loss": 0.0017,
      "step": 71320
    },
    {
      "epoch": 3.804266666666667,
      "grad_norm": 0.264089971780777,
      "learning_rate": 2.446666666666667e-06,
      "loss": 0.0021,
      "step": 71330
    },
    {
      "epoch": 3.8048,
      "grad_norm": 0.2522708475589752,
      "learning_rate": 2.4400000000000004e-06,
      "loss": 0.0028,
      "step": 71340
    },
    {
      "epoch": 3.8053333333333335,
      "grad_norm": 0.12956580519676208,
      "learning_rate": 2.4333333333333335e-06,
      "loss": 0.0033,
      "step": 71350
    },
    {
      "epoch": 3.8058666666666667,
      "grad_norm": 0.5244396328926086,
      "learning_rate": 2.4266666666666666e-06,
      "loss": 0.0019,
      "step": 71360
    },
    {
      "epoch": 3.8064,
      "grad_norm": 0.5682869553565979,
      "learning_rate": 2.42e-06,
      "loss": 0.0027,
      "step": 71370
    },
    {
      "epoch": 3.8069333333333333,
      "grad_norm": 0.357708603143692,
      "learning_rate": 2.4133333333333332e-06,
      "loss": 0.0029,
      "step": 71380
    },
    {
      "epoch": 3.8074666666666666,
      "grad_norm": 0.3902634382247925,
      "learning_rate": 2.4066666666666668e-06,
      "loss": 0.0025,
      "step": 71390
    },
    {
      "epoch": 3.808,
      "grad_norm": 0.2464539110660553,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.0028,
      "step": 71400
    },
    {
      "epoch": 3.808533333333333,
      "grad_norm": 0.21244095265865326,
      "learning_rate": 2.3933333333333334e-06,
      "loss": 0.0026,
      "step": 71410
    },
    {
      "epoch": 3.809066666666667,
      "grad_norm": 0.39569321274757385,
      "learning_rate": 2.386666666666667e-06,
      "loss": 0.0017,
      "step": 71420
    },
    {
      "epoch": 3.8096,
      "grad_norm": 0.20796626806259155,
      "learning_rate": 2.38e-06,
      "loss": 0.002,
      "step": 71430
    },
    {
      "epoch": 3.8101333333333334,
      "grad_norm": 0.12682753801345825,
      "learning_rate": 2.373333333333333e-06,
      "loss": 0.0018,
      "step": 71440
    },
    {
      "epoch": 3.8106666666666666,
      "grad_norm": 0.16623178124427795,
      "learning_rate": 2.3666666666666667e-06,
      "loss": 0.0023,
      "step": 71450
    },
    {
      "epoch": 3.8112,
      "grad_norm": 0.5271704792976379,
      "learning_rate": 2.36e-06,
      "loss": 0.0028,
      "step": 71460
    },
    {
      "epoch": 3.811733333333333,
      "grad_norm": 0.1500789225101471,
      "learning_rate": 2.3533333333333334e-06,
      "loss": 0.0018,
      "step": 71470
    },
    {
      "epoch": 3.812266666666667,
      "grad_norm": 0.14239703118801117,
      "learning_rate": 2.346666666666667e-06,
      "loss": 0.0016,
      "step": 71480
    },
    {
      "epoch": 3.8128,
      "grad_norm": 0.178333580493927,
      "learning_rate": 2.34e-06,
      "loss": 0.0016,
      "step": 71490
    },
    {
      "epoch": 3.8133333333333335,
      "grad_norm": 0.09540289640426636,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 0.0028,
      "step": 71500
    },
    {
      "epoch": 3.8138666666666667,
      "grad_norm": 0.2860572636127472,
      "learning_rate": 2.326666666666667e-06,
      "loss": 0.0018,
      "step": 71510
    },
    {
      "epoch": 3.8144,
      "grad_norm": 0.061699844896793365,
      "learning_rate": 2.32e-06,
      "loss": 0.0027,
      "step": 71520
    },
    {
      "epoch": 3.8149333333333333,
      "grad_norm": 0.25653231143951416,
      "learning_rate": 2.3133333333333333e-06,
      "loss": 0.002,
      "step": 71530
    },
    {
      "epoch": 3.8154666666666666,
      "grad_norm": 0.35653263330459595,
      "learning_rate": 2.306666666666667e-06,
      "loss": 0.0021,
      "step": 71540
    },
    {
      "epoch": 3.816,
      "grad_norm": 0.257300466299057,
      "learning_rate": 2.3e-06,
      "loss": 0.0023,
      "step": 71550
    },
    {
      "epoch": 3.816533333333333,
      "grad_norm": 0.19906310737133026,
      "learning_rate": 2.2933333333333335e-06,
      "loss": 0.0017,
      "step": 71560
    },
    {
      "epoch": 3.817066666666667,
      "grad_norm": 0.47804614901542664,
      "learning_rate": 2.2866666666666667e-06,
      "loss": 0.002,
      "step": 71570
    },
    {
      "epoch": 3.8176,
      "grad_norm": 0.17444591224193573,
      "learning_rate": 2.28e-06,
      "loss": 0.0025,
      "step": 71580
    },
    {
      "epoch": 3.8181333333333334,
      "grad_norm": 0.2095547467470169,
      "learning_rate": 2.2733333333333337e-06,
      "loss": 0.0017,
      "step": 71590
    },
    {
      "epoch": 3.8186666666666667,
      "grad_norm": 0.19566494226455688,
      "learning_rate": 2.266666666666667e-06,
      "loss": 0.0032,
      "step": 71600
    },
    {
      "epoch": 3.8192,
      "grad_norm": 0.2540764808654785,
      "learning_rate": 2.26e-06,
      "loss": 0.0029,
      "step": 71610
    },
    {
      "epoch": 3.819733333333333,
      "grad_norm": 0.15709172189235687,
      "learning_rate": 2.2533333333333335e-06,
      "loss": 0.0034,
      "step": 71620
    },
    {
      "epoch": 3.820266666666667,
      "grad_norm": 0.12327991425991058,
      "learning_rate": 2.2466666666666666e-06,
      "loss": 0.0017,
      "step": 71630
    },
    {
      "epoch": 3.8208,
      "grad_norm": 0.5018746852874756,
      "learning_rate": 2.24e-06,
      "loss": 0.0017,
      "step": 71640
    },
    {
      "epoch": 3.8213333333333335,
      "grad_norm": 0.4225529134273529,
      "learning_rate": 2.2333333333333333e-06,
      "loss": 0.0027,
      "step": 71650
    },
    {
      "epoch": 3.8218666666666667,
      "grad_norm": 0.1728309988975525,
      "learning_rate": 2.226666666666667e-06,
      "loss": 0.0035,
      "step": 71660
    },
    {
      "epoch": 3.8224,
      "grad_norm": 0.185371533036232,
      "learning_rate": 2.2200000000000003e-06,
      "loss": 0.0017,
      "step": 71670
    },
    {
      "epoch": 3.8229333333333333,
      "grad_norm": 0.23899655044078827,
      "learning_rate": 2.2133333333333335e-06,
      "loss": 0.0025,
      "step": 71680
    },
    {
      "epoch": 3.8234666666666666,
      "grad_norm": 0.2898520827293396,
      "learning_rate": 2.2066666666666666e-06,
      "loss": 0.0016,
      "step": 71690
    },
    {
      "epoch": 3.824,
      "grad_norm": 0.26271572709083557,
      "learning_rate": 2.2e-06,
      "loss": 0.0028,
      "step": 71700
    },
    {
      "epoch": 3.824533333333333,
      "grad_norm": 0.33762630820274353,
      "learning_rate": 2.1933333333333332e-06,
      "loss": 0.0019,
      "step": 71710
    },
    {
      "epoch": 3.8250666666666664,
      "grad_norm": 0.08159814774990082,
      "learning_rate": 2.1866666666666668e-06,
      "loss": 0.0028,
      "step": 71720
    },
    {
      "epoch": 3.8256,
      "grad_norm": 0.35668015480041504,
      "learning_rate": 2.1800000000000003e-06,
      "loss": 0.0027,
      "step": 71730
    },
    {
      "epoch": 3.8261333333333334,
      "grad_norm": 0.1741112321615219,
      "learning_rate": 2.1733333333333334e-06,
      "loss": 0.003,
      "step": 71740
    },
    {
      "epoch": 3.8266666666666667,
      "grad_norm": 0.10891437530517578,
      "learning_rate": 2.166666666666667e-06,
      "loss": 0.0017,
      "step": 71750
    },
    {
      "epoch": 3.8272,
      "grad_norm": 0.10885803401470184,
      "learning_rate": 2.16e-06,
      "loss": 0.0024,
      "step": 71760
    },
    {
      "epoch": 3.827733333333333,
      "grad_norm": 0.32676437497138977,
      "learning_rate": 2.1533333333333336e-06,
      "loss": 0.0019,
      "step": 71770
    },
    {
      "epoch": 3.828266666666667,
      "grad_norm": 0.17683124542236328,
      "learning_rate": 2.1466666666666667e-06,
      "loss": 0.0016,
      "step": 71780
    },
    {
      "epoch": 3.8288,
      "grad_norm": 0.27088311314582825,
      "learning_rate": 2.14e-06,
      "loss": 0.0016,
      "step": 71790
    },
    {
      "epoch": 3.8293333333333335,
      "grad_norm": 0.1520942598581314,
      "learning_rate": 2.1333333333333334e-06,
      "loss": 0.0017,
      "step": 71800
    },
    {
      "epoch": 3.8298666666666668,
      "grad_norm": 0.4993430972099304,
      "learning_rate": 2.126666666666667e-06,
      "loss": 0.0034,
      "step": 71810
    },
    {
      "epoch": 3.8304,
      "grad_norm": 0.3805050551891327,
      "learning_rate": 2.12e-06,
      "loss": 0.0022,
      "step": 71820
    },
    {
      "epoch": 3.8309333333333333,
      "grad_norm": 0.4430074989795685,
      "learning_rate": 2.1133333333333336e-06,
      "loss": 0.0021,
      "step": 71830
    },
    {
      "epoch": 3.8314666666666666,
      "grad_norm": 0.18800608813762665,
      "learning_rate": 2.1066666666666667e-06,
      "loss": 0.0028,
      "step": 71840
    },
    {
      "epoch": 3.832,
      "grad_norm": 0.47167593240737915,
      "learning_rate": 2.1000000000000002e-06,
      "loss": 0.0021,
      "step": 71850
    },
    {
      "epoch": 3.832533333333333,
      "grad_norm": 0.4722733497619629,
      "learning_rate": 2.0933333333333338e-06,
      "loss": 0.0018,
      "step": 71860
    },
    {
      "epoch": 3.8330666666666664,
      "grad_norm": 0.22096659243106842,
      "learning_rate": 2.0866666666666665e-06,
      "loss": 0.0029,
      "step": 71870
    },
    {
      "epoch": 3.8336,
      "grad_norm": 0.19912059605121613,
      "learning_rate": 2.08e-06,
      "loss": 0.0024,
      "step": 71880
    },
    {
      "epoch": 3.8341333333333334,
      "grad_norm": 0.26114609837532043,
      "learning_rate": 2.0733333333333335e-06,
      "loss": 0.0026,
      "step": 71890
    },
    {
      "epoch": 3.8346666666666667,
      "grad_norm": 0.1811332106590271,
      "learning_rate": 2.0666666666666666e-06,
      "loss": 0.0023,
      "step": 71900
    },
    {
      "epoch": 3.8352,
      "grad_norm": 0.18487092852592468,
      "learning_rate": 2.06e-06,
      "loss": 0.0028,
      "step": 71910
    },
    {
      "epoch": 3.835733333333333,
      "grad_norm": 0.28408023715019226,
      "learning_rate": 2.0533333333333333e-06,
      "loss": 0.0018,
      "step": 71920
    },
    {
      "epoch": 3.836266666666667,
      "grad_norm": 0.23793189227581024,
      "learning_rate": 2.046666666666667e-06,
      "loss": 0.0026,
      "step": 71930
    },
    {
      "epoch": 3.8368,
      "grad_norm": 0.48276305198669434,
      "learning_rate": 2.0400000000000004e-06,
      "loss": 0.0028,
      "step": 71940
    },
    {
      "epoch": 3.8373333333333335,
      "grad_norm": 0.3261655271053314,
      "learning_rate": 2.033333333333333e-06,
      "loss": 0.0015,
      "step": 71950
    },
    {
      "epoch": 3.8378666666666668,
      "grad_norm": 0.5810084939002991,
      "learning_rate": 2.0266666666666666e-06,
      "loss": 0.0016,
      "step": 71960
    },
    {
      "epoch": 3.8384,
      "grad_norm": 0.21150535345077515,
      "learning_rate": 2.02e-06,
      "loss": 0.0031,
      "step": 71970
    },
    {
      "epoch": 3.8389333333333333,
      "grad_norm": 0.23162351548671722,
      "learning_rate": 2.0133333333333333e-06,
      "loss": 0.0024,
      "step": 71980
    },
    {
      "epoch": 3.8394666666666666,
      "grad_norm": 0.2573232650756836,
      "learning_rate": 2.006666666666667e-06,
      "loss": 0.0021,
      "step": 71990
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.2449290156364441,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.0017,
      "step": 72000
    },
    {
      "epoch": 3.840533333333333,
      "grad_norm": 0.11036226153373718,
      "learning_rate": 1.9933333333333334e-06,
      "loss": 0.0028,
      "step": 72010
    },
    {
      "epoch": 3.8410666666666664,
      "grad_norm": 0.16640208661556244,
      "learning_rate": 1.986666666666667e-06,
      "loss": 0.0021,
      "step": 72020
    },
    {
      "epoch": 3.8416,
      "grad_norm": 0.2568207085132599,
      "learning_rate": 1.98e-06,
      "loss": 0.0039,
      "step": 72030
    },
    {
      "epoch": 3.8421333333333334,
      "grad_norm": 0.4345218539237976,
      "learning_rate": 1.9733333333333332e-06,
      "loss": 0.0027,
      "step": 72040
    },
    {
      "epoch": 3.8426666666666667,
      "grad_norm": 0.10384709388017654,
      "learning_rate": 1.9666666666666668e-06,
      "loss": 0.003,
      "step": 72050
    },
    {
      "epoch": 3.8432,
      "grad_norm": 0.30083003640174866,
      "learning_rate": 1.96e-06,
      "loss": 0.0022,
      "step": 72060
    },
    {
      "epoch": 3.8437333333333332,
      "grad_norm": 0.09206521511077881,
      "learning_rate": 1.9533333333333334e-06,
      "loss": 0.0034,
      "step": 72070
    },
    {
      "epoch": 3.844266666666667,
      "grad_norm": 0.27998974919319153,
      "learning_rate": 1.946666666666667e-06,
      "loss": 0.0023,
      "step": 72080
    },
    {
      "epoch": 3.8448,
      "grad_norm": 0.1613653004169464,
      "learning_rate": 1.94e-06,
      "loss": 0.002,
      "step": 72090
    },
    {
      "epoch": 3.8453333333333335,
      "grad_norm": 0.1055527776479721,
      "learning_rate": 1.9333333333333336e-06,
      "loss": 0.0018,
      "step": 72100
    },
    {
      "epoch": 3.8458666666666668,
      "grad_norm": 0.09391500800848007,
      "learning_rate": 1.9266666666666667e-06,
      "loss": 0.0019,
      "step": 72110
    },
    {
      "epoch": 3.8464,
      "grad_norm": 0.7055426239967346,
      "learning_rate": 1.92e-06,
      "loss": 0.0025,
      "step": 72120
    },
    {
      "epoch": 3.8469333333333333,
      "grad_norm": 0.13192175328731537,
      "learning_rate": 1.9133333333333334e-06,
      "loss": 0.0018,
      "step": 72130
    },
    {
      "epoch": 3.8474666666666666,
      "grad_norm": 0.4388154447078705,
      "learning_rate": 1.9066666666666667e-06,
      "loss": 0.002,
      "step": 72140
    },
    {
      "epoch": 3.848,
      "grad_norm": 0.37296628952026367,
      "learning_rate": 1.9e-06,
      "loss": 0.0023,
      "step": 72150
    },
    {
      "epoch": 3.848533333333333,
      "grad_norm": 0.16304491460323334,
      "learning_rate": 1.8933333333333333e-06,
      "loss": 0.0015,
      "step": 72160
    },
    {
      "epoch": 3.8490666666666664,
      "grad_norm": 0.08798165619373322,
      "learning_rate": 1.8866666666666669e-06,
      "loss": 0.0018,
      "step": 72170
    },
    {
      "epoch": 3.8496,
      "grad_norm": 0.10767722874879837,
      "learning_rate": 1.8800000000000002e-06,
      "loss": 0.0019,
      "step": 72180
    },
    {
      "epoch": 3.8501333333333334,
      "grad_norm": 0.4387032687664032,
      "learning_rate": 1.8733333333333335e-06,
      "loss": 0.0015,
      "step": 72190
    },
    {
      "epoch": 3.8506666666666667,
      "grad_norm": 0.7618626952171326,
      "learning_rate": 1.8666666666666669e-06,
      "loss": 0.0021,
      "step": 72200
    },
    {
      "epoch": 3.8512,
      "grad_norm": 0.2797987461090088,
      "learning_rate": 1.86e-06,
      "loss": 0.0019,
      "step": 72210
    },
    {
      "epoch": 3.8517333333333332,
      "grad_norm": 0.21712873876094818,
      "learning_rate": 1.8533333333333333e-06,
      "loss": 0.0024,
      "step": 72220
    },
    {
      "epoch": 3.8522666666666665,
      "grad_norm": 0.19909444451332092,
      "learning_rate": 1.8466666666666666e-06,
      "loss": 0.0017,
      "step": 72230
    },
    {
      "epoch": 3.8528000000000002,
      "grad_norm": 0.3311961591243744,
      "learning_rate": 1.84e-06,
      "loss": 0.0021,
      "step": 72240
    },
    {
      "epoch": 3.8533333333333335,
      "grad_norm": 0.23753198981285095,
      "learning_rate": 1.8333333333333335e-06,
      "loss": 0.0019,
      "step": 72250
    },
    {
      "epoch": 3.8538666666666668,
      "grad_norm": 0.3883461654186249,
      "learning_rate": 1.8266666666666668e-06,
      "loss": 0.0016,
      "step": 72260
    },
    {
      "epoch": 3.8544,
      "grad_norm": 0.254690945148468,
      "learning_rate": 1.8200000000000002e-06,
      "loss": 0.0022,
      "step": 72270
    },
    {
      "epoch": 3.8549333333333333,
      "grad_norm": 0.46787405014038086,
      "learning_rate": 1.8133333333333335e-06,
      "loss": 0.0025,
      "step": 72280
    },
    {
      "epoch": 3.8554666666666666,
      "grad_norm": 0.16985824704170227,
      "learning_rate": 1.806666666666667e-06,
      "loss": 0.0021,
      "step": 72290
    },
    {
      "epoch": 3.856,
      "grad_norm": 0.11878436803817749,
      "learning_rate": 1.8e-06,
      "loss": 0.002,
      "step": 72300
    },
    {
      "epoch": 3.856533333333333,
      "grad_norm": 0.15568557381629944,
      "learning_rate": 1.7933333333333332e-06,
      "loss": 0.0019,
      "step": 72310
    },
    {
      "epoch": 3.8570666666666664,
      "grad_norm": 0.594978928565979,
      "learning_rate": 1.7866666666666668e-06,
      "loss": 0.0019,
      "step": 72320
    },
    {
      "epoch": 3.8576,
      "grad_norm": 0.1994541436433792,
      "learning_rate": 1.7800000000000001e-06,
      "loss": 0.0032,
      "step": 72330
    },
    {
      "epoch": 3.8581333333333334,
      "grad_norm": 0.4734404981136322,
      "learning_rate": 1.7733333333333334e-06,
      "loss": 0.0022,
      "step": 72340
    },
    {
      "epoch": 3.8586666666666667,
      "grad_norm": 0.41618549823760986,
      "learning_rate": 1.7666666666666668e-06,
      "loss": 0.0023,
      "step": 72350
    },
    {
      "epoch": 3.8592,
      "grad_norm": 0.26249775290489197,
      "learning_rate": 1.76e-06,
      "loss": 0.0017,
      "step": 72360
    },
    {
      "epoch": 3.8597333333333332,
      "grad_norm": 0.25467878580093384,
      "learning_rate": 1.7533333333333336e-06,
      "loss": 0.0017,
      "step": 72370
    },
    {
      "epoch": 3.8602666666666665,
      "grad_norm": 0.2668153941631317,
      "learning_rate": 1.7466666666666665e-06,
      "loss": 0.002,
      "step": 72380
    },
    {
      "epoch": 3.8608000000000002,
      "grad_norm": 0.1649923026561737,
      "learning_rate": 1.7399999999999999e-06,
      "loss": 0.0016,
      "step": 72390
    },
    {
      "epoch": 3.8613333333333335,
      "grad_norm": 0.2318660318851471,
      "learning_rate": 1.7333333333333334e-06,
      "loss": 0.0016,
      "step": 72400
    },
    {
      "epoch": 3.861866666666667,
      "grad_norm": 0.10976620763540268,
      "learning_rate": 1.7266666666666667e-06,
      "loss": 0.0015,
      "step": 72410
    },
    {
      "epoch": 3.8624,
      "grad_norm": 0.16421829164028168,
      "learning_rate": 1.72e-06,
      "loss": 0.0018,
      "step": 72420
    },
    {
      "epoch": 3.8629333333333333,
      "grad_norm": 0.23811711370944977,
      "learning_rate": 1.7133333333333334e-06,
      "loss": 0.0019,
      "step": 72430
    },
    {
      "epoch": 3.8634666666666666,
      "grad_norm": 0.5661740303039551,
      "learning_rate": 1.706666666666667e-06,
      "loss": 0.0018,
      "step": 72440
    },
    {
      "epoch": 3.864,
      "grad_norm": 0.20217181742191315,
      "learning_rate": 1.7000000000000002e-06,
      "loss": 0.0022,
      "step": 72450
    },
    {
      "epoch": 3.864533333333333,
      "grad_norm": 0.3294360935688019,
      "learning_rate": 1.6933333333333336e-06,
      "loss": 0.0018,
      "step": 72460
    },
    {
      "epoch": 3.8650666666666664,
      "grad_norm": 0.112270787358284,
      "learning_rate": 1.6866666666666667e-06,
      "loss": 0.0017,
      "step": 72470
    },
    {
      "epoch": 3.8656,
      "grad_norm": 0.17114152014255524,
      "learning_rate": 1.68e-06,
      "loss": 0.0031,
      "step": 72480
    },
    {
      "epoch": 3.8661333333333334,
      "grad_norm": 0.2946676015853882,
      "learning_rate": 1.6733333333333333e-06,
      "loss": 0.0036,
      "step": 72490
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 0.1636703610420227,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 0.0019,
      "step": 72500
    },
    {
      "epoch": 3.8672,
      "grad_norm": 0.16436149179935455,
      "learning_rate": 1.6600000000000002e-06,
      "loss": 0.0032,
      "step": 72510
    },
    {
      "epoch": 3.8677333333333332,
      "grad_norm": 0.10263506323099136,
      "learning_rate": 1.6533333333333335e-06,
      "loss": 0.0027,
      "step": 72520
    },
    {
      "epoch": 3.8682666666666665,
      "grad_norm": 0.19471924006938934,
      "learning_rate": 1.6466666666666669e-06,
      "loss": 0.0023,
      "step": 72530
    },
    {
      "epoch": 3.8688000000000002,
      "grad_norm": 0.24541597068309784,
      "learning_rate": 1.6400000000000002e-06,
      "loss": 0.0019,
      "step": 72540
    },
    {
      "epoch": 3.8693333333333335,
      "grad_norm": 0.42311081290245056,
      "learning_rate": 1.6333333333333333e-06,
      "loss": 0.0021,
      "step": 72550
    },
    {
      "epoch": 3.869866666666667,
      "grad_norm": 0.1514052152633667,
      "learning_rate": 1.6266666666666666e-06,
      "loss": 0.0016,
      "step": 72560
    },
    {
      "epoch": 3.8704,
      "grad_norm": 0.22347518801689148,
      "learning_rate": 1.62e-06,
      "loss": 0.003,
      "step": 72570
    },
    {
      "epoch": 3.8709333333333333,
      "grad_norm": 0.2657281458377838,
      "learning_rate": 1.6133333333333333e-06,
      "loss": 0.0031,
      "step": 72580
    },
    {
      "epoch": 3.8714666666666666,
      "grad_norm": 0.09137309342622757,
      "learning_rate": 1.6066666666666668e-06,
      "loss": 0.0018,
      "step": 72590
    },
    {
      "epoch": 3.872,
      "grad_norm": 0.6318472623825073,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.0019,
      "step": 72600
    },
    {
      "epoch": 3.872533333333333,
      "grad_norm": 0.2459530234336853,
      "learning_rate": 1.5933333333333335e-06,
      "loss": 0.0034,
      "step": 72610
    },
    {
      "epoch": 3.8730666666666664,
      "grad_norm": 0.4077065587043762,
      "learning_rate": 1.5866666666666668e-06,
      "loss": 0.0025,
      "step": 72620
    },
    {
      "epoch": 3.8736,
      "grad_norm": 0.2884959280490875,
      "learning_rate": 1.5800000000000003e-06,
      "loss": 0.0019,
      "step": 72630
    },
    {
      "epoch": 3.8741333333333334,
      "grad_norm": 0.35991108417510986,
      "learning_rate": 1.5733333333333332e-06,
      "loss": 0.0028,
      "step": 72640
    },
    {
      "epoch": 3.8746666666666667,
      "grad_norm": 0.3081721365451813,
      "learning_rate": 1.5666666666666666e-06,
      "loss": 0.0023,
      "step": 72650
    },
    {
      "epoch": 3.8752,
      "grad_norm": 0.35958051681518555,
      "learning_rate": 1.56e-06,
      "loss": 0.0024,
      "step": 72660
    },
    {
      "epoch": 3.8757333333333333,
      "grad_norm": 0.13845935463905334,
      "learning_rate": 1.5533333333333334e-06,
      "loss": 0.0017,
      "step": 72670
    },
    {
      "epoch": 3.8762666666666665,
      "grad_norm": 0.4546701908111572,
      "learning_rate": 1.5466666666666668e-06,
      "loss": 0.0019,
      "step": 72680
    },
    {
      "epoch": 3.8768000000000002,
      "grad_norm": 0.2230021059513092,
      "learning_rate": 1.54e-06,
      "loss": 0.002,
      "step": 72690
    },
    {
      "epoch": 3.8773333333333335,
      "grad_norm": 0.5756543278694153,
      "learning_rate": 1.5333333333333334e-06,
      "loss": 0.0018,
      "step": 72700
    },
    {
      "epoch": 3.877866666666667,
      "grad_norm": 0.07433033734560013,
      "learning_rate": 1.5266666666666667e-06,
      "loss": 0.0034,
      "step": 72710
    },
    {
      "epoch": 3.8784,
      "grad_norm": 0.1599169671535492,
      "learning_rate": 1.52e-06,
      "loss": 0.0015,
      "step": 72720
    },
    {
      "epoch": 3.8789333333333333,
      "grad_norm": 0.11319488286972046,
      "learning_rate": 1.5133333333333334e-06,
      "loss": 0.0025,
      "step": 72730
    },
    {
      "epoch": 3.8794666666666666,
      "grad_norm": 0.39466744661331177,
      "learning_rate": 1.5066666666666667e-06,
      "loss": 0.0017,
      "step": 72740
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.3404437303543091,
      "learning_rate": 1.5e-06,
      "loss": 0.0031,
      "step": 72750
    },
    {
      "epoch": 3.880533333333333,
      "grad_norm": 0.0713273286819458,
      "learning_rate": 1.4933333333333334e-06,
      "loss": 0.0019,
      "step": 72760
    },
    {
      "epoch": 3.8810666666666664,
      "grad_norm": 0.13060316443443298,
      "learning_rate": 1.4866666666666667e-06,
      "loss": 0.0024,
      "step": 72770
    },
    {
      "epoch": 3.8816,
      "grad_norm": 0.09485092014074326,
      "learning_rate": 1.4800000000000002e-06,
      "loss": 0.0019,
      "step": 72780
    },
    {
      "epoch": 3.8821333333333334,
      "grad_norm": 0.2538575530052185,
      "learning_rate": 1.4733333333333333e-06,
      "loss": 0.0026,
      "step": 72790
    },
    {
      "epoch": 3.8826666666666667,
      "grad_norm": 0.1460486352443695,
      "learning_rate": 1.4666666666666667e-06,
      "loss": 0.0033,
      "step": 72800
    },
    {
      "epoch": 3.8832,
      "grad_norm": 0.2405703067779541,
      "learning_rate": 1.46e-06,
      "loss": 0.0026,
      "step": 72810
    },
    {
      "epoch": 3.8837333333333333,
      "grad_norm": 0.34642162919044495,
      "learning_rate": 1.4533333333333335e-06,
      "loss": 0.0024,
      "step": 72820
    },
    {
      "epoch": 3.8842666666666665,
      "grad_norm": 0.25024956464767456,
      "learning_rate": 1.4466666666666667e-06,
      "loss": 0.0025,
      "step": 72830
    },
    {
      "epoch": 3.8848000000000003,
      "grad_norm": 0.28697946667671204,
      "learning_rate": 1.44e-06,
      "loss": 0.0022,
      "step": 72840
    },
    {
      "epoch": 3.8853333333333335,
      "grad_norm": 0.10450582206249237,
      "learning_rate": 1.4333333333333333e-06,
      "loss": 0.0031,
      "step": 72850
    },
    {
      "epoch": 3.885866666666667,
      "grad_norm": 0.3386106789112091,
      "learning_rate": 1.4266666666666668e-06,
      "loss": 0.0018,
      "step": 72860
    },
    {
      "epoch": 3.8864,
      "grad_norm": 0.2305876612663269,
      "learning_rate": 1.4200000000000002e-06,
      "loss": 0.0019,
      "step": 72870
    },
    {
      "epoch": 3.8869333333333334,
      "grad_norm": 0.2983216941356659,
      "learning_rate": 1.4133333333333333e-06,
      "loss": 0.002,
      "step": 72880
    },
    {
      "epoch": 3.8874666666666666,
      "grad_norm": 0.2628914415836334,
      "learning_rate": 1.4066666666666668e-06,
      "loss": 0.0019,
      "step": 72890
    },
    {
      "epoch": 3.888,
      "grad_norm": 0.2996496856212616,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 0.0023,
      "step": 72900
    },
    {
      "epoch": 3.888533333333333,
      "grad_norm": 0.10740546137094498,
      "learning_rate": 1.3933333333333335e-06,
      "loss": 0.002,
      "step": 72910
    },
    {
      "epoch": 3.8890666666666664,
      "grad_norm": 0.3018563985824585,
      "learning_rate": 1.3866666666666666e-06,
      "loss": 0.0032,
      "step": 72920
    },
    {
      "epoch": 3.8895999999999997,
      "grad_norm": 0.2696758806705475,
      "learning_rate": 1.3800000000000001e-06,
      "loss": 0.002,
      "step": 72930
    },
    {
      "epoch": 3.8901333333333334,
      "grad_norm": 0.1730729043483734,
      "learning_rate": 1.3733333333333335e-06,
      "loss": 0.0021,
      "step": 72940
    },
    {
      "epoch": 3.8906666666666667,
      "grad_norm": 0.3916834592819214,
      "learning_rate": 1.3666666666666668e-06,
      "loss": 0.0033,
      "step": 72950
    },
    {
      "epoch": 3.8912,
      "grad_norm": 0.1575678139925003,
      "learning_rate": 1.36e-06,
      "loss": 0.0029,
      "step": 72960
    },
    {
      "epoch": 3.8917333333333333,
      "grad_norm": 0.12186041474342346,
      "learning_rate": 1.3533333333333334e-06,
      "loss": 0.0018,
      "step": 72970
    },
    {
      "epoch": 3.8922666666666665,
      "grad_norm": 0.08936787396669388,
      "learning_rate": 1.3466666666666668e-06,
      "loss": 0.0018,
      "step": 72980
    },
    {
      "epoch": 3.8928000000000003,
      "grad_norm": 0.33765551447868347,
      "learning_rate": 1.34e-06,
      "loss": 0.0021,
      "step": 72990
    },
    {
      "epoch": 3.8933333333333335,
      "grad_norm": 0.16023240983486176,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.0018,
      "step": 73000
    },
    {
      "epoch": 3.893866666666667,
      "grad_norm": 0.2701049745082855,
      "learning_rate": 1.3266666666666667e-06,
      "loss": 0.0024,
      "step": 73010
    },
    {
      "epoch": 3.8944,
      "grad_norm": 0.14930790662765503,
      "learning_rate": 1.32e-06,
      "loss": 0.0024,
      "step": 73020
    },
    {
      "epoch": 3.8949333333333334,
      "grad_norm": 0.3320743441581726,
      "learning_rate": 1.3133333333333334e-06,
      "loss": 0.0028,
      "step": 73030
    },
    {
      "epoch": 3.8954666666666666,
      "grad_norm": 0.21853452920913696,
      "learning_rate": 1.3066666666666667e-06,
      "loss": 0.0019,
      "step": 73040
    },
    {
      "epoch": 3.896,
      "grad_norm": 0.3872077167034149,
      "learning_rate": 1.3e-06,
      "loss": 0.0026,
      "step": 73050
    },
    {
      "epoch": 3.896533333333333,
      "grad_norm": 0.07852574437856674,
      "learning_rate": 1.2933333333333334e-06,
      "loss": 0.0019,
      "step": 73060
    },
    {
      "epoch": 3.8970666666666665,
      "grad_norm": 0.6880567669868469,
      "learning_rate": 1.2866666666666667e-06,
      "loss": 0.0019,
      "step": 73070
    },
    {
      "epoch": 3.8975999999999997,
      "grad_norm": 0.47206100821495056,
      "learning_rate": 1.28e-06,
      "loss": 0.0021,
      "step": 73080
    },
    {
      "epoch": 3.8981333333333335,
      "grad_norm": 0.12184952944517136,
      "learning_rate": 1.2733333333333334e-06,
      "loss": 0.0033,
      "step": 73090
    },
    {
      "epoch": 3.8986666666666667,
      "grad_norm": 0.31199151277542114,
      "learning_rate": 1.2666666666666667e-06,
      "loss": 0.0018,
      "step": 73100
    },
    {
      "epoch": 3.8992,
      "grad_norm": 0.34060022234916687,
      "learning_rate": 1.26e-06,
      "loss": 0.0018,
      "step": 73110
    },
    {
      "epoch": 3.8997333333333333,
      "grad_norm": 0.5832935571670532,
      "learning_rate": 1.2533333333333335e-06,
      "loss": 0.0028,
      "step": 73120
    },
    {
      "epoch": 3.9002666666666665,
      "grad_norm": 0.3223997950553894,
      "learning_rate": 1.2466666666666667e-06,
      "loss": 0.0019,
      "step": 73130
    },
    {
      "epoch": 3.9008000000000003,
      "grad_norm": 0.10296371579170227,
      "learning_rate": 1.24e-06,
      "loss": 0.0017,
      "step": 73140
    },
    {
      "epoch": 3.9013333333333335,
      "grad_norm": 0.39622586965560913,
      "learning_rate": 1.2333333333333333e-06,
      "loss": 0.0023,
      "step": 73150
    },
    {
      "epoch": 3.901866666666667,
      "grad_norm": 0.17995408177375793,
      "learning_rate": 1.2266666666666669e-06,
      "loss": 0.0017,
      "step": 73160
    },
    {
      "epoch": 3.9024,
      "grad_norm": 0.28090259432792664,
      "learning_rate": 1.2200000000000002e-06,
      "loss": 0.0019,
      "step": 73170
    },
    {
      "epoch": 3.9029333333333334,
      "grad_norm": 0.32556337118148804,
      "learning_rate": 1.2133333333333333e-06,
      "loss": 0.0023,
      "step": 73180
    },
    {
      "epoch": 3.9034666666666666,
      "grad_norm": 0.17768682539463043,
      "learning_rate": 1.2066666666666666e-06,
      "loss": 0.0027,
      "step": 73190
    },
    {
      "epoch": 3.904,
      "grad_norm": 0.9438424110412598,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 0.0032,
      "step": 73200
    },
    {
      "epoch": 3.904533333333333,
      "grad_norm": 0.4267178177833557,
      "learning_rate": 1.1933333333333335e-06,
      "loss": 0.0032,
      "step": 73210
    },
    {
      "epoch": 3.9050666666666665,
      "grad_norm": 0.14763906598091125,
      "learning_rate": 1.1866666666666666e-06,
      "loss": 0.0021,
      "step": 73220
    },
    {
      "epoch": 3.9055999999999997,
      "grad_norm": 0.4196322560310364,
      "learning_rate": 1.18e-06,
      "loss": 0.0027,
      "step": 73230
    },
    {
      "epoch": 3.9061333333333335,
      "grad_norm": 0.15521390736103058,
      "learning_rate": 1.1733333333333335e-06,
      "loss": 0.0035,
      "step": 73240
    },
    {
      "epoch": 3.9066666666666667,
      "grad_norm": 0.1726396232843399,
      "learning_rate": 1.1666666666666668e-06,
      "loss": 0.0017,
      "step": 73250
    },
    {
      "epoch": 3.9072,
      "grad_norm": 0.29491421580314636,
      "learning_rate": 1.16e-06,
      "loss": 0.0041,
      "step": 73260
    },
    {
      "epoch": 3.9077333333333333,
      "grad_norm": 0.2128874808549881,
      "learning_rate": 1.1533333333333334e-06,
      "loss": 0.002,
      "step": 73270
    },
    {
      "epoch": 3.9082666666666666,
      "grad_norm": 0.2286134660243988,
      "learning_rate": 1.1466666666666668e-06,
      "loss": 0.0017,
      "step": 73280
    },
    {
      "epoch": 3.9088000000000003,
      "grad_norm": 0.12280023097991943,
      "learning_rate": 1.14e-06,
      "loss": 0.0019,
      "step": 73290
    },
    {
      "epoch": 3.9093333333333335,
      "grad_norm": 0.30192023515701294,
      "learning_rate": 1.1333333333333334e-06,
      "loss": 0.0032,
      "step": 73300
    },
    {
      "epoch": 3.909866666666667,
      "grad_norm": 0.6556512713432312,
      "learning_rate": 1.1266666666666667e-06,
      "loss": 0.0021,
      "step": 73310
    },
    {
      "epoch": 3.9104,
      "grad_norm": 0.5302535891532898,
      "learning_rate": 1.12e-06,
      "loss": 0.002,
      "step": 73320
    },
    {
      "epoch": 3.9109333333333334,
      "grad_norm": 0.10999836772680283,
      "learning_rate": 1.1133333333333334e-06,
      "loss": 0.002,
      "step": 73330
    },
    {
      "epoch": 3.9114666666666666,
      "grad_norm": 0.23638224601745605,
      "learning_rate": 1.1066666666666667e-06,
      "loss": 0.0017,
      "step": 73340
    },
    {
      "epoch": 3.912,
      "grad_norm": 0.25814223289489746,
      "learning_rate": 1.1e-06,
      "loss": 0.0019,
      "step": 73350
    },
    {
      "epoch": 3.912533333333333,
      "grad_norm": 0.25092872977256775,
      "learning_rate": 1.0933333333333334e-06,
      "loss": 0.0028,
      "step": 73360
    },
    {
      "epoch": 3.9130666666666665,
      "grad_norm": 0.29471391439437866,
      "learning_rate": 1.0866666666666667e-06,
      "loss": 0.0019,
      "step": 73370
    },
    {
      "epoch": 3.9135999999999997,
      "grad_norm": 0.34155070781707764,
      "learning_rate": 1.08e-06,
      "loss": 0.0025,
      "step": 73380
    },
    {
      "epoch": 3.9141333333333335,
      "grad_norm": 0.27100464701652527,
      "learning_rate": 1.0733333333333334e-06,
      "loss": 0.0043,
      "step": 73390
    },
    {
      "epoch": 3.9146666666666667,
      "grad_norm": 0.2662685811519623,
      "learning_rate": 1.0666666666666667e-06,
      "loss": 0.0019,
      "step": 73400
    },
    {
      "epoch": 3.9152,
      "grad_norm": 0.16011077165603638,
      "learning_rate": 1.06e-06,
      "loss": 0.002,
      "step": 73410
    },
    {
      "epoch": 3.9157333333333333,
      "grad_norm": 0.2523099482059479,
      "learning_rate": 1.0533333333333333e-06,
      "loss": 0.0019,
      "step": 73420
    },
    {
      "epoch": 3.9162666666666666,
      "grad_norm": 0.2063521295785904,
      "learning_rate": 1.0466666666666669e-06,
      "loss": 0.0018,
      "step": 73430
    },
    {
      "epoch": 3.9168,
      "grad_norm": 0.333819717168808,
      "learning_rate": 1.04e-06,
      "loss": 0.0025,
      "step": 73440
    },
    {
      "epoch": 3.9173333333333336,
      "grad_norm": 0.5376331210136414,
      "learning_rate": 1.0333333333333333e-06,
      "loss": 0.003,
      "step": 73450
    },
    {
      "epoch": 3.917866666666667,
      "grad_norm": 0.27365049719810486,
      "learning_rate": 1.0266666666666666e-06,
      "loss": 0.0021,
      "step": 73460
    },
    {
      "epoch": 3.9184,
      "grad_norm": 0.3268510401248932,
      "learning_rate": 1.0200000000000002e-06,
      "loss": 0.0037,
      "step": 73470
    },
    {
      "epoch": 3.9189333333333334,
      "grad_norm": 0.07610135525465012,
      "learning_rate": 1.0133333333333333e-06,
      "loss": 0.0019,
      "step": 73480
    },
    {
      "epoch": 3.9194666666666667,
      "grad_norm": 0.14756417274475098,
      "learning_rate": 1.0066666666666666e-06,
      "loss": 0.0019,
      "step": 73490
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.16554728150367737,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.003,
      "step": 73500
    },
    {
      "epoch": 3.920533333333333,
      "grad_norm": 0.23460274934768677,
      "learning_rate": 9.933333333333335e-07,
      "loss": 0.0029,
      "step": 73510
    },
    {
      "epoch": 3.9210666666666665,
      "grad_norm": 0.3670273721218109,
      "learning_rate": 9.866666666666666e-07,
      "loss": 0.0023,
      "step": 73520
    },
    {
      "epoch": 3.9215999999999998,
      "grad_norm": 0.1751735806465149,
      "learning_rate": 9.8e-07,
      "loss": 0.0024,
      "step": 73530
    },
    {
      "epoch": 3.9221333333333335,
      "grad_norm": 0.3525906205177307,
      "learning_rate": 9.733333333333335e-07,
      "loss": 0.0029,
      "step": 73540
    },
    {
      "epoch": 3.9226666666666667,
      "grad_norm": 0.15220580995082855,
      "learning_rate": 9.666666666666668e-07,
      "loss": 0.0019,
      "step": 73550
    },
    {
      "epoch": 3.9232,
      "grad_norm": 0.3135209381580353,
      "learning_rate": 9.6e-07,
      "loss": 0.0027,
      "step": 73560
    },
    {
      "epoch": 3.9237333333333333,
      "grad_norm": 0.18485422432422638,
      "learning_rate": 9.533333333333333e-07,
      "loss": 0.0034,
      "step": 73570
    },
    {
      "epoch": 3.9242666666666666,
      "grad_norm": 0.151055708527565,
      "learning_rate": 9.466666666666667e-07,
      "loss": 0.0032,
      "step": 73580
    },
    {
      "epoch": 3.9248,
      "grad_norm": 0.2229897826910019,
      "learning_rate": 9.400000000000001e-07,
      "loss": 0.0016,
      "step": 73590
    },
    {
      "epoch": 3.9253333333333336,
      "grad_norm": 0.17517676949501038,
      "learning_rate": 9.333333333333334e-07,
      "loss": 0.0016,
      "step": 73600
    },
    {
      "epoch": 3.925866666666667,
      "grad_norm": 0.34339264035224915,
      "learning_rate": 9.266666666666667e-07,
      "loss": 0.0024,
      "step": 73610
    },
    {
      "epoch": 3.9264,
      "grad_norm": 0.27002108097076416,
      "learning_rate": 9.2e-07,
      "loss": 0.0017,
      "step": 73620
    },
    {
      "epoch": 3.9269333333333334,
      "grad_norm": 0.19862240552902222,
      "learning_rate": 9.133333333333334e-07,
      "loss": 0.0017,
      "step": 73630
    },
    {
      "epoch": 3.9274666666666667,
      "grad_norm": 0.22735610604286194,
      "learning_rate": 9.066666666666667e-07,
      "loss": 0.0017,
      "step": 73640
    },
    {
      "epoch": 3.928,
      "grad_norm": 0.15628334879875183,
      "learning_rate": 9e-07,
      "loss": 0.0022,
      "step": 73650
    },
    {
      "epoch": 3.928533333333333,
      "grad_norm": 0.2775248885154724,
      "learning_rate": 8.933333333333334e-07,
      "loss": 0.0017,
      "step": 73660
    },
    {
      "epoch": 3.9290666666666665,
      "grad_norm": 0.33864325284957886,
      "learning_rate": 8.866666666666667e-07,
      "loss": 0.0028,
      "step": 73670
    },
    {
      "epoch": 3.9295999999999998,
      "grad_norm": 0.3464381694793701,
      "learning_rate": 8.8e-07,
      "loss": 0.0019,
      "step": 73680
    },
    {
      "epoch": 3.9301333333333335,
      "grad_norm": 0.10243450105190277,
      "learning_rate": 8.733333333333333e-07,
      "loss": 0.0017,
      "step": 73690
    },
    {
      "epoch": 3.9306666666666668,
      "grad_norm": 0.2517448365688324,
      "learning_rate": 8.666666666666667e-07,
      "loss": 0.0038,
      "step": 73700
    },
    {
      "epoch": 3.9312,
      "grad_norm": 0.1330854445695877,
      "learning_rate": 8.6e-07,
      "loss": 0.0026,
      "step": 73710
    },
    {
      "epoch": 3.9317333333333333,
      "grad_norm": 0.3947466313838959,
      "learning_rate": 8.533333333333335e-07,
      "loss": 0.0025,
      "step": 73720
    },
    {
      "epoch": 3.9322666666666666,
      "grad_norm": 0.46885403990745544,
      "learning_rate": 8.466666666666668e-07,
      "loss": 0.0018,
      "step": 73730
    },
    {
      "epoch": 3.9328,
      "grad_norm": 0.2686096429824829,
      "learning_rate": 8.4e-07,
      "loss": 0.0016,
      "step": 73740
    },
    {
      "epoch": 3.9333333333333336,
      "grad_norm": 0.1439744085073471,
      "learning_rate": 8.333333333333333e-07,
      "loss": 0.0018,
      "step": 73750
    },
    {
      "epoch": 3.933866666666667,
      "grad_norm": 0.12668298184871674,
      "learning_rate": 8.266666666666668e-07,
      "loss": 0.0027,
      "step": 73760
    },
    {
      "epoch": 3.9344,
      "grad_norm": 0.13658224046230316,
      "learning_rate": 8.200000000000001e-07,
      "loss": 0.0016,
      "step": 73770
    },
    {
      "epoch": 3.9349333333333334,
      "grad_norm": 0.1929015815258026,
      "learning_rate": 8.133333333333333e-07,
      "loss": 0.0027,
      "step": 73780
    },
    {
      "epoch": 3.9354666666666667,
      "grad_norm": 0.3852963149547577,
      "learning_rate": 8.066666666666666e-07,
      "loss": 0.0023,
      "step": 73790
    },
    {
      "epoch": 3.936,
      "grad_norm": 0.40504759550094604,
      "learning_rate": 8.000000000000001e-07,
      "loss": 0.002,
      "step": 73800
    },
    {
      "epoch": 3.936533333333333,
      "grad_norm": 0.12675251066684723,
      "learning_rate": 7.933333333333334e-07,
      "loss": 0.0025,
      "step": 73810
    },
    {
      "epoch": 3.9370666666666665,
      "grad_norm": 0.1042928472161293,
      "learning_rate": 7.866666666666666e-07,
      "loss": 0.0021,
      "step": 73820
    },
    {
      "epoch": 3.9375999999999998,
      "grad_norm": 0.2405257672071457,
      "learning_rate": 7.8e-07,
      "loss": 0.0018,
      "step": 73830
    },
    {
      "epoch": 3.9381333333333335,
      "grad_norm": 0.12475766241550446,
      "learning_rate": 7.733333333333334e-07,
      "loss": 0.0023,
      "step": 73840
    },
    {
      "epoch": 3.9386666666666668,
      "grad_norm": 0.2675335705280304,
      "learning_rate": 7.666666666666667e-07,
      "loss": 0.0017,
      "step": 73850
    },
    {
      "epoch": 3.9392,
      "grad_norm": 0.1586608737707138,
      "learning_rate": 7.6e-07,
      "loss": 0.0024,
      "step": 73860
    },
    {
      "epoch": 3.9397333333333333,
      "grad_norm": 0.25505128502845764,
      "learning_rate": 7.533333333333334e-07,
      "loss": 0.0027,
      "step": 73870
    },
    {
      "epoch": 3.9402666666666666,
      "grad_norm": 0.28451672196388245,
      "learning_rate": 7.466666666666667e-07,
      "loss": 0.0031,
      "step": 73880
    },
    {
      "epoch": 3.9408,
      "grad_norm": 0.2997336983680725,
      "learning_rate": 7.400000000000001e-07,
      "loss": 0.002,
      "step": 73890
    },
    {
      "epoch": 3.9413333333333336,
      "grad_norm": 0.642623245716095,
      "learning_rate": 7.333333333333333e-07,
      "loss": 0.0019,
      "step": 73900
    },
    {
      "epoch": 3.941866666666667,
      "grad_norm": 0.21866275370121002,
      "learning_rate": 7.266666666666668e-07,
      "loss": 0.0026,
      "step": 73910
    },
    {
      "epoch": 3.9424,
      "grad_norm": 0.09806816279888153,
      "learning_rate": 7.2e-07,
      "loss": 0.0031,
      "step": 73920
    },
    {
      "epoch": 3.9429333333333334,
      "grad_norm": 0.36566755175590515,
      "learning_rate": 7.133333333333334e-07,
      "loss": 0.0024,
      "step": 73930
    },
    {
      "epoch": 3.9434666666666667,
      "grad_norm": 0.15345363318920135,
      "learning_rate": 7.066666666666666e-07,
      "loss": 0.0015,
      "step": 73940
    },
    {
      "epoch": 3.944,
      "grad_norm": 0.1553286761045456,
      "learning_rate": 7.000000000000001e-07,
      "loss": 0.0031,
      "step": 73950
    },
    {
      "epoch": 3.9445333333333332,
      "grad_norm": 0.14003024995326996,
      "learning_rate": 6.933333333333333e-07,
      "loss": 0.0018,
      "step": 73960
    },
    {
      "epoch": 3.9450666666666665,
      "grad_norm": 0.2523409128189087,
      "learning_rate": 6.866666666666667e-07,
      "loss": 0.0017,
      "step": 73970
    },
    {
      "epoch": 3.9455999999999998,
      "grad_norm": 0.3492687940597534,
      "learning_rate": 6.8e-07,
      "loss": 0.0023,
      "step": 73980
    },
    {
      "epoch": 3.9461333333333335,
      "grad_norm": 0.26989662647247314,
      "learning_rate": 6.733333333333334e-07,
      "loss": 0.002,
      "step": 73990
    },
    {
      "epoch": 3.9466666666666668,
      "grad_norm": 0.14219781756401062,
      "learning_rate": 6.666666666666667e-07,
      "loss": 0.0027,
      "step": 74000
    },
    {
      "epoch": 3.9472,
      "grad_norm": 0.08523549139499664,
      "learning_rate": 6.6e-07,
      "loss": 0.0025,
      "step": 74010
    },
    {
      "epoch": 3.9477333333333333,
      "grad_norm": 0.12892290949821472,
      "learning_rate": 6.533333333333334e-07,
      "loss": 0.002,
      "step": 74020
    },
    {
      "epoch": 3.9482666666666666,
      "grad_norm": 0.11523865163326263,
      "learning_rate": 6.466666666666667e-07,
      "loss": 0.0032,
      "step": 74030
    },
    {
      "epoch": 3.9488,
      "grad_norm": 0.572441041469574,
      "learning_rate": 6.4e-07,
      "loss": 0.0027,
      "step": 74040
    },
    {
      "epoch": 3.9493333333333336,
      "grad_norm": 0.35824885964393616,
      "learning_rate": 6.333333333333333e-07,
      "loss": 0.002,
      "step": 74050
    },
    {
      "epoch": 3.949866666666667,
      "grad_norm": 0.6055330634117126,
      "learning_rate": 6.266666666666668e-07,
      "loss": 0.0025,
      "step": 74060
    },
    {
      "epoch": 3.9504,
      "grad_norm": 0.5376031994819641,
      "learning_rate": 6.2e-07,
      "loss": 0.0021,
      "step": 74070
    },
    {
      "epoch": 3.9509333333333334,
      "grad_norm": 0.20567743480205536,
      "learning_rate": 6.133333333333334e-07,
      "loss": 0.0018,
      "step": 74080
    },
    {
      "epoch": 3.9514666666666667,
      "grad_norm": 0.7187410593032837,
      "learning_rate": 6.066666666666666e-07,
      "loss": 0.002,
      "step": 74090
    },
    {
      "epoch": 3.952,
      "grad_norm": 0.21866336464881897,
      "learning_rate": 6.000000000000001e-07,
      "loss": 0.0031,
      "step": 74100
    },
    {
      "epoch": 3.9525333333333332,
      "grad_norm": 0.3608408272266388,
      "learning_rate": 5.933333333333333e-07,
      "loss": 0.0023,
      "step": 74110
    },
    {
      "epoch": 3.9530666666666665,
      "grad_norm": 0.2992402911186218,
      "learning_rate": 5.866666666666667e-07,
      "loss": 0.0029,
      "step": 74120
    },
    {
      "epoch": 3.9536,
      "grad_norm": 0.30375322699546814,
      "learning_rate": 5.8e-07,
      "loss": 0.0021,
      "step": 74130
    },
    {
      "epoch": 3.9541333333333335,
      "grad_norm": 0.22938591241836548,
      "learning_rate": 5.733333333333334e-07,
      "loss": 0.0017,
      "step": 74140
    },
    {
      "epoch": 3.9546666666666668,
      "grad_norm": 0.28753775358200073,
      "learning_rate": 5.666666666666667e-07,
      "loss": 0.0018,
      "step": 74150
    },
    {
      "epoch": 3.9552,
      "grad_norm": 0.32210680842399597,
      "learning_rate": 5.6e-07,
      "loss": 0.0024,
      "step": 74160
    },
    {
      "epoch": 3.9557333333333333,
      "grad_norm": 0.20511792600154877,
      "learning_rate": 5.533333333333334e-07,
      "loss": 0.0018,
      "step": 74170
    },
    {
      "epoch": 3.9562666666666666,
      "grad_norm": 0.13111960887908936,
      "learning_rate": 5.466666666666667e-07,
      "loss": 0.0021,
      "step": 74180
    },
    {
      "epoch": 3.9568,
      "grad_norm": 0.49413636326789856,
      "learning_rate": 5.4e-07,
      "loss": 0.0017,
      "step": 74190
    },
    {
      "epoch": 3.9573333333333336,
      "grad_norm": 0.21736612915992737,
      "learning_rate": 5.333333333333333e-07,
      "loss": 0.0018,
      "step": 74200
    },
    {
      "epoch": 3.957866666666667,
      "grad_norm": 0.5078782439231873,
      "learning_rate": 5.266666666666667e-07,
      "loss": 0.0025,
      "step": 74210
    },
    {
      "epoch": 3.9584,
      "grad_norm": 0.12443011999130249,
      "learning_rate": 5.2e-07,
      "loss": 0.0027,
      "step": 74220
    },
    {
      "epoch": 3.9589333333333334,
      "grad_norm": 0.4887430667877197,
      "learning_rate": 5.133333333333333e-07,
      "loss": 0.0039,
      "step": 74230
    },
    {
      "epoch": 3.9594666666666667,
      "grad_norm": 0.3432218134403229,
      "learning_rate": 5.066666666666667e-07,
      "loss": 0.0023,
      "step": 74240
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.1624930500984192,
      "learning_rate": 5.000000000000001e-07,
      "loss": 0.0026,
      "step": 74250
    },
    {
      "epoch": 3.9605333333333332,
      "grad_norm": 0.399294912815094,
      "learning_rate": 4.933333333333333e-07,
      "loss": 0.0025,
      "step": 74260
    },
    {
      "epoch": 3.9610666666666665,
      "grad_norm": 0.19698090851306915,
      "learning_rate": 4.866666666666667e-07,
      "loss": 0.0018,
      "step": 74270
    },
    {
      "epoch": 3.9616,
      "grad_norm": 0.16343232989311218,
      "learning_rate": 4.8e-07,
      "loss": 0.0016,
      "step": 74280
    },
    {
      "epoch": 3.962133333333333,
      "grad_norm": 0.22277583181858063,
      "learning_rate": 4.7333333333333334e-07,
      "loss": 0.0029,
      "step": 74290
    },
    {
      "epoch": 3.962666666666667,
      "grad_norm": 0.17354509234428406,
      "learning_rate": 4.666666666666667e-07,
      "loss": 0.0027,
      "step": 74300
    },
    {
      "epoch": 3.9632,
      "grad_norm": 0.3358932435512543,
      "learning_rate": 4.6e-07,
      "loss": 0.0017,
      "step": 74310
    },
    {
      "epoch": 3.9637333333333333,
      "grad_norm": 0.13704177737236023,
      "learning_rate": 4.5333333333333337e-07,
      "loss": 0.0017,
      "step": 74320
    },
    {
      "epoch": 3.9642666666666666,
      "grad_norm": 0.2855667173862457,
      "learning_rate": 4.466666666666667e-07,
      "loss": 0.0039,
      "step": 74330
    },
    {
      "epoch": 3.9648,
      "grad_norm": 0.2686454653739929,
      "learning_rate": 4.4e-07,
      "loss": 0.004,
      "step": 74340
    },
    {
      "epoch": 3.9653333333333336,
      "grad_norm": 0.33801954984664917,
      "learning_rate": 4.3333333333333335e-07,
      "loss": 0.0019,
      "step": 74350
    },
    {
      "epoch": 3.965866666666667,
      "grad_norm": 0.5527819991111755,
      "learning_rate": 4.2666666666666673e-07,
      "loss": 0.0025,
      "step": 74360
    },
    {
      "epoch": 3.9664,
      "grad_norm": 0.26380598545074463,
      "learning_rate": 4.2e-07,
      "loss": 0.0017,
      "step": 74370
    },
    {
      "epoch": 3.9669333333333334,
      "grad_norm": 0.48131608963012695,
      "learning_rate": 4.133333333333334e-07,
      "loss": 0.002,
      "step": 74380
    },
    {
      "epoch": 3.9674666666666667,
      "grad_norm": 0.1329776495695114,
      "learning_rate": 4.0666666666666666e-07,
      "loss": 0.0018,
      "step": 74390
    },
    {
      "epoch": 3.968,
      "grad_norm": 0.1614549607038498,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 0.0027,
      "step": 74400
    },
    {
      "epoch": 3.9685333333333332,
      "grad_norm": 0.1289890557527542,
      "learning_rate": 3.933333333333333e-07,
      "loss": 0.0019,
      "step": 74410
    },
    {
      "epoch": 3.9690666666666665,
      "grad_norm": 0.13287292420864105,
      "learning_rate": 3.866666666666667e-07,
      "loss": 0.002,
      "step": 74420
    },
    {
      "epoch": 3.9696,
      "grad_norm": 0.19723722338676453,
      "learning_rate": 3.8e-07,
      "loss": 0.0018,
      "step": 74430
    },
    {
      "epoch": 3.970133333333333,
      "grad_norm": 0.3282260596752167,
      "learning_rate": 3.7333333333333334e-07,
      "loss": 0.0025,
      "step": 74440
    },
    {
      "epoch": 3.970666666666667,
      "grad_norm": 0.18796946108341217,
      "learning_rate": 3.6666666666666667e-07,
      "loss": 0.0029,
      "step": 74450
    },
    {
      "epoch": 3.9712,
      "grad_norm": 0.1377800554037094,
      "learning_rate": 3.6e-07,
      "loss": 0.0019,
      "step": 74460
    },
    {
      "epoch": 3.9717333333333333,
      "grad_norm": 0.1488461196422577,
      "learning_rate": 3.533333333333333e-07,
      "loss": 0.0017,
      "step": 74470
    },
    {
      "epoch": 3.9722666666666666,
      "grad_norm": 0.11468710005283356,
      "learning_rate": 3.4666666666666665e-07,
      "loss": 0.0023,
      "step": 74480
    },
    {
      "epoch": 3.9728,
      "grad_norm": 0.12529581785202026,
      "learning_rate": 3.4e-07,
      "loss": 0.0017,
      "step": 74490
    },
    {
      "epoch": 3.9733333333333336,
      "grad_norm": 0.42375656962394714,
      "learning_rate": 3.3333333333333335e-07,
      "loss": 0.002,
      "step": 74500
    }
  ],
  "logging_steps": 10,
  "max_steps": 75000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 40,
  "trial_name": null,
  "trial_params": null
}
